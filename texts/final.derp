Last month, I gave a presentation about paid crowdsourcing in the humanities at SDH-SEMI. Below are my notes.IThe rhetorical model in the humanities is appreciation: we believe that by paying attention to an object of interest, we can explore it, find new dimensions within it, notice things about it that have never been noticed before, and increase its value.John Unsworth, 2004In a 2004 talk, John Unsworth characterized the dominant model of the humanities as one of appreciation– rigorous and qualitative. By examining a work from multiple angles and multiple contexts, our belief is that we can learn “notice things about it that have never been noticed before, and increase its value.“ Such research does not easily lend itself to large-scales like quantitative work does: qualitative undertakings, ones of concentrated appreciation, are restrained by the amount of human involvement available.However, as we explore new ways to utilize our digital environment for humanities research, so-called ‘big data’ approaches are not only becoming possible but inevitable. The archival efficiency of computers coupled with the digitization efforts of historians, librarians, and digital humanists has resulted in endless bytes of data to understand and call our own, while the offline limitations of scale have left a large area of questions thus far unturned.In these democratic days, any investigation in the trustworthiness and peculiarities of popular judgments is of interest.Francis Galton, 1907There are numerous approaches for scaled up humanities research. Today, I’ll speak of one in particular: crowdsourcing. In doing so, I’ll describe how crowdsourcing is currently being undertaken and share a project of my own – one where semi-anonymous online users rewrote Jonothon Swift’s A Modest Proposal – as one approach to crowdsourcing workflow.So-called big data is an important area of growth in our field. This year, Unsworth himself has spoken a few times about the need for librarians and other scholars to start thinking about approaches to large-scale research. The reason is that, even though computing allows us to make arguments from data, “data still requires interpretation, and you can still make better and worse interpretations, and more or less compelling arguments” (John Unsworth, 2012). In field questions at another talk, Unsworth argued that regardless of whether we welcome big data, somebody is going to do it, so we should start thinking about how it should be done.In his own forward-looking blog post earlier this year, Dan Cohen made a similar observation. He writes, “all of us are facing either present-day or historical archives of almost unimaginable abundance, and we need sophisticated methods for finding trends, anomalies, and specific documents that could use additional attention.”Finally, Ted Underwood has written that,DH doesn’t have to be identified with scale. But the fact remains that problems of scale constitute a huge blind spot for individual researchers-Ted Underwood, 2012Underwood added that problems of scale “also define a problem that we know computers can help us explore.” However, I will argue for a broader, more humanistic understanding of the computer’s role in tackling problems of scale. Yes, on one end of our tradition we quantify our data for computers to make sense of – as seen in text analysis, text and data mining, and visualization. With crowdsourcing, we can make use of another feature of computing: its ability to connect large groups of people and efficiently distribute works (and work) to them.Crowdsourcing is an approach for achieving large conceptual tasks through large-scale distributed collaboration. The efficiency of digital technologies in connected people has allowed coherent organization of many contributors, resulting in large-scale or rapidly realized projects that use human reasoning and cognition.Contributors to crowdsourcing projects are often interested volunteers. For example, in DH, we have the Suda Online and Transcribe Bentham projects.Suda Online, or SOL, is a translation of the Suda, a byzantine Greek encyclopedia. The Suda is a particularly gnarly work, written in a transitional language and with many grammatical and factual errors. This makes translation difficult and annotation necessary. Thus, in 1998, a number of scholars on a classics mailing list resolved to translate it together, opening the doors to anybody (with the necessary skills, of course, politely vetted) and deciding on a continuous online publication of the work-in-progress rather than waiting for a final edition. The day the precursor to Wikipedia was announced in 1999, Ross Scaife joked on the discussion list that “they must have gotten this idea from the Suda on Line…” . Fourteen years later, SOL is nearing completion and is already an often-referenced work among students.Another volunteer crowdsourcing project is the Transcribe Bentham Project. Transcribe Bentham is a manuscript transcription project at University College London (UCL) testing the feasibility of outsourcing transcription work to members of the public. The project was seeking to digitize 12,500 folios of manuscripts by philosopher and early UCL champion Jeremy Bentham, a subset of 60,000 held by UCL Library Services (Moyle et al. 2010). The Transcribe Bentham project proved to be popular among retirees and academics-on-leave.Volunteer labor is an effective approach to large abstract projects, if there is a reason to participate. With SOL, for example, students sometimes contribute translations as part of their studies. Others, as senior editor David Whitehead explained to me, volunteer “for the fun of it.”However, for volunteer labor, you need a motivating factor. Crowdsourcing is not simply a case of “if you build it they will come” – a point I explore and hammer out in my 2010 thesis. Furthermore, it takes time to attract participants. Finally, the complexity of balancing user motivations, publicity, and managing community-development means that there is a possibility of failure. For projects that need a guarantee of completion, volunteer contributions are not the most desirable approach (especially to your funder).For more tedious tasks, where there is little or no intrinsic motivation for volunteers, crowdsourcing projects can also compensate online workers. Platforms like Amazon’s Mechanical Turk exist for this purpose, emphasizing micro-payments for piecemeal tasks. The goal is to split up human labor tasks into small parts and treat their completion in the same manner as one treats computational cycles. Turk is human intelligence with the convenience of machine processes.Turk is great for projects with tedious tasks, where many people conducting a few tedious tasks is often more humane that asking a few people to perform many. It is also useful when qualitative labour is needed quickly. Finally, for participation-critical tasks, where you need a guarantee of concluding the task, paid crowdsourcing is much more reliable than volunteer crowdsourcing.However, there’s another side of paid crowdsourcing.First, there are potential detriments to the work at the heart of the crowdsourcing. It can grow costly, and there is an additional, financial motivation for users to game the system.If [Tom Sawyer] had been a great and wise philosopher, like the writer of this book, he would now have comprehended that Work consists of whatever a body is obliged to do, and that Play consists of whatever a body is not obliged to do.Mark TwainThere is also a notable difference between extrinsically and intrinsically motivated work.For example, (Mason and Watts 2009) found that users given a word search on mechanical Turk would find more words when they had less financial incentives to do so. Panos Ipeirotis has blogged a concise, capable literature review on this topic.While the balance of benefits and detriments to your project will tilt in one way or another depending on your needs, there are also possible detriments to users. In treating workers as computing cycles, as Amazon encourages, there is a risk of dehumanizing workers: forgetting that you are in fact working with people. This may result in unrealistic expectations of quality, low wages, or unethical work. It’s a possibility that Jonathan Zittrain calls digital sweatshops.On Turk forums, one can see countless posts of workers complaining about shady or unfair requesters, as well as posts of requestors overly standoffish. In just one of many examples, a requester put up badly designed tasks and then withheld all the payment when workers did not perform to their expectations, showing surprise when they started hearing from dissatisfied workers. Their surprise was stunning, as their withholding of payment for work was not in malicious intent but rooted in a belief that payment simply hadn’t been deserved.Examples like this are a risk, but not a necessity. When volunteers crowdsource, they have to be treated with decency because their very participation hinges on the requesting party’s sincerity and tact. When paying people however, and doing so in a system that keeps them at arm’s length, it is the duty of the requester to remember such qualities.IILast year, I set out to utilize Mechanical Turk for a literary crowdsourcing undertaking. My purposes were pedagogical: to demonstrate paid crowdsourcing in the humanities while foregrounding the issues that requestors must be aware of when doing so. To this purpose, I crowdsourced a modern colloquial update to Jonathan Swift’s A Modest Proposal (1729), hoping to instill the satire with some of its original shock though a naive rewrite by semi-anonymous paid workers.Swift is not a new object of study in DH. In 1967, Louis T. Milic published a computational text analysis of his work in A Quantitative Approach to the Style of Jonathan Swift (Worldcat; see also, review in Computers and the Humanities). Patricia Köster later emulated this study in a 1971 attempt to identify unattributed works suspected to be by Swift based on their style.In his famous pamphlet, Swift describes the plight of the impoverished in Ireland before offering a solution: for the poor to sell their children to the wealthy for food. Swift was parodying the language of social engineering essays of the time: ones that look at the big picture of society’s woes while disregarding the humanity of the people involved in these “solutions”. Swift began in a familiar style, casually progressing into its unspeakable proposition.Yet a modern read of A Modest Proposal as a familiar satire is different from a completely naive read. Clark and Gerrig (1984) argue that the strength of the pamphlet lies in readers first taking Swift seriously and gradually realizing the pretense, comforted that others would not realize it at all. However, it is difficult to mistake such a famous work as a serious essay today, dulling some of the edge. Furthermore, the language, while arguably effective in its cold formality, might be inaccessible to some today.In creating a colloquial crowdsourced rewrite, I tried to reinstate some of the shock from the original, to see if the satirical effectiveness survives. At the same time a satire needs a purpose, not simply to shock. The choice of Swift was intentional, paralleling paid crowdsourcing’s worst-case scenario with Swift’s jab at cold, dehumanizing social engineering solutions.In this way, part of the audience for the rewrite was the workers themselves. Workers were removed from the context, rewriting the pamphlet sentence-by-sentence. What would they think, looking at this sentence written in such unassuming prose and deciphering it, only to realize the terrible proposition that they’ve written? Would they realize it?The task was done in two steps: rewriting and voting, adopting Bernstein et al’s Find-Fix-Verify crowdsourcing pattern. “Find-Fix-Verify splits complex crowd intelligence tasks into a series of generation and review stages that utilize independent agreement and voting to produce reliable results. … This process prevents errant crowd workers from contributing too much, too little, or introducing errors into the document.” (2010) Bernstein and his colleagues applied this pattern to a series of crowdsourced editing tools, which allow you to outsource editing from Microsoft Word to Mechanical Turk workers.In the Swift project, the Fix and Verify steps were adapted. In the first stage, rewriting, the original essay was divided into individual sentences. Each worker was given a single sentence and asked to rewrite it in simple English. In order to deter cheating, character-limits were included as artificial restraints. The limit provided was 140 characters: the length of a Tweet or text message. This resulted in an additional layer reflecting our modern colloquial language, and was inspired by the co-founder of text messaging, Friedhelm Hillebrand, who came up with the character limit after a series of informal experiments convinced him than any single thought can be conveyed in 160 characters.For each sentence in a Modest Proposal, three different users were tasked with rewriting it. Once three rewritten sentences were collected, that sentence was sent to the next step: the voting stage. In this stage, workers were presented with the original sentence and the rewrite candidates. Here, they voted on the rewrite that best embodied the original line. Once again, multiple workers voted, and the candidate sentence with the majority of votes was chosen.So, how exactly did it turn out? Below are two rewritten lines. The first line does a fairly good job of creating a simplified sentence. The second example, however, is likely too bare. Nonetheless, there’s something about the plainness of the rewritten sentence that I find particularly compelling about example 2.Example sentence 1Original:For this kind of commodity will not bear exportation, and flesh being of too tender a consistence, to admit a long continuance in salt, although perhaps I could name a country, which would be glad to eat up our whole nation without it.Rewritten:This commodity can’t be exported as the flesh is too tender to remain salted for long, but I know a country that would eat ours without it.Example sentence 2Original:There is likewise another great advantage in my scheme, that it will prevent those voluntary abortions, and that horrid practice of women murdering their bastard children, alas! too frequent among us! sacrificing the poor innocent babes I doubt more to avoid the expense than the shame, which would move tears and pity in the most savage and inhuman breast.Rewritten:My idea is good because it stops the sad practice of abortion, which is murder and should break your heart.Generally the workflow was effective. Tasks were completed quickly and effectively. Once everything was set up, most of the tasks were completed within 3 hours, though some tougher sentences straggled for longer. The main issue was that voting sometimes favored sentences that had more words in common with the original rather than those which were more drastically simplified. Still, the rewritten essay kept it’s cold feel: of a writer making a argument that by all means appears persuasive, except for the fact that it forgets the humanity of the pawns involved.Since the format matched, I briefly began publishing the essay on Twitter. However, while it was interesting to consider the effect of seeing the isolated context of the sentences in a Twitter stream (see my original post), ultimately the Twitter account got to some lines that I could not stomach pushing out to a public feed.Crowdsourcing with volunteers forces researchers to consider the crowds and offer them a satisfying intrinsic reward, but once compensation is introduced users simply become workers. While not inherently bad, it introduces a slippery slope to an exploitative relationship. Through irony, A Modest Proposal criticizes the dehumanization of the poor. It is only appropriate that nameless workers would give it a fresh layer of discomfort.With thanks to Stan Ruecker for suggesting that this project be shared.by Beth Harris, Ph.D. and Steven Zucker, Ph.D., Deans, Art and History, Khan AcademyOur schools and libraries are being radically re-imagined for the digital age, but what about our museums? The New York Public Library, for example, is bravely (and controversially) rethinking its Fifth Avenue flagship building. Last month, MIT and Harvard announced edX, a partnership to offer free online courses, and last fall, Stanford offered three massive open online courses (MOOC) to hundreds of thousands of students for free, and Khan Academy provided 6.1 million unique users with free instruction in March 2012 alone. Museums, on the other hand, have remained largely insular and focused on their institutional identity. So perhaps it’s no surprise that the most recent digital innovation comes not from the museums themselves but from Google, which launched the second iteration of the Google Art Project last month.Google faces numerous challenges among academics; nevertheless, we should recognize that Google’s Art Project has done something extraordinary for both museums and for education. A small team based in London persuaded more than 150 museums from around the world to share more than 32,400 high-resolution images beyond their own institutional boundaries.This is a really big deal.For the first time in history it is easy for non-specialists to explore and closely examine art from museums across the globe on a single website. There have been other initiatives that have moved in this direction, but never with the scope or functionality of the Google Art Project. The Art Project isn’t finished. It needs more museums and more art. It needs improved search and filtering tools. And the public needs better ways to discover and contribute new narratives about art’s history. Despite these weaknesses, the educational potential is tremendous.Meanwhile, many museum professionals (Nina Simon, Nancy Proctor, Seb Chan to name just a few), have been grappling for some time with the question of the future of the museum (see AAM’s Center for the Future of the Museum). And those of us who have worked in the area of museum technology have asked specifically about the future of the museum website. Koven Smith, Director of Technology at the Denver Art Museum asked more than a year ago, “What things do museums do *exclusively* because of tradition? If you were building a museum from scratch, what would you do differently?” Since born-digital institutions often succeed where legacy institutions struggle, this is an important question. Mia Ridge, whose blog Open Objects, has addressed this question often, responded, “a museum invented now would be conversational and authoritative – here’s this thing, and here’s why it’s cool.”The question of conversation is key and it’s been central to Smarthistory.khanacademy.org’s pedagogy. In many ways, scholarship at its best is conversation. But up until now, museums have conversed very little with one another—either on or offline.Here are two examples of how the Google Art Project opens the conversation. In 1889, Vincent van Gogh painted three canvases depicting his bedroom in Arles; these now reside in three different museums. Only the van Gogh Museum in Amsterdam illustrates another version on its website and remarkably, none of the three museums link to the paintings at the other institutions.Now imagine a student studying Édouard Manet’s 1863 painting, Olympia, in Janson’s History of Art textbook. The book mentions Titian’s Venus of Urbino as an important source for Manet, but doesn’t reproduce this older, Renaissance painting. Even at 1152 pages, the Janson text must be extremely selective for reasons of space and cost. Of course a museum website has no such constraints. Nevertheless, although the Musée d’Orsay, home to Olympia, also mentions the Titian, it provides no link to the painting or to the Galleria degli Uffizi where the painting hangs.In contrast, the Google Art Project allows visitors to create and share a gallery where these paintings can be viewed side by side; it also includes links to their respective museum collections (where they exist). Imagine the educational impact if museums put together online galleries like this one and included commentary from curators at multiple institutions aimed at a non-scholarly audience.The Art Project succeeds in large part because it relies on museum expertise—one of the great strengths of the museum in the digital age. Too often museums don’t surface in a simple Google image search. General searches are more likely to return unreliable sites hawking reproductions. In contrast, visitors to the Art Project access current and reliable information. In fact, the Art Project highlights the relative scarcity of educational text and video provided by museums about their permanent collections, the very content the public is looking for. Museums need to create more free content aimed at a general audience and to do so within the broader context of art’s history. Museums employ curatorial staff that, like college and university faculty, have deep knowledge in their areas of expertise, yet too little of that expertise makes its way onto the museum website. Instead, expensive, narrowly targeted, scholarly exhibition catalogs remain the focus of museum publishing.Naturally, linking and creating content is not free, but it doesn’t have to be prohibitively expensive either. Links go bad and have to be updated and there is, of course, always a resource issue. There are ready solutions but such issues are framed by a bigger concern. Namely, should museums point visitors away from their own collection? Museums don’t use the word competitor, but this concept informs such considerations. In an era when education is increasingly occurring outside of traditional learning institutions, we believe that the role of the museum is increasingly important.There are some hopeful signs. Recently, a few museums (The National Gallery of Art, The Brooklyn Museum, and the Metropolitan Museum of Art for example) have begun to offer public domain images for download. We hope more museums will recognize that in the digital era, the old model of controlling and charging for reproductions of public domain work flies in the face of their mission. Museums, and the artists’ rights organizations (such as ARS and VAGA) and the estates they work with, need to do far more to make the shared cultural heritage they hold in trust, accessible. Peter Samis, Associate Curator, Interpretive Media at the San Francisco Museum of Modern Art recently asked, “Are the artworks ours to give? Are they ours to withhold?”Now that the value of a global art platform is evident, will museums think differently about sharing resources with each other and the public? The Google Art Project shows what can happen when museums work in parallel; now imagine what could happen if museums choose to work together.Disclaimer: Harris and Zucker created the Khan Academy videos on the Google Art ProjectPossibly Related Posts:Khan Academy Contributes to the Google Art ProjectMy friends Beth Harris and Steven Zucker, formerly of smARThistory,...Blackboard Contributing Code to Open Source ProjectAccording to Scott Rosenbaum, a business intelligence consultant, Blackboard has...Mellon Foundation RIT Project Concludes a SuccessThis is a guest post by Jim Farmer. Apologies to...Embanet Joins as Sakai Project Affiliate to Offer HostingThis is big news. As far as I know, this...Pan-African Open Source Course Management System ProjectI have just had a few interesting email exchanges with...I am immensely grateful to Eric Kwakkel of Leiden University for drawing my attention via Twitter at the weekend to an important piece of recent work which to my mind provides a model for the sort of innovation we should be developing in the digital humanities. It is a completely experimental approach which doesn't produce a sustainable digital resource, raise questions about standards or encourage us to integrate data in different fashions, but it is more provocative and thought-provoking than a thousand lavishly-funded TEI online editions. Of course, there is room within the big tent of the digital humanities for all such approaches, but my anxiety is that the Digital Humanities, as it grows increasingly complacent, inward-looking and risk-averse, will lose touch with this kind of avowedly experimental work, which was perhaps more commonplace fifteen years ago than it is now. The BBC story which Erik posted described a piece of research by the St Andrews art historian Kathryn Rudy under the headline 'Secrets Revealed by Dirty Books from Medieval Times' and suggested that measurement of dirt on medieval manuscripts could indicate which pages were most frequently handled by their medieval owners. My immediate reaction was to feel doubtful about the validity of such an approach, knowing how frequently major libraries frequently clean manuscripts. However, reference to the full article, published in the Journal of Historians of Netherlandish Art and available from the St Andrew's Institutional Repository here, revealed a much more subtle and important piece of research. It is common in medieval manuscripts to see how oil and dirt from constant handling discolours certain pages. Dr Rudy used a device called a densitometer which measures the reflectivity of a surface in a way that will not damage the manuscript. As pages are handled, then the surface of the vellum becomes darker. Densitometer readings will in theory indicate which pages were most frequently handled: Dr Rudy offers fascinating analyses of the way in which the densitometry data provides evidence of how different owners of particular manuscripts made use of them, and in particular which sections of the manuscript they read most often. Securing this information was not easy - Dr Rudy used as densitometer on about 200 manuscripts, but only got useful information on 10% of them. As I suspected, one of the main problems is modern cleaning of manuscripts. Large institutions such as the British Library and the Victoria and Albert Museum have historically tended to clean the surface of manuscript pages at the same time as rebinding or repairing them, and the huge swathes of rebinding of medieval and other manuscripts which caused such immense damage and loss of evidence in the British Library up to the 1970s also destroyed evidence which could now be explored by the densitometer. Historically, as the conservation wiki notes, bread crumbs were often used for this surface cleaning. The Conservation Wiki gives the following advice for baking bread for use in cleaning your manuscripts: 'Bread has been historically used as a surface cleaning material, but is no longer in general use. Bread should be baked without oils, yeast, or (potentially abrasive) salt. (SD) Traditionally, day old bread was preferred, as it was not as moist as fresh bread and may have had “tooth” to facilitate better cleaning. Crusts were removed and the bread was pressed into the paper surface with a rolling motion. (EO) Residual bread may support mold growth. (RA)'. Another issue in the use of densitometers with manuscripts not noted by Dr Rudy is that modern usage of manuscripts also causes discoloration. In a volume which contains a number of different medieval codices bound together, it often striking how a well known section which has received a great deal of scholarly attention is very seriously discoloured, whereas a less well-known part of the manuscript is much cleaner. Because of the way in which the reconstruction of the Cotton Library in the British Library was undertaken and documented, there are two or three medieval manuscripts which are shown in all the catalogues as destroyed but which were in fact restored and have been preserved. These manuscripts have not been touched by more than half-a-dozen people since they were restored in the middle of the nineteenth century. It is striking how very much cleaner these volumes are than those Cotton Manuscripts which have been regularly consulted in the Manuscripts Reading Room during that time. There is clearly a great deal to do in developing this new method of manuscript densitometry, and this is a task which should be taken up by scholars working in the digital humanities. Nevertheless, the scholarship of Dr Rudy's first experimental use of this technique is very striking and it is difficult to disagree with Dr Rudy's claim that 'We can add densitometrical analysis to the manuscript scholar's toolbox of forensic techniques, which also includes the use of ultraviolet (UV) light or other techniques to help to disclose texts that have been scratched out'. The potential value of densitometry is not restricted to manuscripts. It would be interesting to compare how different owners approached the same copy of a book by analysing some early printed books. Or we could take a library like that of Thomas Jefferson or Edward Gibbon, and analyse which books they were most interested in. There is a huge new potential field of investigation here. At the end of her excellent article, Dr Rudy enters an important plea: 'As we listen to the last gasp of the physical book, it is important to think about this material evidence and what it represents. What we have to gain by digitization and by abandoning the book as a physical object may be negated by what we have to lose'. She goes on: make a similar plea that, as libraries continue to digitize medieval illuminations, they continue to grant access to the physical objects, which always hold more evidence than we first perceive. The Koninklijke Bibliotheek in The Hague, which preserves many of the examples taken up in this study, for example, has been in the forefront of digitizing images from its illuminated manuscripts, but at the same time has reduced the opening hours of its reading rooms. But they have done so partly because the reading rooms are frequently empty. It would seem that manuscript historians are largely content to study a digital copy from home if it exists. The convenience of digital facsimiles might be heralding the end of codicological approaches to manuscript studies. This is lamentable, as there is much subtle information stored in the physical object'. This is a real challenge, and scholars working in the digital humanities must wonder how far, in their naive techno-enthusiasm, they are culpable here. By giving us new means of exploring and investigating cultural artefacts such as books and manuscripts, digital technologies made access to and engagement with original objects more and not less important. Yet too often scholars working in the digital humanities give out the message that what counts is data and information, and that this can somehow be investigated in a fashion disconnected from its physical roots. This is a route to a major cultural disaster. We may throw up out hands in horror at the Victorian and early twentieth century destruction of bindings and other aspects of medieval manuscripts, but the digital humanities is actively colluding in encouraging approaches which are potentially equally destructive. We can help avert this looming disaster by showing how digital technologies give us more tools to engage with the original manuscript and printed book, and by leading a renewed engagement with books and manuscripts in library and archive reading rooms. The slogan of many librarians in the 1990s was 'access not collections'. Practitioners of the digital humanities should aim to replace this with 'collections and access'.In previous posts, I’ve shown how WordSeer can be used to explore small, well-defined questions: what word did Shakespeare use for ‘beautiful’? Is the occurrence of the word ‘love’ the same in the comedies and tragedies? This post is different. WordSeer has now developed enough to support a simple, but complete, exploratory analysis.As one answer, we’ll see how WordSeer suggests that when love is a major plot point, the language referring to women changes to become more physical, and the language referring to men becomes more sentimental. You can watch a screencast here, or just read this post.We began our analysis with the question, “what are some things that are portrayed as ‘his’ and some things that are ‘hers’?. A typical keyword search returns an unstructured lists of results, and a standard approach in literature study is to view them in a concordance. This is a list of all the sentences in which a word occurs, with the target word aligned in the center of the view, exposing the contexts to its left and right, sorted in some way. WordSeer uses the word tree concordance visualization which makes common contexts easier to see by grouping them in a tree-like structure.Figure 1. Word Tree for the word "her" generated by WordSeer.The word tree for her is shown in Figure 1 above. Some words like beauty stand out, but constructions like her own muddy the picture. The problem lies in the different ways in which his and her are used. The word his is always a possessive pronoun, and word sequences containing his would nearly always be relevant. However, her can also be a 3rd-person pronoun, and will yield constructions like “I told her that X” and “I gave her the Y”.Figure 2. The grammatical relationships made searchable by WordSeer.With WordSeer, we can get around this problem with grammatical search.The system uses natural language processing (NLP) to extract relationships between words, and allows users to specify both keywords and relationships between them. In the tool’s search interface, pairs of words are specified using input boxes, and the relationship between them is selected from a drop-down menu (Figure 2). Leaving a word-input box blank returns all matches.With this feature, we can take advantage of the fact that possessive relationships between words can be automatically detected, to express our question precisely: “what are all the words with which his has a possessive relationship?”. The results are shown in Figure 3 below.Figure 3. Grammatical search results for possessed-by hisComparing these words with those for her (Figure 4 below) reveals immediate differences. The word father is most common for her, with husband, and son close behind. Several body parts enter the picture: eyes, hand, face, tongue, lips, cheek. A picture emerges: women’s most commonly-mentioned possessions are their male relatives and their bodies.Figure 4. Grammatical search results for possessed-by herVisualization, Reading, and Hypothesis-GenerationOur next question was whether this physical, male-dominated picture of women was consistent, or whether it changed in different types of plays. We used the tool’s collections feature to divide the plays into comedies, tragedies, and histories – the three most commonly-accepted categorizations of Shakespeare’s plays. We also created pre-1600, and post-1600 categories to check whether there were temporal differences.Figure 5. Initial collectionsCollections were created using the “collections” bay, a collapsible window at the bottom of the screen. We added the appropriate plays through the document listing (sortable and filterable by date, title, full-text search, grammatical search, and length).We used the tool’s newspaper-strip visualization (Figure 6) to compare the prevalence of the two categories of words in different types of plays. Each play is represented as a long column. Within each column, small, colored horizontal blocks (corresponding to 10 sentences each) highlight the presence of a match.Figure 6. Comparing the prevalence of body parts possessed-by her (eyes, lips, cheeks, and face)(blue) and relatives possessed-by her (husband, father, sons, daughters, children) (orange) in the comedies. Each column is a comedy, represented in alternating shades of grey. Hovering over a column (e.g. “Much Ado About Nothing" above) darkens it and displays the title. Hovering over a highlighted block displays the matching sentence.The results for the tragedies collection were similar to the results for comedies (Figure 6) but in histories (Figure 7), an interesting pattern emerged. It seemed that body parts (blue) were somewhat less prevalent in these plays, but family (orange) remained unchanged.Figure 7. Comparing the prevalence of body parts possessed-by her (eyes, lips, cheeks, and face)(blue) and relatives possessed-by her (husband, father, sons, daughters, children) (orange) in the histories. Each column is a play, represented in alternating shades of grey.Hypothesis-building: close reading, annotation, and explorationWordSeer supports quick, large-scale analysis through search and visualization, but in all cases maintains links back to the source text. Hovering over a blue or orange highlighted block in Figures 6 or 7 brings up a popup displaying the matching sentence. Clicking opens the reading interface to that point (Figure 8). The full text of the document is loaded, and the system automatically scrolls to the relevant sentence, and highlights it.Figure 8. WordSeer’s reading interface. If the document is subdivided into sections, these appear on the right as a table of contents.Hovering over a few body-part results quickly led to a new hypothesis. In our rough sample, many of the mentions sounded romantic. We used the reading and annotating interface to follow up on this by clicking on the highlighted blocks in the newspaper-column visualization.Figure 9. Highlighting text creates a snippet, to which tags and notes can be attached.We selected the speeches referring to body parts and tagged them by the topics they seemed to contain. It soon became apparent that many of the mentions were speeches by a lover.Our hypothesis was strengthened when we viewed related words. For exploration of style and language, WordSeer uses computational linguistics to calculate words commonly used in similar contexts, or commonly used within a 10-sentence window of each other. Clicking on any word while reading brings up a small window showing related words.Figure 10. Related words for faceIn our example, the the related words for body-parts (e.g. Figure 10 for face) strengthened our growing suspicion that female body part mentions were associated with romance. The popup shows that other body parts are frequently mentioned, along with love, fair, and sweet.Assembling EvidenceWe created a final pair of categories focusing on love: not-love-stories for plays in which love is not a major plot point, and love-stories for plays in which it is. When we reorganized the plays along these lines, the results were immediate.Figure 11. Visualization of the love-stories collection comparing the prevalence of body parts possessed-by her (blue) and relatives possessed-by her (orange).In the love-stories (Figure 11), we see both body parts and male relatives. By contrast, the not-love-stories visualization (Figure 12) shows predominantly male relatives, and hovering over the occurrences of body parts reveals a gloomy picture of her tear-stained cheeks and her sorrowful eyes.Figure 12. Visualization of the not-love-stories collection comparing the prevalence of body parts possessed-by her (blue) and relatives possessed-by her (orange).The grammatical search results (below) agree with the newspaper-strip visualizations and related words. We see more physical attributes possessed-by her in the in the love-stories collection (Figure 13a) than in the not-love collection (13b).Figure 13a. her possessions in the love-stories collectionFigure 13b. her possessions in the not-love-stories collectionThe grammatical search results show that the language around men changes as well (Figures 14a and 14b below). In the not-love case, the only woman to appear is mother, at number 20, but in the love case, wife takes first place, followed by favor. Compared to the physical language for women, these words have a more sentimental quality.Figure 14a. his possessions in the love-stories collectionFigure 14b. his possessions in the not-love-stories collectionThus, we see that, while a male-dominated picture of both men and women is always present, physical aspects are more prominent for women in plays about love. For men, the more sentimental aspects come to the fore.ConclusionWordSeer is being developed through case studies. This means we observe scholars working with texts, figure out what they need, and then try to translate it into interactions, text mining algorithms, and visualizations. Therefore, when the time comes to demonstrate it, I always think examples work better than anything else.So what do the literature scholars among you think of this simple example? How might it be improved, and made more convincing? What are its flaws? What would you have done? Please comment, even if it is to criticize. It would be great to hear your thoughts.Like this:Be the first to like this.Below is the text of my inaugural lecture. If you would like to listen to it, you can find it on Soundcloud. The two are not exactly the same, because I believe in giving lectures not reading them. Enjoy.... The monologue in a crowdsourced world: have digital resources rendered the inaugural lecture obsolete? The longer I work in DH, and the more I consider what the digital medium makes possible the more the idea of me standing up and telling people what I think and thus by implication what they might think seems frankly bizarre. I increasingly dislike the idea of the single voice speaking with some kind of a spurious authority. One of the great assets of the digital, and what it encourages and enables is multiple voices entering into a dialogue and creating new knowledge out of conversation and discussion. In what follows, therefore, I propose to look carefully at this apparent contradiction. Even in the physical word, there are, I believe, better ways to generate knowledge, through dialogue and conversation. I think that one of the reasons for my unease with the idea of the single person lecture is that, as a student I knew it as an optional extra rather than the core of the educative process (lectures were not compulsory for Cambridge undergraduates and this remains the case). Cambridge teaching relies on the supervision- a discussion between an academic and one or two students- as the foundation of teaching and learning in the arts and humanities. I was lucky enough to be taught by some of the greatest international authorities yet it was never assumed that their voice in the conversation was necessarily more important than mine. Far more important than who was talking was the quality of thought expressed and the nature of knowledge that emerged from the dialogue, and I think that's quite right. I don’t propose to talk about users of digital resources in the humanities, and cultural heritage…again. I thought it might be time to take pity on people: if there are any inhabitants of planet Zog who haven’t heard me talking about this, you could always download some of my publications from UCL Discovery. Nevertheless, I propose to apply some of the techniques that we use in user studies, and apply them to the phenomenon of the Inaugural lecture as a case study. Stan Ruecker my colleague on the INKE project uses what he calls the affordance strength model to assess whether digital resources and interfaces are fit for purpose. This allows him to compare the actual use of an artefact or resource, digital or physical, against its potential utility and suggest changes to design and functionality that might improve it. This approach can help explain, for example why despite the production of ever more complex digital reading devices, many of us still prefer to read print because it has affordances that digital cannot yet match.For example we can make notes on a piece of paper, doodle, fold it up, carry it easily, etc in a way that even the most sophisticated digital readers cannot match. Stan makes clear that the concept of affordance is a very complex one, and there is excellent discussion of affordances in the excellent new book: Visual Interface Design for Cultural Heritage, that Stan has co-written with Milena Radzikowska and Stefan Sinclair. But my very basic explanation of an affordance would be a property that an object possesses that we are aware we can use. It's not a very elegant description, so here is an example from the book of a situation in which various affordances interact. For example, a cat may afford petting by its owner; the petting affords pleasure for the cat; the petting affords pleasure for the owner; the petting and the cat’s pleasure afford a sense of companionship for the cat owner (and arguably for the cat, too). The pet-ability of the cat is a mechanical affordance. The pleasure of the two creatures involved is an affective affordance. The companionship is a social affordance. It is possible to have any of these affordances without the others. The cat may still afford companionship even if it is not currently in the mood for being petted. The cat may also afford petting but fail to experience pleasure, and so on. The cat is also unlike the book in that its willingness to afford petting in the first place is volitional – the book cannot actively resist reading, by, for instance, jumping up on top of the refrigerator. Given that affordances can be nested in these various ways, it is not necessary to perceive all the details of an affordance in order to be able to identify and begin using it.....With respect to petting the cat, the person does not have to anticipate that the petting may result in a sense of companionship – it is enough for either the owner or the cat to initiate the negotiation and see where it leads. (Ruecker, Radzikowska and Sinclair, 2011, p94) I propose to use a version of this method to assess the IL’s current affordances and possible future utility. Another method that we have used extensively is what Ann Blandford calls use in context. This means studying what users actually do with digital resources in the context of their usual work, rather than forcing them to complete set tasks in a lab, and it takes into account the importance of the cultural and professional context within which people work. We have, for example, braved knee deep mud to study archaeologists at Roman Silchester, so I think it’s robust enough for the task in front of me. I am assuming that both the person giving the lecture and the audience are users, and the institutional context we need to take into account is that of UCL in particular, but also the wider academic and historical context, and that of my own professional history. Affordances are to some extent dependent on the user's perception of them, so the list that follows is mine, but based on what I can gather about inaugural lectures and their purpose from talking to other academics, and from reading university websites. They are as follows:Communication of researchInteractivityOrdealPaying backPublic EngagementInclusiveness/TeamsCelebrationSocial occasion/networking Communicating your research One of the ideas that is mentioned regularly as the purpose of the inaugural is to tell people about my research, whether that is at UCL, or to engage with the wider public. I had to be reminded that there are in fact people who don’t yet do DH, and part of the reason for me doing this lecture might be to persuade them that they’d enjoy it. Quite honestly I am not convinced that there is any corner of the known universe that hasn’t been reached by the relentless digital wave of publicity that is UCLDH. The fact that our posters are now on display at the ODH in Washington as a result of Melissa’s tweets and blogs is surely evidence of the huge potential for outreach that is innate in digital media. However, it is important to consider this affordance. One of the stated aims of inaugural lectures is that they should give some idea of the kind of research field in which people work. In this again I feel the affordances of the lecture form are lacking because of its monologic nature. All of my work has been about giving others a voice. If I succeed I should no longer need to speak at all really. When I began work in DH it was assumed that users should not be seen or heard. Their views were unimportant and their only purpose was to adopt all the cool tools and techniques that the clever expert DH people designed for them, and they should be grateful and uncritical. We know what was good for them, in effect. If they failed to do so, it was because they were ignorant, Luddite, old fashioned or just plain stubborn. They did not know what was good for them, in other words. If my work has achieved anything it is to fight against such assumptions and insist that users of digital resources do know what they need, and that if they don't find it they will not use things that are unfit for their needs. In this I was, at one point, something of a lone voice, but I insisted that it was worth me speaking, because I was doing so to give voice to those whose opinions were ignored. I am delighted to find that opinions are now changing, users are being consulted and their views listened to. If my voice is lost in the clamour of ideas, views and demands from the voices of those users, and that such views are taken seriously and design decisions taken on this basis then that is the greatest success that I could wish for. But how in the end can this happen? The only way to create such resources is for users, designers and those who study user needs, behaviours and requirements to work together. Once again, where is the place for the lone voice in this process? DH is, in almost every way that we can imagine, a collaborative field. We have to learn to work together and understand the different languages that are spoken by different partners in the dialogue: geeks, humanities scholars, information professionals, technical support people and indeed the public. In that sense, therefore, the voice of the DH scholar is of use as an interpreter between different languages and cultures. But interpreters cannot, but the nature of their job, exist in isolation. It is perhaps significant that there are, in relative terms, so many excellent female scholars in DH and in user studies more widely. One might argue that girls are constantly socialised to the idea of communication, creating community and interpreting between people who don’t understand each other. This is not always easy, and if it doesn’t work, can be the downfall of apparently good projects, but when it works properly is one of the great joys of doing DH research, where conversations from different viewpoints result in insights that no one individual could have produced. I would far rather work in research teams that stress community and dialogue than publish single authored monograph, and it may be that this is why so many of us in DH have come to the same conclusion. Of course it is partly due to the speed of technological change in our field. Nobody really wants to read about 5 year old technologies. When we do publish books: they tend to be multiple authored. It's as if we have a sense that DH is about a multiplicity of voices and perspectives, and thus that hundreds of pages of a single voice would be to misrepresent the diversity of the field. So I really do not feel that the monologic lecture can give a real idea of how research in DH works. Lectures are beset by problems of physical constraints. We can only fit a certain number of people into a lecture theatre: there are always limits to the number of questions that may be asked, and of the time possible for answers. There are of course some very interesting and complex questions about the comparison of physical presence and digital surrogacy which we are only beginning to understand, for example in the context of museum studies. Helen Chatergee’s work at UCL Museums suggests that when we handle real objects, different part of our brains respond than when we see a digital surrogate, and similar results have been achieved in studies of visitors to art galleries. It is also clear that despite the early, exaggerated enthusiasm for a pure form of e-learning the reality is that most students prefer a face to face experience of university education because of its social aspects. For example there are many excellent XML tutorial materials on the web, but students still prefer to come to UCLDIS to be taught XML because, despite my great respect for my former boss Lou Burnard, it's easier to work out why your code won't validate with the aid of a friendly demonstrator than a cardboard programmer. I remain to be convinced, however, that this applies to one-off lectures. Digital media offer a far more flexible and appropriate way to communicate DH research. DH is a global field, and we can enter into conversations with members of our community worldwide using blogs, social media and crowd sourcing techniques. Webcasting or podcasting a lecture means that nobody really needs to be physically present to hear me talk any more. But if I blog or tweet about these subjects it becomes a more equal, multi vocal dialogue. Anyone, anywhere in the world can read a blog at any time, or indeed listen to a podcast. They can leave comments or tweet and be part of the discussion either with me, or other members of the 'audience'. There is no limit on the number of questions or comments that can be made: for people who feel shy of asking a question in public it may be easier to comment on a blog or to tweet especially if they wish to do so anonymously. Such a dialogue may be carried on over an extended time period and does not require an 'audience' to be present at a particular time and place. There is also far less implied or actual hierarchy present: it would seem odd if audience members stuck up a discussion amongst themselves at a lecture, even it if was inspired by the themes discussed, yet we relatively often see multi vocal discussion in the comments sections of blogs or on Twitter or Facebook. It might also seem rude if people got up and left during a lecture yet when reading a blog or series of blog posts we can stop, skip, re-read and come back hours or even days later, as is convenient, and the writer need never know or be offended. I also had to draw up an invitation list for my inaugural lecture, but one of the reasons I prefer blogs and Twitter to Facebook is that I don’t have to invite people to join me when I discuss DH in those media. Anyone can follow me, or read a blog, and I rather like the sense that I have never met many of the people who do so, in the case of a blog, I may never know. Somehow it’s liberating to talk to such an audience, whereas talking to a distinguished crowd face to face is frankly terrifying. The ordeal This brings me to the need to digress, briefly about another possible affordance: the inaugural as ritual ordea. It's been described to me as the entry fee to the professorial club. This makes it sound rather like some awful fraternity hazing ritual, but we might pause to look at this, at least briefly. I can report that if such lectures are meant to be provide a frightening ordeal then in my case at least, that really works! I can’t see how anything digital could match this affordance, unless the new professor were seriously technophobic. But talking of phobias, there are surely quicker, more efficient ways of terrifying people than making them do a lecture. Those who fear heights could be made to walk about on the college roof: arachnophobes could be sent to the UCL Grant Museum to play with the spiders; people like me who are claustrophobic could be locked in a small dark space for a while. I don’t think anyone would feel that was an appropriate thing to do to a new professor, so surely we can dismiss the idea of the ordeal as a serious aim……can’t we? Putting something back There is also the more serious idea lurking behind the idea of an entry fee: that new professors should give something back to the community. This is a laudable aim, but I cannot see how someone standing up and giving a lecture achieves this, and indeed it rather reinforces the image of professors as "personal glory seekers", or "backstabbing assholes who take the credit for other people's work" as a recent article in the THE reported. The same study on which the THE reports suggests that Professors should take a greater role in intellectual leadership and mentoring. So, instead of giving a lecture, a more useful way to give back, or pay the entry fee might be to require all new professors to mentor a more junior colleague for a year. This is most likely to be a real world activity, but it might have a digital component, depending on how geeky both people were. The digital alternative Digital media are, not surprisingly, the best way to communicate the nature of my own research field- DH. However, I shall also go on to argue that the affordances we have discussed above for dialogue and sharing of information work better than a lecture for sharing any types of research with the wider UCL community. One good example of how colleagues can communicate their research to each other is through blogging, and particularly a simultaneous blogging event such as the Day of DH. Participants sign up to be part of the day and are then encouraged to record what we are doing and reflect on their work and the progress of our subject, and to read each other’s work and comment. This writing has been analysed, using text analysis technique and treated as a crowd sourced publication on the themes and development of DH. The global commitment to the Day of DH seems to me to indicate that dialogue and the equality of many voices is regarded as central to what we do, but it also works well in other fields: UCLDH PhD student Lorna Richardson used this model very successfully for the Day of Archaeology which she organised last year. It’s possible to imagine a similar event at UCL, where we chose a day and blogged about our work. I think it would be fascinating to read about what my colleagues are doing all day. This would not have to be limited to professors: it could showcase the work of entire research teams or groups and could, indeed should, include early career researchers, postdocs and PhD students. Arguably their research needs more exposure rather than that of professors who are supposed already to have a global reputation after all. Or, if we are thinking of it as an alternative to inaugural lectures, then we might ask newly appointed professors to blog about their work, for example over a week, or longer if they wanted to. These blogs could, as in the Day of DH, be linked to a common interface, and other new professors might add their comments, as indeed any readers could. Bloggers could provide links to artefacts, images, designs, music, buildings etc depending on what they work on, and there could also be links to UCL Discovery, so that if readers found the blog sufficiently interesting, or relevant to their own research, they could download academic articles. The audience may leave a lecture fired up with enthusiasm to download articles, but I think it's quite doubtful whether they actually do so, especially if they stay for the party afterwards. The inaugural model also seems to speak to an older model of academia, where everyone had time to find out what everyone else was doing, and might be able to understand it when they did. Now, if we are serious researchers we don't have time to go to all the inaugurals even in our school, let alone UCL, that time is better spent on our own research. Disciplines are also far more specialised, so the idea of the professor as polymath is seldom true. We get promoted because we are experts in our fields and we become so by a pattern of publication in specialist journals that precludes the ability to develop a broader outlook. You don’t have time to read very widely if you have to produce the publications and grant applications that RAE/REF and promotion criteria demand. In the real world, if I want to find out what colleagues work on I don't want to have to wait until they give a lecture: I'll use digital resources, look up their webpage, follow them on Twitter, find out if they blog, download articles from UCL Discovery. This gives me a far more comprehensive picture of their work, far more quickly than listening to them give a lecture. This could be a new way to foster interdisciplinarity at UCL, whereby people might stumble upon someone who is working in an area of shared interest. It could also be a genuine vehicle for pubic engagement, since the commenting function and potential linkage with Twitter would allow those outside UCL to take part in the conversation. Public Engagement Of course these kind of blogs would work very well as a vehicle for Public Engagement. This has also been suggested as a purpose of the inaugural lecture. I can't see how this can be possible, because a lecture is a one to many medium of expression, and without the ability to ask any questions there is no possibility of two way interaction: under the UCL definition, therefore this cannot count as public engagement. Steve Cross, UCL’s head of public engagement, also tells me that very few people from outside UCL come to Lunch hour lectures, that are specifically designed for the public. Yet I know that the podcast of my LHL on Twitter has reached far beyond UCL- people tweet to tell me so and the numbers of downloads of public engagement podcasts such as those from UCL CASA’s Global Lab are very impressive, and clearly growing. So it's arguable that even if we think about such things as communication of research to the public the digital form is at least as good, if not better. However, digital resources really are very good vectors for Public Engagement: they make it possible for those outside academic to engage with our ideas and even become part of the research process. Because of the stress in DH on collaboration and the need for interpretation and communication it is perhaps not surprising that as a field we have taken to Public Engagement very happily. I’m very proud of the various public physical public engagement activities in which various members of UCLDH, including our students, have taken part, including Bright Club, creative writing workshops and a popup exhibition at the UCL Art Museum. But it is not surprising that the combination of digital resources and the UCL belief in PE and inclusion have produced are two of the most exciting crowd sourcing projects in the world. Transcribe Bentham allows people to engage with original historical sources online in a way that was, until recently, only the preserve of scholars and archivists. It’s wonderful that it has won a Prix Ars Electronica, and caught the imagination of the global media, such as the New York Times, but even more important is that fact that so many members of the public have taken part and contributed transcriptions to the resource. After my lecture the party took place in the UCL Grant Museum, not just because it’s a beautiful space full of fascinating exhibits, but also so that people could use the QRator iPads. Our work on QRator, a collaboration with UCL CASA's Tales of Things, means that users can now express their ideas about museum objects, rather than passively clicking an interactive display or reading a conventional museum label. In doing so they enter into a dialogue with the exhibits, the museum curators and other visitors, whether they are physically present, or commenting on Twitter or via the Tales of Things website. This is a true dialogue, one might even say crowdsourced interpretation, and would have been impossible without the aid of digital technologies. Our work on social media and crowdsourcing once again privileges many voices over one, and is thus, entirely appropriate for DH. I must admit though that some critics of digital diversity appear to feel that in this scenario there is no place for expertise and the role of the teacher, curator, editor or other form of expert is thereby undermined. There still remains a reactionary academic distrust in the idea that social media can ever be used to a serious purpose, and a fear that allowing normal people to voice their views is fundamentally disruptive and disreputable. I disagree with this view which seems contrary to UCL’s founding principles of openness and inclusivity. Cuncti adsint meritaeque expectent praemia palmae: Let all come who by their merit deserve the greatest rewards. Surely the affordance of the digital medium to allow expert voices to mix and converse with those of the interested public is far more powerful than that of the lone voice speaking. If we are too afraid to discuss our views with others, whether within or outside academia, what kind of experts are we? Interactivity and inclusiveness It therefore seems to me that the digital medium allows for a more inclusive approach to academic research, whereby users are not only consulted but become part of the process of discovery and interpretation. There’s also another aspect to inclusivity, and that is in the sense of not discriminating against certain groups. The physical lecture therefore seems to me to offer potential barriers to gender neutrality and family friendliness as compared to at least some uses of digital media. The early evening timing of academic rituals such as lectures and seminars also seems to assume a very dated model where male academics worked and their wives or servants dealt with family and practical things. Having such things in the early evening is only convenient if someone else is shopping for and making dinner and looking after children or older relatives, and you do not have a long commute home afterwards. For most of us this is not the case; early evenings are especially difficult when it comes to maintaining a healthy balance between work and family life. Given that, unfortunately, studies continue to show that women still do the larger share of caring responsibilities and housework, even if they work full time, it may be especially difficult for them to manage this conflict. Yet I know that my DH colleague Melissa Terras found that the use of Twitter and reading blogs helped her keep up with her field while on maternity leave: you can read twitter a 3am while balancing babies and an iPhone. Melissa has missed a lot of inaugurals while she’s been on leave though…. Including the team I might also argue that the inaugural lecture form is not only unfriendly to women in the audience, but also to female presenters. The most recent Athena ASSET survey of women in STEM subjects demonstrates that most women prefer to attribute their success to working with an excellent team of other researchers and to the support their receive from their partner and family. I am very definitely one of these. My team know, because I tell them all the time, that they are the most wonderful group of DH researchers on the planet ever, and I could in no way have achieved a fraction of what I have done without them. I don’t say that kind of thing to my husband, but I should, because the same is true- even if he doesn’t do DH. But the serious point is that this tends not to be the case for men, who, the report suggests, tend to see their own ability as the main reason for their success. It follows from this that the inaugural lecture is a particularly masculine form, stressing as it does the achievements of the individual. I would have preferred some kind of event in which my team could have shared not just in the celebration, but in the presentation, and it feels uncomfortable for me to be singled out in this fashion. To use a cycling image, I’m like the person who wins the Tour de France. I may be the one who, literally, gets to stand on the podium this time, but I could never have achieved it without my team working for me, sheltering me from the wind, setting the pace up the climbs, helping me on a bad day, after a puncture or a crash, leading me out in the sprints. I might take the glory, but they do so much of the unseen, unappreciated work, without which it would not be possible. It appears that this may not simply be my own choice, it’s just that I am typically female in terms of who I credit for my success, and who therefore I wish I could include in its celebration. Celebration It’s perhaps significant that some of us are wondering whether the single person lecture is appropriate at all as a way to celebrate achievement in DH. During this year’s Zampolli Lecture, at DH11 several of us wondered on Twitter whether this was an incongruous event, given that the honorand, Chad Gaffield was talking a great deal about the work of his team. We felt that almost every DH scholar of note now works with a research team, and that therefore it might be more appropriate to have some kind of an event that celebrates the most excellent DH team, or the most effective DH team worker. But it must be noted that digital social networks can be a vector for celebration themselves. One of the most delightful aspects of my field is that it’s usual to congratulate individuals and team on their success using Twitter, Facebook or comments on blogs. It’s great to know that we don’t feel it diminishes us as scholars to celebrate the success of others online. Thus the inaugural lecture works well as a celebration if it’s for an individual scholar, but I think it’s less appropriate for team-based research. In my view, though, we already have an excellent way for individuals to celebrate at UCL- The Provost’s Promotion party. This is a delightful occasion at which everyone invited is celebrating their promotion, not just professors, and is able to bring a guest, often a family member, or colleague who has supported them and helped make the promotion possible. It can’t celebrate the whole team, but it gets closer to it than a lecture. I think therefore, that when we compare the affordances of digital resources and the one-off individual lecture, the digital proves to be at least as good, if not better in almost every category and it is especially ineffective when it comes to expressing the nature of my own field. And yet, the objection might be raised that we still feel that it’s very important that DHers from all over the world should meet at various conferences and workshops, especially the annual DH conference. Why does this physical meeting still matter? As Ann Blandford has found, the informal, social parts of conferences are the most useful in terms of ideas generated through serendipitous discovery. This is part of the reason for UCLDH digital excursions, where the talk is always short, but the drinking and discussion is as long and enjoyable as possible. People might think we enjoy such occasions: how wrong they are. We only do it for the research networking possibilities, honest. So it turns out that the really important part of this whole process is not the lecture at all, it is the party that follows. My colleagues in other parts of the world, who could not attend my lecture might watch a webcast but nobody has yet invented the digital equivalent of the party, even via social media. Even if they were tweeting away with a glass of wine in one hand and an iPhone in the other, it’s almost impossible to replicate the atmosphere generated by a real, physical party. So this, after all is the affordance that we cannot yet surpass in digital fashion, which is probably why we in DH take partying so seriously. Conclusion So let’s have a look at the affordances that I’ve described above and how physical lectures and digital media compare. Physical lectures are clearly massively superior when it comes to giving people a serious fright. Neither medium offers a very effective way to pay back to the scholarly community, but other ways to do this, such as mentoring, would be predominantly face to face activities. Lectures compare badly to digital media when it comes to being interactive, and allowing users and those outside academia to take part in the research process. I also believe that digital media are far more effective as a way to communicate research whether within or outside academia. If we use such things as connected blogs then digital media also offer a way to include and celebrate the activities of a research team. The physical lecture also does little to dispel the image of the professor as stuffy, self-absorbed and disconnected from the wider public or colleagues; the early evening timing harks back to a world where men attended lectures and women looked after the home. We need, therefore, to be aware that in persisting with the physical form we are doing little to challenge these kind of academic stereotypes. How effective a lecture is as a way to celebrate seems to me to depend on the type of person and the kind of research they carry out. For an extravert single scholar who loves the adrenalin of performance then I am sure they must be wonderful. But for people such as me, who prefer to celebrate with their team and supporters, and fade happily into the background, attracting as little personal attention as possible, then they are, as my engineering colleagues might say, suboptimal. One of the most powerful things that we gain through the use of digital resources and media is options for ways to communicate and exchange information, express ourselves and conduct our research. We can send email, blog, tweet, Facebook, share pictures, videos, music: we can be an active participant who creates information or prefer to read, lurk and take things in. None of this excludes the possibility of reading a printed book, visiting a museum, listening to a concert or going to a movie with friends. It’s up to us to decide how we want to mix the digital and physical in our own informational and social world. The media we use depend on individual preferences, and what we want to say about ourselves. No one thing is right or wrong: we need to find the most appropriate tool or medium for what we want to achieve. This is as true in our academic as our social lives. Thus I would argue that in academia we should be open to the same kind of complex informational landscape: why not allow for a variety of forms physical and digital that will achieve communicative objectives, why not change the mixture as technologies change? In doing this we might wish to include the traditional lecture in the repertoire of channels, but if we do we need to be clear about our motivations for doing so. If we persist with the traditional form of the inaugural, it is because we want to say something about belonging to a historical academic form and tradition of public academic performance not because it’s genuinely the best way convey information about our work, or our disciplines to colleagues and the interested public. The one affordance of the inaugural process that we cannot begin to match in the digital form however, is the party. It looks as though it might be some time before we can find a digital equivalent for that.researching and teaching digital history as the art of annotation.Homer Simpson as Historian? From Josh Smith, Annotating an Image in WPF.As I continue to plan out this spring’s Digitizing Folk Music History course with the ace librarians, archivists, and technologists at Northwestern University’s library, I keep returning to the concept of annotation as a core concern for digital historians.I suspect that literary scholars have done a lot of thinking about annotation, but have historians? Chauncey Monte-Sano has a good post about teaching annotation on the teachinghistory.org website. Her post is directed toward K-12 education (important!). But I think the art of annotation also has bigger implications for historical teaching at the undergraduate and graduate levels. So too, it proposes new modes of research, publication, and scholarly communication in the field of history.If we broaden the term annotation to mean commentary on other objects, texts, data, and information, it becomes both practically and metaphorically the very stuff of historical interpretation. It is the assembly and analysis of artifacts, documents, media, statistics, and other kinds of evidence. It is the very glue that holds evidence and argument together in primary research. Which is to say that it is the stuff of argumentation, the art of building an interpretation out of close, creative, convincing analysis of evidence.Annotation is not only a key to historical research; it also plays a key role in historiography. Reviews, forums, debates, discussions are, in some sense, annotations of annotations, queries and commentary about the annotative linkages between evidence and interpretation made by others.It seems to me that annotation and the digital go well together, for the digital is geared toward enabling, representing, and capturing the flow of argument and argumentation. And annotation, at its essence, is just this flow. Therefore I would argue that the digital can take annotation places that print could not. Now, I love print, and I think it remains a unique and essential technology for historical scholarship, far better than the digital at many things. But print tends to freeze things in place (part of what makes it great). The digital seems potentially far better at allowing us to develop, track, and push forward the stream of interpretations upon interpretations, evidence upon evidence, readings upon readings of the past.This points toward new modes of historical scholarship that the digital can add to our repertoire of analytic tools. Certainly infrastructure building is needed: we do not quite yet have the right tools, interfaces, database structures, affordances, interoperabilities, and other technical capacities to get the flow of annotation going. CommentPress and its new iterations are one starting point. Other tools are in development as well, not only for text annotation, but also for multiple modes and forms of evidence (audio, video, maps, visualizations, quantitative data, and so on). But we are not there yet (that’s okay, we’re just getting started).I think there is an additional conversation we need to have about the relationship of annotation to tagging. How do we think through the qualitative dimensions of tagging more fully? One way would be to develop forums for comparing different historians’ decisions for tagging logics and specifications across projects. What similarities are we seeing in tagging a database of folk music festival documents and one of railroads in the nineteenth century, for instance? What differences? We need to better name, articulate, and identify the logics that guide tagging decisions. Might annotation of tags and semantic web design be one way to approach this fascinating but tricky issue?Finally, we also need to develop new incentives and ideals of scholarly contribution. In particular I am drawn to the idea of the nanopublication currently being developed in the sciences (shout out to Claire Stewart for describing this concept to me). The nanopublication is the smallest unit of intellectual and analytic contribution to a research question. It’s easier to describe and track this kind of contribution in the sciences: a scholar might be able to connect one data point to another in a specific and easily identifiable way. In history and the humanities, the contributions are murkier, at once more individual and more collective, sometimes lining up in an orderly fashion but usually criss-crossing in a complex grid of vectors based on different assumptions, starting points, sensibilities, investments, attitudes, perceptions, and positions. Nonetheless, I think there is something to the concept of the nanopublication that aligns well with the art of annotation as historical interpretation in the digital medium.My WordPress blog is not yet set up for you to annotate this post (!), but I welcome comments and thoughts about the concept of the art annotation as a key aspect in the development of digital history. This entry was posted on Friday, January 20th, 2012 at 11:07 am and is filed under Annotation, Digital Historiography, Digital History, Digital Humanities, Digital Humanities Critical Discourse, Digital Humanities Methodologies, Digital Labor, Digitizing Folk Music History, Digitizing Folk Music History Spring 2012, Graduate Education, Interface Design, Metadata, Open Source, Peer Review, Undergraduate Education. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site.Collection Achievements and Profiles System and DPLA Crawler ServicesThis is a quick strawman proposal for what the Digital Public Library of America should build as the first parts of a generative platform. This document is not in a finished state, but just as the DPLA has been good at opening up its process with the Beta Sprint, I wanted to release this document early even in this unfinished state.I attended the December DPLA Technical Workshop in Cambridge and was inspired by the discussion there. I hope that this document makes it clearer some of the approaches I and others at that meeting were advocating. I shared this with the DPLA Interim Development Team a couple of weeks ago, and now that development has started I thought I would share it here as well.While the first iteration of the DPLA platform may be set and on its way, I still wanted to share one vision of what a generative platform for aggregations might involve. The main point is to get the DPLA to the aggregations they likely need to present at some point. This document leaves aside the question of whether creating aggregations is a good idea. The desire to create aggregations is a big, often unquestioned, assumption of big digital library projects. I think what is set out below is one simple architecture for accomplishing aggregations in a very Web-centered way while potentially having more reuse outside of just aggregations.This proposal gives a high-level overview of one possible DPLA technical architecture. This gives the idea of what a beginning of a scalable, extensible DPLA platform could look like. The architecture starts with a foundation in the distributed digital collections which already exist on the Web. The platform set out here works with the way the Web works while allowing the DPLA to meet its goals. As a result it will also help cultural heritage organizations to meet their goals for greater discoverability of their collections.Collecting and keeping track of the those existing collections is the job of the Collection Achievements and Profiles System (CAPS). The metadata CAPS collects can be reused to do focused crawls of digital collections through a DPLA Crawl Service. The results of those crawls can be analyzed, and the data used for a variety of applications including topical or format aggregations, mashups, visualizations, and other internal and external tools.This architecture can be summarized through the following diagram.The rest of this document begins with an overview of how these major components fit together. It then goes into more detail about the architecture and technical components required to implement the two major pieces of this platform:Collection Achievements and Profiles SystemCrawler Services: Crawler Services are further divided into the Raw Crawler Service and the Analyzed Crawl Service.The foundation for the DPLA platform would be the distributed collections on the Web. The DPLA can help make these distributed collections more discoverable on the open Web and enable new services. A user with a browser can already enter the address for these digital collections and get something useful back. In this scenario any digital collection on the Web can be a part of the DPLA. The collection (and hosting institution) is not required to implement any new metadata format, gateway, or API. The existing published HTML pages are enough to gain the initial benefits of a DPLA. Collections can choose to adopt other Web standards or provide more information on their collections to gain more of the benefits of the DPLA and the Web at large.The technical barriers to entry into the DPLA are purposefully low to maximize participation. The DPLA has an opportunity to be a truly big tent approach to solving the problems of making America’s cultural heritage accessible and discoverable. When suggesting digital collections adopt standards or make changes, this proposal gives preference to asking digital collections to optimize for the Web for broad applicability. The technical decisions made here always choose what would make the system simpler and easier for producers of digital collections over what would be easier for the DPLA or other aggregators.The Collection Achievements and Profiles System (CAPS) is an editable directory of Web-accessible digital collections. In its most basic form Collection Profiles hold the name and URL of digital collections. Achievements are a way to expand Collection Profiles through gathering discrete pieces of data about collections and their institutions. In order to validate various Achievements, CAPS can request pages and resources from a collection Website. Full Collection Profiles with all completed Achievements are available through a simple API.The Raw Crawler Service finds new collections to crawl through CAPS. The Raw Crawler Service can then launch crawls of a collection Web site. The raw crawl data can be made available to external developers who want to do their own analysis of the raw crawl data and build new services.The Analyzed Crawl Service makes use of the raw crawl data to extract data and text from pages. CAPS can use this data to perform work like validating Achievements, assigning automated tags to collections, and confirming the health of Web sites. The DPLA could use this analyzed crawl data to create various aggregations, search interfaces, and other services for digital collections that are only possible through having this central data store. External developers could access the analyzed crawl data to create their own aggregations, mashups, and other services.The DPLA can create a generative platform through using the existing digital collections on the Web and adding value. Each major component of the DPLA platform can make its data available to the world to enable the creation of novel new services and new creative works.The first component that the DPLA could build is a Web application which allows for collecting basic information about collections. We call this the Collection Achievements and Profiles System (CAPS). DPLA Collection Profiles provide a mechanism for the DPLA to host a centralized Web-based, editable directory of collections on the Internet. DPLA Collection Achievements provide a mechanism for progressively expanding Collection Profile descriptions, promote standards adoption, validate adherence to standards, and progressively engage the community. While the initial barrier of entry is low, CAPS encourages digital collection managers to adopt standards that benefit the discoverability of their collections and benefit the goals of the DPLA.Before reading the following technical aspects of the CAPS proposal it would be best to familiarize yourself with the Collection Achievements and Profiles System documentation. This detailed documentation was done as part of a DPLA Beta Sprint submission, and it forms the initial thinking for this work, including a narrative, wireframes, and Achievement ideas.Technical Components of CAPSCAPS can be modeled with the following diagram. Elements in blue are managed by the DPLA. Other colors refer to various external contributors and consumers.Following is a description of the major components of CAPS.The CAPS Web application allows for editing and managing Collection Profiles and Achievements (information about collections on the Web). Collection managers and DPLA volunteers can create and update Profiles and Achievement data through Web forms. A researcher looking for collections (on a particular topic, in a geographic region, or other relevant facet) could also discover collections and see all information about the collections.The Web application can request pages or other resources for a collection on the Web using the stored URL. For instance when a Collection Profile is first created the URL is validated for being well-formed and then the page is requested to check that it returns a 200 OK status code. Other Achievement validations could also request information from the site (e.g. robots.txt, sitemap). To insure that the CAPS application returns a timely response for editors, CAPS can defer some of these longer running processes and validations to a background job queue.CAPS would require a persistent data store for Collection Profiles and Achievement data. Periodically a data dump of all data could be created for consumption by external aggregators, crawlers, and service providers. Access to all of the data or searches for slices of data would be available through a Web API. Consumers of the API could be aggregators, crawlers, and other service providers. Through the API the DPLA could also provide other services like aggregated sitemaps. Having multiple ways (data dump and API) for accessing the data, lowers the barrier for developers both internally and externally to build new and interesting applications.Standards through AchievementsThrough Achievements the DPLA can encourage the use of various standards which can make Collection Profiles more useful.Initial effort can be put into Achievements which can be automatically detected, therefore requiring minimal effort from contributing collections. The DPLA can adopt Achievements for Web standards that will improve the discoverability of collections on the open Web. For instance it is possible to automatically check whether the site allows for crawlers (robots.txt) and has a sitemap of the most important pages to crawl (sitemap protocol). When digital collections implement these kinds of standards it benefits all consumers of digital collections resources, including the DPLA and search engines.These same Achievements will have interconnections with the rest of the architecture laid out here. For instance automated Achievement validations can require analyzed crawl data to be confirmed.Other Achievements which would benefit libraries and museums could also figure prominently. Knowing the hours of operation and geographic location of the access point to the physical collections, could help encourage visits to a library or museum. CAPS can provide the data to start making connections between the digital and the physical.Since Achievements add a named, small, discrete piece of information to a Collection Profile, the code required to implement an Achievement is relatively small and self-contained. The starter set of Achievements could create a basic functional system that can be delivered quickly to help bootstrap the rest of the DPLA effort. Achievements can be incrementally added over time. Communities and developers could work to create and incubate new Achievements around new standards before they become part of the DPLA core platform. Achievements are another way in which the DPLA could continue to spur innovation around digital collections standards and services.Crawler Services are responsible for coordinating robots to crawl digital collections sites, analyzing the data, and making it available.Benefits of Crawler ServicesRather than using new or existing niche library protocols, the DPLA could make use of common, ubiquitous Web protocols and standards. Encouraging standards (through Achievements) that help the DPLA do its work to crawl digital collections, will also aid the discoverability of digital collections on the open Web.The data created through the Crawler Services is important background information for CAPS to validate some Achievements. Certain standards which the DPLA may want to promote through Achievements, would require requesting multiple pages from a collection. For instance validating a sitemap could involve requesting each of the listed URLs. The resulting data could be used to confirm the presence of listed URLs.Analyzed crawl data could form the basis of various DPLA aggregations and services. Making this data available will also encourage other developers to create applications using digital collections.Architecture of Crawler ServicesCrawler Services can be split into two interrelated, but separate, applications. The Raw Crawler Service is responsible for coordinating crawls of digital collections sites and making the raw crawl data available. The Analyzed Crawl Service is responsible for extracting data and meaning from the raw crawl data to enable DPLA services. Building them as two independent applications can allow much of their development to happen in parallel.Raw Crawler ServiceThe Raw Crawler Service uses the data collected by CAPS to discover digital collections to crawl. Crawls could fall into different categories. CAPS or other DPLA services could require a focused crawl of a collection to be triggered for timely data. Extensive crawls of digital collections sites could also be made.The final product of the Raw Crawler Service is a store of the pages crawled along with technical metadata. Technical metadata would include when the page was last crawled and the HTTP headers returned with the request including the status code.This data could be made available to external developers who want to conduct their own research or analysis on this slice of the Web. Both an API and data dump could be made available. Whether the API only provides for discovery of available raw crawl data or actually returns crawl data, is an open question. Because of the size of the corpus, it may be that the raw crawl data is made available in a lower cost way through cloud services. (See the Common Crawl for more on how this might work.)The Raw Crawler Service would require an application to coordinate robots, a data store for raw crawl data, and a database for technical metadata about the crawl data. A Web application would also be needed to create the API service.Analyzed Crawl ServiceThe DPLA could also provide an Analyzed Crawl Service. This service analyzes the raw crawl data to extract data and text from the raw crawl data. At this stage it can also begin to make connections across repositories. For various ways the DPLA can get to item-level data through crawl analysis, see Solving the Item-Level Problem on the Web. With crawls resulting in the full text of the page there is the the potential to provide rich item-level data without reliance on niche protocols.The initial consumer of this service would be the DPLA. The resulting data could be used as the source metadata about collection pages underlying new DPLA aggregations and services. The data could be made available to external developers to create new aggregations, mashups, and services.This high-level overview of a DPLA platform architecture is intended to spur discussion. There are many possibilities for what a DPLA technical architecture may look like. Presented here is a technical architecture which would enable the DPLA to function in the way that the Web works. Development could be scaffolded quickly and immediately begin to provide real benefits from the DPLA effort.If the model set out here is not followed, the hope is that some of the principles here will remain in the DPLA effort. Allowing for content producers at all levels of technological sophistication to be part of a big tent DPLA effort is an important underlying principle. Technically, the DPLA can insure that at every level of the platform that the data and metadata it creates is made easily accessible for reuse.Is there any merit to this kind of approach for digital library aggregations? Feedback welcome in the comments.This text is copyright Jason Ronallo and licensed under a Creative Commons Attribution 3.0 Unported License.We have the advantage of arriving late to the game.In the cut-throat world of high-tech venture capitalism, the first company with a good idea often finds itself at the mercy of latecomers. The latecomer’s product might be better-thought-out, advertised to a more appropriate market, or simply prettier, but in each case that improvement comes through hindsight. Trailblazers might get there first, but their going is slowest, and their way the most dangerous.Digital humanities finds itself teetering on the methodological edge of many existing disciplines, boldly going where quite a few have gone before. When I’ve blogged before about the dangers of methodology appropriation, it was in the spirit of guarding against our misunderstanding of foundational aspects of various methodologies. This post is instead about avoiding the monsters already encountered (and occasionally vanquished) by other disciplines.If a map already exists with all the dragons' hideouts, we should probably use it. (Image from the Carta Marina)A collective guffaw probably accompanied my defining digital humanities as a “new” discipline. Digital humanities itself has a rich history dating back to big iron computers in the 1950s, and the humanities in general, well… they’re old. Probably older than my grandparents.The important point, however, is that we find ourselves in a state of re-definition. While this is not the first time, and it certainly will not be the last, this state is exceptionally useful in planning against future problems. Our blogosphere cup overfloweth with definitions of and guides to the digital humanities, many of our journals are still in their infancy, and our curricula are over-ready for massive reconstruction. Generally (from what I’ve seen), everyone involved in these processes are really excited and open to new ideas, which should ease the process of avoiding monsters.Most of the below examples, and possible solutions, are drawn from the same issues of bias I’ve previouslydiscussed. Also, the majority are meta-difficulties. While some of the listed dangers are avoidable when writing papers and doing research, most are discipline-level systematic. That is, despite any researcher’s best efforts, the aggregate knowledge we gain while reading the newest exciting articles might fundamentally mislead us. While these dangers have never been wholly absent from the humanities, our recent love of big data profoundly increases their effect sizes.An architect from Florida might not be great at designing earthquake-proof housing, and while earthquakes are still a distant danger, this shouldn’t really affect how he does his job at home. If the same architect moves to California, odds are he’ll need to learn some extra precautions. The same is true for a digital humanist attempting to make inferences from lots of data, or from a bunch of studies which all utilize lots of data. Traditionally, when looking at the concrete and particular, evidence for something is necessary and (with enough evidence) sufficient to believe in that thing. In aggregate, evidence for is necessary but not sufficient to identify a trend, because that trend may be dwarfed by or correlated to some other data that are not available.Don't let Florida architects design your California home. (Image by Claudio Núñez, through Wikimedia Commons)The below lessons are not all applicable to DH as it exists today, and of course we need to adapt them to our own research (their meaning changes in light of our different material of study), however they’re still worth pointing out and, perhaps, may be guarded against. Many traditional sciences still struggle with these issues due to institutional inertia. Their journals have acted in such a way for so long, so why change it now? Their tenure has acted in such a way for so long, so why change it now? We’re already restructuring, and we have a great many rules that are still in flux, so we can change it now.Anyway, I’ve been dancing around the examples for way too long, so here’s the meat:The problem here is actually two-fold, both for the author of a study, and for the reader of several studies. We’ll start with the author-centric issues.Sampling and Selection Bias in Experimental DesignPeople talk about sampling and selection biases in different ways, but for the purpose of this post we’ll use wikipedia’s definition:Selection bias is a statistical bias in which there is an error in choosing the individuals or groups to take part in a scientific study.…A distinction, albeit not universally accepted, of sampling bias [from selection bias] is that it undermines the external validity of a test (the ability of its results to be generalized to the rest of the population), while selection bias mainly addresses internal validity for differences or similarities found in the sample at hand. In this sense, errors occurring in the process of gathering the sample or cohort cause sampling bias, while errors in any process thereafter cause selection bias.In this case, we’ll say a study exhibits a sampling error if the conclusions drawn from the data at hand, while internally valid, does not actually hold true for the world around it. Let’s say I’m analyzing the prevalence of certain grievances in the cahiers de doléances from the French Revolution. One study showed that, of all the lists written, those from urban areas were significantly more likely to survive to today. Any content analysis I perform on those lists will bias the grievances of those people from urban areas, because my sample is not representative. Conclusions I draw about grievances in general will be inaccurate, unless I explicitly take into account which sort of documents I’m missing.Selection bias can be insidious, and many varieties can be harder to spot than sampling bias. I’ll discuss two related phenomena of selection bias which lead to false positives, those pesky statistical effects which leave us believing we’ve found something exciting when all we really have is hot air.Data DredgingThe first issue, probably the most relevant to big-data digital humanists, is data dredging. When you have a lot of data (and increasingly more of us have just that), it’s very tempting to just try to find correlations between absolutely everything. In fact, as exploratory humanists, that’s what we often do: get a lot of stuff, try to understand it by looking at it from every angle, and then write anything interesting we notice. This is a problem. The more data you have, the more statistically likely it is that it will contain false-positive correlations.Google has lots of data, let’s use them as an example! We can look at search frequencies over time to try to learn something about the world. For example, people search for “Christmas” around and leading up to December, but that search term declines sharply once January hits. Comparing that search with searches for “Santa”, we see the two results are pretty well correlated, with both spiking around the same time. From that, we might infer that the two are somehow related, and would do some further studies.Unfortunately, Google has a lot of data, and a lot of searches, and if we just looked for every search term that correlated well with any other over time, well, we’d come up with a lot of nonsense. Apparently searches for “losing weight” and “2 bedroom” are 93.6% correlated over time. Perhaps there is a good reason, perhaps there is not, but this is a good cautionary tale that the more data you have, the more seemingly nonsensical correlations will appear. It is then very easy to cherry pick only the ones that seem interesting to you, or which support your hypothesis, and to publish those.Comparing searches for "losing weight" (blue) against "2 bedroom" (red) over time, using Google Trends.Cherry PickingThe other type of selection bias leading to false positives I’d like to discuss is cherry picking. This is selective use of evidence, cutting data away until the desired hypothesis appears to be the correct one. The humanities, not really known for their hypothesis testing, are not quite as likely to be bothered by this issue, but it’s still something to watch out for. This is also related to confirmation bias, the tendency for people to only notice evidence for that which they already believe.Much like data dredging, cherry picking is often done without the knowledge or intent of the research. It arises out of what Simmons, Nelson, and Simonsohn (2011) call researcher degrees of freedom. Researchers often make decisions on the fly:Should more data be collected? Should some observations be excluded? Which conditions should be combined and which ones compared? Which control variables should be considered? Should specific measures be combined or transformed or both?…The problem, of course, is that the likelihood of at least one (of many) analyses producing a falsely positive finding [that is significant] is [itself necessarily significant]. This exploratory behavior is not the by-product of malicious intent, but rather the result of two factors: (a) ambiguity in how best to make these decisions and (b) the researcher’s desire to find a statistically significant result.When faced with decisions of how to proceed with analysis, we will almost invariably (and inadvertently) favor the decision that results in our hypothesis seeming more plausible.If I go into my favorite dataset (The Republic of Letters!) trying to show that Scholar A was very similar to Scholar B in many ways, odds are I could do that no matter who the scholars were, so long as I had enough data. If you take a cookie-cutter to your data, don’t be surprised when cookie-shaped bits come out the other side.Sampling and Selection Bias in Meta-AnalysisThere are copious examples of problems with meta-analysis. Meta-analysis is, essentially, a quantitative review of studies on a particular subject. For example, a medical meta-analysis could review data from hundreds of small studies testing the side-effects of a particular medicine, bringing them all together and drawing new or more certain conclusions via the combination of data. Sometimes these are done to gain a larger sample size, or to show how effects change across different samples, or to provide evidence that one non-conforming study was indeed a statistical anomaly.A meta-analysis is the quantitative alternative to something every one of us in academia does frequently: read a lot of papers or books, find connections, draw inferences, explore new avenues, and publish novel conclusions. Because quantitative meta-analysis is so similar to what we do, we can use the problems it faces to learn more about the problems we face, but which are more difficult to see. A criticism oft-lobbed at meta-analyses is that of garbage in – garbage out; the data used for the meta-analysis is not representative (or otherwise flawed), so the conclusions as well are flawed.There are a number of reasons why the data in might be garbage, some of which I’ll cover below. It’s worth pointing out that the issues above (cherry-picking and data dredging) also play a role, because if the majority of studies are biased toward larger effect sizes, then the overall perceived effect across papers will appear systematically larger. This is not only true of quantitative meta-analysis; when every day we read about trends and connections that may not be there, no matter how discerning we are, some of those connections will stick and our impressions of the world will be affected. Correlation might not imply anything.Before we get into publication bias, I will write a short aside that I was really hoping to avoid, but really needs to be discussed. I’ll dedicate a post to it eventually, when I feel like punishing myself, but for now, here’s my summary ofThe Problems with PMost of you have heard of p-values. A lucky few of you have never heard of them, and so do not need to be untrained and retrained. A majority of you probably hold a view similar to a high-ranking, well-published, and well-learned professor I met recently. “All I know about statistics,” he said, “is that p-value formula you need to show whether or not your hypothesis is correct. It needs to be under .05.” Many of you (more and more these days) are aware of the problems with that statement, and I thank you from the bottom of my heart.Let’s talk about statistics.The problems with p-values are innumerable (let me count the ways), and I will not get into most of them here. Essentially, though, the calculation of a p-value is the likelihood that the results of your study did not appear by random chance alone. In many studies which rely on statistics, the process works like this: begin with a hypothesis, run an experiment, analyze the data, calculate the p-value. The researcher then publishes something along the lines of “my hypothesis is correct because p is under 0.05.”Most people working with p-values know that it has something to do with the null hypothesis (that is, the default position; the position that there is no correlation between the measured phenomena). They work under the assumption that the p-value is the likelihood that the null hypothesis is true. That is, if the p-value is 0.75, it’s 75% likely that the null hypothesis is true, and there is no correlation between the variables being studied. Generally, the cut-off to get published is 0.05; you can only publish your results if it’s less than 5% likely that the null hypothesis is true, or more than 95% likely that your hypothesis is true. That means you’re pretty darn certain of your result.Unfortunately, most of that isn’t actually how p-values work. Wikipedia writes:The p-value is the probability of obtaining a test statistic at least as extreme as the one that was actually observed, assuming that the null hypothesis is true.In a nutshell, assuming there is no correlation between two variables, what’s the likelihood that they’ll appear as correlated as you observed in your experiment by chance alone? If your p-value is .05, that means it’s 5% likely that random chance caused your variables to be correlated. That is, one in every twenty studies (5%) that get a p-value of 0.05 will have found a correlation that doesn’t really exist.Wikipedia's image explaining p-values.To recap: p-values say nothing about your hypothesis. They say, assuming there is no real correlation, what’s the likelihood that your data show one anyway? Also, in the scholarly community, a result is considered “significant” if p is less than or equal to 0.05. Alright, I’m glad that’s out of the way, now we’re all on the same footing.Publication BiasesThe positive results bias, the first of many interrelated publication biases, simply states that positive results are more likely to get published then negative or inconclusive ones. Authors and editors will be more likely to submit and accept work if the results are significant (p < .05). The file drawer problem is the opposite effect: negative results are more likely to be stuck in somebody’s file drawer, never to see the light of day. HARKing (Hypothesizing After the Results Are Known), much like cherry-picking above, is when, if during the course of a study many trials and analyses occur, only the “significant” ones are ever published.Let’s begin with HARKing. Recall that a p-value is (basically) the likelihood that an effect occurred by chance alone. If one research project consisted of 100 different trials and analyses, if only 5 of them yielded significant results pointing toward the author’s hypothesis, those 5 analyses likely occurred by chance. They could still be published (often without the researcher even realizing they were cherry-picking, because obviously non-fruitful analyses might be stopped before they’re even finished). Thus, again, more positive results are published than perhaps there ought to be.Let’s assume some people are perfect in every way, shape, and form. Every single one of their studies is performed with perfect statistical rigor, and all of their results are sound. Again, however, they only publish their positive results – the negative ones are kept in the file drawer. Again, more positive results are being published than being researched.Who cares? So what that we’re only seeing the good stuff?The problem is that, using common significance testing of p < 0.05, 5% of published, positive results ought to have occurred by chance alone. However, since we cannot see the studies that haven’t been published because their results were negative, those 5% studies that yielded correlations where they should not have are given all the scholarly weight. One hundred small studies are done on the efficacy of some medicine for some disease; only five by chance find some correlation – they are published. Let’s be liberal, and say another three are published saying there was no correlation between treatment and cure. Thus, an outside observer will see that the evidence is stacked in the favor of the (ineffectual) medication.xkcd take on significance values. (comic 882)The Decline EffectA recent much-discussed article by John Lehrer, as well as countless studies by John Ioannidis and others, show two things: (1) a large portion of published findings are false (some of the reasons are shown above). (2) The effects of scientific findings seem to decline. A study is published, showing a very noticeable effect of some medicine curing a disease, and further tests tend show that very noticeable effect declining sharply. (2) is mostly caused by (1). Much ink (or blood) could be spilled discussing this topic, but this is not the place for it.Biases! Everywhere!So there are a lot of biases in rigorous quantitative studies. Why should humanists care? We’re aware that people are not perfect, that research is contingent, that we each bring our own subjective experiences to the table, and they shape our publications and our outlooks, and none of those are necessarily bad things.The issues arise when we start using statistics, or algorithms derived using statistics, and other methods used by our quantitative brethren. Make no mistake, our qualitative assessments are often subject to the same biases, but it’s easy to write reflexively on one’s own position when they are only one person, one data-point. In the age of Big Data, with multiplying uncertainties for any bit of data we collect, it is far easier to lose track of small unknowns in the larger picture. We have the opportunity of learning from past mistakes so we can be free to make mistakes of our own.Ioannidis’ most famous article is, undoubtedly, the polemic “Why Most Published Research Findings Are False.” With a statement like that, what hope is there? Ioannidis himself has some good suggestions, and there are many floating around out there; as with anything, the first step is becoming cognizant of the problems, and the next step is fixing them. Digital humanities may be able to avoid inheriting these problems entirely, if we’re careful.We’re already a big step ahead of the game, actually, because of the nearly nonsensical volumes of tweets and blog posts on nascent research. In response to publication bias and the file drawer problem, many people suggest a authors submit their experiment to a registry before they begin their research. That way, it’s completely visible what experiments on a subject have been run that did not yield positive results, regardless of whether they eventually became published. Digital humanists are constantly throwing out ideas and preliminary results, which should help guard against misunderstandings through publication bias. We have to talk about all the effort we put into something, especially when nothing interesting comes out of it. The fact that some scholar felt there should be something interesting, and there wasn’t, is itself interesting.At this point, “replication studies” means very little in the humanities, however if we begin heading down the road where replication studies become more feasible, our journals will need to be willing to accept them just as they accept novel research. Funding agencies should also be just as willing to fund old, non-risky continuation research as they are the new exciting stuff.Other institutional changes needed for us to guard against this sort of thing is open access publications (so everyone draws inferences from the same base set of research), tenure boards that accept negative research and exploratory research (again, not as large of an issue for the humanities), and restructured curricula that teach quantitative methods and their pitfalls, especially statistics.On the ground level, a good knowledge of statistics (especially Bayesian statistics, doing away with p-values entirely) will be essential as more data becomes available to us. When running analysis on data, to guard against coming up with results that appear by random chance, we have to design an experiment before running it, stick to the plan, and publish all results, not just ones that fit our hypotheses. The false-positive psychology paper I mentioned above actually has a lot of good suggestions to guard against this effect:Authors must decide the rule for terminating data collection before data collection begins and report this rule in the article.Authors must collect at least 20 observations per cell or else provide a compelling cost-of-data-collection justification.Authors must list all variables collected in a studyAuthors must report all experimental conditions, including failed manipulations.If observations are eliminated, authors must also report what the statistical results are if those observations are included.If an analysis includes a covariate, authors must report the statistical results of the analysis without the covariate.Reviewers should ensure that authors follow the requirements.Reviewers should be more tolerant of imperfections in results.Reviewers should require authors to demonstrate that their results do not hinge on arbitrary analytic decisions.If justifications of data collection or analysis are not compelling, reviewers should require the authors to conduct an exact replication.This list of problems and solutions is neither exhaustive nor representative. That is, there are a lot of biases out there unlisted, and not all the ones listed are the most prevalent. Gender and power biases come to mind, however they are well beyond anything I could intelligently argue, and there are issues of peer-review and retraction rates that are an entirely different can of worms.Also, the humanities are simply different. We don’t exactly test hypothesis, we’re not looking for ground truths, and our publication criteria are very different from that of the natural and social sciences. It seems clear that the issues listed above will have some mapping on our own research going forward, but I make no claims at understanding exactly how or where. My hope in this blog post is to raise awareness of some of the more pressing concerns in quantitative studies that might have bearing on our own studies, so we can try to understand how they will be relevant to our own research, and how we might guard against it.I pledge to be a good scholarly citizen. This includes:Opening all data generated by me for the purpose of a publication at the time of publication.Opening all code generated by me for the purpose of a publication at the time of publication.Freely distributing all published material for which I have the right, and fighting to retain those rights in situations where that is not the case.Fighting for open access of all materials worked on as a co-author, participant in a grant, or consultant on a project.I pledge to support open access by:Only reviewing for journals which plan to release their publications openly.Donating to free open source software initiatives where I would otherwise have paid for proprietary software.Citing open publications if there is a choice between two otherwise equivalent sources.I pledge never to let work get in the way of play.I pledge to give people chocolate occasionally if I think they’re awesome._Welcome to the scottbot irregular. My name’s Scott, and the US Government has for some reason seen fit to give me money to study Science. It’s ‘Science’ with a capital ‘S’ because I’m not studying individual aspects of the world using science, but rather studying Science in general as a social, historical, philosophical, and intellectual phenomenon. What’s worse, I’m attempting to do it scientifically. This blog is my attempt at giving the country its money’s worth. Also, I kinda would love feedback on my eventual dissertation. See? Everybody wins.scott b. weingartis pretty clueless about a lot of things. This is his attempt to be less so.[This is somewhat out of date. Please stand by for new information!]Hello World!Student of History & Philosophy of Science and Information Science at Indiana University.You’ve managed to stumble across my little corner of the internet. I’m currently a student and researcher in the HPS and SLIS departments at IUB under two of the most interesting and capable professors I’ve had the fortune to meet: Colin Allen and Katy Börner. I studied history of science and computer engineering at UF, where I slaved researched for the infinitely patient Robert A. Hatch, who taught me more in four short years than I’d yet learned in aggregate over my entire life.Early InPhO Concept MapThese days, I split my time between classes, the Indiana Philosophy Ontology Project (InPhO) and the Cyberinfrastructure for Network Science Center (CNS). At InPhO I program and design visual, navigable representations of our dynamically generated taxonomy of ideas; analyze relational networks (influenced, disagreed with, etc.) from our Thinkers database; and map and compare philosophical ontologies. The CNS keeps me busy with all sorts of scientometric analyses, and I am also involved in the development of large scale network analysis software such as the NWB, creating workflows, providing software feedback, writing documentation and teaching workshops.Co-authorship network created using the Network Workbench ToolResearchHow do changes in communication structures and technologies affect scientific discourse and collaboration?Science is totally rad. So I study it.There are all sorts of ways to study science, of course, and you can’t leave out even one if you want to understand Science as a whole. That means taking a look at its philosophy, history, anthropology, culture and all sorts of other things as well (perhaps even sociology!). It also means looking at (gasp) the science itself, because no self-respecting scholar should claim to understand physics and physicists without being able to calculate the distance the bullet travels before it falls.My overarching research is in modeling and mapping the growth of science on a large scale – thematically, geographically and temporally – hoping eventually to reveal what conditions yield the most rapid rate of discovery and innovation. Looking back, we see times when scientific progress lurches forward at alarming rates, times when studies come to a halt, times when great minds exposit to deaf ears. Sometimes the reasons are obvious: burned libraries, overthrown empires, new sources of funding, technological breakthroughs, wars that need to be won. But these are heavy brush-strokes painted across the canvas of history.If we could somehow view the whole of scientific endeavors for the last thousand years, across every topic and in every city, with the same fine granularity used to research modern-day science, imagine how much we could learn. By zooming out and looking for “hot spots” of innovation in the history of science, and by understanding the environment in which these hot spots formed, we can learn how to induce those same ideal conditions in modern day research.If the synthesis of new ideas in physics tends to come from young researchers working on their own and with backgrounds in other fields, funds can be allotted to make sure more of those exist. If medical innovations come fastest when small groups of experts collaborate, or if science in general runs smoother in small-world type collaborative networks rather than completely connected networks, that information can be used to focus funding in just the right way to improve the rate of innovation.The closest we can come to that fine granularity, to understanding science across contexts, is by using as many research tools as we can find. We must be comfortable working in whatever discipline with whatever methodology is necessary to find the answers sought. Huge historical data sets will be a must. Scientometricians and others in related fields do an amazing job of learning the structure of modern science, but that structure is necessarily bound to the mediums it inhabits. Modern science is a beast of national laboratories, e-mails, universities, cited journals, click-throughs, conferences and page hits.Marshall McLuhan may or may not have been correct when he claimed “the medium is the message,” but there is no doubt that the medium plays a large role in how science is adopted, disseminated and studied. That role cannot be understood without stepping back and viewing all of the alternatives – correspondences, scientific societies, book transcriptions, etc.Dutch Republic of Letters created in collaboration with The Huygens InstituutThe task, then, is to collect as much data as possible, as far back as we can. We should track where books traveled within Medieval Europe and Asia; who corresponded with whom, how often, and about what during the Early Modern period; who taught whom and where scientists studied; how many books were published in what languages; what universities had copies of which journals; where shared resources traveled.This is an impossible amount of data, of course, and can only exist if created collaboratively and in the spirit of openness. These are not ideas to be copyrighted – they are numbers and data points, and they should be accessible and compatible and aggregated in one place. A History of Science Data Commons, so to speak. More on that project coming soon.Trying to understand all of it at once is a big task… and absolutely impossible.  I’ve sliced myself two pieces of the pie that are hopefully manageable and definitely inseparable:Periods of rapid scientific production and progress.Inflection points in scientific communication and collaboration.Changes in communication structures and technologies obviously affect scientific progress deeply, and it is exactly what those effects are that I hope to uncover. Scientific revolutions and media revolutions, what a tired subject! Well, perhaps, but there are two very good reasons they’re overstudied: they’re terribly important, and nobody’s got them right yet.InterestsCourtney and I contact jugglingThankfully for my friends and family I do not work 24/7. When not working, I can often be found juggling, attending renaissance festivals, geocaching, camping, campaigning for rationality, and reading science fiction & fantasy novels. When I feel guilty about not working, but not enough to actually get back to work, I read about physics, cognitive science and linguistics. I am also perpetually writing a history of the obscure art of contact juggling.Juggling has been a big part of my life for nearly a decade now; I was president of Objects in Motion (UF Juggling Club) for a few years and brought the club from 3 to 30 active members, taught lessons at Groovolution dance studio, and performed with Circle & Spice in Bloomington. I’m now involved in the IU Juggling Club and juggle irregularly at the Bloomington Farmer’s Market. I have performed as far north as Calgary, as far east as Amsterdam, as far west as Los Angeles, all the way south in Miami, and all sorts of places in between.None of that would have been possible without my good friends and co-performers in the Spherocity contact juggling troupe: Matt, Jay, Cory, Courtney, Steve, and Leighanna. Thanks to Nick, Nicole, Leah, Ian and the rest of the crew, Objects in Motion keeps growing larger and better and I miss them terribly. And if you’re reading this, Sierra, you should start juggling again.Juggling knives in CalgaryAs if there’s not enough on my plate already, I’m also involved in two wonderful pseudo-academic organizations. I co-founded Sophosessions with Warren C. Moore, the coolest cat I know, in my junior year at UF. The group still meets a little more than monthly and allows its two-dozen members to present talks on whatever they feel like, from Chinese calligraphy to Zen Buddhism to advanced fractal mathematics to building robots. Then everyone goes to Ben & Jerry’s. I still webcast into meetings whenever I can, but it’s just not the same without the ice-cream.The Venerable IU Beer & Algorithms Club fills two Monday nights a month, and I get to listen to a bunch of Computer Science and Math graduates present their favorite algorithms in gory detail, all while eating a tasty meal and enjoying an equally tasty beverage. What could be better?Here is my paper for the Marxism and New Media conference at Duke this weekend. It largely overlaps with my recent MLA paper, but it is rather different in many respects as well so I will just put the whole thing up despite the repetition. In any case, I am trying to beat it into shape for a more formal publication venue.The Political Economy of Digital Media and EducationBenjamin J RobertsonFirst, let me shill for ebr and ask anyone who is interested in submitting their paper to us for publication to speak to me at lunch or later today. You can also find me online, on Twitter, etc.Second, let me say that this is perhaps the worst paper title I have ever come up with.This paper is a continuation of one that I gave at MLA two weeks ago, with a much better, if less informative title: “Digital Anamnesis.” My aim for that paper, and for this, is to think through my hesitation with regard to the new, history, form, and meaning. Briefly put, and not saying anything new as yet I think, I value new forms and processes of discourse, ones that seek to overcome limitations inherited from the past in order to make meaning in new ways. These forms and processes would have to, perhaps, ignore history and the methods of meaning making it affords us. However, I also value history, however problematic, insofar as it allows us to contextualize, understand, and make judgments about the new. In my MLA paper, and with further elaboration here, I consider received forms and processes of scholarship, especially as such scholarship (which is being challenged by digital media) operates within a political economy of academic employment and instruction and intellectual discourse. My concern, specifically, has to do with the manner in which the discourse surrounding what we still call the job market has been inflected by the advent and valorization of the so-called digital humanities. Dh has, it seems to me, implicitly promised young scholars jobs if they are able to write code, create databases, or otherwise interact with networked computers in an expert manner, often by prioritizing alternative academic, or alt-ac, careers. My purpose is not to argue against the value of DH broadly, but to question how DH or new media interacts with and informs the political economy of academic instruction, production, and employment in the humanties.My MLA paper was part of a panel organized by David Golumbia: “Digital Literary Studies: When Will it End?” which has the distinction of being name-checked by Stanley Fish in a New York Times op-ed. Given where we are, and the appositeness of Fish’s comments on the MLA convention generally in the context of this paper, I will start with him as a way into my argument. Fish tells us that he cannot attend MLA, but that he has read the program and can therefore weigh in on its shortcomings, which, it turns out, are legion. He writes, “I was pleased to see that the program confirmed an observation I made years ago: while disciplines like physics or psychology or statistics discard projects and methodologies no longer regarded as cutting edge, if you like the way literary studies were done in 1950 or even 1930, there will be a department or a journal that allows you to proceed as if nothing had happened in the last 50 or 75 years.” Ignoring that session titles are rarely useful for understanding what sessions are actually about or the directions they might take, we can see here Fish, apparently at any rate, critiquing his (former?) profession for failing to advance. In some respect, he is no doubt correct. I recall Michael Berube writing somewhere that most undergraduate courses are methodologically organized by practices of close reading and simple historicism. These practices, in fact, still dominate if silently, I think, even more advanced humanistic discourse. As I hope to make clear, I am rather perplexed by the question of what to do about this “failure” to move forward with new practices of reading, writing, and thinking.In any case, Fish then goes on to reminisce about how everyone used to talk about postmodernism (which seems to be a proxy for “theory” broadly), but no one does anymore. So, it seems we do move on, but not in the manner that Fish wants or expects. He writes: “What happened then, and inevitably, was that after an exciting period of turmoil and instability, the alien invader was domesticated and absorbed into the mainstream, forming part of a new orthodoxy that would subsequently be made to tremble by a new insurgency.” It’s not at all clear what Fish’s point is here, whether he wants a continued instability or is happy to see it pass.And, finally, we get to what is for my purposes the point, Fish’s criticism of digital humanities, or new media studies, or whatever you want to call it—the new insurgency before which the now staid and neutered postmodernism-informed profession trembles. DH is the “rough beast” that has replaced postmodernism as the destabilizing force that threatens “what we do.” As an aside: it seems to me the height of ignorance to equate postmodernism (which has been variously understood as a theoretical position, a style, and a historical period) with digital humanities (which seems to be becoming a methodological position, but has been understood more as a practical, pedagogical, and sometimes theoretical engagement with the hardware and software that increasingly dictate the manner and scope of our practices). Nevertheless, DH is Fish’s target, and he writes:Once again, as in the early theory days, a new language is confidently and prophetically spoken by those in the know, while those who are not are made to feel ignorant, passed by, left behind, old. If you see a session on “Digital Humanities versus New Media” and you’re not quite sure what either term means you might think you have wandered into the wrong convention. When the notes explaining the purpose of a session on “Digital Material” include the question “Is there gravity in digital worlds?”, you might be excused for wondering whether you have become a character in a science fiction movie. And when a session’s title is “Digital Literary Studies: When Will it End?”, you might find yourself muttering, “Not soon enough.”And here is the question: does this “not soon enough” reveal a longing to return to the proper practices of humanistic discourse or a longing for the incorporation of DH into those practices in such a way that it becomes part of the new orthodoxy? It seems uncontroversial to state that theory or postmodernism has transformed the profession, whether positively or negatively. Maybe no one “does” theory the way they use to, but we need look no further than the title of the recent collection Theory after “Theory” to recognize that its legacy remains. Is this “theory” a domesticated one, one that has lost its power to subvert as a result of our acceptance of it? I certainly cannot answer that question. Rather, in the remainder of this paper, I will address what I see as Fish’s hesitation in the face of digital media as a transformative force in the humanities in order to open up a discussion of the political economy of our profession.To that end, I begin with Bernard Stiegler and his work on anamnesis.Stiegler and anamnesisStiegler raises the subject of anamnesis in Technics and Time Volume I, where at first introduction it refers to much what Plato meant by it in the Meno: the true knowledge possessed by the soul prior to birth that is recalled during life. In the Meno, Socrates leads an apparently uneducated slave through a complex geometrical problem without giving him any instruction, thus proving that the slave must have already possessed knowledge of geometry. My initial response to Golumbia’s provocative question about the end of digital literary studies had to do with this understanding of anamnesis and the danger I see in work that treats the digital as always already there. I see this danger in ill-conceived uses of media archaeological or similar approaches that seek to demonstrate the digital nature of moments prior to the digital. Just as nature became a book after the Gutenberg technology, it has now become a computer and, it turns out, it has always been a computer; we are only just now recollecting that knowledge at the prodding of latter-day Socrateses. Some of the more radical claims about the effects of computers on the world come out of such work. Siegfried Zielinski discusses such claims, which came with the “inflation” of the number of definitions of “media” in the 1990s, as follows: “Media and future became synonymous. If you didn’t engage with what was then baptized media, you were definitely passé.” Such assumptions became more entrenched with the addition of “digital” to “media,” or to anything really. He writes: “The digital became analogous to the alchemists’ formula for gold, and it was endowed with infinite powers of transformation” (32). In any case, given that Golumbia’s own work as well as that of others has already dismantled such thought, and given a provocative blog post by John Protevi to which I will turn later, I found my thoughts turning instead to Stiegler’s reformulation of anamnesis.In Taking Care of Youth and the Generations (another awkward title as it were), Stiegler argues that the major (to which is opposed the minor in terms of legal standing or social position with regard to maturity; eg someone not yet 18), must care for youth by fostering in it long circuits of attention that can withstand the force of short-circuiting technologies (such as those we refer to when we say “new media”). (To jump ahead, I am sympathetic with Stiegler on this issue insofar as I adhere to inherited practices of meaning-making, but nonetheless cannot completely agree with his position because it seems to install those practices in a position of superiority to all possible new practices. Of course, Stiegler will always claim to be working pharmacologically with regard to all techniques and technologies, but he seems rather conservative to me in this respect.) In any case, anamnesis, in Taking Care, becomes associated with the literacy Kant discusses in his essay on Enlightenment. Notably, Stiegler will take Foucault to task for failing to address this aspect of Kant in his myriad discussions of that essay. For Stiegler, anamnesis becomes the proper mode of individuation for youth. He writes:The true, the just, and beautiful have an effect on me, transcending my understanding as such: they transform me. This intrinsic transcendence of the understanding by its object is what requires the individuation of “the one who knows” by what he knows (its object), where the knower is transformed even as the object being constructed is transformed in return. Plato calls this individuation “anamnesis.” (110, original emphasis)My issue with Stiegler at this point has less to do with the residue in “anamnesis” of its previous formulation, in which it referred to the true and eternal knowledge of the soul, than with the manner in which he now aligns anamnesis with what he calls long circuits and the establishment and maintenance of these long circuits within literacy as discussed by Kant. More specifically, my issue with Stiegler derives from his insistence on long circuits, which is to say older psychotechniques such as reading and writing (and by extension what we might call scholarship) as the proper and only effective answers to the short circuits and short-circuiting tendencies of the psychotechnologies of digital media.But that is a critique for another day, as here I want to think about anamnesis in terms of the institutional memory of academia. In Taking Care Stiegler neatly sums up academic training as part of a general organology. “A scholarly education, as the interiorization of organology, consists entirely of psychotechniques for capturing and fashioning attention, transforming it into nootechniques through the interiorization of disciplinary criteria” (65). By “organology” Stiegler refers to a general practice and study of the connection between human organs (ie the body and mind, broadly), technical organs (that is, technologies), and human organizations (such as schools). “Psychotechniques” are practices of individuation that cultivate attention (such as the book), as Alex Galloway glosses, and are distinguished from “psychotechnologies” which short circuit attention. “Nootechniques” are practices of transindivuation having to do with a “we” rather than an “I”; hence the connection here to “disciplinary criteria”, the standards of a group. Stiegler continues this passage as follows:Embedded in these criteria are the rules governing the practice of any organology—such as the rules for rewriting in mathematics, as the anamnesis of the long circuits grounding those rules in reason (that is, by going back to axioms) transferred through the course work assigned by teachers in training programs. Certain organs—the eye, the hand, the brain—must be coordinated for reading and writing to take place, but the entire body must first be trained to sit for long periods of time. (65)We can see the connection here between Stiegler’s organology and academic training in the humanities. We learn to sit and read, to sit and write. Our human organs become coordinated with technical organs, namely those having to do with reading and writing—not only books but furniture. We learn that perhaps the most valuable thing we can own is a good office chair that will prevent chronic back pain. Moreover, this coordination of human organ and technical organ, of person, book, chair, etc. also coordinates with an organization, namely the school (or, as Stiegler prefers, the skolieon). And, again to jump ahead, we see in this complex coordination the instantiation of a discipline but also of a tertierary retention. Stored in the literacy practices of the humanities are not only certain contents—novels, our thoughts on novels, etc. Also stored there is human gesture, a time of reading and writing, in the same manner that the assembly line stores the gesture of the industrial worker that preceded it. It is here that I become concerned with DH and its technologies and it is here that we can turn to the political economy of academic instruction, production, and employment.Evaluating digital scholarshipI will return to Stiegler in a moment, but first I want to briefly turn my attention to the recent special section of Profession on evaluating digital scholarship as an example of the discourse on this political economy. Before I say anything that appears to be negative about the section, first let me express how impressive it is, and how valuable it is as a contribution to the development of the profession. Any “issues” I have with these essays are bound up in a problem I find in Stiegler: a fundamental inability, which belongs I think to all of us, to conceive of a present that does not rely in some way on the past. In the context of the special section, the problem becomes thinking of what is not-scholarship (that is, not what tries and yet fails to be scholarship but what is not-scholarship insofar as it is something else) in a positive manner, where “positive” means both “good” and “extant.” What is not-scholarship, which I am here aligning with what Stiegler critiques as short-circuiting psychotechnologies even if the contributors to Profession are not, can only be thought insofar as it is reduced to scholarship or meaning, which is to say negatively, in terms of what it refuses to be or return us to.After all, when we speak of “evaluating” digital work, we speak of finding its value, we speak of situating it in a political economy which by necessity extends into the past. Where else might we ground such formulations of value? This issue is not one of nominalism, of what we call this new work or practice, but rather of the establishment or destruction (that is short-circuiting) of a connection of the present to the past and hence the future. Stiegler’s long circuits always loop back into the past, back into literacy and into reading and writing in terms that Kant established for the Enlightenment. We write for a literate public, for a public that can read our writing (or we ought to). We “care” for youth and the relationships among generations by establishing and maintaining programming institutions capable of “long-circuiting.” (And, as an aside, while I am in many ways sympathetic to Stiegler’s overall idea here, there remains in it a creepy paternalism redolent of one of Plato’s more odious ideas: the philosopher king.) What happens when we stop reading or writing, when we stop producing things that can be understood according to the values established by reading and writing?I sense in some of these essays a certain hesitation. I do not mean a faltering, nor do I mean a hesitation born of some sort of nervousness. Rather, I mean something closer to what Todorov articulates with regard to the fantastic. Todorov characterizes the fantastic as a genre by the hesitation it produces in its readers. This hesitation is a reader’s inability reader to finally decide the ontology of the (apparent) ghost or other (alleged) supernatural being or event present in the text. If, in the end, the ghost was not a ghost but has a rational and natural explanation, the reader finds herself in the context of the uncanny, in which natural things appear to be something beyond what they are. If, by contrast, the ghost is truly a ghost, we find ourselves in the context of the marvelous, which admits to a supernatural dimension to or beyond everyday experience.The hesitation in the present context works as follows. On one hand we find an insistence that digital scholarship not be shoehorned into print culture—that it not be reduced to the uncanny, that it not become the same old thing dressed up in an unexpected or otherwise strange manner. Against this tendency, for example, Schreibman, Mandell, and Olsen write in their introduction to the special section: “Current debates in the field of of the digital humanities about the divergent practices of ‘close’ and ‘distant’ reading are really a screen for deeper changes called for by the advent of new media. Digital technologies do more than propose new ways of thinking, as did theory; they require new modes of being” (126, my emphasis). Against the uncanny, they posit the marvelous.On the other hand, we find an inability to think or enact this mode of being beyond the constraints of the terms that precede and, of course, in-form our thoughts of it. Here we find a tendency against the marvelous that leads us back to the uncanny, as if to say that we can obey no laws, no criteria, but those which we already have, those with which we are comfortable and familiar. For example, earlier in their introduction, Schreibman, Mandell, and Olsen write: “digital scholarship needs to be recognized not only as scholarship but also as literary scholarship” (125). This injunction comes in the context of their argument against claims that digital work that involves long-neglected practices of bibliography, editing, and philology is merely service. I agree with them 100% on this issue. At the same time, and I doubt that there is anyone here who would not fall into similar language, they cannot avoid the fundamental category or form of “scholarship,” the insertion of the digital into pre-established and ongoing long circuits according to which knowledge practices have been, are, and likely will continue to be evaluated and valorized. The book, in a world that recognizes digital work as literary scholarship, might no longer be the privileged form of scholarship when it comes to T&P decisions, but scholarship and its long circuits, which come from a culture fully in-formed by the book, will remain.Again, my aim here is not to undermine or disparage this work, which is outstanding. For example, I think that Bethany Nowviskie’s essay on collaborative writing is excellent and demonstrates and overcomes, perhaps indirectly, the fears academics have about losing the individual nature of their work. In addition to these essays, I would also recommend Kathleen Fitzpatrick’s recent book Planned Obsolescence and Michael Gold’s Debates in the Digital Humanities as entry points into this debate. In any case, again, my critique, such that it is, is a soft one. The issue here remains that we might wish for the new, might even seek to identify, think, or build it. However, we hesitate at its threshold or, perhaps more correctly, we hesitate between the new and the old and thus render the digital as the fantastic, tending towards the marvelous but always burdened by the uncanny. Perhaps it is right that we do so, and I admit that this hesitation comforts me. However, I must admit that even as I remain convinced that we need to take Stiegler’s concerns about long and short circuits seriously, I believe we would be doing ourselves a disservice if we decided on the uncanny once and for all as he seems to suggest we do. In this respect, I find the hesitation in Schreibman, Mandell, and Olsen described above preferable and more productive than Stiegler’s subordination of hypomnesis (or technical memory) to anamnesis and his privileging of older and more familiar hypomnemata to those newer and therefore still in flux.The political economy of academic instruction, production, and employmentIf Stiegler’s anamnesis provided the first inspiration for this paper, John Protevi’s call, in the context of his discipline of philosophy, for a move away from a discourse of the “job market” that assumes that one enters such a market in the late stages of or even after the PhD and towards what he calls a discussion of “the political economy of philosophy instruction” provides the second. Protevi argues that philosophers need to change their frame of reference and stop referring to “the job market” as something that happens either late in the PhD or afterwards. I would suggest, in this context, we think along similar lines and therefore discuss the political economy of academic instruction and production, or something similar.One’s interaction with this political economy begins well before one is ABD. A successful run at the market has its roots perhaps as far back as high school and the process of undergraduate admissions. If this suggestion seems hyberbolic, I hope we can agree that success at the undergraduate level, and thus admission to a top-ranked PhD program, certainly helps with the job hunt that waits for a prospective academic in the future. Current discussions about how to advise undergraduates about going (or, more often, not going) into a PhD program in the humanities demonstrate that the market does not begin during graduate school. In fact, it might end during graduate school. As Marc Bousquet convincingly argues, for many PhD candidates, the best (or perhaps only) academic job they will ever have will have been the one they had during graduate school. In Bousquet’s argument, PhDs are not the product of a system of instruction that feeds a demand for intellectual labor, but the by-product of a systemic demand for cheap and relatively uneducated instructors. That is, it is the PhD student and her cheap labor, not the finished PhD and her more expensive labor, that the system demands. Once the PhD is over, the student has priced herself right out of the profession (insofar as the profession has become more comprised of term-based, non-tenure-track, and casualaized labor). As such any discussion or analysis of a well-defined and discrete job market does us a disservice. Moreover, anyone who thinks that the system is broken also does us a disservice. I am reminded of Galloway and Thacker’s argument that epidemics are not the result of networks that fail, but of networks that work all too well. The crisis in academic employment is not that of a system that has failed, but the stabilization of a system that operates exactly as it is meant to operate.It seems to me no coincidence that what Bousquet calls “informationalism” and Golumbia calls “computationalism” rise in prominence at roughly the same time, from the 1950s onward, but really taking root in the 1960s before coming to fruition in the 80s. I am oversimplifying in the extreme, and I don’t want to push on this point too hard here or suggest a causal relationship between these things. I do not think computers, or the forms of thought they engender, are to blame for our job crisis, but I do want to think about the increased emphasis, from HASTAC and elsewhere, on getting PhD students involved in projects under the umbrella of Digital Humanities and what this emphasis means. The stories I have heard about how graduate school worked decades ago suggest that one did not need to attend conferences or to publish as a PhD student. Now, one had better present at several conferences and probably ought to publish one article before “going on the marker”. We still decry this professionalization even as we increasingly ask students to network through Facebook and Twitter, take part in the development of digital tools and resources, etc. Often, it seems to me, these digital projects have less to do with the scholarship of a given PhD studentAnd even as we consider the relation of DH or new media to that side of academic employment we should recognize another thing that the special section of Profession I just discussed makes clear, namely that the political economy of academic employment does not end once one has a job. One must produce and produce in a recognizable manner; one must produce scholarship, one must research. Of course, this research, or its program (and there is that word again) will have begun for most if not all scholars in graduate school. To get a job, one must go to the right place to study the right thing; to keep a job, one must continue to study that thing and demonstrate that study through scholarship. But for those students who work in the as yet non-traditional forms of scholarship made possible by new media and DH’s emphasis on it, it will no longer be “simply” a case of going to the right school to study the right thing and subsequently producing scholarship that follows from this study (as if that were simple). One will have to, for the near if not foreseeable future, justify what one does as scholarship if what one does is in part determined by or involved with new media.To be clear: current graduate students increasingly devote time to digital projects. These projects include theses in media studies (which remain the most familiar form of digital work); work developing and using databases, etc.; professional and semi-professional blogging activities (such as the work undertaken by HASTAC scholars); the production of digital editions; etc. Some of them, no doubt, engage in this work because it promises jobs. Few of them, I imagine, understand as yet the fact that getting these jobs will only be a first step in a longer process of justifying their work.As an aside, we should also think in terms of this political economy in broader terms than I am even suggesting here, as I am not accounting much for teaching and those colleges and universities that privilege it over research. Moreover, discussion of this political economy should include analysis of increasing demands for and emphasis on alt-ac jobs, which seem to involve placing people trained as scholars into staff jobs where they make decisions rather than contemplate the consequences of those positions in the manner that scholars might. I realize that I am uninformed on the alt-ac debate in all of its dimensions, but it holds for me many problems. These include the aforementioned “deskilling” or proletarianization of the PhD who now must give up scholarship for management; the increasing requirement that one have a PhD to do what was likely once a job that required a bachelor’s or master’s degree; and trickle down effect that this last issue has on employment more broadly. In fact, as Protevi argues, I believe that when we discuss the political economy I am naming here we must think it in relation to other sectors of the broader economy.To head towards a conclusion, I want to throw one idea out there. In For a New Critique of Political Economy, Stiegler summarizes his previous arguments about memory and technics: “all technical objects constitute an intergenerational support of memory which, as material culture, overdetermines learning and mnesic activities. [. . .] A child arrives into a world in which tertiary retention [memory stored in technical objects] both precedes and awaits it, and which, precisely constitutes its world as world.” Beyond all of the specific projects and types of projects new media offers to scholars in the humanities, I think we need to think through the tertiary retentions it engenders. Stiegler argues that we need better understanding of technics because technics informs so much of what we do. Zielinski tells us that media is deeply inhuman and discusses its deep time. I wonder what we are creating when we work in new media. I do not wonder about our writing or our editions, but how we are transforming our individual and collective being. I realize that this question is not new, but I nonetheless raise it here in order to highlight the fact that by working in new media now, we are setting up for the future an intergenerational support of memory that we do not understand. Again, I am uncomfortable with what I see in Stiegler as a sort of paternalism. I am likewise uncomfortable with the insistence by some pundits that we should discourage undergraduates from entering the profession. “Taking care” often seems to involve a “knowing better” that does not allow for self-discovery in the traditional sense. Nonetheless, I understand the impulse to care and cannot in the end discredit it.In any case, I think we can see here how Stiegler is useful for thinking through political economy in the context of technics. After all, if one returns to Technics and Time, one can see that economy ha slong been one of his concerns. He writes there: “there is no work without technics, no economic theory that is not a theory of work, of surplus profit, of means of production and investment.”With this in mind I want to make several suggestions:that we continue to consider how new media operate in the political economy of academic employment and education much the way the contributors to Professionalready havehowever, we should extend this consideration to account for graduate training and research projectswe might consider this question in terms of graduate funding (should we be providing them with technology and the space in which to use it effectively?), in terms of graduate teaching (especially in the context of the writing lab, a space with which many tenured profs continue to be unfamiliar but has become a disciplinary requirement for many who teach or would teach first year writing), in terms of media as object, in terms of media as tool, etc.further, we should think about new media as a tertiary retention of academic knowledgeI mean not only that we think about it in terms of the database that organizes our journals or our research objectsI mean, additionally, that we need to understand how new media stores the book, the logic of the book; that it does seems obvious, as new media was produced by a culture entirely conditioned by print (if we buy McLuhan’s argument); understanding new media as a tertiary retention of “book logic” (itself a redundant term) might be a first step in recognizing that the call to call digital work “scholarship” does not go nearly far enough in that it only returns new media to the logic that informed it but does not do enough to create the logic that it might informI mean, also, that we need to understand how new media and its attendant ergonomic supplements (the desk, the chair, the laptop stand, the KSM and now touchscreen interfaces) store the gestures, the embodiment as well as the mind the academicwe should understand as well how new media is a tertiary retention of academia in relation to its status of tertiary retention of other fields (especially that of business; again it seems to me no coincidence that the rise of digital work in the humanities coincides with the casualization of its workforce and what Bousquet calls the informationalization of the university, although this relationship remains murky to me)in short, we need to think of new media in the context of Stiegler’s organologystill need to discuss, perhaps, Stiegler’s critique of Marxnamely, S says Marx fails to separate the working class from the proletariatthis is because the proletariat is something that happens to workers as they are deskilled, as their knowledge of work becomes tertiary retention (as what they did is automated)however, I think that this is unfair to Marx, as at the time it was mainly the working class who were proletarianized, I thinkwith the advent of expert systems, of cybernetics and the information age, we get the proletarianization of management and the bourgeoisieI think that this proletarianization happens later for the most part as a development of capitalism away from the industrial model and the bourgeoisie/proletariat splitthe current split (which is newer) of hacker/vectoralist, we see the spread of proletarianization to the vectoralistsconsider in this context Bousquet’s discussion of the Yeshiva decision and the fact that capitalism has learned to consolidate the classes to either redirect/redistribute class antagonisms or to do away with them altogetheras the proletariat become managers, so too do managers become proletairianizedmake sure to address Stiegler’s failure to think of new weapons, a la Deleuzeneed to discuss the manner in which digital technologies STORE academic knowledge in the manner that the assembly line stores industrial gestures (that we once human); this is the process of proletarianization for Stiegler; how does this relate to new media in the humanities?that would account for workforce casualization issues that in part determine the nature of graduate instruction. Protevi’s vocabulary is both useful and instructive for those of us involved in the study and teaching of language, literature,and culture, and when we use it to address the issues to which I now turn, we can easily see the manner in which such a political economy not only begins for each of us well before we enter the market, but how it extends to the time after we have left it (if we ever do).However, Stiegler, rather than dismissing anamnesis as what he might call “mystagogy,” instead excises from it its more Platonic valences and installs it in his thinkingStiegler, whatever reservations I have about him as described above, offers us a way to think about this issue in terms of institutional memory. First, consider Stiegler’s description of scholarly education from Taking Care of Youth and the Generations, which I think is apt here:In For a New Critique of Political Economy, Stiegler writes:The process of grammatization is the technical history of memory, in which hypomneisc memory continually reintroduces the constitution of a tension within anamnesic memory. This anamnesic tension is exterieorized in the form of works of the mind [or of spirit, espirit], through which epochs of psychosocial individuation and disindividuation are pharmacologically configured.I have far more to say on this issue, but I just want to suggest for now that we need to think about the above issues in terms of this political economy. To that end, we might think about new media technologies as what Stiegler calls tertiary retentions insofar as these technologies store not only our content but the logic that subtends it. Insofar as new media has become a tertiary retention of academia, it creates a deep time of academia, but one that is, I think, different than the deep time of the book and “scholarship.” How that deep time works and how it connects with other sectors of the economy—funding, long term employment rates, etc—not to mention the future, now becomes the question.Like this:One blogger likes this. This entry was posted on 21 January 2012 at 18:35 and is filed under Uncategorized with tags digital humanities, intellectual discourse, mla paper. You can follow any responses to this entry through the RSS 2.0 feed You can leave a response, or trackback from your own site.By PastPlayerJan 19, 12This is a guest article from Adam Chapman, a PhD Candidate at the University of Hull in the Media, Culture and Society department. He received a History BA (Hons) from Leeds Metropolitan University and a Cultural History MA at the University of Liverpool before continuing on to study at Hull. Adam’s research focuses on the videogame as an historical form and seeks to weave existing (mainly postmodernist) historical theory and analysis, with game-focused research that emphasizes the unique qualities of games and play as well as, more recently, Gibsonian psychology. Accordingly, he is attempting to develop an analytical framework for historical videogames that includes both action/agency and narrative/representation. Unsurprisingly, Adam is also an avid gamer and when he is not studying he is normally to be found playing some kind of videogame as his digital alter-ego ‘Woodlandstar’. Like most academics that study games, Adam has found he has to restrict the amount of time he spends doing ‘research’ of this kind.’Few could now deny that contemporary game series like Civilization or Assassin’s Creed constitute history. However, such a broad term does not suffice to infer the approach that analysis of these new historical texts requires. Any apprehension of these hugely-popular historicalvideogames must be grounded in an approach that privileges understanding of the videogame form (and the varying structures this entails) and its integral rolein the production and reception of historical meaning, rather than solely, or even primarily, the content of specific products as historical narratives. This is especially pertinent given the relatively early nature of the serious consideration of historical videogames. Examining the content of particular histories according to what we believe to be accurate is of course a natural part of analysis. However, in terms of modern videogames (at the least), such a focus often really only serves to re-inform us about popular history rather than increasing our understanding of the opportunities for engaging with discourse about the past (and the nature of this discourse) that this new historical form can offer.Proper analysis of content cannot even occur without consideration of the structures that create and represent it. Content cannot be seen to stand independent of form, history cannot be understood as a category separate from the modes in which it is written, coded, filmed, played, read or viewed. Whilst analysing the historical content of particular videogames can give us some basic information this tells us nothing of how content is created in terms of its stylistic and epistemological approach and nothing of how (or even if) players experience this content. Similarly, such a focus tells us nothing about the opportunities for exploring discourse about the past through play: what actions players can perform and do perform (and the necessity of these actions), when they play. This last concern is integral to understanding games, because, unlike the majority of historical forms, videogames, as well as creating ‘traditional’ opportunities for negotiation of meaning, are actively configured by their audiences. In essence, this means when we play we may well be ‘reading’ (i.e. interpreting and negotiating historical signifiers and narrative) but we are also ‘doing’ (i.e. playing). It is only by focusing on form that we can properly include this latter category of action and create an analytical approach that fuses Salen and Zimmerman’s three schemas of games: play, rules and culture (2004, 102), whilst allowing the consideration of the player’s role in the negotiation and fusion, of this triad. The overt focus on content in some of the scholarly analyses of existing historical videogames is troubling for this very reason.With these concerns in mind, this article is a call for a refocusing of academic work on historical videogames. A call for an approach that does not get detained by primarily examining the particular historical content of each game (i.e. historical accuracy or what a game ‘says’ about a particular period it depicts) but instead tries to establish an analytical framework that privileges analysis of form (i.e. how the particular audio-visual-ludic structures of the game operate to produce meaning and allow the player to explore/configure discourse about the past). The benefit of this is that we do not just gain knowledge of a particular historical representation but instead, conclusions about form (a particular game-structure’s operations) are then transferable to an understanding of games made up of similar ludic (and audio-visual) elements.If a cautionary tale about the problems with privileging content over form is needed, then we can turn to the example of historical film, a form that has often been rejected on the basis of individual texts historical content. Critiques of particular historical films were assumed to be indicative of some kind of structural inability of film as a mode of historical expression. Many scholars concluded that film could not constitute ‘proper history’. It took a number of theorists (particularly the seminal work of Robert A. Rosenstone) to outline that accusations of ‘poor information loads’ or ‘discursive weakness’, were not only often unjustified and selective, but based on unfair comparisons. Not, as it might first seem, to the elusive past itself, but to the history found in books. This is an, often unconscious, ontological discrepancy whereby the notion of ‘accuracy’ or ‘truth’ is collapsed with and thus, taken to mean ‘in alignment with the narratives of book-history’.Obviously the aim of the developers of historical videogames like Civilization or Brother’s in Arms (as well as to create entertaining games),is not to create history as it can be represented in a book but as it can be represented in a videogame. Analysis on the basis of content alone, almost invariably involves comparisons with historical narratives constructed and received in book form, which is often troublingly understood as the only form capable of producing ‘proper’ history. Most often these narratives are used as the benchmark for establishing truth or accuracy and thus, the examination of content. Such comparisons are also based on a confusion between the evidence of the past and the history that is written about it. The evidence of the past is often unavailable for reconsideration and rarely stands independent of (most often, narrative) interpretation and as such, these written interpretations are taken to be history (or more accurately, the past) itself, rather than history as it can be written, which naturally cannot be bluntly compared to history as it can be played. As Rosenstone (2006) repeatedly outlines, expecting history on film to be that of the book, merely transposed to an inferior form, is intensely problematic. Instead, history on film must be considered on its own terms. Aware of this type of flawed analysis, we are now presented with an opportunity to avoid the same mistakes that were made in the consideration of historical film. We can only do this by approaching historical videogames on their own terms and this can only be achieved by beginning to build an understanding that privileges transferable understandings of form over fixed analysis of individual historical content.Games will likely never produce the same opportunities for discourse as a book, but then why should they? Analysis on the basis of content usually involve uncomfortable comparisons of this kind and can result in mistaken conclusions about the representational capability of the videogame as an historical form, rather than the limitations of and concerns surrounding, histories which can be interpreted as ‘popular’ or ‘commercial’. Each form utilizes different structures that, considered alongside one another as part of a larger trans-media meta-discourse, create much more interesting collaborative opportunities for establishing historical understanding than one or the other alone.Examining only content also necessarily involves asking questions about what is included or left out of a particular videogames representation. This is rarely a useful question beyond the basis of a general common sense. Historical videogames are, like all histories, mimetic cultural products. Naturally, this involves a productive and often creative, process of evidence selection and emplotment. Thus, as Carr notes, ‘criticising a simulation for being reductive is nonsensical… [endnote]… That would be like disparaging a map for not being life-size’ (Carr 2007, 234 & endnote 6). Selectivity and reductionism is a natural ‘flaw’ of history (and all representation). This is no different in those histories that are written in books. This point becomes even more explicit when we consider ‘simulation…is perhaps the best translation of the Greek mimesis’ (Genette 1983, 15).Analysis on the basis of content using a comparative method such as this often does not even produce particularly useful results. We can of course sometimes confidently highlight obvious anachronisms and misplaced objects but historical videogames mostly relinquish the telling of the experiences of specific historical agents and, favour instead typical historical environments, scenarios and experiences. Given this, in the majority of cases (particularly given the implied audience), how much is to be actually gained by knowing, for instance, that certain shoes were not genuinely available until the 1490’s rather than the 1470’s, or that a particular character, though historically typical, did not truly exist? Relatively little, compared to the ‘feel’ of a period, the life, colour, action and processes (with which the book can struggle) and which can be easily communicated in games. More than this, in games we can wilfully discover these things, often as an (inter)active part of them, configuring our experience. It is only by focusing on form that we can understand how the game can produce meaning in these, arguably, new ways, that neither book nor cinema can effectively utilize. Examination of a particular history has to involve an understanding of the form through which it operates as these aspects can never be seen to truly stand apart. History is not a ‘thing’ that can be understood as separate from the forms in which it is produced, received and argued.Historical videogames must be understood on their own terms, yet, still without relinquishing our understanding of the basic tenets of historical theory as they universally apply to history as a practice within any form (e.g. history is referential and representational). Admittedly, striking the balance between these concerns can be challenging. Accepting this challenge means considering historical videogames on a quite different basis, that does not completely exclude analysis of content and yet seeks to understand how the nature and the meanings produced by this are wholly dependent on the form of the text in both production and reception. Such an approach is more trying in the sense that content cannot be evaluated on only its own terms. Returning to Salen and Zimmerman’s schemas, historical content in games is a concern that balances unnervingly between rules, play and cultureand as such can only be understood by understanding the structures of the game through which it is created and disseminated. It is only by understanding the interplays between form and content that we can really gain any comprehension of the (often troubling) category we know as history, which is always anchored within the mediums in which it is created and received.Understanding this requires a new approach to historical videogames, one that involves analyzing the structures that produce meaning. Structures which create opportunities for players to negotiate meaning in the ways that we are familiar with from other more ‘passive’ media but also to actively configure their own historical experience. In short, this means continually returning to and refocusing on, the agency which the player wields and which allows a somewhat unique form of engagement with historical discourse. This also means understanding the aesthetics of historical description that are utilized in historical videogames. This almost always includes audio-visual design, semiotic structures with which we are (hopefully) somewhat familiar with from historical film. However, in the videogame, even this element largely depends on the rules of the game and the opportunities for player action that these create. A large part of the aesthetics of games such as Assassin’s Creed are actually algorithms, that though written logically are still subjective aesthetics that attempt to represent historical experience through producing signs to be read and responses to be acted upon. In short, in any historical videogame, the aesthetics of historical description also function at a ludic level, producing a form of ‘procedural rhetoric’ (Bogost 2007) that, depending on a particular games (or genres) structures can influence virtually all of the other historical signifiers through which the game produces meaning. Understanding of this can only be approached through a focus that privileges form rather than individual content.Having identified combinations of these audio-visual-ludic structures, we can then approach other games that operate similarly with an understanding of what opportunities for historical meaning-making they are likely to offer. This is transferable knowledge that is likely to remain so, even when faced with the historical games of the near-future.It is defining and understanding these structures and how they operate in games, including the whole raft of new aesthetics that this implies, which is the most important task facing historians or other scholars interested in historical videogames. When we look at one game’s content, we understand no more than that. If analysis of content is necessary, then surely it is better left to those scholars that specialize in the historical period that the game tries to represent? However, as scholars that wish to study historical videogames then our first concern must be the form that exerts influence over virtually every aspect of production and reception. And which, in its relation to the historian/developer’s choices, decides the content. When we look at the videogame form in this way we can, I hope, begin to create a cohesive understanding of how games represent the past and what structures create particular opportunities for players to explore, understand and interact with these representations.Of course advocating an approach and demonstrating it are different matters and as such my next post will be an analysis of the Assassin’s Creed seriesthat I hope displays that this focus on different concerns can take us back to the basics of what the game achieves or offers and accordingly, help us to think about the game in new ways.This approach is far from complete and I make this call in a collaborative spirit. If taken up, this will no doubt become a complex analytical approach to construct but one which will benefit our understanding of this new form of historical expression. I also realise that this post is probably in many regards ‘preaching to the converted’ and many of the excellent pieces on this site display an understanding of the importance of analysing the structures at play within videogames if we are to understand the medium as a historical form and therefore, games as history at all. In a sense, this call to privilege form over content is also a simple point. However, I do believe that it is one worth making explicitly if we are to further develop a cohesive approach to historical videogames. Here at the relatively early point in the mediums life we are well placed to together offer the beginnings of an understanding of how and what, videogames enable in terms of playfully engaging, configuring and experiencing, discourse about the past.ReferencesBogost, I. 2007. Persuasive games. Massachusetts: MIT Press.Carr, D. 2007. The Trouble with Civilization. In Videogame//player/text, ed. T. Krzywinska and B. Atkins. 222-236. Manchester: Manchester University Press.Firaxis Games. 2010. Sid Meier’s Civilization V. 2K Games.Gearbox Software. 2008. Brothers in Arms: Hell’s Highway. Ubisoft.Genette, G. 1990 [1983]. Narrative discourse revisited, trans. J.E. Lewin. New York: Cornell University Press.Rosenstone, R. 2006. History on Film/ Film on History. London: Pearson.Salen, K & Zimmerman, E. 2004. Rules of Play: Game Design Fundamentals. Massachusetts: MIT Press.Ubisoft Montreal. 2011. Assassin’s Creed: Revelations. Ubisoft.[Flickr Image courtesy of Man of Wax / Creative Commons Licensed]Collaboratively edited and authored, Play the Past is dedicated to thoughtfully exploring and discussing the intersection of cultural heritage (very broadly defined) and games/meaningful play (equally broadly defined). Play the Past contributors come from a wide variety of backgrounds, domains, perspectives, and motivations (for being interested in both games and cultural heritage) – a fact which is evident in the wide variety of topics we tackle in our posts.It is very important to note that Play the Past isn’t just about the intersection of cultural heritage and digital games, its also about non-digital games (boardgames, tabletop games, collectible card games, etc.), alternate reality games (ARGs), barely games (a term originally coined by Russel Davies – no, not the Doctor Who Russel Davies – and built upon by our very own Rob McDougall), and playful mechanics (or “gamifying” as its been recently called).We are also very interested in exploring the spectrum of approaches to games – from the more “philosophical” (as some might call it) games studies side of things, to the more practically applied serious games/meaningful play side of things (and just about everything betwixt and between).Inspired by ProfhackerCredit where credit is due. One of the most important inspirations for Play the Past comes from directly ProfHacker. You can see the fingerprints of ProfHacker all over Play the Past. From the way we do business behind the scenes to our commenting and community policy (which is pretty much shamelessly lifted verbatim from ProfHacker). This is no great surprise as Ethan Watrall (Play the Past’s editor) was in the first batch of ProfHacker writers. In this regard, we are extremely grateful to ProfHacker – and particularly ProfHacker’s two Editors: George H. Williams (@georgeonline on Twitter) and Jason B. Jones (@jbj on Twitter).Interested in Contributing?Interested in contributing to Play the Past (either on a regular basis or as a “one shot” guest author)? We’d love to hear from you. Drop us a line here and tell us a little bit about yourself and how you might contribute.We are committed to fostering an environment characterized by generosity, creativity, and (as corny as it might sound) kindness. Comments on this blog are an important part of creating that environment, and this comment policy aims to communicate our values to new readers and encourage comments that will build up the online community here.Thoughtful comments (even when–and often especially if–disagreeing) are encouraged and appreciated.No snark allowed (see David Denby on definition of snark). While snark certainly has its virtues, this blog provides a space for people to be inexperienced at something, or even wrong, to facilitate learning. That’s harder to do in the face of either persistent or “drive-by” snark.Play the Past should be a community built through regular contributions made by recognized–but not necessarily “real name”–contributors. Some commenters’ identities reveal their real names; other commenters use pseudonyms. Our online identities are built from our comments here and our presence–as commenters and authors–in other places on the web, in print, at conferences. Play the Past welcomes commenters–whether anonymous, psuedonymous, or publically identified–who are committed to creating a rich and respectful dialogue. We want commenters to be able to explore the complexities of Play the Past posts; we want commenters to inquire and debate; we want everyone to be able to learn from the conversation.Links & images are encouraged. Gratuitous linking back to your own site is discouraged. Links in the “website” field should point only to profile pages or to personal websites.Gravatars are strongly encouraged.Jeremy AntleyJeremy Antley is a writer/student/gamer who received his MA in History with a focus on the Russian Imperial period from the University of Kansas in 2007. While currently in the middle of researching the immigration of Russian Old Believers to Oregon in the mid-60′s for his doctoral ambitions, Jeremy also finds studying the life and culture of Russian peasants to be a fascinating topic. He looks at topics of digital culture, games and, of course, Russian history at his blog, Peasant Muse. Those interested in hearing the twitter ramblings of someone crazy enough to love Russian peasants would do well to check-out Jeremy’s handle- @jsantley. Having played many epic games of Axis & Allies as an undergraduate, Jeremy now plays a variety of board and console (xbox 360) games with his friends in his current residence of Portland, Oregon. Jeremy’s love of board games informs his current interest in how players modify their games and how looking at board games as platforms can inform historical inquiries.Kevin BallestriniKevin Ballestrini teaches Latin and Mythology at the Norwich Free Academy in Connecticut. He has received an M.A. and B.A. in Classics from the University of Colorado and University of Connecticut respectively. In addition to experience teaching in a traditional classroom setting, Kevin is deploying the first fully practomimetic introductory language course at the high school level this year in a section of Latin I. He hopes that the experience will enhance student engagement and connection to life and culture in ancient Rome. As an avid technology enthusiast, he maintains his blog, Techna Virumque Cano (http://kevinbal.blogspot.com) where he discusses the intersection of technology and his teaching. Kevin is also the leader of a Lord of the Rings Online kinship and has a great interest in exploring how games contribute to the development of (online) communities just as the bardic tradition contributed to the development of ancient communities. You can also find Kevin on Twitter at http://www.twitter.com/kballestrini.Emily Joy BembeneckEmily Bembeneck is a Ph.D. candidate at the University of Michigan in the Department of Classical Studies. She primarily works on narrative and character development in ancient epic and modern video games, both graphical and text-based. Other interests of hers include the image of the hero, Greek tragedy and social catharsis, cultural and individual identity through play, immersion, and game design, She teaches classes in Latin, Greek and Roman history, ancient war and entertainment, among other things related to the Classical world She is an active contributor at http://www.greywardens.com where she writes on the narrative structure of Bioware’s Dragon Age RPG franchise. Her current projects include working on images of Rome in ancient and modern culture, developing a Flash-based application that combines components of social play with narrative creation, and designing a Dragon Age module that explores Euripides’ Medea through post-primary narrative. When not being all academic and studious, she is likely either playing an elf in some virtual world or spending time with her two young sons. Find her online at Ada Play and on Twitter at http://www.twitter.com/adarel.Andrew D. DevenneyAndrew D. Devenney is currently a Visiting Assistant Professor of World History at Grand Valley State University in Allendale, Michigan. A life-long and avid gamer of both video and role-playing games, Andrew has found elements of game-based learning, new media, and the digital humanities unconsciously seeping into his classroom over the last few years. As such, he has begun to experiment relentlessly on his students with these new ideas, techniques, and shiny toys, and is having a grand time doing it. Sometime in the relatively near future, Andrew hopes to deploy a new course that will explore global history through the medium of gaming (which means the class may very well morph into a giant, heavily modded, credit-based game of Dungeons and Dragons). Andrew can be found online at http://andrewdevenney.net and on Twitter as @adevenney.Shawn GrahamShawn Graham is Assistant Professor of Digital Humanities in the Department of History at Carleton University in Ottawa, Canada. He’s been blogging as ‘The Electric Archaeologist‘ since 2007, documenting his interest & experiments in game-based learning, agent based modeling, and other aspects of digital media for archaeological teaching and research. On Twitter, he’s at http://www.twitter.com/electricarchaeo. He’s published amongst other things a number of agent-based simulations on aspects of the Roman world, and has explored using Civilization mods in his distance-education classrooms. He received his PhD in Archaeology from the University of Reading in 2002, where he was interested in complexity & evolving networks in antiquity (especially in Rome). When he’s not geeking out over the latest tech toys or things archaeological, he is chief cider maker at Coronation Hall Cider Mills and playing Wii games with his family. If only there was wiiCivilization…Matthew KirschenbaumMatthew Kirschenbaum is Associate Professor in the Department of English at the University of Maryland, Associate Director of the Maryland Institute for Technology in the Humanities (MITH, an applied thinktank for the digital humanities), and Director of Digital Cultures and Creativity, a new “living/learning” program in the Honors College. Kirschenbaum speaks and writes often on topics in the digital humanities and new media; his work has received coverage in the Atlantic, New York Times, National Public Radio, Wired, Boing Boing, Slashdot, and the Chronicle of Higher Education. He has been pushing cardboard counters around on hexagonal grids since his early teens. See http://www.mkirschenbaum.net for more.Katy MeyersKaty Meyers is a graduate student in the Department of Anthropology,with a concentration in mortuary archaeology, at Michigan StateUniversity. She is currently a fellow in both the Cultural Heritage Informatics Initiative and the Campus Archaeology Program. She is also an editor of GradHacker, and the game designer for an Ancient Egyptbased mod of Civilization V. When she somehow finds free time she loves playing first person shooter and adventure video games, and is amajor fan of Milton-Bradley board game classics. She’s interested inthe role of games for outreach and education in archaeology and cultural heritage, as well as the biases they create. Katy can befound online at her blog http://www.bonesdontlie.com and on Twitter at @bonesdonotlie.Jeremiah McCallJeremiah McCall has been teaching high school history for the past decade, mostly at Cincinnati Country Day School. His first professional love is high school teaching, especially designing instructional strategies that will engage and challenge his students to learn and grow. In addition to more conventional courses, Jeremiah also teaches senior elective on (tabletop) historical simulation design, and the intersection of serious games and contemporary global issues. Jeremiah’s primary training is in history with a PhD in ancient history from Ohio State University; he authored a book on the cavalry of the Roman Republic and is currently writing a historical biography of the Roman aristocrat M. Claudius Marcellus to be published by Pen and Sword press. He has recently completed a guidebook for teachers who wish to use simulation games in the history class. Titled Gaming the Past: Using Video Games to Teach Secondary History, the book was published by Routledge in June, 2011. As an extension of his teaching philosophy — that history is primarily the study and evaluation of competing interpretations of the past — McCall has conducted numerous classroom implementations of historical simulations as historical interpretations. He maintains the website gamingthepast.net, one of the primary sites devoted to the use of historical simulations in classroom teaching. He also plays far more video games — particularly RPGs and strategy — than you’d think he could find time for.Rob MacDougallRob MacDougall is Assistant Professor of History at the University of Western Ontario and Associate Director of UWO’s Centre for American Studies, where he teaches United States history, the history of technology, and digital history methods. His research centers on the history of communication–he has just written a book on the early days of the telephone and is beginning a new book on the circulation of bad ideas. He blogs sporadically at Old is the New New and is on the Twitter at http://twitter.com/robotnik. A life-long gamer, Rob is interested in the history of gaming and in using games and play to encourage more playful historical thinking. His own play is largely non-digital of late–tabletop RPGs with his gamer buddies and make-believe with his kids–but he looks forward to seeing how good computer games will have gotten by the time he gets tenure.Rebecca MirRebecca Mir is a museum educator, researcher, and writer at various institutions in New York City. She will receive an M.A. in Decorative Arts, Design History, and Material Culture from the Bard Graduate Center in May 2012. She received her Bachelor’s degree in Art History from Indiana University of Pennsylvania in 2010. She often thinks about how (and why) artifacts and cultures are (mis)represented in video games and how museums are using games to engage and educate audiences. Rebecca can be found on Twitter as @hellenophile.Trevor OwensTrevor Owens is a digital archivist with the National Digital Information Infrastructure and Preservation Program (NDIIP) at the Library of Congress and a doctoral student in the Graduate School of Education at George Mason University. Before joining the Library of Congress he worked as the community lead for the Zotero project at the Center for History and New Media helped organize the first two meetings of the Games, Learning, and Society Conference. He received a bachelors degree in the history of science form the University of Wisconsin, and a masters degree in American History with and emphasis on digital history from George Mason University. Trevor has spent considerable amounts of time playing all iterations of Civilization, but is also a big fan of role playing games (everything from Earthbound to Fallout). Trevor has published on the history of children’s books about Einstein and Curie, the discursive practices of Civ Modders, and the role of digital research tools in scholarship and teaching. When not researching, writing, or gaming, he also enjoys playing the violin. Trevor can be found online at http://www.trevorowens.org and on Twitter at http://www.twitter.com/tjowensRoger TravisRoger Travis is an Associate Professor of Classics in the Department of Literatures, Cultures & Languages of the University of Connecticut. He is also the Director of the Video Games and Human Values Initiative at UConn, an interdisciplinary online nexus for scholarly activities like monthly symposia and “playversations.” He received his Bachelor’s degree in classics from Harvard College, and his Ph.D. in comparative literature from the University of California, Berkeley before arriving at UConn in 1997. He has published on Homeric epic, Greek tragedy, Greek historiography, the 19th C. British novel, HALO, and the massively-multiplayer online role-playing game He has been President of the Classical Association of New England and of the Classical Association of Connecticut. He writes the blog Living Epic about the fundamental connection between ancient epic and the narrative video game, and is a founder and contributor of the collaborative blog Play the Past. In the 2009-2010 academic year, Roger offered the first courses ever designed entirely as practomimes.Mark SampleMark Sample is an Assistant Professor of contemporary literature and new media studies in the Department of English at George Mason University. In addition to his work on electronic literature, videogames, and code studies, Mark is an outspoken advocate of open source pedagogy and open source research. In recognition of his commitment to innovation in teaching, Mark was the recipient of George Mason’s 2010 Teaching Excellence Award. Mark is a regular contributor to ProfHacker, and can also be found online at samplereality.com or on Twitter as @samplereality.Ethan WatrallEthan Watrall is an Assistant Professor in the Department of Anthropology and Associate Director of Matrix:The Center for Humane Arts, Letters & Social Sciences Online at Michigan State University. In addition, Ethan is a Principal Investigator in the Games for Entertainment & Learning Lab, and co- founder of both the undergraduate Specialization and Game Design Development and the MA in Serious Game Design at Michigan State University. Ethan teaches (and has taught) in a wide variety of areas including cultural heritage informatics, ancient Egyptian social history & archaeology, archaeology and pop culture, user centered & user experience design, game design, serious game design, game studies. and history of various forms of popular and entertainment media (comics and digital games to name a few). When he’s not being all professorial, he’s a world class comic book nerd (Killowog is so his favorite Green Lantern), a sci-fi dork (he’ll argue to the grave that Tom Baker is the best Doctor ever), and an avid player of all sorts of games (digital, board, and tabletop). Ethan can be found online at http://www.captainprimate.com, and on Twitter at http://www.twitter.com/captain_primateLecture at CERN, Geneva, Switzerland, 18 April 2011: A new talk about open access to academic or scientific information, with a bit of commentary about YouTube Copyright School. ;2012 Ahn, J., Plaisant, C., Shneiderman, B. (June 2012)A Task Taxonomy for Network Evolution Analysis HCIL-2012-13 [Link to Report] Sopan, A. (May 2012)Monitoring Scientific Conference: Real-time Visualization and Retrospective Analysis of the Backchannel Conversation HCIL-2012-12 [Link to Report] Dunne, C., Shneiderman, B. (May 2012)Motif Simplification: Improving Network Visualization Readability with Fan and Parallel Glyphs HCIL-2012-11 [Link to Report] Liu, R., Chao, T., Plaisant, C., Shneiderman, B. (April 2012)ManyLists : Product Comparison Tool Using Spatial Layouts with Animated Transitions HCIL-2012-10 [Link to Report] Pantazos, K., Tarkan, S., Plaisant, C., Shneiderman, B. (April 2012)Promoting Timely Completion of Multi-Step Processes - A Visual Approach to Retrospective Analysis HCIL-2012-09 [Video] [Link to Report] Wongsuphasawat, K. (April 2012)Interactive Exploration of Temporal Event Sequences Ph.D Dissertation from the Department of Computer Science HCIL-2012-08 [Link to Report] Tao, C., Wongsuphasawat, K., Clark, K., Plaisant, C., Chute, C. (April 2012)Towards Event Sequence Representation, Reasoning and Visualization for HER Data Published in: Proc. 2nd ACM International Health Informatics Symposium, ACM Press, New York (2012), 801-805. HCIL-2012-07 [Link to Report] Monroe, M., Wongsuphasawat, K., Plaisant, C., Shneiderman, B., Millstein, J., Gold, S. (April 2012)Exploring Point and Interval Event Patterns: Display Methods and Interactive Visual Query HCIL-2012-06 [Video] [Link to Report] Cheng, H., Plaisant, C., Shneiderman, B. (April 2012)Identifying and Measuring Associations of Temporal Events HCIL-2012-05 [Video] [Link to Report] Guerra Gómez, J., Buck-Coleman, A., Plaisant, C., Shneiderman, B. (April 2012)TreeVersity: Interactive Visualizations for Comparing Two Trees with Structure and Node Value Changes HCIL-2012-04 [Link to Report] Chao, T., Plaisant, C., Shneiderman, B. (January 2012)Twinlist: Overview and general implementation description HCIL-2012-03 [Link to Report] Sopan, A., Rey, P., Ahn, J., Plaisant, C., Shneiderman, B. (February 2012)The Dynamics of Web-Based Community Safety Groups: Lessons Learned from the Nation of Neighbors HCIL-2012-02 [Link to Report] Hara, K., Hajiaghayi, M., Bederson, B. (January 2012)FluTCHA: Using Fluency to Distinguish Humans from Computers HCIL-2012-01 [Link to Report]2011 Chao, T. (November 2011)Visual techniques for medical reconciliation: spatial metaphor, animated explanation, and flexible decision-making HCIL-2011-33 [Link to Report] Sopan, A. (December 2011)Application of ManyNets to Analyze Dynamic Networks and Compare Online Communities: A case study with Nation of Neighbors HCIL-2011-32 [Link to Report] Lam, H., Bertini, E., Isenberg, P., Plaisant, C., Carpendale, S. (December 2011)Empirical Studies in Information Visualization: Seven Scenarios Published in: IEEE Transactions on Visualization and Computer Graphics, 30 Nov. 2011. IEEE computer Society Digital Library. IEEE Computer Society, http://doi.ieeecomputersociety.org/10.1109/TVCG.2011.279 [Published Version] HCIL-2011-31 [Link to Report] Guerra Gómez, J., Shneiderman, B. (December 2011)Understanding social relationships from photo collection tags HCIL-2011-30 [Link to Report] Guerra Gómez, J. (December 2011)MySocialTree: Browsing the Facebook Feed using hierarchies HCIL-2011-29 [Link to Report] Rotman, D., Preece, J., Hammock, J., Procita, K., Hansen, D., Parr, C., Lewis, D., Jacobs, D. (November 2011)Dynamic Changes in Motivation in Collaborative Citizen-Science Projects In Proc. CSCW 2012. February 11-15, 2012, Seattle, Washington. HCIL-2011-28 [Link to Report] Norman, K. (November 2011)Assessing the Components of Skill Necessary for Playing Video Games HCIL-2011-27 [Link to Report] Quinn, A., Bederson, B. (November 2011)Appsheet: Efficient use of web workers to support decision making HCIL-2011-26 [Link to Report] Hu, C., Resnik, P., Kronrod, Y., Bederson, B. (September 2011)Deploying MonoTrans Widgets in the Wild To appear in Proceedings of CHI 2012. HCIL-2011-25 [Link to Report] Rodrigues, E., Milic-Frayling, N., Smith, M., Shneiderman, B., Hansen, D. (October 2011)Group-In-a-Box Layout for Multi-faceted Analysis of Communities Published in Proc. IEEE Conference on Social Computing, IEEE Press, Piscataway, NJ (October 2011). HCIL-2011-24 [Link to Report] Foss, E., Hutchinson, H., Druin, A., Brewer, R., Lo, P., Sanchez, L., Golub, E.Childrens Search Roles at Home: Implications for Designers, Researchers, Educators, and Parents HCIL-2011-23 [Link to Report] Guerra Gómez, J., Buck-Coleman, A., Plaisant, C., Shneiderman, B. (October 2011)Interactive Visualizations for Comparing Two Trees With Structure and Node Value Changes HCIL-2011-22 [Link to Report] Yeh, T., Wongsuphasawat, K., Shneiderman, B., Davis, L. (September 2011)Making GUIs Narcissistic: Toolkit for Managing Space and Occlusion by Visual Introspection HCIL-2011-21 [Link to Report] Claudino, L., Khamis, S., Liu, R., London, B., Pujara, J., Plaisant, C., Shneiderman, B. (September 2011)Facilitating Medication Reconciliation with Animation and Spatial layout To appear in Proceedings of the Workshop on Interactive Healthcare Systems (WISH2011). HCIL-2011-20 [Link to Report] Rotman, D., Procita, K., Hansen, D., Parr, C., Preece, J. (July 2011)Supporting Content Curation Communities: The Case of the Encyclopedia of Life HCIL-2011-19 [Link to Report] Bonsignore, E., Hansen, D., Kraus, K., Ruppel, M. (August 2011)Alternate Reality Games as a Platform for Practicing 21st Century Literacies HCIL-2011-18 [Link to Report] Hansen, D., Jacobs, D., Lewis, D., Biswas, A., Rotman, D., Preece, J. (July 2011)Odd Leaf Out: Improving visual recognition with games HCIL-2011-17 [Link to Report] Dunne, C., Shneiderman, B., Gove, R., Klavans, J., Dorr, B. (July 2011)Rapid Understanding of Scientific Paper Collections: Integrating Statistics, Text Analytics, and Visualization To appear in JASIST: Journal of the American Society for Information Science and Technology. HCIL-2011-16 [Link to Report] Tarkan, S., Plaisant, C., Shneiderman, B., Hettinger, A. (July 2011)Improving Timely Clinical Lab Test Result Management: A Generative XML Process Model to Support Medical Care HCIL-2011-15 [Link to Report] Khurana, U., Nguyen, V., Cheng, H., Ahn, J., Chen, X., Shneiderman, B. (June 2011)Visual Analysis of Temporal Trends in Social Networks Using Edge Color Coding and Metric Timelines To appear in Proc. IEEE Conference on Social Computing 2011 (October 2011, Boston, MA), IEEE Press, Piscataway, NJ. HCIL-2011-14 [Link to Report] Violi, N., Shneiderman, B., Hanson, A., Rey, P. (June 2011)Motivation for Participation in Online Neighborhood Watch Communities: An Empirical Study Involving Invitation Letters To appear in Proc. IEEE Conference on Social Computing 2011 (October 2011, Boston, MA), IEEE Press, Piscataway, NJ. HCIL-2011-13 [Link to Report] Gove, R., Gramsky, N., Kirby, R., Sefer, E., Sopan, A., Dunne, C., Shneiderman, B., Taieb-Maimon, M. (June 2011)NetVisia: Heat Map and Matrix Visualization of Dynamic Social Network Statistics and Content In SocialCom '11:Proc. IEEE 3rd International Conference on Social Computing (October 2011), 19-26. [Published Version] HCIL-2011-12 [Link to Report] Gove, R. (May 2011)Understanding Scientific Literature Networks: Case Study Evaluations of Integrating Visualizations and Statistics M.S. Thesis from the Department of Computer Scienc HCIL-2011-11 [Link to Report] Walsh, G., Brown, Q., Druin, A., Amos, C. (April 2011)Social Networking as a Vehicle to Foster Cross-Cultural Awareness To appear in IDC 2011. HCIL-2011-10 [Link to Report] Ahn, J., Plaisant, C., Shneiderman, B. (April 2011)A Task Taxonomy of Network Evolution Analysis HCIL-2011-09 [Link to Report] Markowitz, E., Bernstam, E., Herskovic, J., Zhang, J., Shneiderman, B., Plaisant, C., Johnson, T. (April 2011)Medication Reconciliation: Work Domain Ontology, Prototype Development, and a Predictive Model Submitted to AMIA 2011 under review. HCIL-2011-07 [Link to Report] Tarkan, S., Plaisant, C., Shneiderman, B., Hettinger, A. (March 2011)Reducing Missed Laboratory Results: Defining Temporal Responsibility, Generating User Interfaces for Test Process Tracking, and Retrospective Analyses to Identify Problems Submitted to AMIA 2011 under review HCIL-2011-06 [Link to Report] Shneiderman, B. (March 2011)Foreword: The Expanding Impact of Human-Computer Interaction Foreword for Jacko, J. (Editor), Handbook of Human-Computer Interaction: 3rd Edition, Taylor & Francis (to appear 2012). HCIL-2011-05 [Link to Report] Guha, M., Druin, A., Fails, J. (Feburary 2011)How Children Can Design the Future Published at HCII 2011. HCIL-2011-04 [Link to Report] Shneiderman, B. (March 2011)Technology-Mediated Social Participation: The Next 25 Years of HCI Challenges Keynote: Proc. HCI International Conference, Springer (to appear July 2011). HCIL-2011-03 [Link to Report] Gove, R., Dunne, C., Shneiderman, B., Klavans, J., Dorr, B. (January 2011)Evaluating Visual and Statistical Exploration of Scientific Literature Networks In VL/HCC'11:Proc. 2011 IEEE Symposium on Visual Languages and Human-Centric Computing (2011), 217-224. [Published Version] HCIL-2011-02 [Link to Report] Wang, T., Wongsuphasawat, K., Plaisant, C., Shneiderman, B. (January 2011)Extracting Insights from Electronic Health Records: Case Studies, a Visual Analytics Process Model, and Design Recommendations Published in Journal of Medical Systems (2011) PMID 21541691. [Published Version] HCIL-2011-01 [Link to Report]2010 Norman, K. (December 2010)Development of Instruments to Measure ImerseAbility of Individuals and ImmersiveNess of Video Games HCIL-2010-33 [Link to Report] Ho, P., Wang, T., Wongsuphasawat, K., Plaisant, C., Shneiderman, B., Smith, M., Roseman, D. (December 2010)Monitoring and Improving Quality of Care with Interactive Exploration of Temporal Patterns in Electronic Health Records HCIL-2010-32 [Link to Report] Bonsignore, E. (February 2010)The Use of StoryKit: Design Implications for Intergenerational Mobile Storytelling HCIL-2010-31 [Link to Report] Golbeck, J., Robles, C., Turner, K. (November 2010)Predicting Personality with Social Media HCIL-2010-30 [Link to Report] Guha, M. (July 2010)Understanding the Social and Cognitive Experiences of Children Involved in Technology Design Processes HCIL-2010-29 [Link to Report] Ahn, J., Taieb-Maimon, M., Sopan, A., Plaisant, C., Shneiderman, B. (November 2010)Temporal Visualization of Social Network Dynamics: Prototypes for Nation of Neighbors Proc. Of Social Computing, Behavioral-Cultural Modeling and Prediction conference, pp. 309-316, 2011. [Published Version] HCIL-2010-28 [Link to Report] Bederson, B., Quinn, A. (November 2010)Web Workers Unite, Addressing Challenges of Online Laborers HCIL-2010-27 [Link to Report] Quinn, A., Bederson, B. (November 2010)Human Computation, Charting The Growth Of A Burgeoning Field HCIL-2010-26 [Link to Report] Norman, K., Norman, K. (November 2010)Comparison of Relative Versus Absolute Pointing Devices Submitted to the UMD DRUM system as Tech Report LAPDP2010TR04. HCIL-2010-25 [Link to Report] Sharma, P., Khurana, U., Scharrenbroich, M., Locke, J. (June 2010)Speeding up Network Layout and Centrality Measures with NodeXL and the Nvidia CUDA Technology HCIL-2010-24 [Link to Report] Elsayed, T., Ture, F., Lin, J. (October 2010)Brute-Force Approaches to Batch Retrieval: Scalable Indexing with MapReduce, or Why Bother? HCIL-2010-23 [Link to Report] Wongsuphasawat, K., Guerra Gómez, J., Plaisant, C., Wang, T., Taieb-Maimon, M., Shneiderman, B. (September 2010)LifeFlow: Visualizing an Overview of Event Sequences In Proceedings of the 2011 annual conference on Human factors in computing systems (CHI '11). ACM, New York, NY, USA, 1747-1756. DOI=10.1145/1978942.1979196 [Published Version] HCIL-2010-22 [Link to Report] Hu, C., Bederson, B., Resnik, P. (September 2010)MonoTrans2: An Asynchronous Human Computation System to Support Monolingual Translation HCIL-2010-21 [Link to Report] Golbeck, J., Hansen, D. (September 2010)Computing Political Preference among Twitter Followers HCIL-2010-20 [Link to Report] Rind, A., Wang, T., Aigner, W., Miksch, S., Wongsuphasawat, K., Plaisant, C., Shneiderman, B. (September 2010)Interactive Information Visualization for Exploring and Querying Electronic Health Records: A Systematic Review For a copy of the paper please contact plaisant@cs.umd.edu HCIL-2010-19 [Link to Report] Walsh, G. (May 2010)Developing DisCo: A distributed co-design, on-line tool Submitted to the College of Information Studies doctoral program in fulfillment of the integrative paper requirement for admission to candidacy at the University of Maryland. HCIL-2010-18 [Link to Report] Guerra Gómez, J., Wongsuphasawat, K., Wang, T., Pack, M., Plaisant, C. (August 2010)Analyzing Incident Management Event Sequences with Interactive Visualization In Proceedings of the Transportation Research Board 90th annual meeting, The National Academies, Washington, DC (2011) [Published Version] HCIL-2010-17 [Link to Report] Shneiderman, B., Dunne, C., Sharma, P., Wang, P. (August 2010)Innovation Trajectories for Information Visualizations: Comparing Treemaps, Cone Trees, and Hyperbolic Trees To appear in Information Visualization. HCIL-2010-16 [Link to Report] Wang, T. (May 2010)Interactive Visualization Techniques for Searching Temporal Categorical Data Ph.D Dissertation from the Department of Computer Science HCIL-2010-15 [Link to Report] Huang, D., Lin, J.Scaling Populations of a Genetic Algorithm for Job Shop Scheduling Problems using MapReduce HCIL-2010-14 [Link to Report] Hansen, D., Smith, M., Shneiderman, B.EventGraphs: Charting Collections of Conference Connections Accepted to the Social Networking and Communities Mini-Track of the Forty-Forth Annual Hawaii International Conference on System Sciences (HICSS), Jan 4-7, 2011. Kauai, Hawaii. HCIL-2010-13 [Link to Report] Wang, T., Wongsuphasawat, K., Plaisant, C., Shneiderman, B. (June 2010)Visual Information Seeking in Multiple Electronic Health Records: Design Recommendations and A Process Model Published in Proceedings of the 1st ACM International Informatics Symposium (IHI '10) (2010) 46-55. [Published Version] HCIL-2010-12 [Link to Report] Quinn, A., Bederson, B., Yeh, T., Lin, J. (May 2010)CrowdFlow: Integrating Machine Learning with Mechanical Turk for Speed-Cost-Quality Flexibility HCIL-2010-09 [Link to Report] Yeh, T., White, B., Davis, L., Katz, B. (May 2010)Searching the Web Using Screenshots HCIL-2010-08 [Link to Report] Resnik, P., Hu, C., Buzek, O., Bederson, B. (May 2010)Using Monolingual Human Computation to Improve Language Translation via Targeted Paraphrase Will appear in the proceedings of the 2010 Conference on Empirical Methods on Natural Language Processing (EMNLP 2010). HCIL-2010-07 [Link to Report] Rios-Berrios, M., Sharma, P., Lee, T., Schwartz, R., Shneiderman, B. (May 2010)TreeCovery : Coordinated Dual Treemap Visualization for Exploring the Recovery Act HCIL-2010-06 [Link to Report] Gregory, M., Shneiderman, B. (May 2010)Shape Identification in Temporal Data Sets HCIL-2010-05 [Link to Report] Sopan, A., Noh, A., Lee, G., Rosenfeld, P., Karol, S., Shneiderman, B. (May 2010)Community Health Map: A Geospatial and Multivariate Data Visualization Tool for Public Health Datasets HCIL-2010-04 [Link to Report] Guha, M., Druin, A., Fails, J. (April 2010) Investigating the Impact of Design Processes on Children In press, IDC 2010, Barcelona, Spain HCIL-2010-03 [Link to Report] Fails, J., Druin, A., Guha, M. (April 2010)Mobile Collaboration: Collaboratively Reading and Creating Childrens Stories on Mobile Devices In press, IDC 2010, Barcelona, Spain HCIL-2010-02 [Link to Report] Sopan, A., Freire, M., Taieb-Maimon, M., Golbeck, J., Shneiderman, B., Shneiderman, B. (April 2010)Exploring Data Distributions: Visual Design and Evaluation Published in International Journal of Human-Computer Interaction (Volume 28, Issue 7, 2012). [Published Version] HCIL-2010-01 [Link to Report]2009 Filippova, D., Shneiderman, B. (January 2009)Interactive Exploration of Multivariate Categorical Data: Exploiting Ranked Criteria to Reveal Patterns and Outliers HCIL-2009-38 [Link to Report] Kang, H., Kang, H., Plaisant, C., Plaisant, C., Elsayed, T., Elsayed, T., Oard, D., Oard, D. (December 2009)Making Sense of Archived E-mail: Exploring the Enron Collection with NetLens Published in Journal of the American Society for Information Science and Technology, 61 , 4 (2010) 723-744. [Published Version] HCIL-2009-37 [Link to Report] Hansen, D.Overhearing the Crowd: An Empirical Examination of Conversation Reuse in a Technical Support Community Published in Proceedings of the Fourth International Conference on Communities and Technologies (University Park, PA, USA, June 25 - 27, 2009). C&T '09. ACM, New York, NY, 155-164. [Published Version] HCIL-2009-36 [Link to Report] Wang, T., Wongsuphasawat, K., Plaisant, C., Shneiderman, B. (June 2010)Exploratory Search Over Temporal Event Sequences: Novel Requirements, Operations, and a Process Model in Proc. Of Third Workshop on Human-Computer Interaction and Information Retrieval (HCIR 2009). HCIL-2009-35 [Link to Report] Fails, J. (August 2009)MOBILE COLLABORATION FOR YOUNG CHILDREN: READING AND CREATING STORIES HCIL-2009-34 [Link to Report] Druin, A., Foss, E., Hutchinson, H., Golub, E., Hatley, L. (December 2009)Children's Roles using Keyword Search Interfaces at Home New York Times Article: [Published Version] Published in ACM CHI 2010 Conference on Human Factors in Computing Systems HCIL-2009-33 [Link to Report] Golbeck, J., Grimes, J., Rogers, A. (December 2009)Twitter Use by the U.S. Congress HCIL-2009-32 [Link to Report] Zalinger, J., Freier, N., Freire, M., Shneiderman, B.Reading Ben Shneidermans Email: Identifying Narrative Elements in Email Archives HCIL-2009-31 [Link to Report] Freire, M., Plaisant, C., Shneiderman, B., Golbeck, J. (September 2009)ManyNets: An Interface for Multiple Network Analysis and Visualization In Proceedings of the 28th international conference on Human factors in computing systems (CHI '10). ACM, New York (2010) 213-222 [Published Version] HCIL-2009-30 [Link to Report] Walsh, G., Druin, A., Guha, M., Foss, E., Golub, E., Hatley, L., Bonsignore, E., Franckel, S. (October 2009)Layered Elaboration: A New Technique for Co-Design with Children HCIL-2009-29 [Link to Report] Bederson, B., Hu, C., Resnik, P. (October 2009)Translation by Iterative Collaboration between Monolingual Users Published as: Bederson, B.B., Hu, C., & Resnik, P. (2010) Translation by Iteractive Collaboration between Monolingual Users, Proceedings of Graphics Interface (GI 2010), 39-46. HCIL-2009-28 [Link to Report] Golbeck, J., Hu, C. (October 2009)Impact of Visualization Methods on Interaction with Search Results HCIL-2009-27 [Link to Report] Golbeck, J. (October 2009)A Single Strong Disagreement Ruins a Recommender: Improving Recommendation Accuracy with a Simple Statistic HCIL-2009-26 [Link to Report] Walsh, G., Golbeck, J. (October 2009)Curator: A Game with a Purpose for Collection Recommendation HCIL-2009-25 [Link to Report] Golbeck, J. (October 2009)The More People I Meet, The More I Like My Dog: A Study of Pet-Oriented Social Networks on the Web HCIL-2009-24 [Link to Report] Quinn, A., Bederson, B. (October 2009)A Taxonomy of Distributed Human Computation A more recent version of this report is available as HCIL-2010-26. HCIL-2009-23 [Link to Report] Quinn, A., Bederson, B., Bonsignore, E., Druin, A. (October 2009)StoryKit: Designing a Mobile Application for Story Creation By Children And Older Adults HCIL-2009-22 [Link to Report] Bederson, B. (October 2009)The Promise of Zoomable User Interfaces HCIL-2009-21 [Link to Report] Wongsuphasawat, K., Plaisant, C., Taieb-Maimon, M., Shneiderman, B. (October 2009)Querying Event Sequences by Exact Match or Similarity Search: Design and Empirical Evaluation HCIL-2009-20 [Link to Report] Bonsignore, E., Dunne, C., Rotman, D., Smith, M., Capone, T., Hansen, D., Shneiderman, B. (August 2009)First steps to NetViz Nirvana: Evaluating social network analysis with NodeXL In SIN '09: Proc. International Symposium on Social Intelligence and Networking. IEEE Computer Society Press. HCIL-2009-19 [Link to Report] Shneiderman, B. (July 2009)iParticipate.gov: A National Initiative for Social ParticipationScience 323 (March 13, 2009), 1426-1427. [Published Version] HCIL-2009-18 [Link to Report] Hansen, D., Rotman, D., Bonsignore, E., Milic-Frayling, N., Rodrigues, E., Smith, M., Shneiderman, B. (September 2009)Do You Know the Way to SNA?: A Process Model for Analyzing and Visualizing Social Media Data HCIL-2009-17 [Link to Report] Bederson, B., Quinn, A., Druin, A. (May 2009)Designing the Reading Experience for Scanned Multi-lingual Picture Books on Mobile Phones HCIL-2009-16 [Link to Report] Druin, A., Bederson, B., Quinn, A. (May 2009)Designing Intergenerational Mobile Storytelling HCIL-2009-15 [Link to Report] Wang, T., Deshpande, A., Shneiderman, B. (May 2009)A Temporal Pattern Search Algorithm for Personal History Event Visualization Wang, T.; Deshpande, A.; Shneiderman, B.; , "A Temporal Pattern Search Algorithm for Personal History Event Visualization," Published in Knowledge and Data Engineering, IEEE Transactions on, vol.PP, no.99, pp.1, 0. doi: 10.1109/TKDE.2010.257 [Published Version] HCIL-2009-14 [Link to Report] Dunne, C., Shneiderman, B. (May 2009)Improving Graph Drawing Readability by Incorporating Readability Metrics: A Software Tool for Network Analysts HCIL-2009-13 [Link to Report] Vuillemot, R., Clement, T., Plaisant, C., Kumar, A. (April 2009)Whats Being Said Near "Martha"? Exploring Name Entities in Literary Text Collections Vuillemot, R.; Clement, T.; Plaisant, C.; Kumar, A.; , "What's being said near "Martha"? Exploring name entities in literary text collections," IEEE Symposium on Visual Analytics Science and Technology, VAST 2009 (2009) 107-114. doi: 10.1109/VAST.2009.5333248 [Published Version] HCIL-2009-12 [Link to Report] Smith, M., Shneiderman, B., Milic-Frayling, N., Rodrigues, E., Barash, V., Dunne, C., Capone, T., Perer, A., Gleave, E. (April 2009)Analyzing (Social Media) Networks with NodeXLProc. Communities & Technologies Conference, Springer (June 2009). HCIL-2009-11 [Link to Report] Costello, L., Grinstein, G., Plaisant, C., Scholtz, J. (April 2009)Advancing User-Centered Evaluation of Visual Analytic Environments through ContestsInformation Visualization, 8 (2009) 230-238 [Published Version] HCIL-2009-10 [Link to Report] Wang, T., Plaisant, C., Shneiderman, B., Spring, N., Roseman, D., Marchand, G., Mukherjee, V., Smith, M. (April 2009)Temporal Summaries: Supporting Temporal Categorical Searching, Aggregation and Comparison Published in IEEE Transactions on Visualization and Computer Graphics (2009) 1049-1056 [Published Version] HCIL-2009-09 [Link to Report] Wongsuphasawat, K., Shneiderman, B. (April 2009)Finding Comparable Temporal Categorical Records: A Similarity Measure with an Interactive Visualization Is to appear in Proceedings of IEEE VAST 2009. HCIL-2009-08 [Link to Report] Hu, C., Rose, A., Bederson, B. (March 2009)Locating Text in Scanned Books Published as: Hu, C., Rose, A., Bederson, B.B.* (2009) Locating Text in Scanned Books. In Proceedings of the Joint Conference on Digital Libraries (JCDL 2009), Poster. HCIL-2009-07 [Link to Report] Bederson, B., Quinn, A., Druin, A. (March 2009)Designing the Reading Experience for Scanned Multi-lingual Picture Books on Mobile Phones Published as: Bederson, B.B., Quinn, A., Druin, A. (2009) Designing the Reading Experience for Scanned Multi-lingual Picture Books on Mobile Phones. In Proceedings of the Joint Conference on Digital Libraries (JCDL 2009), Short Paper, ACM Press, New York, NY, 305-308. HCIL-2009-06 [Link to Report] Chen, R., Rose, A., Bederson, B. (March 2009)How People Read Books Online: Mining and Visualizing Web Logs for Use Information HCIL-2009-05 [Link to Report] Druin, A., Foss, E., Hatley, L., Golub, E., Guha, M., Fails, J., Hutchinson, H. (February 2009)How Children Search the Internet with Keyword Interfaces HCIL-2009-04 [Link to Report] Tarkan, S., Sazawal, V., Druin, A., Foss, E., Golub, E., Hatley, L., Khatri, T., Massey, S., Walsh, G., Torres, G. (January 2009)Designing a Novice Programming Environment with Children HCIL-2009-03 [Link to Report] Druin, A., Bederson, B., Rose, A., Weeks, A. (January 2009)From New Zealand to Mongolia: Co-Designing and Deploying a Digital Library for the Worlds Children This article in currently "In Press" and will be published in a special issue of: Children, Youth and Environments (http://www.colorado.edu/journals/cye/): Children in Technological Environments: Interaction, Development, and Design, Editors: N.G. Freier & P. H. Kahn HCIL-2009-02 [Link to Report] Lin, J., Bahety, A., Konda, S., Mahindrakar, S. (January 2009)Low-Latency, High-Throughput Access to Static Global Resources within the Hadoop Framework HCIL-2009-01 [Link to Report]2008 Shneiderman, B. (July 2009) Copernican challenges face those who suggest that collaboration, not computation are the driving energy for socio-technical systems that characterize Web 2.0.Science 319 (March 7, 2008), 1349-1350. [Published Version] HCIL-2008-42 [Link to Report] Golbeck, J. (December 2008)Weaving a Web of TrustScience, Vol 321, September 19, 2008. HCIL-2008-41 [Link to Report] Golbeck, J., Rothstein, M. (December 2008)Linking Social Networks on the Web with FOAF HCIL-2008-40 [Link to Report] Golbeck, J. (December 2008)Trust and Nuanced Profile Similarity in Online Social Networks HCIL-2008-39 [Link to Report] Hendler, J., Golbeck, J. (December 2008)Metcalfe's Law, Web 2.0, and the Semantic Web HCIL-2008-38 [Link to Report] Golbeck, J., Halaschek-Wiener, C. (December 2008)Trust-Based Revision for Expressive Web Syndication HCIL-2008-37 [Link to Report] Golbeck, J. (December 200)The Dynamics ofWeb-based Social Networks: Membership, Relationships, and Change HCIL-2008-36 [Link to Report] Aris, A., Shneiderman, B., Qazvinian, V., Radev, D. (December 2008)Visual Overviews for Discovering Key Papers and Influences Across Research Fronts Published in Journal of the American Society for Information Systems and Technology 60, 11 (November 2009), 2219-2228. HCIL-2008-35 [Link to Report] Grinstein, G., Plaisant, C., Laskowski, S., O'Connell, T., Scholtz, J., Whiting, M. (November 2008)VAST 2008 Challenge: Introducing mini-challenges VAST 2008 Challenge: Introducing mini-challenges, VAST '08. IEEE Symposium on Visual Analytics Science and Technology (2008)195-196 [Published Version] HCIL-2008-34 [Link to Report] Clement, T., Plaisant, C., Vuillemot, R. (November 2008)The Story of One: Humanity scholarship with visualization and text analysis in Proc. Of the Digital Humanities Conference (DH 2009) HCIL-2008-33 [Link to Report] Jong, C., Rajkumar, P., Siddiquie, B., Clement, T., Plaisant, C., Shneiderman, B. (November 2008)Interactive Exploration of Versions across Multiple Documents to appear in Proc. of the Digital Humanities Conference (DH 2009) HCIL-2008-32 [Link to Report] Hutchinson, H., Druin, A., Bederson, B. (November 2008)Supporting Elementary-Age Childrens Searching and Browsing: Design and Evaluation Using the International Childrens Digital Library Published as: Hutchinson, H., Bederson, B.B., Druin, A., (2007) Supporting Elementary-Age Children's Searching and Browsing: Design and Evaluation Using the International Children's Digital Library, Journal of the American Society for Information Science and Technology, John Wiley & Sons, 58 (11), 1618-1630. HCIL-2008-31 [Link to Report] Lieberman, M., Taheri, S., Guo, H., Mir-Rashed, F., Yahav, I., Aris, A., Shneiderman, B. (October 2008)Visual Exploration Across Biomedical Databases Published in IEEE/ACM Transactions on Computational Biology and Bioinformatics (March/April 2011). [Published Version] HCIL-2008-30 [Link to Report] Aris, A. (August 2008)Visualizing and Exploring Networks Using Semantic Substrates Ph.D. Dissertation from the Department of Computer Science HCIL-2008-29 [Link to Report] Lin, J. (July 2008)Scalable Language Processing Algorithms for the Masses: A Case Study in Computing Word Co-occurrence Matrices with MapReduceProceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP 2008), pages 419-428, October 2008, Honolulu, Hawaii. [Published Version] HCIL-2008-28 [Link to Report] Shneiderman, B. (June 2008)Extreme Visualization: Squeezing a Billion Records into a Million PixelsProc. ACM SIGMOD 2008 Conference, ACM, New York (June 2008). HCIL-2008-27 [Link to Report] Filippova, D., Lee, J., Olea, A., VanDaniker, M., Wongsuphasawat, K. (May 2008)Exploring Clusters in Geospatial Datasets HCIL-2008-26 [Link to Report] Parr, C. (May 2008)Ecological Informatics InternetJorgensen, S.E., ed. Encyclopedia of Ecology. Oxford, UK: Elsevier. HCIL-2008-25 [Link to Report] Parr, C. (May 2008)Open Sourcing Ecological DataBioScience, 4, 309-310. HCIL-2008-24 [Link to Report] Parafiynyk, A., Parr, C., Sachs, J., Finin, T. (May 2008)Adding Semantics to Social Websites for Citizen Science Proceedings of the AAAI 2007 Semantic e-Science workshop., Vancouver, Canada. HCIL-2008-23 [Link to Report] Lee, B., Robertson, G., Czerwinski, M., Parr, C. (May 2008)CandidTree: Visualizing Structural Uncertainty in Similar HierarchiesInformation Visualization, 6, 233-246.Proceedings of Interact 2007, Lecture Notes in Computer Science 4663, 250-263 HCIL-2008-21 [Link to Report] Guha, M., Druin, A., Fails, J. (May 2008)Designing with and for children with special needs: An inclusionary modelTo appear in Interaction Design and Children, June 2008. HCIL-2008-20 [Link to Report] Liao, C., Guimbretière, F., Loeckenhoff, C. (May 2008)Pen-top Feedback for Paper-based InterfacesProceedings of ACM UIST06, pp. 291 -220. [Published Version] HCIL-2008-19 [Link to Report] Liao, C., Guimbretière, F., Anderson, R., Linnell, N., Prince, C., Razmov, V. (May 2008)PaperCP: Exploring the Integration of Physical and Digital Affordances for Active LearningProceedings of INTERACT 07, pp 15 - 28. [Published Version] HCIL-2008-18 [Link to Report] Yeh, R., Liao, C., Klemmer, S., Guimbretière, F., Lee, B., Kakarodov, B., Stamberger, J., Paepcke, A. (May 2008)ButterflyNet: A Mobile Capture and Access System for Field Biology ResearchProceedings of ACM CHI 06, pp. 571- 580 [Published Version] HCIL-2008-17 [Link to Report] Liao, C., Guimbretière, F., Hinckley, K., Hollan, J. (May 2008)PapierCraft: A Gesture-Based Command System for Interactive PaperACM Transactions on Computer-Human Interaction (TOCHI) archive,Volume 14 , Issue 4 (January 2008) [Published Version] HCIL-2008-16 [Link to Report] Quinn, A., Hu, C., Arisaka, T., Rose, A., Bederson, B. (May 2008)Readability of Scanned Books in Digital LibrariesProceeding of the Twenty-Sixth Annual SIGCHI Conference on Human Factors in Computing Systems (Florence, Italy, April 05 - 10, 2008). CHI '08. ACM, New York, NY, 705-714. [Published Version] HCIL-2008-15 [Link to Report] Golub, E., Druin, A., Komlodi, A., Resnik, P., Preece, J., Fails, J., Hou, W., Barin, T., Clamage, A. (May 2008)Exploring Cross-Language Communication for Children via a Word Guessing Game HCIL-2008-14 [Link to Report] Plaisant, C., Lam, S., Shneiderman, B., Smith, M., Roseman, D., Marchand, G., Gillam, M., Feied, C., Handler, J., Rappaport, H. (May 2008)Searching Electronic Health Records for Temporal Patterns in Patient Histories: A Case Study with Microsoft Amalga AMIA Annual Symposium Proceedings (2008) 601-605. [Published Version] HCIL-2008-13 [Link to Report] Druin, A. (April 2008)Designing Online Interactions: What Kids Want and What Designers KnowInteractions Magazine, May/June 2008. [Published Version] HCIL-2008-12 [Link to Report] Druin, A. (April 2008)Lifelong Interactions: My Father's Kitchen Table HCIL-2008-11 [Link to Report] Aris, A., Shneiderman, B. (April 2008)A Node Aggregation Strategy to Reduce Complexity of Network Visualization using Semantic Substrates HCIL-2008-10 [Link to Report] Hu, C., Quinn, A., Rose, A., Bederson, B., Arisaka, T. (February 2008)Enhancing Readability of Scanned Picture Books HCIL-2008-09 [Link to Report] Lin, J., Smucker, M. (February 2008)How Do Users Find Things with PubMed? Towards Automatic Utility Evaluation with User SimulationsProceedings of the 31th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2008),pages 19-26, July 2008, Singapore. [Published Version] HCIL-2008-07, LAMP-TR-148 [Link to Report] Kules, B., Wilson, M., Schraefel, M., Shneiderman, B. (February 2008)From Keyword Search to Exploration: How Result Visualization Aids Discovery on the Web HCIL-2008-06 [Link to Report] Friedler, S., Tan, Y., Peer, N., Shneiderman, B. (February 2008)Enabling teachers to explore grade patterns to identify individual needs and promote fairer student assessment To appear in Computers & Education in 2008 HCIL-2008-05 [Link to Report] Plaisant, C., Grinstein, G., Scholtz, J., Whiting, M., O'Connell, T., Laskowski, S., Chien, L., Tat, A., Wright, W., Gorg, C., Liu, Z., Parekh, N., Singhal, K., Stasko, J. (January 2008)Evaluating Visual Analytics: The 2007 Visual Analytics Science and Technology Symposium ContestIEEE Computer Graphics and Applications, 28, 2, 12-21 (2008) [Published Version] HCIL-2008-04 [Link to Report] Perer, A., Shneiderman, B. (January 2008)Integrating Statistics and Visualization: Case Studies of Gaining Clarity during Exploratory Data Analysis Proceedings of ACM CHI 2008, pp.265-274 [Published Version] HCIL-2008-03 [Link to Report] Hawes, T., Lin, J., Resnik, P. (January 2008)Elements of a Computational Model for Multi-Party Discourse: The Turn-Taking Behavior of Supreme Court Justices Journal of the American Society for Information Science and Technology, 2009, in press. [Published Version] HCIL-2008-02, LAMP-TR-147 [Link to Report] Lin, J. (January 2008)PageRank without hyperlinks: Reranking with PubMed related article networks for biomedical text retrievalBMC Bioinformatics, 2008, 9:270 [Published Version] HCIL-2008-01, LAMP-TR-146 [Link to Report]2007 Shneiderman, B. (July 2009)Creativity Support Tools: Accelerating Discovery and InnovationCommunications of the ACM 50, 12 (December 2007), cover story, 20-32. [Published Version] HCIL-2007-31 [Link to Report] Bederson, B. (November 2007)No Road, Drive: The ICDL Goes to the Mongolian Countryside International Children's Digital Library [Published Version] In Maryland International Spring 08 Edition [Published Version] HCIL-2007-30 [Link to Report] Tue Dao, H., Bazinet, A., Berthier, R., Shneiderman, B.NASDAQ Velocity and Forces: An Interactive Visualization of Activity and Change HCIL-2007-29 [Link to Report] Wang, T., Plaisant, C., Quinn, A., Stanchak, R., Shneiderman, B., Murphy, S. (January 2008)Aligning Temporal Data by Sentinel Events: Discovering Patterns in Electronic Health RecordsProc. of ACM Conference on Human Factors in Computing Systems (2008) 457-466 [Published Version] HCIL-2007-28 [Link to Report] Aris, A., Shneiderman, B. (December 2007)Designing Semantic Substrates for Visual Network ExplorationInformation Visualization, 6, 4 (2007), 281-300. [Published Version] HCIL-2007-27 [Link to Report] Perer, A., Shneiderman, B. (December 2007)Systematic Yet Flexible Discovery: Guiding Domain Experts through Exploratory Data AnalysisProceedings of the International Conference on Intelligent User Interfaces, (IUI 2008). HCIL-2007-26 [Link to Report] Namata, G., Staats, B., Getoor, L., Shneiderman, B. (November 2007)A dual-view approach to interactive network visualizationProceedings of the sixteenth ACM conference on Conference on information and knowledge management, 2007, p.939 - 942. [Published Version] HCIL-2007-25 [Link to Report] Karlson, A. (November 2007)Interface and Interaction Design for One-Handed Mobile Computing Ph.D. Dissertation from the Department of Computer Science HCIL-2007-24 [Link to Report] Shneiderman, B. (November 2007)Crisis and opportunity in computer scienceCommunications of the ACM 48, 11, November 2005, p. 27 - 28. [Published Version] HCIL-2007-23 [Link to Report] Shneiderman, B. (November 2007)A Telescope for High-Dimensional DataComputing Science & Engineering 8, 2, March - April 2006, pp. 48 - 53. [Published Version] HCIL-2007-22 [Link to Report] Shneiderman, B., Bederson, B., Drucker, S. (November 2007)Find that photo! Interface strategies to annotate, browse, and shareCommunications of the ACM, 49, 4, April 2006, p. 69 - 71. [Published Version] HCIL-2007-21 [Link to Report] Shneiderman, B. (November 2007)Discovering Business Intelligence Using Treemap Visualizations b-eye April 11, 2006. [Published Version] HCIL-2007-20 [Link to Report] Shneiderman, B., Preece, J. (November 2007)PUBLIC HEALTH: 911.govScience 315, 5814, 16 February 2007, p. 944. [Published Version] HCIL-2007-19 [Link to Report] Shneiderman, B. (November 2007)Human Responsibility for Autonomous AgentsIntelligent Systems 22, 2, March - April 2007, p. 60 - 61. [Published Version] HCIL-2007-18 [Link to Report] Shneiderman, B. (November 2007)25 years of CHI conferences: capturing the exchange of ideasACM Interactions 14, 2, March - April 2007, p. 24-31. [Published Version] HCIL-2007-17 [Link to Report] Gregory, M., Don, A., Zheleva, E., Tarkan, S., Plaisant, C., Shneiderman, B. (October 2007)Shape Search in Temporal Data to Facilitate Knowledge Discovery: A User Interface to find Spikes, Sinks and Slopes HCIL-2007-16 [Link to Report] Druin, A., Xie, B., Fails, J., Massey, S., Golub, E., Schneider, K., Kruskal, A. (September 2007)Connecting Generations: Developing Co-Design Methods for Older Adults and Children HCIL-2007-15 [Link to Report] Karlson, A., Bederson, B. (September 2007)One-Handed Touchscreen Input for Legacy Applications One-Handed Touchscreen Input for Legacy Applications. Proceedings of ACM CHI 2008, pp. 1399-1408 [Published Version] HCIL-2007-14 [Link to Report] Chipman, L. (September 2007)Collaborative Technology for Young Children's Outdoor Education Ph.D. Dissertation from the Department of Computer Science HCIL-2007-13 [Link to Report] Golub, E. (August 2007)PhotoCropr: A first step towards computer-supported automatic generation of photographically interesting cropping suggestions HCIL-2007-12 [Link to Report] Bederson, B., Clamage, A., Plaisant, C. (January 2008)Enhancing In-Car Navigation Systems with Personal ExperienceProceedings of the Transportation Research Board 87th annual meeting, The National Academies, Washington, DC (2008) 1-11[ Published Version] Also Journal of the Transportation Research Board of the National Academies, 2064 (2008) 33-42 [Published Version] HCIL-2007-11 [Link to Report] Lin, J., DiCuccio, M., Grigoryan, V., Wilbur, W. (July 2007)Exploring the Effectiveness of Related Article Search in PubMed Information Processing & Management, 44(5):1771-1783, 2008. [Published Version] HCIL-2007-10, CS-TR-4877, UMIACS-TR-2007-36 [Link to Report] Perer, A., Shneiderman, B. (May 2007)Systematic Yet Flexible Discovery: Guiding Exploratory Data Analysis HCIL-2007-09 [Link to Report] Don, A., Zheleva, E., Gregory, M., Tarkan, S., Auvil, L., Clement, T., Shneiderman, B., Plaisant, C. (May 2007)Discovering interesting usage patterns in text collections: integrating text mining with visualizationProceedings of the sixteenth ACM conference on Conference on information and knowledge management, 2007, p. 213 - 222. [Published Version] HCIL-2007-08 [Link to Report] Preece, J., Shneiderman, B., Jaeger, P., Qu, Y. (May 2007)Community Response Grids for Older Adults: Motivations, Usability, and Sociability HCIL-2007-07 [Link to Report] Zhang, P., Plettenberg, L., Klavans, J., Oard, D., Soergel, D. (May 2007)Task-based Interaction with an Integrated Multilingual, Multimedia Information System: A Formative Evaluation HCIL-2007-06 [Link to Report] Buono, P., Plaisant, C., Simeone, A., Aris, A., Shneiderman, B., Shmueli, G., Jank, W. (April 2007)Similarity-Based Forecasting with Simultaneous Previews: A River Plot Interface for Time Series Forecasting Similarity-Based Forecasting with Simultaneous Previews: A River Plot Interface for Time Series Forecasting,Proc. of the 11th International Conference Information Visualization (IV '07), 2007, 191-196 [Published Version] HCIL-2007-05 [Link to Report] Karlson, A., Bederson, B. (April 2007)Direct Versus Indirect Input Methods for One-Handed Touchscreen Mobile Computing Revised version at HCIL-2007-14 HCIL-2007-04 [Link to Report] Karlson, A., Bederson, B. (April 2007)ThumbSpace: Generalized One-Handed Input for Touchscreen-Based Mobile Devices Published as: Karlson, A., & Bederson, B. B.* (2007) ThumbSpace: Generalized One-Handed Input for Touchscreen-Based Mobile Devices, Proceedings of INTERACT 2007, 324-338. HCIL-2007-03 [Link to Report] Druin, A., Weeks, A., Massey, S., Bederson, B. (January 2007)Children's Interests and Concerns when using the International Children's Digital Library: A Four Country Case Study Published as Druin, A., Weeks, A., Massey, S., & Bederson, B. B. (2007) Childrens Interests and Concerns When Using the International Childrens Digital Library: A four country case study, Proceedings of the Joint Conference on Digital Libraries (JCDL 2007), 167-176. Available in the ACM Digital Library. [Published Version] HCIL-2007-02 [Link to Report] Druin, A., Weeks, A., Bederson, B., Massey, S. (January 2007)Final Report on IMLS Funded Grant #LG02020026-02: APPENDICES HCIL-2007-01 [Link to Report]2006 Kules, B., Shneiderman, B. (July 2009)Users can change their web search tactics: Design guidelines for categorized overviewsInformation Processing and Management 44, 2 (2008), 463-484. [Published Version] HCIL-2006-29 [Link to Report] Golub, E. (December 2006)Adding Societal Impact and Reflection to Information Technology Fluency Classes HCIL-2006-28 [Link to Report] Apitz, G., Bederson, B., Shneiderman, B. (December 2006)Interrupted Coordinated Activities HCIL-2006-27 [Link to Report] Bederson, B. (2006)No Hotel, Tent: The International Childrens Digital Library Goes to Mongolia HCIL-2006-26 [HTML]  [Link to Report] Perer, A., Shneiderman, B. (November 2006)Balancing Systematic and Flexible Exploration of Social NetworksIEEE Transactions on Visualization and Computer Graphics, 12, 5, (October 2006), 693 - 700. HCIL-2006-25 [Link to Report] Parr, C., Lee, B., Bederson, B. (September 2006)Ecolens: Integration and Interactive Visualization of Ecological Datasets EcoLens: Integration and Interactive Visualization of Ecological Datasets, Journal of Ecological Informatics, Elsevier, 2 (1), 61-69, 2007. HCIL-2006-24 [HTML]  [Link to Report] Guha, M., Druin, A., Montemayor, J., Chipman, L. (August 2006)A Theoretical Model of Children's Storytelling using Physically-Oriented Technologies (SPOT)Journal of Educational Multimedia and Hypermedia, 16 (4), pp. 389-410, (October 2007). HCIL-2006-23 [HTML]  [Link to Report] Zhao, H., Shneiderman, B., Plaisant, C., Lazar, J. (August 2006)Data sonification for users with visual impairments: A case study with geo-referenced dataACM Transactions on Computer Human Interaction 15, 1 (May 2008), Article 4. [Published Version] HCIL-2006-22 [HTML]  [Link to Report] Suh, B. (July 2006)Image Management Using Pattern Recognition Systems Ph.D. Dissertation from the Department of Computer Science HCIL-2006-21 [Link to Report] Lee, B. (May 2006)Interactive Visualizations for Trees and Graphs Ph.D. Dissertation from the Department of Computer Science HCIL-2006-20 [Link to Report] Shneiderman, B., Aris, A. (June 2006)Network Visualization by Semantic SubstratesIEEE Transactions on Visualization and Computer Graphics, 12, 5, (October 2006), 733 - 740. HCIL-2006-19 [HTML]  [Link to Report] White, R., Kules, B., Bederson, B. (May 2006)Exploratory Search Interfaces: Categorization, Clustering and BeyondSIGIR Forum, volume 39, issue 2, December 2005 HCIL-2006-18 [HTML]  [Link to Report] Shneiderman, B., Fischer, G., Czerwinski, M., Resnick, M., Myers, B., Candy, L., Edmonds, E., Eisenberg, M., Giaccardi, E., Hewett, T., Jennings, P., Kules, B., Nakakoji, K., Nunamaker, J., Pausch, R., Selker, T., Sylvan, E. (May 2006)Creativity Support Tools: Report from a U.S. National Science Foundation Sponsored WorkshopInternational Journal of Human-Computer Interaction, 20 (2). 61-77 HCIL-2006-17 [Link to Report] White, R., Kules, B., Drucker, S., Schraefel, M. (May 2006)Supporting Exploratory SearchCommunications of the ACM, 49(4), 36-39. [Published Version] HCIL-2006-16 [Link to Report] Kules, B., Kustanowitz, J., Shneiderman, B. (2006)Categorizing Web Search Results into Meaningful and Stable Categories using Fast-Feature techniques Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital libraries (Chapel Hill, NC, USA, June 11 - 15, 2006). JCDL '06. ACM Press, New York, NY. 210-219. [Published Version] HCIL-2006-15 [HTML]  [Link to Report] Kules, B. (April 2006)Supporting Exploratory Web Search with Meaningful and Stable Categorized Overviews Ph.D. Dissertation from the Department of Computer Science HCIL-2006-14 [Link to Report] Lee, B., Plaisant, C., Parr, C., Fekete, J., Henry, N. (May 2006)Task Taxonomy for Graph VisualizationProceedings of BELIV '06, pp.81-85. HCIL-2006-13 [HTML]  [Link to Report] Shneiderman, B., Plaisant, C. (May 2006)Strategies for Evaluating Information Visualization Tools: Multi-dimensional In-depth Long-term Case StudiesProceedings of BELIV '06, pp.38-43. HCIL-2006-12 [HTML]  [Link to Report] Parhi, P., Karlson, A., Bederson, B. (May 2006)Target Size Study for One-Handed Thumb Use on Small Touchscreen DevicesProceedings of MobileHCI 2006, ACM Press, pp. 203-210. HCIL-2006-11 [HTML]  [Link to Report] Kang, H., Plaisant, C., Lee, B., Bederson, B. (May 2006)NetLens: Iterative Exploration of Content-Actor Network DataProceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2006), 91 - 98. HCIL-2006-10 [HTML]  [Link to Report] Cockburn, A., Karlson, A., Bederson, B. (April 2006)A Review of Focus and Context InterfacesTo appear in ACM Computing Surveys as A Review of Overview+Detail, Zooming, and Focus+Context Interfaces. ACM Computing Surveys. HCIL-2006-09 [HTML]  [Link to Report] Perer, A., Smith, M. (April 2006)Contrasting Portraits of Email Practices: Visual Approaches to Reflection and Analysis Proceedings of  Conference on Advanced Visual Interfaces (AVI). 2006, ACM Press: Venezia, Italy, p.389-395. HCIL-2006-08 [HTML]  [Link to Report] Kim, B., Lee, B., Seo, J. (April 2006)Visualizing Concordance of Sets HCIL-2006-07 [HTML]  [Link to Report] Zhao, H. (April 2006)Interactive Sonificaton of Abstract Data - Framework, Design Space, Evaluation, and User Tool Ph.D. Dissertation from the Department of Computer Science HCIL-2006-06 [Link to Report] Herrnson, P., Bederson, B. (March 2006)A Study of Vote Verification Technology Conducted for the Maryland State Board of Elections HCIL-2006-05 [HTML]  [Link to Report] Lee, B., Parr, C., Plaisant, C., Bederson, B., Veksler, V., Gray, W., Kotfila, C. (February 2006)TreePlus: Interactive Exploration of Networks with Enhanced Tree LayoutsIEEE Transactions Transactions on Visualization and Computer Graphics, 12, 6, 1414-1426. HCIL-2006-04 [HTML]  [Link to Report] Chipman, L., Druin, A., Beer, D., Fails, J., Guha, M., Simms, S. (February 2006)A Case Study of Tangible Flags: A Collaborative Technology to Enhance Field Trips To appear in Interaction Design and Children 2006, pp. 1-8. HCIL-2006-03 [HTML]  [Link to Report] Karlson, A., Bederson, B. (January 2006)Understanding Single Handed Use of Handheld Devices. Lumsden, Jo (Ed.), Handbook of Research on User Interface Design and Evaluation for Mobile Technology, Idea Group Reference, 86-101, 2007. HCIL-2006-02 [HTML]  [Link to Report] Plaisant, C., Rose, J., Yu, B., Auvil, L., Kirschenbaum, M., Smith, M., Clement, T., Lord, G. (January 2006)Exploring Erotics in Emily Dickinson's Correspondence with Text Mining and Visual InterfacesProceedings of the 6th ACM/IEEE Joint Conference on Digital Libraries, JCDL 06, 141-150 (nominated for Best Paper award) [Published Version] HCIL-2006-01 [HTML]  [Link to Report]2005 Hutchinson, H., Bederson, B., Druin, A. (December 2005)The Evolution of the International Children's Digital Library Searching and Browsing InterfaceProceedings of the 2006 Conference on Interaction Design and Children, 2006, 105-112. [Published Version] HCIL-2005-33 [HTML]  [Link to Report] Hutchinson, H. (December 2005)Children's Interface Design for Searching and Browsing Ph.D. Dissertation from the Department of Computer Science HCIL-2005-32 [HTML]  [Link to Report] Kules, B., Shneiderman, B. (December 2005)Using meaningful and stable categories to support exploratory web search: Two formative studies HCIL-2005-31 [HTML]  [Link to Report] Plaisant, C., Shneiderman, B., Baker, H., Duarte, N., Haririnia, A., Klinesmith, D., Lee, H., Velikovich, L., Wanga, A., Westhoff, M. (November 2005)Personal Role Management: Overview and a Design Study of Email for University StudentsTo appear in: Kaptelinin, V., Czerwinski, M. (Eds) Integrated Digital Work Environments: Beyond the Desktop, MIT Press, (This is an expansion on HCIL-2003-30), (2007) 143-170. HCIL-2005-30 [HTML]  [Link to Report] Shneiderman, B., Bederson, B. (October 2005)Maintaining Concentration to Achieve Task CompletionProceedings of DUX 2005, no. 9. [Published Version] HCIL-2005-29 [Link to Report] Zhao, H., Plaisant, C., Shneiderman, B. (October 2005)A Framework for Auditory Data Exploration and Evaluation with Geo-referenced Data Sonification This report was updated and improved by TR HCIL-2006-22 HCIL-2005-28 [HTML]  [Link to Report] Hutchinson, H., Druin, A., Bederson, B., Reuter, K., Rose, A., Weeks, A. (August 2005)How do I Find Blue Books About Dogs? The Errors and Frustrations of Young Digital Library UsersProceedings of HCII 2005, Las Vegas, NV (CD-ROM). HCIL-2005-27 [HTML]  [Link to Report] Perer, A., Shneiderman, B. (Sept 2005)Beyond Threads: Identifying Discussions in Email Archives Extended Abstracts of IEEE Information Visualization (InfoVis). 2005, IEEE Press: Minneapolis, USA, p. 41 - 42. HCIL-2005-26 [Link to Report] Fails, J., Karlson, A., Shahamat, L., Shneiderman, B. (April 2006)A Visual Interface for Multivariate Temporal Data: Finding Patterns of Events over TimeProceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2006), 167-174. HCIL-2005-25 [HTML]  [Link to Report] Hutchinson, H., Bederson, B., Druin, A. (Sept 2005)Interface Design for Children's Searching and Browsing HCIL-2005-24 [HTML]  [Link to Report] Lee, B., Parr, C., Plaisant, C., Bederson, B. (Sept 2005)Visualizing Graphs as Trees: Plant a Seed and Watch it GrowProceedings of Graph Drawing 2005, pp. 516-518. HCIL-2005-23 [HTML]  [Link to Report] Seo, J., Shneiderman, B. (August 2005)Knowledge Discovery in High Dimensional Data: Case Studies and a User Survey for an Information Visualization ToolIEEE Transactions on Visualization and Computer Graphics 12, (May/June, 2006), 311-322. HCIL-2005-22, CS-TR-4743, UMIACS-TR-2005-46, ISR-TR-2005-100 [HTML]  [Link to Report] Kustanowitz, J., Shneiderman, B. (July 2005)Hierarchical layouts for photo librariesIEEE MultiMedia, 13, 4, Oct-Dec 2006, 62-72. HCIL-2005-21, CS-TR-4744, UMIACS-TR-2005-47, ISR-TR-2005-101 [HTML]  [Link to Report] Seo, J. (May 2005)Information Visualization Design for Multidimensional Data: Integrating the Rank-By-Feature Framework with Hierarchical Clustering Ph.D. Dissertation from the Dept. of Computer Science HCIL-2005-20, CS-TR-4745, UMIACS-TR-2005-48 [HTML]  [Link to Report] Kules, B., Shneiderman, B. (May 2005)Catergorized Overviews for Web Search Results: Two Exploratory Studies in the U.S. Government Domain HCIL-2005-19, CS-TR-4746, UMIACS-TR-2005-49 [Link to Report] Smith, B. (May 2005)Multiple Small Maps as an Information Seeking Tool HCIL-2005-17, CS-TR-4748, UMIACS-TR-2005-51 [HTML]  [Link to Report] Komlodi, A., Alburo, J., Preece, J., Druin, A., Elkiss, A., Resnik, P. (May 2005)Evaluation a Cross-Cultural Children's Online Book Community: Sociability, Usability, and Cultural Exchange To appear in Interacting with Computers HCIL-2005-15, CS-TR-4749, UMIACS-TR-2005-52 [HTML]  [Link to Report] Norman, K., Campbell, S. (May 2005)Pet Enumeration: Usability Testing of U.S. Census Bureau Data Collection Methods Avoiding Protected Personal Data HCIL-2005-14, CS-TR-4750, UMIACS-TR-2005-53 [HTML]  [Link to Report] Zhao, H., Plaisant, C. (May 2005)InterSon: Interactive Sonification for Geo-referenced Data Exploration for the Vision Impaired HCIL-2005-13, CS-TR-4751, UMIACS-TR-2005-54 [HTML]  [Link to Report] Seo, J., Shneiderman, B. (May 2005)Using Categorical Information in Multidimensional Data Sets: Interactive Partition and Cluster Comparison HCIL-2005-12, CS-TR-4752, UMIACS-TR-2005-55, ISR-TR-2005-102 [HTML]  [Link to Report] Liao, C., Guimbretière, F., Hinckley, K. (May 2005)PapierCraft: A System for Interactive PaperProceedings of the 18th Annual ACM Symposium on User Interface Software and Technology, 2005, 241-244. [Published Version] HCIL-2005-11, CS-TR-4753, UMIACS-TR-2005-56 [HTML]  [Link to Report] Kang, H., Hawala, S., Zayatz, L. (April 2005)IDFinder: Data Visualization for Checking Re-identifiability in MicrodataTo appear in the Proc. of International Conference on Human Computer Interaction 2005 HCIL-2005-10, CS-TR-4704, UMIACS-TR-2005-12 [HTML]  [Link to Report] Fails, J., Druin, A., Guha, M., Chipman, L., Simms, S. (April 2005)Child's Play: A Comparison of Desktop and Physical Interactive EnvironmentsProceeding of the 2005 conference on Interaction design and children, 2005, 48-55. [Published Version] HCIL-2005-09, CS-TR-4705, UMIACS-TR-2005-13 [HTML]  [Link to Report] Perer, A., Shneiderman, B., Oard, D. (March 2005)Using Rhythms of Relationships to Understand Email ArchivesJournal of the American Society of Information Science & Technology 57, 14 (2006), 1936-1948. HCIL-2005-08, CS-TR-4706, UMIACS-TR-2005-14, ISR-TR-2005-82 [HTML]  [Link to Report] Parr, C., Espinosa, R., Myers, P. (March 2005)Serving Computational Ecology From a Digital Library HCIL-2005-07, CS-TR-4707, UMIACS-TR-2005-15 [HTML]  [Link to Report] Parr, C., Cummings, M. (March 2005)Data Sharing in Ecology and Evolution: Why Not?Trends in Ecology and Evolution, July 2005. HCIL-2005-06, CS-TR-4708, UMIACS-TR-2005-16 [HTML]  [Link to Report] Eaton, C., Plaisant, C., Drizd, T. (March 2005)Visualizing Missing Data: Classification and Empirical StudyProc. of INTERACT 2005, Springer, Berlin (861-872). Springer Link: http://dx.doi.org/10.1007/11555261_68 HCIL-2005-05, CS-TR-4709, UMIACS-TR-2005-17 [HTML]  [Link to Report] Axelrod, A., Golbeck, J., Shneiderman, B. (February 2005)Generating and Querying Semantic Web Environments for Photo Libraries HCIL-2005-04, CS-TR-4710, UMIACS-TR-2005-18, ISR-TR-2005-75 [HTML]  [Link to Report] Kustanowitz, J., Shneiderman, B. (February 2005)Meaningful Presentations of Photo Libraries: Rationale and Applications of Bi-Level Radial Quantum LayoutsProc. ACM/IEEE Joint Conference on Digital Libraries,June 2005, 188-196. Winner Best Student Paper Award [Published Version] HCIL-2005-03, CS-TR-4711, UMIACS-TR-2005-19, ISR-TR-2005-74 [HTML]  [Link to Report] Plaisant, C., Shneiderman, B. (February 2005)Show Me! Guidelines for Producing Recorded DemonstrationsProceedings of 2005 IEEE Symposium on Visual Languages and Human-Centric Computing,  (VL/HCC'05), 171 - 178. [Published Version] HCIL-2005-02, CS-TR-4712, UMIACS-TR-2005-20, ISR-TR-2005-72 [HTML]  [Link to Report] Aris, A., Shneiderman, B., Plaisant, C., Shmueli, G., Jank, W. (February 2005)Representing Unevenly-Spaced Time Series Data for Visualization and Interactive ExplorationProc. of INTERACT 2005,  Springer, Berlin (835-846). Springer Link http://dx.doi.org/10.1007/11555261_66 HCIL-2005-01, CS-TR-4713, UMIACS-TR-2005-21, ISR-TR-2005-73 [HTML]  [Link to Report]2004 Baehrecke, E., Dang, N., Babaria, K., Shneiderman, B. (June 2004)Visualization and Analysis of Microarray and Gene Ontology Data with TreemapsBMC Bioinformatics 2004,5:84 (28 June 2004) HCIL-2004-39, CS-TR-4714, UMIACS-TR-2005-22 [HTML]  [Link to Report] Kules, B., Shneiderman, B. (January 2005)Categorized Graphical Overviews for Web Search Results: An Exploratory Study Using U.S. Government Agencies as a Meaningful and Stable StructureProc. Third Annual Workshop on HCI Research in MIS, December 2004, Washington, DC Organized by AIS SIGHCI (December 2004), 20-23. http://business.wm.edu/scott.mccoy/hci04_proceedings_only.pdf HCIL-2004-38, CS-TR-4715, UMIACS-TR-2005-23, ISR-TR-2005-71 [HTML]  [Link to Report] Karlson, A., Bederson, B., SanGiovanni, J. (January 2005)AppLens and LaunchTile: Two Designs for One-Handed Thumb Use on Small DevicesProc. of SIGCHI Conference on Human Factors in Computing Systems, (CHI 2005), 201-210. [Published Version] HCIL-2004-37, CS-TR-4716, UMIACS-TR-2005-24 [HTML]  [Link to Report] Plaisant, C. (January 2005)Information Visualization and the Challenge of Universal AccessIn Exploring Geovisualization (Eds, J. Dykes, A. MacEachren and M.J. Kraak), Oxford: Elsevier (2005) 53-82. HCIL-2004-36, CS-TR-4717, UMIACS-TR-2005-25 [HTML]  [Link to Report] Zhao, H., Plaisant, C., Shneiderman, B. (January 2005)"I Hear the Pattern" - Interactive Sonification of Geographical Data PatternsACM SIGCHI 2005 Extended Abstracts on Human Factors in Computing Systems [Published Version] HCIL-2004-35, CS-TR-4718, UMIACS-TR-2005-26, ISR-TR-2005-64 [HTML]  [Link to Report] Lee, B., Czerwinski, M., Robertson, G., Bederson, B. (January 2005)Understanding Research Trends in Conferences Using PaperLensExtended Abstracts of the SIGCHI Conference on Human Factors in Computing Systems, (CHI 2005), 1969-1972. [Published Version] HCIL-2004-34, CS-TR-4719, UMIACS-TR-2005-27 [HTML]  [Link to Report] Good, L., Stefik, M., Bederson, B. (November 2004)A Comparison of Zoomable User Interfaces and Folders For Grouping Visual Objects HCIL-2004-33, CS-TR-4641, UMIACS-TR-2004-83 [HTML]  [Link to Report] Seo, J., Shneiderman, B. (November 2004)Knowledge Integration Framework for Information Visualization In Hemmje, M, Niederee, C., and Risse, T. (Editors), From Intergrated Publication and Information Systems to Virtual Information and Knowledge Environments, Springer Lecture Notes in Computer Science, Berlin (2005), 207-220. HCIL-2004-32, CS-TR-4640, UMIACS-TR-2004-82, ISR-TR-2005-63 [HTML]  [Link to Report] Seo, J., Shneiderman, B. (November 2004)A Rank-by-Feature Framework for Interactive Exploration of Multidimensional DataInformation Visualization, 4, 2 (June 2005), 99-113. HCIL-2004-31, CS-TR-4639, UMIACS-TR-2004-81, ISR-TR-2005-62 [HTML]  [Link to Report] Plaisant, C., Fekete, J., Grinstein, G.Promoting Insight Based Evaluation of Visualizations: From Contest to Benchmark Repository Promoting Insight Based Evaluation of Visualizations: From Contest to Benchmark Repository, ACM Transactions on Visualization and Computer Graphics , 14, 1 (2008) 120-134 [Published Version] HCIL-2004-30 [HTML]  [Link to Report] Shneiderman, B. (November 2004)Why Not Make User Interfaces Better Than 3D Reality?IEEE Computer Graphics & Applications 23, 6 (November/December 2003), 12-15. HCIL-2004-29, CS-TR-4638, UMIACS-TR-2004-80, ISR-TR-2005-61 [HTML]  [Link to Report] Marchionini, G., Haas, S., Plaisant, C., Shneiderman, B., Hert, C. (November 2004)Project Highlight: Toward a Statistical Knowledge NetworkProc. National Conference on Digital Government Research, (2004), 93-94 http://www.dgrc.org/dgo2004 [Published Version] HCIL-2004-28, CS-TR-4637, UMIACS-TR-2004-79, ISR-TR-2005-60 [HTML]  [Link to Report] Shneiderman, B. (November 2004)Designing For Fun: How to Make User Interfaces More FunACM Interactions 11, 5 (Sept-Oct 2004), 48-50. [Published Version] HCIL-2004-27, CS-TR-4636, UMIACS-TR-2004-78, ISR-TR-2005-59 [HTML]  [Link to Report] Hochheiser, H., Shneiderman, B. (November 2004)Dynamic Query Tools for Time Series Data Sets, Timebox Widgets for Interactive ExplorationInformation Visualization 3, 1 (March 2004), 1-18. HCIL-2004-26, CS-TR-4635, UMIACS-TR-2004-77, ISR-TR-2005-58 [HTML]  [Link to Report] Buono, P., Aris, A., Plaisant, C., Khella, A., Shneiderman, B. (October 2004)Interactive Pattern Search in Time SeriesProceedings of Conference on Visualization and Data Analysis, VDA 2005, SPIE, Washington DC (2005) 175-186. HCIL-2004-25, CS-TR-4634, UMIACS-TR-2004-76, ISR-TR-2005-57 [HTML]  [Link to Report] Hutchinson, H., Rose, A., Bederson, B., Weeks, A., Druin, A. (September 2004)The International Children's Digital Library: A Case Study in Designing for a Multi-Lingual, Multi-Cultural, Multi-Generational AudienceInformation Technology and Libraries, American Library Association, March 2005, 24, 1, 4-12. HCIL-2004-24, CS-TR-4650, UMIACS-TR-2005-11 [HTML]  [Link to Report] Zhao, H., Smith, B., Norman, K., Plaisant, C., Shneiderman, B. (May 2004)Interactive Sonification of Choropleth Maps: Design and EvaluationIEEE multimedia, Special issue on Interactive Sonification,  12, 2 (2005) 26 -35. [Published Version] HCIL-2004-23, CS-TR-4651, ISR-TR-2005-56 [HTML]  [Link to Report] Conroy, K., Levin, D., Guimbretière, F. (May 2004)ProofRite: A Paper-Augmented Word Processor This paper has been submitted to UIST 2004. HCIL-2004-22, CS-TR-4652 [HTML]  [Link to Report] Apitz, G., Guimbretière, F. (May 2004)CrossY A Crossing Based Drawing ApplicationProc. Of the 17th annual ACM Symposium on User Interface Software and Technology (2004) Sante Fe, NM, USA, 3-12. [Published Version] HCIL-2004-21, CS-TR-4653 [HTML]  [Link to Report] Khella, A., Bederson, B. (May 2004)Pocket PhotoMesa: A Zooming Image Browser for PDA'sProc. of Mobile and Ubiquitous Multimedia (MUM 2004), ACM Press, p.19-24. HCIL-2004-20, CS-TR-4654 [HTML]  [Link to Report] Plaisant, C. (May 2004)The Challenge of Information Visualization Evaluation Proc. of Conf. on Advanced Visual Interfaces AVI'04 (2004), p.109-116. [Published Version] HCIL-2004-19, CS-TR-4655 [HTML]  [Link to Report] Kustanowitz, J., Shneiderman, B. (May 2004)Motivating Annotation for Digital Photographs: Lowering Barriers While Raising Incentives HCIL-2004-18, CS-TR-4656, ISR-TR-2005-55 [HTML]  [Link to Report] Norman, K., Panizzi, E. (May 2004)Levels of Automation and User Participation in Usability Testing HCIL-2004-17, CS-TR-4657 [HTML]  [Link to Report] Druin, A. (May 2004)The Role of Books, Libraries, Technology, and Culture in Children's Lives: An International case study HCIL-2004-16, CS-TR-4658 [HTML]  [Link to Report] Suh, B., Bederson, B. (April 2004)Semi-Automatic Photo Annotation Strategies Using Event Based Clustering and Clothing Based Person RecognitionInteracting With Computers, Elsevier, 19, 4 (Jul. 2007), 524-544. [Published Version] HCIL-2004-15, CS-TR-4659 [HTML]  [Link to Report] Klein, C., Bederson, B. (April 2004)Benefits of Animated ScrollingProceedings of Extended Abstracts of Human Factors in Computing Systems (CHI 2005) ACM Press, Short Paper, pp. 1965-1968. HCIL-2004-14, CS-TR-4660 [HTML]  [Link to Report] Seo, J., Shneiderman, B. (October 2004)A Rank-by-Feature Framework for Unsupervised Multidimensional Data Exploration Using Low Dimensional ProjectionsProc. IEEE Information Visualization Symposium, IEEE Press, Austin, TX (October 2004), 65-72. [Published Version] HCIL-2004-13, CS-TR-4661, ISR-TR-2005-54 [HTML]  [Link to Report] Lazar, J., Jones, A., Shneiderman, B. (April 2004)Workplace user frustration with computers: An exploratory investigation of the causes and severityBehaviour & Information Technology 25 3 (May-June 2006), 239-251. HCIL-2004-12, CS-TR-4662, ISR-TR-2005-53 [HTML]  [Link to Report] Chintalapani, G., Plaisant, C., Shneiderman, B. (April 2004)Extending the Utility of Treemaps with Flexible HierarchyProc. International Conference on Information Visualization, (2004), 335-344. HCIL-2004-10, CS-TR-4663, ISR-TR-2005-52 [HTML]  [Link to Report] Golub, E. (March 2004)On Audience Activities During PresentationsJournal of Computing Sciences in Colleges, 20, 3, February 2005, pp. 38-47. HCIL-2004-09, CS-TR-4664 [HTML]  [Link to Report] Golub, E. (March 2004)Supporting Faculty Goals During Student Presentations via Electronic Note-TakingFrontiers in Education 2, FIE 2004, pp. F4E13-F4E16 [Published Version] HCIL-2004-08, CS-TR-4665 [HTML]  [Link to Report] Mosolov, A., Bederson, B. (March 2004)Evaluation of Serial Periodic, Multi-Variable Data Visualizations HCIL-2004-07, CS-TR-4666 [HTML]  [Link to Report] Parr, C., Lee, B., Campbell, D., Bederson, B. (March 2004)Visualizations for Taxonomic and Phylogenetic TreesBioinformatics, Oxford University Press, 20 (17), pp. 2997-3004. HCIL-2004-06, CS-TR-4667 [HTML]  [Link to Report] Kang, H., Shneiderman, B. (February 2004)Personal media exploration: A spatial interface to user-designed semantic regionsJournal of Visual Languages and Computing 17, 3 (2006), 254-283. HCIL-2004-05, CS-TR-4668, ISR-TR-2005-51 [HTML]  [Link to Report] Zhao, H., Plaisant, C., Shneiderman, B., Duraiswami, R. (February 2004)Sonification of Geo-Referenced Data for Auditory Information Seeking: Design Principle and Pilot StudyProc. International Conference on Auditory Displays (2004), (http://www.icad.org) HCIL-2004-04, CS-TR-4669, ISR-TR-2005-36 [HTML]  [Link to Report] Seo, J., Shneiderman, B. (February 2004)Understanding Clusters in Multidimensional Spaces: Making Meaning by Combining Insights from Coordinated Views of Domain Knowledge HCIL-2004-03, CS-TR-4670, ISR-TR-2005-50 [HTML]  [Link to Report] Reuter, K., Druin, A. (January 2004)Bringing Together Children and Books: An Initial Descriptive Study of Children's Book Searching and Selection Behavior in a Digital LibraryProc. American Society for Information Science and Technology Conference (ASIST), Providence, RI. HCIL-2004-02, CS-TR-4671 [HTML]  [Link to Report] Guha, M., Druin, A., Chipman, L., Fails, J., Simms, S. (January 2004)Mixing Ideas: A New Technique for Working with Young Children as Design PartnersProc of Interaction Design and Children (IDC) 2004 Conference, 35-42. [Published Version] HCIL-2004-01, CS-TR-4672 [HTML]  [Link to Report]2003 Montemayor, J.Physical Programming: Tools for Kindergarten Children to Author Physical Interactive Environments Ph.D. Dissertation from the Department of Computer Science HCIL-2003-46 [Link to Report] Eaton, C., Plaisant, C., Drizd, T. (December 2003)The Challenge of Missing and Uncertain Data In Visualization 2003 Conference poster abstract compendium, IEEE, (2003) 40-41 [Published Version] HCIL-2003-45, CS-TR-4673 [Link to Report] Seo, J., Bakay, M., Chen, Y., Hilmer, S., Shneiderman, B., Hoffman, E. (December 2003)Building a Coherent Data Pipeline in Microarray Data Analyses: Optimization of Signal/Noise Ratios Using an Interactive Visualization Tool and a Novel Noise Filtering MethodBioinformatics 20, (2004), 2534-2544. http://bioinformatics.oupjournals.org/cgi/content/abstract/20/16/2534?etoc HCIL-2003-44, CS-TR-4674, ISR-TR-2005-49 [HTML]  [Link to Report] Kang, H. (November 2003)Managing and Exporing Personal Media Using Semantic Regions: A Spatial Interface Supporting User-Defined Mental Models Ph.D. Dissertation from the Dept. of Computer Science HCIL-2003-43, CS-TR-4675 [Link to Report] Hutchinson, H. (November 2003)Children's Interface Design for Hierarchical Search and Browse In ACM SIGCAPH Newsletter. ACM Press, pp. 11-12. [Published Version] HCIL-2003-42, CS-TR-4676 [HTML]  [Link to Report] Good, L. (October 2003)Zoomable User Interfaces for the Authoring and Delivery of Slide Presentations Ph.D. Dissertation from the Dept. of Computer Science HCIL-2003-41, CS-TR-4677 [HTML]  [Link to Report] Parr, C., Lee, B., Campbell, D., Bederson, B. (October 2003)TaxonTree: Visualizing Biodiversity InformationProceedings of AVI, (2004), 320-327. HCIL-2003-40, CS-TR-4678 [HTML]  [Link to Report] Druin, A. (October 2003)What Children Can Teach Us: Developing Digital Libraries for Children with Children A revised version of this paper will be published in Library Quarterly HCIL-2003-39, CS-TR-4679 [HTML]  [Link to Report] Plaisant, C., Bederson, B., Clamage, A., Hutchinson, H., Druin, A. (October 2003)Shared Family Calendars: Promoting Symmetry and AccessibilityACM Transactions on Computer-Human Interaction, 13, 3 (2006) 313 - 346. [Published Version] HCIL-2003-38, CS-TR-4680 [HTML]  [Link to Report] Bederson, B. (October 2003)Interfaces for Staying in the Flow Published as: Bederson, B. B. (2004) Interfaces for Staying in the Flow. In Ubiquity, ACM Press, (5) 27. HCIL-2003-37, CS-TR-4681 [HTML]  [Link to Report] Rivadeneira, A., Bederson, B. (October 2003)A Study of Search Result Clustering Interfaces: Comparing Textual and Zoomable User Interfaces HCIL-2003-36, CS-TR-4682 [HTML]  [Link to Report] Golub, E. (September 2003)Handwritten Slides on a TabletPC in a Discrete Mathematics Course To appear in the Proceeding of the 35th SIGCSE Technical Symposium on Computer Science Education, 2004, 51-55. [Published Version] HCIL-2003-35, CS-TR-4683 [HTML]  [Link to Report] Golub, E. (September 2003)Using the BIRD Note-taking System During In-Class Presentations: An Example in an HCI Class HCIL-2003-34, CS-TR-4684 [HTML]  [Link to Report] Shneiderman, B. (September 2003)Promoting Universal Usability with Multi-Layer Interface DesignACM Conference on Universal Usability, ACM Press, New York (2003), 1-8. [Published Version] HCIL-2003-33, CS-TR-4685, ISR-TR-2005-48 [HTML]  [Link to Report] Fekete, J., Wang, D., Dang, N., Aris, A., Plaisant, C. (August 2003)Overlaying Graph Links on TreemapsIn Information Visualization 2003 Symposium Poster Compendium, IEEE, (2003) 82-83 HCIL-2003-32, CS-TR-4686 [HTML]  [Link to Report] Ceaparu, I., Shneiderman, B. (June 2003)Finding Governmental Statistical Data on the Web: A study of categorically organized links for the FedStats Topics PageJournal of the American Society of Information Science & Technology 55, 11 (2004), 1008-1015 HCIL-2003-31, CS-TR-4513, UMIACS-TR-2003-81, ISR-TR-2005-47 [HTML]  [Link to Report] Baker, H., Duarte, N., Haririnia, A., Klinesmith, D., Lee, H., Velikovich, L., Wanga, A., Westhoff, M., Plaisant, C. (June 2003)A Design Study of the Integration of Email and Role Management for University Students (This TR was revised and expanded into HCIL-2005-30) HCIL-2003-30, CS-TR-4687 [HTML]  [Link to Report] Lazar, J., Bessiere, K., Ceaparu, I., Robinson, J., Shneiderman, B. (May 2003)Help! I'm Lost: User Frustration in Web Navigation IT&SOCIETY, (1), 3, WINTER 2003, 18-26. http://www.ITandSociety.org [Published Version] HCIL-2003-29, CS-TR-4688, ISR-TR-2005-46 [HTML]  [Link to Report] Munzner, T., Guimbretière, F., Tasiran, S., Zhang, L., Zhou, Y. (April 2003)TreeJuxtaposer: Scalable Tree Comparison using Focus+Context with Guaranteed Visibility To appear in proceedings of SigGraph 2003. [Published Version] HCIL-2003-28, CS-TR-4489, UMIACS-TR-2003-58 [Link to Report] Knudtzon, K., Druin, A., Kaplan, N., Summers, K., Chisik, Y., Kulkarni, R., Moulthrop, S., Weeks, H., Bederson, B. (April 2003)Starting an Intergenerational Technology Design Team: A Case StudyProc. Interaction Design and Children (IDC' 2003), Lancashire, England, 51-58. HCIL-2003-27, CS-TR-4689 [HTML]  [Link to Report] Zhao, P., Seo, J., Wang, Z., Wang, Y., Shneiderman, B., Hoffman, E. (April 2003)In vivo filtering of in vitro MyoD target data: An approach for identification of biologically relevant novel downstream targets of transcription factorsComptes Rendus Biologies 326 (2003), 1049-1065. HCIL-2003-26, CS-TR-4690, ISR-TR-2005-45 [Link to Report] Seo, J., Shneiderman, B. (April 2003)Interactive Exploration of Multidimensional Microarray Data: Scatterplot Ordering, Gene Ontology Browser, and Profile Search HCIL-2003-25, CS-TR-4486, UMIACS-TR-2003-55, ISR-TR-2005-68 [HTML]  [Link to Report] Seo, J., Bakay, M., Zhao, P., Chen, Y., Clarkson, P., Shneiderman, B., Hoffman, E. (April 2003)Interactive Color Mosaic and Dendrogram Displays for Signal/Noise Optimization in Microarray Data AnalysisProceedings IEEE International Conference on Multimedia and Expo, July 6-9, 2003, Baltimore, MD (http://www.icme2003.com) [Published Version] HCIL-2003-24, ISR-TR-2005-44 [HTML]  [Link to Report] Norman, K., Zhao, H., Shneiderman, B., Golub, E. (June 2003)Dynamic Query Chloropleth Maps for Information Seeking and Decision MakingProc. Human-Computer Interaction International 2003: Volume 2 Theory and Practice, Lawrence Erlbaum Associates, Mahwah, NJ (June 2003), 1263-1267. [Published Version] HCIL-2003-23, CS-TR-4484, UMIACS-TR-2003-53, ISR-TR-2005-43 [HTML]  [Link to Report] Kules, B., Kang, H., Plaisant, C., Rose, A., Shneiderman, B. (April 2003)Immediate Usability: A Case Study of Public Access Design for a Community Photo LibraryInteracting with Computers, 16, 3, December 2004, 1171-1193. HCIL-2003-22, CS-TR-4481, UMIACS-TR-2003-50, ISR-TR-2005-42 [HTML]  [Link to Report] Hourcade, J. (April 2003)User Interface Technologies and Guidelines to Support Children's Creativity, Collaboration, and Learning Ph.D. Dissertation from the Dept. of Computer Science HCIL-2003-21, CS-TR-4480, UMIACS-TR-2003-49 [HTML]  [Link to Report] Hochheiser, H. (April 2003)Interactive Graphical Querying of Time Series and Linear Sequence Data Sets Ph.D. dissertation from the Dept. of Computer Science HCIL-2003-20, CS-TR-4479, UMIACS-TR-2003-48 [Link to Report] Hochheiser, H., Baehrecke, E., Mount, S., Shneiderman, B. (April 2003)Dynamic Querying for Pattern Identification in Microarray and Genomic DataProceedings IEEE International Conference on Multimedia and Expo, July 6-9, 2003, Baltimore, MD (http://www.icme2003.com) [Published Version] HCIL-2003-19, CS-TR-4478, UMIACS-TR-2003-47, ISR-TR-2005-41 [HTML]  [Link to Report] Hourcade, J., Bederson, B., Druin, A. (April 2003)Building KidPad: An Application for Children's Collaborative StorytellingSoftware: Practice & Experience, 34(9), 895-914. HCIL-2003-18, CS-TR-4474, UMIACS-TR-2003-44 [HTML]  [Link to Report] Hourcade, J., Bederson, B., Druin, A., Rose, A., Takayama, Y. (April 2003)The International Children's Digital Library: Viewing Digital Books OnlineInteracting with Computers, 15, 151-167. HCIL-2003-17, CS-TR-4473, UMIACS-TR-2003-43 [HTML]  [Link to Report] Hourcade, J., Bederson, B., Druin, A., Guimbretière, F. (April 2003)Accuracy, Target Reentry and Fitts' Law Performance of Preschool Children Using MiceACM Transactions on Computer Human Interaction(TOCHI). [Published Version] HCIL-2003-16, CS-TR-4472, UMIACS-TR-2003-42 [HTML]  [Link to Report] Chipman, L., Bederson, B., Golbeck, J. (April 2003)SlideBar: Analysis of a linear input deviceJournal of Behavior and Information Technology, 23 (1), pp 1-9. HCIL-2003-15, CS-TR-4471, UMIACS-TR-2003-41 [HTML]  [Link to Report] Guimbretière, F. (April 2003)Paper Augmented Digital DocumentsProceedings of the 16th Annual ACM Symposium on User Interface Software and Technology, 2003, 51 - 60 [Published Version] HCIL-2003-14, CS-TR-4470, UMIACS-TR-2003-40 [HTML]  [Link to Report] Suh, B., Ling, H., Bederson, B., Jacobs, D. (April 2003)Automatic Thumbnail Cropping and its EffectivenessACM Conference on User Interface and Software Technolgy (UIST 2003), pp. 95-104, and won best student paper award. [Published Version] HCIL-2003-13, CS-TR-4469, UMIACS-TR-2003-39 [HTML]  [Link to Report] Lee, B., Bederson, B. (April 2003)Favorite Folders: A Configurable, Scalable File Browser HCIL-2003-12, CS-TR-4468, UMIACS-TR-2003-38 [HTML]  [Link to Report] Zhao, H., Plaisant, C., Shneiderman, B. (March 2003)Improving Accessibility and Usability of Geo-referenced Statistical DataProc. of the Digital Government Research Conference, 147-150, http://www.dgrc.org/dgo2004 [Published Version] HCIL-2003-11, CS-TR-4467, UMIACS-TR-2003-37, ISR-TR-2005-40 [HTML]  [Link to Report] Kang, H., Plaisant, C., Shneiderman, B. (March 2003)New Approaches to Help Users Get Started with Visual Interfaces: Multi-Layered Interfaces and Integrated Initial GuidanceProc. of the Digital Government Research Conference, 141-146, http://www.dgrc.org/dgo2004 [Published Version] HCIL-2003-10, CS-TR-4466, UMIACS-TR-2003-36, ISR-TR-2005-39 [HTML]  [Link to Report] Kules, B., Shneiderman, B., Plaisant, C. (March 2003)Data Exploration with Paired Hierarchical Visualizations: Initial Designs of PairTreesProc. of the Digital Government Research Conference, 255-260, http://www.dgrc.org/dgo2004 HCIL-2003-09, CS-TR-4465, UMIACS-TR-2003-35, ISR-TR-2005-38 [HTML]  [Link to Report] Kules, B., Shneiderman, B. (March 2003)Designing a Metadata-Driven Visual Information Browser for Federal StatisticsProc. of the Digital Government Research Conference, 117-122. HCIL-2003-08, CS-TR-4464, UMIACS-TR-2003-34, ISR-TR-2005-37 [HTML]  [Link to Report] He, D., Wang, J., Oard, D., Nossal, M. (February 2003)Comparing User-assisted and Automatic Query TranslationProceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, 2003, 461. [Published Version] HCIL-2003-07, CS-TR-4453, UMIACS-TR-2003-23 [HTML]  [Link to Report] Kim, J., Oard, D., Soergel, D. (January 2003)Searching Large Collections of Recorded Speech: A Preliminary StudyProceedings of ASIS&T 2003 Annual Meeting, October 19-22, 2003, Long Beach, CA HCIL-2003-06, CS-TR-4450, UMIACS-TR-2003-20 [HTML]  [Link to Report] Montemayor, J., Druin, A., Chipman, L., Guha, M. (January 2003)Sensing, Storytelling, and Children: Putting Users in Control HCIL-2003-05, CS-TR-4446, UMIACS-TR-2003-16 [HTML]  [Link to Report] Kang, H., Shneiderman, B. (January 2003)MediaFinder: An Interface for Dynamic Personal Media Management with Semantic RegionsProceedings of the Extended Abstract conference on human factor and computing systems, CHI2003, 668-669, Fort Lauderdale, April, 2003. [Published Version] HCIL-2003-04, CS-TR-4445, UMIACS-TR-2003-15 [HTML]  [Link to Report] Kang, H. (January 2003)Personal Media Exploration with Semantic Regions Proceedings of the Extended Abstract conference on human factor and computing systems, CHI2003, 764-765, Fort Lauderdale, April, 2003. [Published Version] HCIL-2003-03, CS-TR-4444, UMIACS-TR-2003-14 [HTML]  [Link to Report] Druin, A., Bederson, B., Weeks, A., Grosjean, J., Guha, M., Hourcade, J., Lee, J., Liao, S., Reuter, K., Rose, A., Takayama, Y., Zhang, L. (January 2003)The International Children's Digital Library: Description and Analysis of First UseFirst Monday,http://firstmonday.org/issues/issue8_5/ HCIL-2003-02, CS-TR-4433, UMIACS-TR-2003-04 [HTML]   [Link to Report] Bederson, B., Grosjean, J., Meyer, J. (January 2003)Toolkit Design for Interactive Structured GraphicsIEEE Transactions on Software Engineering(TSE). HCIL-2003-01, CS-TR-4432, UMIACS-TR-2003-03 [HTML]   [Link to Report]2002 Zhao, H., Shneiderman, B. (December 2002)Colour-Coded Pixel-Based Highly Interactive Web Mapping for Georeferenced Data ExplorationInternational Journal of Georgraphical Information Science, 19, 4, 2005. HCIL-2002-26, CS-TR-4431, UMIACS-TR-2003-02, ISR-TR-2005-35 [HTML]   [Link to Report] Herrnson, P., Bederson, B., Abbe, O. (December 2002)An Evaluation of Maryland's New Voting Machine HCIL-2002-25, CS-TR-4429, UMIACS-TR-2002-107 [HTML]   [Link to Report] Hourcade, J. (November 2002)It's Too Small! Implications of Children's Developing Motor Skills on Graphical User Interfaces HCIL-2002-24, CS-TR-4425, UMIACS-TR-2002-104 [HTML]   [Link to Report] Bederson, B., Lee, B., Sherman, R., Herrnson, P., Niemi, R. (October 2002)Electronic Voting System Usability IssuesACM Conference on Computer-Human Interaction (CHI 2003), 145-152. [Published Version] HCIL-2002-23, CS-TR-4414, UMIACS-TR-2002-94 [HTML]   [Link to Report] Ceaparu, I. (October 2002)Governmental Statistical Data on the Web: A Case Study of FedStats Revised December, 2002 HCIL-2002-22, CS-TR-4413, UMIACS-TR-2002-93 [HTML]  [Link to Report] Hutchinson, H., Bederson, B., Plaisant, C., Druin, A. (October 2002)Family Calendar Survey HCIL-2002-21, CS-TR-4412, UMIACS-TR-2002-92 [HTML]   [Link to Report] Guimbretière, F. (September 2002)On Merging Command Selection and Direct Manipulation HCIL-2002-20, CS-TR-4411, UMIACS-TR-2002-91 [Link to Report] Bessiere, K., Ceaparu, I., Lazar, J., Robinson, J., Shneiderman, B. (2004)Social and Psychological Influences on Computer User Frustration In Bucy, E. and Newhagen, J. (eds.) Media Access: Social and Psychological Dimensions of New Technology Use. Mahwah, NJ: Lawrence Erlbaum Associates, 169-192. HCIL-2002-19, CS-TR-4410, UMIACS-TR-2002-90, ISR-TR-2005-34 [HTML]   [Link to Report] Lazar, J., Jones, A., Hackley, M., Shneiderman, B. (2006)Severity and Impact of Computer User Frustration: A Comparison of Student and Workplace UsersInteracting with Computers18(2), 187-207. HCIL-2002-18, CS-TR-4409, UMIACS-TR-2002-89, ISR-TR-2005-33 [HTML]  [Link to Report] Guimbretière, F. (September 2002)Measuring FlowMenu Performance HCIL-2002-17, CS-TR-4408, UMIACS-TR-2002-88 [Link to Report] Hutchinson, H., Bederson, B., Druin, A., Plaisant, C., Mackay, W., Evans, H., Hansen, H., Conversy, S., Beaudouin-Lafon, M., Roussel, N., Lacomme, L., Eiderbäck, B., Lindquist, S., Sundblad, Y., Westerlund, B. (September 2002)Technology Probes: Inspiring Design for and with Families ACM Conference on Computer-Human Interaction (CHI 2003), pp. 17-24. HCIL-2002-16, CS-TR-4407, UMIACS-TR-2002-87 [HTML]   [Link to Report] Keogh, E., Hochheiser, H., Shneiderman, B. (August 2002)An Augmented Visual Query Mechanism for Finding Patterns in Time Series DataProc. Fifth International Conference on Flexible Query Answering Systems (October 27 - 29, 2002, Copenhagen, Denmark), Springer-Verlag, in the series Lecture Notes in Artificial Intelligence. [Published Version] HCIL-2002-15, CS-TR-4398, UMIACS-TR-2002-78, ISR-TR-2005-32 [HTML]   [Link to Report] Shneiderman, B., Kang, H., Kules, B., Plaisant, C., Rose, A., Rucheir, R. (August 2002)A Photo History of SIGCHI: Evolution of Design from Personal to PublicACM Interactions, 9, 3 (May 2002), 17-23. [Published Version] HCIL-2002-14, CS-TR-4397, UMIACS-TR-2002-77, ISR-TR-2005-67 [Link to Report] Druin, A., Chipman, L., Julian, D., Somashekhar, S. (June 2002)How Young Can Our Design Partners Be?Proc. Participatory Design Conference (PDC' 2003),  Malmo, Sweden, 127-131. HCIL-2002-13, CS-TR-4396, UMIACS-TR-2002-76 [HTML]   [Link to Report] Ceaparu, I., Shneiderman, B. (May 2002)Improving Web-based Civic Information Access: A Case Study of the 50 US StatesProc. 2002 International Symposium on Technology and Society (ISTAS'02), IEEE. [Published Version] HCIL-2002-12, CS-TR-4372, UMIACS-TR-2002-52, ISR-TR-2005-31 [HTML]   [Link to Report] Ceaparu, I., Lazar, J., Bessiere, K., Robinson, J., Shneiderman, B. (May 2002)Determining Causes and Severity of End-User FrustrationInternational Journal of Human-Computer Interaction, 17, 3, (2004), 333-356. HCIL-2002-11, CS-TR-4371, UMIACS-TR-2002-51, ISR-TR-2005-30 [HTML]  [Link to Report] Seo, J., Shneiderman, B. (May 2002)Understanding Hierarchical Clustering Results by Interactive Exploration of Dendrograms: A Case Study with Genomic Microarray Data Final version: "Interactively Exploring Hierarchical Clustering Results", IEEE Computer, Volume 35, Number 7, pp. 80-86, July 2002. HCIL-2002-10, CS-TR-4370, UMIACS-TR-2002-50, ISR-TR-2005-29 [HTML]   [Link to Report] Bederson, B., Clamage, A., Czerwinski, M., Robertson, G. (May 2002)DateLens: A Fisheye Calendar Interface for PDAs. Appeared as "DateLens: A Fisheye Calendar Interface for PDAs" in Transactions on Computer-Human Interaction [Published Version] HCIL-2002-09, CS-TR-4368, UMIACS-TR-2002-48 [HTML]   [Link to Report] Golub, E., Shneiderman, B. (May 2002)Dynamic Query Visualizations on World Wide Web Clients: A DHTML Approach for Maps and Scattergrams To appear International Journal of Web Engineering and Technology 1, 1 (2003). HCIL-2002-08, CS-TR-4367, UMIACS-TR-2002-47, ISR-TR-2005-28 [HTML]   [Link to Report] Druin, A., Revelle, G., Bederson, B., Hourcade, J., Lee, J., Campbell, D. (May 2002)A Collaborative Digital Library for Children: A Descriptive Study of Children's Collaborative Behavior and Dialogue Journal of Computer-Assisted Learning, 19 (2), pp. 239-248. HCIL-2002-07, CS-TR-4366, UMIACS-TR-2002-46 [HTML]   [Link to Report] Hochheiser, H., Shneiderman, B. (May 2002)Visual Queries for Finding Patterns in Time Series Data HCIL-2002-06, CS-TR-4365, UMIACS-TR-2002-45, ISR-TR-2005-27 [Link to Report] Plaisant, C., Grosjean, J., Bederson, B. (April 2002)SpaceTree: Supporting Exploration in Large Node Link Tree, Design Evolution and Empirical EvaluationINFOVIS 2002. IEEE Symposium on Information Visualization, 2002, Page(s): 57 -64, Boston, October 2002. [Published Version] HCIL-2002-05, CS-TR-4360, UMIACS-TR-2002-40 [HTML]   [Link to Report] Kang, H., Shneiderman, B. (March 2002)Dynamic Layout Management in a Multimedia Bulletin BoardProceeding of IEEE 2002 Symposia on Human Centric Computing Language and Environments, pp 51-53, Arlington, September 2002. [Published Version] HCIL-2002-04, CS-TR-4346, UMIACS-TR-2002-27, ISR-TR-2005-26 [HTML]   [Link to Report] Norman, K., Pleskac, T. (January 2002)Conditional Branching in Computerized Self-Administered Questionnaires: An Empirical Study HCIL-2002-02, CS-TR-4323, UMIACS-TR-2002-05 [HTML]   [Link to Report] Fekete, J., Plaisant, C. (January 2002)Interactive Information Visualization of a Million ItemsINFOVIS 2002. IEEE Symposium on Information Visualization, 2002, Page(s): 117 -124, Boston, October 2002. [Published Version] HCIL-2002-01, CS-TR-4320, UMIACS-TR-2002-2 [HTML]   [Link to Report]2001 Tang, L., Shneiderman, B. (March 2001)Dynamic Aggregation to Support Pattern Discovery: A Case Study with Web Logs University of Maryland, College Park, MD. Short version appears in Proc. Discovery Science: 4th International Conference 2001, Editors (Jantke, K. P. and Shinohara, A.), Springer-Verlag, Berlin, 464-469. SHORT VERSION [Published Version] HCIL-2001-27, CS-TR-4345, UMIACS-TR-2002-26, ISR-TR-2005-24 [HTML]   [Link to Report] Norman, K. (December 2001)Implementation of Conditional Branching in Computerized Self-Administered Questionnaires HCIL-2001-26, CS-TR-4319, UMIACS-TR-2002-1 [HTML]   [Link to Report] Boltman, A., Druin, A. (November 2001)Children's Storytelling Technologies An edited version was presented in the Proceedings of the American Educational Research Association. HCIL-2001-25, CS-TR-4310, UMIACS-TR-2001-87 [HTML]   [Link to Report] Boltman, A. (October 2001)Children's Storytelling Technologies: Differences in Elaboration and Recall University of Maryland, College of Education, Human Development, Dissertation HCIL-2001-24, CS-TR-4305, UMIACS-TR-2001-82 [HTML]   [Link to Report] Kules, B., Kang, H., Plaisant, C., Rose, A., Shneiderman, B. (October 2001)Immediate Usability: Kiosk Design Principles from the CHI 2001 Photo Library This paper has been updated and replaced by HCIL-2003-22 HCIL-2001-23, CS-TR-4293, UMIACS-TR-2001-71, ISR-TR-2005-25 [Link to Report] Tanin, E. (September 2001)Browsing Large Online Data Using Generalized Query Previews University of Maryland, Computer Science Dept., Dissertation HCIL-2001-22, CS-TR-4292, UMIACS-TR-2001-70, ISR-TR-2005-18 [HTML]   [Video] [Link to Report] Montemayor, J., Druin, A., Simms, S., Churaman, W., D'Armour, A. (September 2001)Physical Programming: Designing Tools for Children to Create Physical Interactive EnvironmentsCHI 2002, ACM Conference on Human Factors in Computing Systems, CHI Letters, 4(1), 299-306. [Published Version] HCIL-2001-21, CS-TR-4288, UMIACS-TR-2001-67 [HTML]   [Link to Report] Browne, H., Bederson, B., Plaisant, C., Druin, A. (September 2001)Designing an Interactive Message Board as a Technology Probe for Family Communication HCIL-2001-20, CS-TR-4284, UMIACS-TR-2001-63 [HTML]   [Link to Report] Mayer, M., Bederson, B. (August 2001)Browsing Icons: A Task-Based Approach for a Visual Web History HCIL-2001-19, CS-TR-4308, UMIACS-TR-2001-85 [HTML]   [Link to Report] Bederson, B., Shneiderman, B., Wattenberg, M. (July 2001)Ordered and Quantum Treemaps: Making Effective Use of 2D Space to Display HierarchiesACM Transactions on Graphics (TOG), 21, (4), October 2002, 833-854. [Published Version] HCIL-2001-18, CS-TR-4277, UMIACS-TR-2001-57, ISR-TR-2005-22 [HTML]   [Link to Report] Shneiderman, B., Hochheiser, H. (July 2001)Universal Usability as a Stimulus to Advanced Interface DesignBehaviour & Information Technology 20, 5 (Sept-Oct 2001), 367-376. HCIL-2001-17, CS-TR-4276, UMIACS-TR-2001-56, ISR-TR-2001-17 [HTML]   [Link to Report] Shneiderman, B. (July 2001)Inventing Discovery Tools: Combining Information Visualization with Data Mining Information Visualization 1, 1 (2002), 5-12. Also appeared in Proc. Discovery Science 4th International Conference 2001. [Published Version] HCIL-2001-16, CS-TR-4275, UMIACS-TR-2001-55, ISR-TR-2005-20 [HTML]   [Link to Report] Konishi, M., Plaisant, C., Shneiderman, B. (July 2001)Enabling Commuters to Find the Best Route: An Interface for Analyzing Driving History LogsProc. Interact 2001, IFIP IOS Press, (2001), 799-800. HCIL-2001-15, CS-TR-4274, UMIACS-TR-2001-54, ISR-TR-2005-19 [HTML]   [Link to Report] Druin, A., Fast, C. (July 2001)The Child as Learner, Critic, Inventor, and Technology Design Partner: An Analysis of Three Years of Swedish Student JournalsInternational Journal for Technology and Design Education, 12(3), 189-213. HCIL-2001-14, CS-TR-4273, UMIACS-TR-2001-53 [HTML]   [Link to Report] Tanin, E., Shneiderman, B. (June 2001)Exploration of Large Online Data Tables Using Generalized Query PreviewsInformation Systems(to appear, 2006). HCIL-2001-13, CS-TR-4266, UMIACS-TR-2001-47 [HTML]   [Link to Report] Plaisant, C. (Editor) (June 2001)2001 Human-Computer Interaction Laboratory Video Reports HCIL-2001-12, CS-TR-4263, UMIACS-TR-2001-46 [HTML] [Video] [Link to Report] Hornbæk, K., Bederson, B., Plaisant, C. (May 2001)Navigation Patterns and Usability of Overview+Detail and Zoomable User Interfaces for Maps Revised version with new title, "Navigation Patterns and Usability of Zoomable User Interfaces with and without an Overview", ACM Transactions on Computer-Human Interaction, Vol. 9, No. 4, pp. 362-389, December 2002. HCIL-2001-11, CS-TR-4267, UMIACS-TR-2001-48 [HTML]   [Link to Report] Bederson, B. (May 2001)Quantum Treemaps and Bubblemaps for a Zoomable Image BrowserACM Conference on User Interface and Software Technology (UIST 2001) as PhotoMesa: A Zoomable Image Browser using Quantum Treemaps and Bubblemaps, pp. 71-80. [Published Version] HCIL-2001-10, CS-TR-4256, UMIACS-TR-2001-39 [HTML]   [Video] [Link to Report] Norman, K., Pleskac, T., Norman, K. (May 2001)Navigational Issues in the Design of On-Line Self-Administered Questionnaires: The Effect of Training and Familiarity HCIL-2001-09, CS-TR-4255, UMIACS-TR-2001-38 [HTML]   [Link to Report] Dang, G., North, C., Shneiderman, B. (April 2001)Dynamic Queries and Brushing on Choropleth MapsProc. International Conference on Information Visualization 2001, 757-764. IEEE Press (July 2001). [Published Version] HCIL-2001-08, CS-TR-4254, UMIACS-TR-2001-37, ISR-TR-2005-17 [HTML]   [Video] [Link to Report] Chipman, L., Plaisant, C., Gahagan, S., Herrmann, J., Hewitt, S., Reaves, L. (April 2001)Understanding Manufacturing Systems with a Learning Historian for User-Directed Experimentation HCIL-2001-07, CS-TR-4243, UMIACS-TR-2001-29 [HTML]   [Link to Report] Shneiderman, B., Wattenberg, M. (April 2001)Ordered Treemap LayoutsProc. IEEE Symposium on Information Visualization 2001, 73-78. IEEE Press, Los Alamitos, CA (October 2001). [Published Version] HCIL-2001-06, CS-TR-4237, UMIACS-TR-2001-26, ISR-TR-2005-16 [HTML]   [Link to Report] Hochheiser, H., Shneiderman, B. (March 2001)Visual Specification of Queries for Finding Patterns in Time-Series DataProceedings of Discovery Science 2001, 441-446. Washington, DC, November 2001. [Published Version] HCIL-2001-05, CS-TR-4236, UMIACS-TR-2001-25, ISR-TR-2005-15 [HTML]  [Video] [Link to Report] Suh, B., Bederson, B. (March 2001)OZONE: A Zoomable Interface for Navigating OntologyProceedings of International Conference on Advanced Visual Interfaces (AVI 2002), ACM, Trento, Italy, 139-143, ACM Press. HCIL-2001-04, CS-TR-4227, UMIACS-TR-2001-16 [HTML]   [Link to Report] Good, L., Bederson, B. (March 2001)CounterPoint: Creating Jazzy Interactive Presentations Published with a new name as "Zoomable User Interfaces as a Medium for Slide Show Presentations", (2002) Information Visualization, 1 (1), Palgrave Macmillan, 35-49. HCIL-2001-03, CS-TR-4225, UMIACS-TR-2001-14 [HTML]   [Link to Report] Hochheiser, H., Shneiderman, B. (February 2001)Universal Usability Statements: Marking the Trail for All UsersACM Interactions,Vol. 8, No. 2, 16-18 March/April 2001 [Published Version] HCIL-2001-02, CS-TR-4307, UMIACS-TR-2001-84 [HTML]   [Link to Report] Shneiderman, B. (February 2001)Bridging the Digital Divide with Universal UsabilityACM Interactions,Vol. 8, No. 2, 11-15, March/April 2001 [Published Version] HCIL-2001-01, CS-TR-4306, UMIACS-TR-2001-83, ISR-TR-2005-14 [HTML]   [Link to Report]2000 Hochheiser, H. (October 2000)Browsers with Changing Parts: A Catalog Explorer for Philip Glass' WebsiteProc. of the ACM Designing Interactive Systems Conference HCIL-2000-26 [HTML]  [Link to Report] Hochheiser, H., Shneiderman, B. (October 2000)Coordinating Overviews and Detail Views of WWW Log DataProc. 2000 Workshop on New Paradigms in Information Visualization and Manipulation, ACM New York (November 2000). HCIL-2000-25, ISR-TR-2005-13 [HTML]  [Link to Report] Plaisant, C. (Editor) (October 2000)2000 and 1999-1991 Retrospective: Human-Computer Interaction Laboratory Video Reports HCIL-2000-24, CS-TR-4196 [HTML] [Video] [Link to Report] Plaisant, C., Bhamidipati, P. (October 2000)Vehicle Speed Information Displays for Public Websites: A Survey of User PreferencesProceeding of Conference on Intelligent Transportation Systems' 2001 (CD ROM proceedings), ITS'2001, Washington, DC, ITS America, Washington DC, June 2001 (http://www.its.org). Must be printed in color. HCIL-2000-23, CS-TR-4194, UMIACS-TR-2000-73 [HTML]   [Link to Report] Norman, K., Slaughter, L., Friedman, Z., Norman, K., Stevenson, R. (October 2000)Dual Navigation of Computerized Self-Administered Questionnaires and Organizational Records HCIL-2000-22, CS-TR-4192, UMIACS-TR-2000-71 [HTML]   [Link to Report] Kreitzberg, C., Shneiderman, B. (October 2000)Making Computer and Internet Usability a PriorityCommon Ground Anthology, Usability Professionals Association, 2000. Revised version reprinted in Branaghan, R. J. (Editor), Design by People for People: Essays on Usability, Usability Professionals Assn, 2001. Chicago, 7-20. HCIL-2000-21, CS-TR-4191, UMAICS-TR-2000-70, ISR-TR-2005-65 [HTML]   [Link to Report] Christian, K., Kules, B., Shneiderman, B., Youssef, A. (September 2000)A Comparison of Voice Controlled and Mouse Controlled Web BrowsingProc. ACM ASSETS 2000 Conference, ACM Press, New York (November 2000) [Published Version] HCIL-2000-20, CS-TR-4188, UMIACS-TR-2000-69, ISR-TR-2005-11 [HTML]   [Link to Report] Revelle, G., Druin, A., Platner, M., Weng, S., Bederson, B., Hourcade, J., Sherman, L. (September 2000)Young Children's Search Strategies and Construction of Search Queries Revised version: A Visual Search Tool for Early Elementary Science Students, Journal of Science Education and Technology (2002), 11(1), 49-57 HCIL-2000-19, CS-TR-4187, UMIACS-TR-2000-68 [HTML]   [Link to Report] Druin, A., Bederson, B., Hourcade, J., Sherman, L., Revelle, G., Platner, M., Weng, S. (September 2000)Designing a Digital Library for Young Children: An Intergenerational Partnership Revised version in the Proceedings of ACM/IEEE Joint Conference on Digital Libraries (JCDL), Virginia, June 2001 (pp. 398-405). HCIL-2000-18, CS-TR-4185, UMAICS-TR-2000-67 [HTML]   [Link to Report] Browne, H., Bederson, B., Druin, A., Sherman, L., Westerman, W., Bederson, B. (Advisor) (September 2000)Designing a Collaborative Finger Painting Application for Children HCIL-2000-17, CS-TR-4184, UMAICS-TR-2000-66 [HTML]   [Link to Report] Plaisant, C., Druin, A., Lathan, C., Dakhane, K., Edwards, K., Vice, J., Montemayor, J. (September 2000)A Storytelling Robot for Pediatric Rehabilitation Revised version: Proc. ASSETS '00, Washington, Nov. 2000, ACM, New York, 50-55. HCIL-2000-16, CS-TR-4183, UMIACS-TR-2000-65 [HTML]   [Video] [Link to Report] North, C. (May 2000)A User Interface for Coordinating Visualizations based on Relational Schemata: Snap-Together Visualization University of Maryland, Computer Science Dept., Doctoral Dissertation [Published Version] HCIL-2000-15 [Link to Report] Tanin, E., Plaisant, C., Shneiderman, B. (May 2000)Broadening Access to Large Online Databases by Generalizing Query PreviewsProc. of the Symposium on New Paradigms in Information Visualization and Manipulation - CIKM, pp. 80-85, 2000. [Published Version] HCIL-2000-14, CS-TR-4139, UMIACS-TR-2000-32, ISR-TR-2005-10 [HTML]   [Link to Report] Bederson, B., Meyer, J., Good, L. (May 2000)Jazz: An Extensible Zoomable User Interface Graphics ToolKit in JavaUIST 2000, ACM Symposium on User Interface Software and Technology, CHI Letters, 2(2), pp. 171-180. HCIL-2000-13, CS-TR-4137, UMIACS-TR-2000-30 [HTML]   [Link to Report] Bederson, B. (May 2000)Fisheye MenusProc. UIST 2000, pp. 217-225, ACM, New York. HCIL-2000-12, CS-TR-4138, UMIACS-TR-2000-31 [HTML]  [Video] [Link to Report] Kim, J., Oard, D., Romanik, K. (May 2000)User Modeling for Information Access Based on Implicit FeedbackProceedings of ISKO-France 2001, July 5-6, Nanterre, France. HCIL-2000-11, CS-TR-4136, UMIACS-TR-2000-29 [HTML]   [Link to Report] Norman, K., Friedman, Z., Norman, K., Stevenson, R. (May 2000)Navigational Issues in the Design of On-Line Self-Administered QuestionnairesBehaviour and Information Technology, 20, 37-45. HCIL-2000-10, CS-TR-4135, UMIACS-TR-2000-28, LAP-TR-2000-01 [HTML]   [Link to Report] Plaisant, C., Komlodi, A. (May 2000)Evaluation Challenges for a Federation of Heterogeneous Information Providers: The Case of NASA's Earth Science Information PartnershipsIEEE 9th Intl. Workshops on Enabling Technologies (WETICE'2000) Evaluating Collaborative Enterprises, June 14-16, Gaithersburg, MD, pp. 130-138, IEEE, Los Alamitos, CA. http://www.mel.nist.gov/msidevent/workshop/wetice-ece/ [Published Version] HCIL-2000-09, CS-TR-4134, UMIACS-TR-2000-27 [HTML]  [Link to Report] Marchionini, G., Hert, C., Liddy, L., Shneiderman, B. (May 2000)Extending User Understanding of Federal Statistics in TablesProceedings on the 2000 conference on Universal Usability, Arlington, VA, 2000, pp. 132 - 138. [Published Version] HCIL-2000-08, CS-TR-4131, UMIACS-TR-2000-24, ISR-TR-2005-9 [HTML]   [Link to Report] Kang, H., Shneiderman, B. (May 2000)Visualization Methods for Personal Photo Collections: Browsing and Searching in the PhotoFinder Proc. IEEE International Conference on Multimedia and Expo (ICME2000), New York City, New York. HCIL-2000-07, CS-TR-4494, UMIACS-TR-2003-63, ISR-TR-2005-8 [HTML]   [Link to Report] Shneiderman, B., Kang, H. (May 2000)Direct Annotation: A Drag-and-Drop Strategy for Labeling PhotosProc. International Conference Information Visualization (IV2000). London, England. Proc. International Conference on Information Visualization 2000, IEEE, Los Alamitos, CA (July 2000), 88-95. HCIL-2000-06, CS-TR-4129, UMIACS-TR-2000-23, ISR-TR-2005-7 [HTML]   [Link to Report] North, C., Shneiderman, B. (May 2000)Snap-Together Visualization: A User Interface for Coordinating Visualizations via Relational SchemataConference Proc. Advanced Visual Interfaces 2000, ACM, New York. HCIL-2000-05, CS-TR-4128, UMIACS-TR-2000-22, ISR-TR-2005-6 [HTML]   [Video] [Link to Report] Semple, P., Allen, R., Rose, A. (2000)Developing an Educational Multimedia Digital Library: Content Preparation, Indexing, and UsageED-MEDIA 2000, Montreal, and reprinted with permission of Assn. for the Advancement of Computing in Education. HCIL-2000-04 [HTML]  [Link to Report] Gandhi, R., Kumar, G., Bederson, B., Shneiderman, B. (March 2000)Domain Name Based Visualization of Web Histories in a Zoomable User InterfaceProc. 11th International Workshop on Database and Expert Systems Applications, includes WebVis 2000: Second International Workshop on Web-Based Information Visualization, IEEE Computer Society, Los Alamitos, CA (2000), 591-598. HCIL-2000-03, CS-TR-4114, UMIACS-TR-2000-12, ISR-TR-2000-8  [Link to Report] Alborzi, H., Druin, A., Montemayor, J., Sherman, L., Taxén, G., Best, J., Hammer, J., Kruskal, A., Lal, A., Plaisant Schwenn, T., Sumida, L., Wagner, R., Hendler, J. (February 2000)Designing StoryRooms: Interactive Storytelling Spaces for ChildrenProc. ACM Desiging Interactive Systems (DIS'2000), NY, 95-100. HCIL-2000-02, CS-TR-4106, UMIACS-TR-2000-06 [HTML]   [Video] [Link to Report] Shneiderman, B. (January 2000)The Limits of Speech Recognition: Understanding Acoustic Memory and Appreciating ProsodyCommunications of the ACM 43, 9 (September 2000), 63-65. HCIL-2000-01, ISR-TR-2005-5 [HTML]  [Link to Report]1999 Plaisant, C. (Editor) (October 2000)1999 Human-Computer Interaction Laboratory Video Reports HCIL-99-34, CS-TR-4195 [HTML] [Link to Report] Zhang, Z., Basili, V., Shneiderman, B. (1999)Perspective-based Usability Inspection: An Empirical Validation of EfficacyEmpirical Software Engineering 4, 1 (March 1999), 43-69. HCIL-99-33, CS-TR-4193, UMIACS-TR-2000-72  [Link to Report] Rose, A., Allen, R., Fulton, K. (1999)Multiple Channels of Electronic Communication for Building a Distributed Learning CommunityProceedings of the Computer Support for Collaborative Learning, CSCL '99, Stanford, CA, 495-502. [Published Version] HCIL-99-32 [HTML]  [Link to Report] Fredrikson, A., North, C., Plaisant, C., Shneiderman, B. (December 1999)Temporal, Geographical and Categorical Aggregations Viewed through Coordinated Displays: A Case Study with Highway Incident DataProceedings of the Workshop on New Paradigms in Information Visualization and Manipulation, Kansas City, Missouri, November 6, 1999 (in conjunction with ACM CIKM'99), ACM New York, 26-34. [Published Version] HCIL-99-31 [HTML]  [Link to Report] Hochheiser, H., Shneiderman, B. (November 1999)Using Interactive Visualizations of WWW Log Data to Characterize Access Patterns and Inform Site Design A revised version appears in ASIS'99 Proceedings of the 62nd Annual Meeting of the American Society for Information Science, Annual Conference October 31-November 4, 1999, Vol. 36, 331-344. Published as Understanding patterns of user visits to web sites: Interactive starfield visualizations of WWW log data, Journal of the American Society for Information Science and Technology 54, 4 (February 2001), 331-343. HCIL-99-30, ISR-TR-99-70 [HTML]   [Link to Report] Tse, T., Vegh, S., Marchionini, G., Shneiderman, B. (November 1999)An Exploratory Study of Video Browsing User Interface Designs and Research Methodologies: Effectiveness in Information Seeking TasksASIS'99 Proc. 62nd Annual Meeting of the American Society for Information Science, Medford, NJ (October 1999), 681-692. HCIL-99-29 [HTML] [Link to Report] Benford, S., Bederson, B., Åkesson, K., Bayon, V., Druin, A., Hansson, P., Hourcade, J., Ingram, R., Neale, H., O'Malley, C., Simsarian, K., Stanton, D., Sundblad, Y., Taxén, G. (November 1999)Designing Storytelling Technologies to Encourage Collaboration Between Young ChildrenProceedings of CHI 2000, The Hague, Netherlands, April 1-6, ACM, New York, 556-563. HCIL-99-28, CS-TR-4087, UMIACS-TR-99-76 [HTML]  [Video] [Link to Report] Bederson, B., Stewart, J., Druin, A. (November 1999)Single Display GroupwareProceedings of the SIGCHI Conference on Human Factors in Computing Systems, 1999, 286 - 293. [Published Version] HCIL-99-27, CS-TR-4086, UMIACS-TR-99-75 [HTML]  [Link to Report] North, C., Shneiderman, B. (October 1999)Snap-Together Visualization: Evaluating Coordination Usage and ConstructionInt'l Journal of Human-Computer Studies special issue on Empirical Studies of Information Visualization, Volume 53, 5 (November 2000), 715-739. HCIL-99-26, CS-TR-4075, UMIACS-TR-99-68 [HTML]  [Link to Report] Montemayor, J., Druin, A., Hendler, J. (October 1999)PETS: A Personal Electronic Teller of Stories Druin, A., Hendler, J. (ed.) Robots for Kids: New Technologies for Learning. Morgan Kaufmann, San Francisco, CA (2000). HCIL-99-25, CS-TR-4074, UMIACS-TR-99-67 [HTML]   [Link to Report] Hochheiser, H., Shneiderman, B. (September 1999)Performance Benefits of Simultaneous Over Sequential Menus as Task Complexity IncreasesInternational Journal of Human Computer Interaction, Volume 12, #2, 173-192. HCIL-99-24, CS-TR-4066, UMIACS-TR-99-60, ISR-TR-99-71 [HTML]   [Link to Report] Druin, A. (September 1999)The Role of Children in the Design of New TechnologyBehaviour and Information Technology (BIT), 2002, 21 (1), 1-25. HCIL-99-23, CS-TR-4058, UMIACS-TR-99-53 [HTML]  [Link to Report] Salter, R. (September 1999)A Client-Server Architecture for Rich Visual History Interfaces HCIL-99-22, CS-TR-4056, UMIACS-TR-99-52 [HTML]  [Link to Report] Chapman, R., Shneiderman, B. (September 1999)Booksites as Web-based Dynamic Supplements to Computer Science Textbooks HCIL-99-21 [HTML]  [Link to Report] Cailleteau, L. (September 1999)Interfaces for Visualizing Multi-Valued Attributes: Design and Implementation Using Starfield Displays (Summer Project report) HCIL-99-20 [HTML]   [Link to Report] Fredrikson, A. (August 1999)Temporal, Geographical and Categorical Aggregations Viewed through Coordinated Displays: A Case Study with Highway Incident Data (Summer Project Report) HCIL-99-19 [HTML]  [Link to Report] Harris, C., Allen, R., Plaisant, C., Shneiderman, B. (June 1999)Temporal Visualization for Legal Case Histories ASIS'99 Proceedings of the 62nd Annual Meeting of the American Society for Information Sciences, Conference October 31-November 4, 1999, Vol. 36, 271-279. HCIL-99-18, CS-TR-4047 [HTML]   [Link to Report] Shneiderman, B. (July 1999)Universal Usability: Pushing Human-Computer Interaction Research to Empower Every Citizen CACM 43, 5 (May 2000), ACM, New York, 84-91. HCIL-99-17, CS-TR-4043, UMIACS-TR-99-43, ISR-TR-99-72 [HTML]  [Link to Report] Shneiderman, B. (July 1999)Supporting Creativity with Advanced Information-Abundant User Interfaces In Earnshaw, R., Guedj, R., Van Dam, A., and Vince, J. (Editors), Human-Centred Computing, Online Communities, and Virtual Environments, Springer-Verlag London (2001), 469-480. HCIL-99-16, CS-TR-4042, UMIACS-TR-99-42, ISR-TR-99-73 [HTML]  [Link to Report] Zaphiris, P., Shneiderman, B., Norman, K. (June 1999)Expandable Indexes Versus Sequential Menus for Searching Hierarchies on the World Wide WebBehaviour & Information Technology, 2002, Vol. 21, No. 3, 201-207. HCIL-99-15 [HTML] [Link to Report] Druin, A. (May 1999)Cooperative Inquiry: Developing New Technologies for Children with ChildrenProceedings of CHI'99, Pittsburgh, PA, USA, May 15-20, ACM, New York, 592-599 HCIL-99-14 [HTML]   [Link to Report] Druin, A., Montemayor, J., Hendler, J., McAlister, B., Boltman, A., Fiterman, E., Plaisant, A., Kruskal, A., Olsen, H., Revett, I., Plaisant Schwenn, T., Sumida, L., Wagner, R. (May 1999)Designing PETS: A Personal Electronic Teller of StoriesProceedings of CHI'99, Pittsburgh, PA, USA, May 15-20, ACM, New York, 326-329 [Published Version] HCIL-99-13 [HTML] [Video] [Link to Report] Ghosh, P., Shneiderman, B. (May 1999)Zoom-Only vs Overview-Detail Pair: A Study in Browsing Techniques as Applied to Patient Histories HCIL-99-12, CS-TR-4028, UMIACS-TR-99-35 [HTML] [Link to Report] Plaisant, C., Rose, A., Rubloff, G., Salter, R., Shneiderman, B. (May 1999)The Design of History Mechanisms and Their Use in Collaborative Educational SimulationsProc. of the Computer Support for Collaborative Learning, CSCL' 99, Palo Alto, CA, 348-359. [Published Version] HCIL-99-11, CS-TR-4027, UMIACS-TR-99-34, ISR-TR-99-74 [HTML]  [Video] [Link to Report] North, C., Shneiderman, B. (May 1999)Snap-Together Visualization: Coordinating Multiple Views to Explore Information HCIL-99-10, CS-TR-4020, UMIACS-TR-99-28 [HTML]  [Link to Report] Potter, R., Shneiderman, B., Bederson, B. (May 1999)Pixel Data Access for End-User Programming and Graphical Macros HCIL-99-09, CS-TR-4019, UMIACS-TR-99-27 [HTML] [Link to Report] Hourcade, J., Bederson, B. (May 1999)Architecture and Implementation of a Java Package for Multiple Input Devices (MID) HCIL-99-08, CS-TR-4018, UMIACS-TR-99-26 [HTML]  [Link to Report] Bederson, B., McAlister, B. (May 1999)Jazz: An Extensible 2D+Zooming Graphics Toolkit in Java See updated report #2000-13 on the same topic HCIL-99-07, CS-TR-4015, UMIACS-TR-99-24 [HTML]  [Link to Report] Hochheiser, H., Shneiderman, B. (March 1999)Understanding Patterns of User Visits to Web Sites: Interactive Starfield Visualization of WWW Log Data - Short Version See 99-30 (November 1999). HCIL-99-06, CS-TR-3989, UMIACS-TR-99-11, ISR-TR-99-3 [Link to Report] Combs, T., Combs, T., Bederson, B. (February 1999)Does Zooming Improve Image Browsing?Proceedings of Digital Libraries '99, ACM, New York,1999, 130-137. HCIL-99-05, CS-TR-3995, UMIACS-TR-99-14 [HTML]  [Link to Report] Murphy, E., Norman, K., Moshinsky, D. (February 1999)VisAGE Usability Study HCIL-99-04 [HTML] [Link to Report] Shneiderman, B., Feldman, D., Rose, A., Ferre Grau, X. (February 1999)Visualizing Digital Library Search Results with Categorical and Hierarchial AxesProc. 5th ACM International Conference on Digital Libraries (San Antonio, TX, June 2-7, 2000), ACM, New York, 57-66. [Published Version] HCIL-99-03, CS-TR-3992, UMIACS-TR-99-12, ISR-TR-99-75 [HTML]   [Link to Report] Hochheiser, H., Shneiderman, B. (February 1999)Understanding Patterns of User Visits to Web Sites: Interactive Starfield Visualization of WWW Log Data See 99-30 (November 1999). HCIL-99-02, CS-TR-3989, UMIACS-TR-99-11, ISR-TR-99-3 [Link to Report] Shneiderman, B. (February 1999)Creating Creativity for Everyone: User Interfaces for Supporting InnovationACM Transactions on Computer-Human Interaction 7, 1 (March 2000), 114-138. Also to appear in Carroll, J. (Ed.) (2001)  HCI in the Millennium, ACM, New York. HCIL-99-01, CS-TR-3988, UMIACS-TR-99-10, ISR-TR-99-4 [HTML]   [Link to Report]1998 Zhang, Z., Basili, V., Shneiderman, B. (1998)An Empirical Study of Perspective Based Usability InspectionProceedings of the Human Factors and Ergonomics Society 42nd Annual Meeting, Santa Monica, CA (1998), 1346-1350. [Published Version] HCIL-98-18  [Link to Report] Plaisant, C., Venkatraman, M., Ngamkajornwiwat, K., Barth, R., Harberts, B., Feng, W.Refining query previews techniques for data with multivalued attributes: The case of NASA EOSDISIEEE Forum on Research and Technology Advances in Digital Libraries (ADL '99), IEEE Computer Society, Los Alamitos, CA, 50-59 HCIL-98-17, CS-TR-4010, UMIACS-TR-99-20 [HTML]  [Link to Report] Plaisant, C. (Editor) (March 1999)1998 Human-Computer Interaction Laboratory Video Reports HCIL-98-16, CS-TR-4007 [HTML] [Link to Report] Shneiderman, B. (Nov/Dec 1998)Educational Journeys on the Web FrontierEDUCOM Review 33,6 (Nov/Dec 1998), 10,12-14 HCIL-98-15 [HTML] [Link to Report] Stewart, J., Bederson, B., Druin, A. (December 1998)Single Display Groupware: A Model for Co-present CollaborationProceedings of CHI'99, Pittsburgh, PA, USA, May 15-20, 1999, ACM, New York, 286-293. HCIL-98-14, CS-TR-3966, UMIACS-98-75 [HTML]  [Link to Report] Kandogan, E. (June 1998)Hierarchial Multi-Window Management with Elastic Layout Dynamics Doctor of Philosophy, 1998 HCIL-98-13, CSC-TR-2007 [HTML] [Link to Report] Meyer, J., Bederson, B. (September 1998)Does a Sketchy Appearance Influence Drawing Behavior? HCIL-98-12, CS-TR-3965, UMIACS-TR-98-74 [HTML]  [Link to Report] Bederson, B., Boltman, A. (September 1998)Does Animation Help Users Build Mental Maps of Spatial InformationProceedings of InfoViz '99, IEEE, Los Alamitos, CA, 28-35. HCIL-98-11, CS-TR-3964, UMIACS-TR-98-73 [HTML]  [Link to Report] Plaisant, C., Tarnoff, P., Keswani, S., Saraf, A., Rose, A. (October 1998)Understanding Transportation Management Systems Performance with a Simulation-Based Learning EnvironmentProceeding of Conference on Intelligent Transportation Systems' 99, ITS'99, Washington, DC, ITS America, Washington DC, http://www.itsa.org (CD ROM proceedings) 1999. HCIL-98-10, CS-TR-3947, UMIACS-TR-98-60, ISR-TR-98-59 [HTML]  [Video] [Link to Report] Fekete, J., Plaisant, C. (1998)Excentric Labeling: Dynamic Neighborhood Labeling for Data VisualizationProceedings of CHI'99, Pittsburgh, PA, USA, May 15-20, 1999, ACM, New York, 512-519. HCIL-98-09, CS-TR-3946, UMIACS-TR-98-59 [HTML]  [Link to Report] Plaisant, C., Mushlin, R., Snyder, A., Li, J., Heller, D., Shneiderman, B. (1998)LifeLines: Using Visualization to Enhance Navigation and Analysis of Patient Records Revised version in 1998 American Medical Informatic Association Annual Fall Symposium (Orlando, Nov. 9-11, 1998), p. 76-80, AMIA, Bethesda MD. HCIL-98-08, CS-TR-3943, UMIACS-TR-98-56 [HTML]  [Video] [Link to Report] Rose, A., Eckard, D., Rubloff, G. (May 1998)An Application Framework for Creating Simulation-Based Learning Environments HCIL-98-07, CS-TR-3907, UMIACS-TR-98-32 [HTML]  [Video] [Link to Report] Hightower, R., Ring, L., Helfman, J., Bederson, B., Hollan, J. (1998)Graphical Multiscale Web Histories: A Study of PadPrintsProceedings of ACM Conference on Hypertext (Hypertext 98), ACM Press, 58-65. [Published Version] HCIL-98-06, CS-TR-3908, UMIACS-TR-98-33 [HTML]  [Link to Report] Li, J., Plaisant, C., Shneiderman, B. (1998)Data Object and Label Placement for Information Abundant Visualizations Proceedings, Workshop on New Paradigms in Information Visualization and Manipulation (NPIV'98) In conjunction with the CIKM'98, ACM, New York, 41-48. [Published Version] HCIL-98-05, CS-TR-3901, UMIACS-TR-98-28, ISR-TR-98-65 [HTML]  [Link to Report] Shneiderman, B., Borkowski, E., Alavi, M., Norman, K. (1998)Emergent Patterns of Teaching/Learning in Electronic ClassroomsEducational Technology Research and Development 46, 4 (1998, 23-42) HCIL-98-04, CS-TR-3889, UMIACS-TR-98-21 [HTML]  [Link to Report] Druin, A., Bederson, B., Boltman, A., Miura, A., Knotts-Callahan, D., Platt, M. (1998)Children as Our Technology Design Partners In Druin, A. (Ed.), The Design of Children's Technology: How we design and why?, Morgan Kaufmann, 1998, pp. 51-72. HCIL-98-03, CS-TR-3887, UMIACS-TR-98-20 [HTML] [Link to Report] Tanin, E., Lotem, A., Haddadin, I., Shneiderman, B., Plaisant, C., Slaughter, L. (1998 (Revised 1999))Facilitating Network Data Exploration with Query Previews: A Study of User Performance and PreferenceBehaviour & Information Technology Vol. 19 No. 6 (2000), pp. 393-403. HCIL-98-02, CS-TR-3879, UMIACS-TR-98-14 [HTML]  [Link to Report] Marchionini, G., Plaisant, C., Komlodi, A. (1998)Interfaces and Tools for the Library of Congress National Digital Library ProgramInformation Processing & Management, 34, 5, pp. 535-555, 1998. Also French version appeared in Document numerique. 2(1), 1998, pages 53-65, Hermes, Paris. HCIL-98-01, CS-TR-3872, UMIACS-TR-98-09 [HTML]  [Link to Report]1997 Shneiderman, B.A Grander Goal: A Thousand-fold Increase in Human CapabilitiesEducom Review, 32, 6 (Nov/Dec 1997), 4-10. HCIL-97-23 [HTML] [Link to Report] Plaisant, C. (Editor) (March 1999)1997 Human-Computer Interaction Laboratory Video Reports HCIL-97-22, CS-TR-4006 [HTML] [Link to Report] Shneiderman, B. (1997)Codex, Memex, Genex: The Pursuit of Tranformational TechnologiesInternational Journal of Human-Computer Interaction 10,2 (1998), 87-106. HCIL-97-21, CS-TR-3862, UMIACS-TR-97-89 [HTML] [Link to Report] Greene, S., Tanin, E., Plaisant, C., Shneiderman, B., Olsen, L., Major, G., Johns, S. (1997)The End of Zero-Hit Queries: Query Previews for NASA's Global Change Master DirectoryInternational Journal Digital Libraries Vol. 2 No.2+3 (1999), pp.79-90 HCIL-97-20, CS-TR-3856, UMIACS-TR-97-84 [HTML] [Link to Report] Plaisant, C., Shneiderman, B., Mushlin, R. (1997)An Information Architecture to Support the Visualization of Personal HistoriesInformation Processing & Management, 34, 5, pp. 581-597, 1998. HCIL-97-19, CS-TR-3855, UMIACS-TR-97-87 [HTML] [Link to Report] North, C., Shneiderman, B. (1997)A Taxonomy of Multiple Window Coordinations HCIL-97-18, CS-TR-3854, UMIACS-TR-9783 [HTML] [Link to Report] Shneiderman, B. (1997)Relate-Create-Donate: An Educational Philosophy for the Cyber-GenerationComputers & Education 31, 1 (1998), 25-39. HCIL-97-17 [HTML] [Link to Report] Greene, S., Marchionini, G., Plaisant, C., Shneiderman, B. (1997)Previews and Overviews in Digital Libraries: Designing Surrogates to Support Visual Information-Seeking Journal of the American Society for Information Science 51, 3 (March 2000), 380-393. HCIL-97-16, CS-TR-3838, UMIACS-TR-97-73, ISR-TR-97-80 [HTML]  [Link to Report] Rose, A., Ding, W., Marchionini, G., Beale Jr., J., Nolet, V. (1997)Building an Electronic Learning Community: From Design to ImplementationProceedings of CHI 98, Los Angeles, CA, 18-23 April 1998, ACM, New York, 203-210 HCIL-97-15, CS-TR-3831, UMIACS-TR-97-67, CLIS-TR-97-12 [HTML] [Link to Report] Tanin, E., Beigel, R., Shneiderman, B. (1997)Design and Evaluation of Incremental Data Structures and Algorithms for Dynamic Query InterfacesProceedings of the 1997 IEEE Information Visualization Workshop, pp. 81-86, 1997 HCIL-97-14, CS-TR-3796, UMIACS-TR-97-46, ISR-TR-97-54 [HTML] [Link to Report] Alonso, D., Rose, A., Plaisant, C., Norman, K. (1997)Viewing Personal History Records: A Comparison of Tabular Format and Graphical Presentation Using LifeLinesBehavior and Information Technology 17, 5, 1998, 249-262. HCIL-97-13, CS-TR-3795, UMIACS-TR-97-45 [HTML] [Link to Report] Oard, D. (1997)Speech-Based Information Retrieval for Digital Libraries HCIL-97-12, CS-TR-3778, UMIACS-TR-97-36, CLIS-TR-97-05, LAMP-TR-015 [HTML] [Link to Report] Ding, W., Marchionini, G. (May 1997)A Study on Video Browsing Strategies HCIL-97-11, CS-TR-3790, UMIACS-TR-97-40, CLIS-TR-97-06 [HTML] [Link to Report] Nation, D., Plaisant, C., Marchionini, G., Komlodi, A. (May 1997)Visualizing Websites Using a Hierarchical Table of Contents Browser: WebTOCProceedings of 3rd Conference on Human Factors and the Web, Denver, Colorado, June 12, 1997. HCIL-97-10, CS-TR-3791, UMIACS-TR-97-41, CLIS-TR-97-08 [HTML] [Link to Report] Doan, K., Plaisant, C., Shneiderman, B., Bruns, T. (1997)Interface and Data Architecture for Query Preview in Networked Information SystemsACM Transactions on Information Systems, July 1999, Vol. 17, No. 3, 320-341. A short early version also appeared in ACM SIGMOD Record, Vol.26, No.1, pp. 75-81 March 1997, as Query Previews for Networked Information Systems: A Case Study with NASA Environmental Data by Doan, K., Plaisant, C., Shneiderman, B., Bruns, B. [Published Version] HCIL-97-09, CS-TR-3792, UMIACS-TR-97-42, ISR-TR-97-57 [HTML] [Link to Report] Kandogan, E., Shneiderman, B. (1997)Elastic Windows: A Hierarchical Multi-Window World-Wide Web BrowserProc. ACM, UIST97, ACM New York (October 1997), 169-177 [Published Version] HCIL-97-08, CS-TR-3789, ISR-TR-97-56 [Link to Report] Marchionini, G., Nolet, V., Williams, H., Ding, W., Beale Jr., J., Rose, A., Gordon, A., Enomoto, E., Harbinson, L. (1997)Content + Connectivity => Community: Digital Resources for a Learning Community HCIL-97-07, CS-TR-3785, CLIS-TR-97-07 [HTML] [Link to Report] Lane, J., Kuester, S., Shneiderman, B. (January 1997)User Interfaces for a Complex Robotic Task: A Comparison of Tiled vs. Overlapped Windows HCIL-97-06, CS-TR-3784, ISR-TR-97-55 [HTML] [Link to Report] Shneiderman, B., Byrd, D., Croft, W. (January 1997)Clarifying Search: A User-Interface Framework for Text SearchesD-Lib Magazine, January 1997Condensed versions published as: Shneiderman, B., A framework for search interfaces, IEEE Software (March/April 1997), 18-20. Revised and shortened version published as: Shneiderman, B., Byrd, D., and Croft, B., Sorting out searching: A user-interface framework for text searches, Communications of the ACM 41, 4 (April 1998), 95-98. HCIL-97-05 [HTML] [Link to Report] Shneiderman, B. (February 1997)Between Hope and FearCommunications of the ACM, February 1997, Vol. 40, No. 2 (59-62) HCIL-97-04 [Link to Report] Alonso, D., Norman, K. (1997)Apparency of Contingencies in Pull Down Menus HCIL-97-03, CS-TR-3793, UMIACS-TR-97-43 [HTML] [Link to Report] Doan, K., Plaisant, C., Shneiderman, B., Bruns, T. (1997)Query Previews for Networked Information Systems: A Case Study with NASA Environmental DataACM SIGMOD Record, Vol. 26, No. 1 (75-81), March 1997 HCIL-97-02 [HTML]  [Link to Report] Shneiderman, B. (January 1997)Direct Manipulation for Comprehensible, Predictable, and Controllable User InterfacesProceedings of IUI97, 1997 International Conference on Intelligent User Interfaces, Orlando, FL, January 6-9, 1997, 33-39. [Published Version] HCIL-97-01 [HTML]  [Link to Report]1996 Plaisant, C., Bruns, T., Shneiderman, B., Doan, K. (1996)Query Previews in Networked Information Systems: the Case of EOSDIS Video in  CHI 97 Video program , Atlanta GA, 22-27 March 1997, ACM, New York NY. A two page summary also appears in CHI 97 Extended Abstracts, ACM New York, 202-203. HCIL-96-19 [HTML] [Video] [Link to Report] Tanin, E., Beigel, R., Shneiderman, B. (1996)Incremental Data Structures and Algorithms for Dynamic Query Interfaces Workshop on New Paradigms in Information Visualization and Manipulation, Fifth ACM International Conference on Information and Knowledge Management (CIKM '96) (Rockville, MD, Nov. 16, 1996) 12-15. Also in SIGMOD Record, Vol. 25, No. 4 (21-24), December 1996 [Published Version] HCIL-96-18, CS-TR-3730, ISR-TR-97-5 [HTML] [Link to Report] Kandogan, E., Shneiderman, B. (October 1996)Elastic Windows: Evaluation of Multi-Window OperationsCHI 97 Proceedings, Atlanta GA, 22-27 March 1997, ACM New York, 250-257 [Published Version] HCIL-96-17, CS-TR-3695, ISR-TR-97-2 [HTML] [Link to Report] Plaisant, C., Marchionini, G., Bruns, T., Komlodi, A., Campbell, L. (October 1996)Bringing Treasures to the Surface: Iterative Design for the Library of Congress National Digital Library ProgramCHI 97 Proceedings, Atlanta GA, 22-27 March 1997, ACM New York, 518-525 HCIL-96-16, CS-TR-3694, CLIS-TR-96-03 [Video] [Link to Report] Ellis, J., Rose, A., Plaisant, C. (September 1996)Putting Visualization to Work: ProgramFinder for Youth PlacementCHI 97 Proceedings, Atlanta GA, 22-27 March 1997, ACM New York, 502-509 HCIL-96-15, CS-TR-3692 [HTML] [Link to Report] Kolker, R., Shneiderman, B. (July 1996)Tools for Creating and Exploiting ContentGetty Art History Information Program, Research Agenda for Networked Cultural Heritage (Santa Monica, CA,1996) 27-30. HCIL-96-14 [HTML] [Link to Report] Shneiderman, B. (July 1996)The Eyes Have It: A Task by Data Type Taxonomy for Information VisualizationsProc. 1996 IEEE Conference on Visual Languages (Boulder, CO, Sept.3-6,1996) 336-343. [Published Version] HCIL-96-13, CS-TR-3665, ISR-TR-96-66 [HTML] [Link to Report] Plaisant, C. (Editor) (June 1996)1996 Human-Computer Interaction Laboratory Video Reports HCIL-96-12, CS-TR-3664 [HTML] [Link to Report] Alonso, D., Norman, K. (May 1996)Apparency of Contingencies in Single Panel Menus HCIL-96-11, CS-TR-3644, CAR-TR-824 [HTML] [Link to Report] Oard, D., Marchionini, G. (May 1996)A Conceptual Framework for Text Filtering Appeared as The State of the Art in Text Filtering in User Modeling and User Adapted Interaction, 7(3)141-178, 1997. HCIL-96-10, CS-TR-3643, CAR-TR-830, EE TR-96-25, CLIS TR-96-02 [HTML] [Link to Report] Marchionini, G., Plaisant, C., Komlodi, A. (May 1996)User Needs Assessment for the Library of Congress National Digital Library HCIL-96-09, CS-TR-3640, CAR-TR-829, CLIS-96-01 [HTML] [Link to Report] Mahajan, R., Shneiderman, B. (May 1996)Visual & Textual Consistency Checking Tools for Graphical User InterfacesIEEE Transactions on Software Engineering 23, 11 (November 1997), 722-735. HCIL-96-08, CS-TR-3639, CAR-TR-828, ISR-TR-96-46 [HTML] [Link to Report] Rose, A., Ellis, J., Plaisant, C., Greene, S. (May 1996)Life Cycle of User Interface Techniques: The DJJ Information System Design Process HCIL-96-07, CS-TR-3637, CAR-TR-826 [HTML] [Link to Report] Greene, S., Rose, A. (May 1996)Information and Process Integration from User Requirements Elicitation: A Case Study of Documents in a Social Services Agency Information and Process Integration in Enterprises: Rethinking Documents, Wakayama, T., et al., ed., Kluwer: Boston, 1998, 143-160. (Proceedings of IPIC '96: Information and Processes Integration Conference "Rethinking Documents", Sloan School of Management, MIT, Cambridge, MA, November 14-15, 1996. HCIL-96-06, CS-TR-3638, CAR-TR-827 [HTML] [Link to Report] Shneiderman, B. (April 1996)Designing Information-Abundant WebsitesInternational Journal of Human-Computer Studies 47 (1997), 5-29. Also  Designing the User Interface, 3rd edition, Addison Wesley. HCIL-96-05, CS-TR-3634, CAR-TR-824, ISR-TR-96-40 [HTML] [Link to Report] Plaisant, C., Rose, A. (March 1996)Exploring LifeLines to Visualize Patient Records A short version of this report appeared as a poster summary in 1996 American Medical Informatic Association Annual Fall Symposium (Washington, DC, Oct. 26-30, 1996), pp. 884, AMIA, Bethesda MD. [Published Version] HCIL-96-04, CS-TR-3620, CAR-TR-819 [HTML] [Link to Report] Paton, N., Doan, K., Díaz, O., Jaime, A. (December 1995)Exploitation of Object-Oriented and Active Constructs in Database Interface DevelopmentProceedings of the 3rd International Workshop on Database Interfaces (IDS3) (Edinburgh, Scotland, July 1996) Springer Verlag, 1-14. http://www.springer.co.uk/eWiC/eWiCAbstracts/IDS3.html HCIL-96-03 [HTML]  [Link to Report] Preece, J., Shneiderman, B. (December 1995)Survival of the Fittest: The Evolution of Multimedia User InterfacesACM Computing Surveys, vol. 27, 4 (Dec. 1995) 558-559. HCIL-96-02 [HTML]  [Link to Report] Korn, F., Shneiderman, B. (February 1996)Navigating Terminology Hierarchies to Access a Digital Library of Medical Images HCIL-96-01 [HTML]  [Link to Report]1995 Plaisant, C., Levy, R., Zhao, W. (November 1995)BizView: Managing Business and Network Alarms Summary of the video available from HCIL as part of the  1995 HCIL Video report. HCIL-95-22 [HTML] [Link to Report] Pointek, J. (November 1995)Data Structures for Dynamic Query Browsing of EOS Data Directories Presentation abstract appears in online Proc. of NASA Science Information Systems Interoperability Conference (University of Maryland, College Park, MD, Nov. 6-9, 1995) HCIL-95-21 [HTML] [Link to Report] North, C., Shneiderman, B., Plaisant, C. (October 1995)User Controlled Overviews of an Image Library: A Case Study of the Visible HumanProc. of the 1st ACM International Conference on Digital Libraries (Bethesda, MD, March 20-23, 1996) 74-82. ACM, New York. In addition a video "Browsing anatomical image databases: A case study of the Visible Human" appeared in CHI 96 Video Program with a two-page video summary in ACM CHI '96 Conference Companion (Vancouver, BC, Canada, April 13-18, 1996) 414-415, http://www.acm.org/sigchi/chi96/proceedings. The video is also available from HCIL as part of the  1995 HCIL Video report. [Published Version] HCIL-95-20, CS-TR-3550, CAR-TR-798, ISR-TR-95-99. [HTML] [Video] [Link to Report] Shneiderman, B., Alavi, M., Norman, K., Borkowski, E. (September 1995)Windows of Opportunity in Electronic ClassroomsCommunications of the ACM, Log on Education column, vol. 38, 11 (Nov. 1995) 19-24. HCIL-95-19, CS-TR-3542, CAR-TR-797 [HTML] [Link to Report] Shneiderman, B., Rose, A. (September 1995)Social Impact Statements: Engaging Public Participation in Information Technology DesignProc. CQL'96, ACM SIGCAS Symposium on Computers and the Quality of Life (Feb. 1996) 90-96. Also appears in Friedman, B. (Editor), Human Values and the Design of Computer Technology, CSLI Publications and Cambridge Univ. Press (1997), 117-133. HCIL-95-18, CS-TR-3537, CAR-TR-796. [HTML] [Link to Report] Plaisant, C. (Editor) (June 1995)1995 Human-Computer Interaction Laboratory Video Reports HCIL-95-17, CS-TR-3532, CAR-TR-795 [HTML] [Link to Report] Doan, K., Plaisant, C., Shneiderman, B. (September 1995)Query Previews in Networked Information SystemsProc. of the Third Forum on Research and Technology Advances in Digital Libraries, ADL '96 (Washington, DC, May 13-15, 1996) IEEE CS Press, 120-129. Also abstract appears as Architecture of dynamic query user interface for networked information systems in on-line Proc. of NASA Science Information Systems Interoperability Conference  (College Park, MD, Nov. 6-9, 1995) HCIL-95-16, CS-TR-3524, CAR-TR-788, ISR-TR-95-90 [HTML] [Link to Report] Plaisant, C., Milash, B., Rose, A., Widoff, S., Shneiderman, B. (September 1995)Life Lines: Visualizing Personal HistoriesACM CHI '96 Conference Proc. (Vancouver, BC, Canada, April 13-18, 1996) 221-227, color plate 518, http://www.acm.org/sigchi/sigchi96/proceedings. The paper also has a corresponding video in the  CHI 96 Video Program ACM, New York. Video also available from HCIL in the  1996 HCIL Video report. HCIL-95-15, CS-TR-3523, CAR-TR-787, ISR-TR-95-88. [HTML] [Link to Report] Kandogan, E., Shneiderman, B. (September 1995)Elastic Windows: Improved Spatial Layout and Rapid Multiple Window OperationsACM Proc. of the Workshop on Advanced Visual Interfaces, AVI '96 (Gubbio, Italy, May 27-29, 1996) 29-38. HCIL-95-14, CS-TR-3522, CAR-TR-786, ISR-TR-95-89. [HTML] [Video] [Link to Report] Carr, D. (May 1995)A Compact Graphical Representation of User Interface Interaction Objects 190 page Doctoral dissertation CSC 949, see 94-09 for condensed version. HCIL-95-13 [Link to Report] Kumar, H., Plaisant, C., Shneiderman, B. (March 1995)Browsing Hierarchical Data with Multi-Level Dynamic Queries and PruningInternational Journal of Human-Computer Studies, Volume 46, No. 1, 103-124 (January 1997). HCIL-95-12, CS-TR-3474, CAR-TR-772, ISR-TR-95-53. [HTML] [Link to Report] Plaisant, C., Shneiderman, B. (May 1995)Organization Overviews and Role Management: Inspiration for Future Desktop Environments IEEE Proc. 4th Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises (Berkeley Springs, WV, April 20-22, 1995) 14-22. [Published Version] HCIL-95-11, CS-TR-3473, CAR-TR-771. [HTML] [Link to Report] Mahajan, R., Shneiderman, B. (April 1995)A Family of User Interface Consistency Checking ToolsProc. of the Twentieth Annual Software Engineering Workshop, SEL-95-004 (Greenbelt, MD, Dec. 1995) NASA Pub., 169-188. HCIL-95-10, CS-TR-3472, CAR-TR-770, ISR-TR-95-52. [HTML] [Link to Report] Slaughter, L., Norman, K., Shneiderman, B. (March 1995)Assessing Users' Subjective Satisfaction with the Information System for Youth Services (ISYS)VA Tech Proc. of Third Annual Mid-Atlantic Human Factors Conference (Blacksburg, VA, March 26-28, 1995) 164-170. HCIL-95-09, CS-TR-3463, CAR-TR-768 [HTML] [Link to Report] Plaisant, C., Rose, A., Shneiderman, B., Vanniamparampil, A. (Revised October 1996)User Interface Reengineering: Low Effort, High Payoff StrategiesIEEE Software, vol.14, 4 (July/August 1997) 66-72. Also translated in Japanese in Nikkei Computer,Nikkei Business Publications, Inc., Tokyo, Japan, no. 430, pp. 151-159, Nov. 1997. HCIL-95-08, CS-TR-3459, CAR-TR-767 [HTML] [Link to Report] Rose, A., Shneiderman, B., Plaisant, C. (February 1995)An Applied Ethnographic Method for Redesigning User InterfacesACM Proc. of DIS 95, Symposium on Designing Interactive Systems: Processes, Practices, Methods & Techniques (Ann Arbor, MI, Aug 23-25, 1995)115-122. [Published Version] HCIL-95-07, CS-TR-3454, CAR-TR-765. [HTML] [Link to Report] Ellis, J., Tran, C., Ryoo, J., Shneiderman, B. (June 1995)Buttons vs. Menus: An Exploratory Study of Pull-Down Menu Selection as Compared to Button Bars HCIL-95-06, CS-TR-3452, CAR-TR-764. [HTML] [Link to Report] Shneiderman, B., Chimera, R., Jog, N., Stimart, R., White, D. (May 1995)Evaluating Spatial and Textual Style of DisplaysProc. of Getting the Best from State of the Art Display Systems, The Society for Information Display (Trafalgar Square, London, Feb. 21-23, 1995). Also appears in MacDonald, L., and Lowe, A. (1997), Display systems: Design and Applications, Chapter 5: Evaluating the spatial and textual style of diplays, pp 83-96. John Wiley & Sons. HCIL-95-05, CS-TR-3451, CAR-TR-763, ISR-TR-95-51. [HTML] [Link to Report] Asahi, T., Turo, D., Shneiderman, B. (January 1995)Visual Decision-Making: Using Treemaps for the Analytic Hierarchy Process Video in CHI '95 Video Program, ACM, New York. A two page video summary also appears in ACM CHI '95 Conference Companion, (Denver, Colorado, May 7-11, 1995) 405-406. Video also available through HCIL as part of the  1994 HCIL Video Report. HCIL-95-04 [HTML]  [Link to Report] Plaisant, C., Shneiderman, B. (May 1995)Organization Overviews and Role Management: Inspiration for Future Desktop Environments Video in CHI '95 Video Program, ACM, New York. A two page video summary also appears in ACM CHI '95 Conference Companion, (Denver, Colorado, May 7-11, 1995) 419-420. Video also available through HCIL as part of the  1994 HCIL Video Report. HCIL-95-03 [HTML]  [Link to Report] Shneiderman, B. (October 1994)Looking for the Bright Side of User Interface AgentsACM Interactions, vol. 2, 1 (Jan. 1995) 13-15. HCIL-95-02 [HTML]  [Link to Report] Shneiderman, B. (January 1995)The Info Superhighway: For the PeopleCommunications of the ACM, Inside Risks column, vol. 38, 1 (Jan. 1995) 162. HCIL-95-01 [HTML]  [Link to Report]1994 Atallah, G., Ball, M., Baras, J., Goli, S., Karne, R., Kelley, S., Kumar, H., Plaisant, C., Roussopoulos, N., Shneiderman, B., Srinivasarao, M., Stathatos, K., Teittinen, M., Whitefield, D.Next Generation Network Management Technology ISR-TR-94-42 Proceedings of the 12th Symposium on Space Nuclear Power and Propulsion/Commercialization, pp. 75-82, Albuquerque, NM, January 8-12, 1995. HCIL-94-19 [Link to Report] Rosenfeld, A., Marchionini, G., Holliday, W., Ricart, G., Faloustos, C., Dick, J., Shneiderman, B. (June 1994)QUEST: QUery Environment for Science TeachingProc. of Digital Libraries '94 (Texas A&M University, College Station, TX) 74-79. Also available at http://atg1.WUSTL.edu/DL94 HCIL-94-18 [Link to Report] Plaisant, C. (Editor) (June 1994)1994 Human-Computer Interaction Laboratory Video Reports HCIL-94-17, CS-TR-3531, CAR-TR-794. [HTML] [Link to Report] Plaisant, C., Jain, V. (April 1994)Dynamaps: Dynamic Queries on a Health Statistics Atlas Video in CHI '94 Video Program, ACM, New York. A two page video summary also appears in ACM CHI '94 Conference Companion, (Boston, MA, April 24-28, 1994) 439-440. Video also available through HCIL as part of the  1993 HCIL Video Report. HCIL-94-16 [HTML]  [Video] [Link to Report] Turo, D. (April 1994)Hierarchical Visualization with Treemaps: Making Sense of Pro Basketball Data Video in CHI '94 Video Program, ACM, New York. A two page video summary also appears in ACM CHI '94 Conference Companion, (Boston, MA, April 24-28, 1994) 441-442. Video also available through HCIL as part of the 1993 HCIL Video Report. HCIL-94-15 [HTML]  [Link to Report] Ahlberg, C., Shneiderman, B. (April 1994)Visual Information Seeking Using the FilmFinder Video in CHI '94 Video Program, ACM, New York. A two page video summary also appears in ACM CHI '94 Conference Companion, (Boston, MA, April 24-28, 1994) 433-434. Video also available through HCIL as part of the  1994 HCIL Video Report. HCIL-94-14 [Video] [Link to Report] Marchionini, G., Barlow, D. (January 1994)Extending Retrieval Strategies to Networked Environments: Old Ways, New Ways, and a Critical Look at WAISJournal of American Society for Information Science, 45 (8) 561-564. HCIL-94-13 [Link to Report] Shneiderman, B. (October 1994)The River Beyond the Rapids: Responsive Services for Responsible UsersConnecting the DOE Community: Partnerships in Information, Info Tech '94 (Oak Ridge, TN, Oct. 25-26,1994) 1-9. Also appears as Comprehensible Predictable, and Controllable User Interfaces in American Programmer, vol.8, 4 (April 1995) 2-7. HCIL-94-12 [HTML]  [Link to Report] Barreau, D. (December 1994)Context as a Factor in Personal Information Management SystemsJournal of American Society for Information Science, 46 (5) 327-339. HCIL-94-11 [Link to Report] Kumar, H. (December 1994)Browsing Hierarchical Data with Multi-Level Dynamic Queries and Pruning Dissertation (105p.), see 95-12 (CS-TR-3474) for condensed version. ISR-MS-95-5. HCIL-94-10 [Link to Report] Carr, D., Jog, N., Kumar, H., Teittinen, M. (September 1994)Using Interaction Object Graphs to Specify and Develop Graphical Widgets HCIL-94-09, CS-TR-3344, CAR-TR-734, ISR-TR-94-69 [Link to Report] Asahi, T., Turo, D., Shneiderman, B. (June 1994)Using Treemaps to Visualize the Analytic Hierarchy ProcessInformation Systems Research, vol. 6, 4 (Dec. 1995) 357-375. HCIL-94-08, CS-TR-3293, CAR-TR-719, ISR-TR-94-57. [HTML] [Link to Report] Kumar, H., Plaisant, C., Teittinen, M., Shneiderman, B. (June 1994)Visual information management for network configuration Part of this article was later published in: Next Generation Network Management Technology, G. Atallah, M. Ball, J. Baras, S. Goli, R. Karne, S. Kelley, H. Kumar, C. Plaisant, N. Roussopoulos, B. Shneiderman, M. Srinivasarao, K. Stathatos, M. Teittinen, and D. Whitefield, Proceedings of the 12th Symposium on Space Nuclear Power and Propulsion/Commercialization, pp. 75-82, Albuquerque, NM, January 8-12, 1995 HCIL-94-07, CS-TR-3288, CAR-TR-716, ISR-TR-94-45 [HTML] [Link to Report] Jog, N., Shneiderman, B. (May 1994)Starfield Information Visualization with Interactive Smooth ZoomingIFIP 2.6 Visual Databases Systems Proc. (Lausanne, Switzerland, March 27-29,1995) 1-10. [Published Version] HCIL-94-06, CS-TR-3286, CAR-TR-714, ISR-TR-94-46. [HTML] [Link to Report] Shneiderman, B., Plaisant, C. (May 1994)The Future of Graphic User Interfaces: Personal Role ManagersPeople and Computers IX, British Computer Society's HCI 94 (Glasgow, Scotland, Aug. 1994) CU Press (Cambridge, U.K.) 3-8. [Published Version] HCIL-94-05, CS-TR-3285, CAR-TR-713, ISR-TR-94-48 [HTML]  [Link to Report] Johnson, B. (August 1993)Treemaps: Visualizing Hierarchical and Categorical Data HCIL-94-04, UMI-94-25057. [Link to Report] Carr, D., Plaisant, C., Hasegawa, H. (revised June 1995)Usability Experiments for the Redesign of a Telepathology Workstation Based on The Design of a Telepathology Workstation: Exploring Remote Images. Interacting with Computers, 11(1), 1998, pp. 33-52. HCIL-94-03, CS-TR-3270, CAR-TR-708 [Link to Report] Plaisant, C., Carr, D., Shneiderman, B. (April 1994)Image Browsers: Taxonomy, Guidelines, and Informal SpecificationsIEEE Software, vol.12, 2 (March 1995) 21-32. HCIL-94-02, CS-TR-3282, CAR-TR-712, ISR-TR-94-39 [HTML] [Link to Report] Shneiderman, B., Lewis, C. (March 1992)Building HCI Partnerships and InfrastructureBehavior & Information Technology 12, 2, 1993, 130-135. HCIL-94-01 [HTML]  [Link to Report]1993 Plaisant, C. (Editor) (June 1993)1993 Human-Computer Interaction Laboratory Video Reports HCIL-93-25, CS-TR-3530, CAR-TR-793 [HTML] [Link to Report] Plaisant, C., Carr, D., Hasegawa, H. (April 1993)Exploring Remote Images: A Telepathology Workstation Video in ACM INTERCHI 93 Video Program (Amsterdam, Netherlands, April 24-29, 1993), video available through ACM SIGGRAPH Video Review, issue 88-89. A one page summary also appears in INTERCHI 93 Proceedings, 518. Video also available through HCIL as part of the  1992 HCIL Video Report. HCIL-93-24 [HTML]  [Link to Report] Potter, R. (April 1993)Guiding Automation with Pixels: A Technique for Programming in the User Interface Video in ACM INTERCHI 93 Video Program (Amsterdam, Netherlands, April 24-29, 1993), video available through ACM SIGGRAPH Video Review, issue 88-89. A one page summary also appears in INTERCHI 93 Proceedings, 530. Video also available through HCIL as part of the  1992 HCIL Video Report. HCIL-93-23 [Link to Report] Norman, K., Wright, P. (Aug. 1993)HyperTools for HyperTexts: Supporting Readers of Electronic Documents HCIL-93-22, CS-TR-3090, CAR-TR-675 [HTML] [Link to Report] Plaisant, C. (Aug. 1993)Facilitating Data Exploration: Dynamic Queries on a Health Statistics Map ASA (Alexandria, VA)1993 Proc. of the Annual Meeting of the American Statistical Association of the Government Statistics Section (San Francisco, CA, Aug. 1993) 18-23. HCIL-93-21 [HTML]  [Link to Report] Carr, D. (Sept. 1993)Specification of Interface Interaction ObjectsACM CHI '94 Conference Proc. (Boston, MA, April 24-28, 1994) 372-378. HCIL-93-20, CS-TR-3142, CAR-TR-687 [HTML] [Link to Report] Potter, R., Maulsby, D. (May 1993)A Test Suite for Programming by DemonstrationWatch What I Do: Programming By Demonstration, Allen Cypher, Ed., MIT Press, Cambridge, MA (1993) 539-591. HCIL-93-19 [HTML]  [Link to Report] Potter, R. (May 1993)Just-in-Time ProgrammingWatch What I Do: Programming By Demonstration, Allen Cypher, Ed., MIT Press (1993) 513-526. HCIL-93-18 [HTML]  [Link to Report] Marchionini, G., Crane, H. (April 1994)Evaluating Hypermedia and Learning: Methods and Results from the Perseus ProjectACM Transactions on Information Systems, vol. 12, 1 (Jan. 1994) 5-34. HCIL-93-17 [Link to Report] Jain, V., Shneiderman, B. (revised Sept. 1993)Data Structures for Dynamic Queries: An Analytical and Experimental EvaluationProc. of the Workshop in Advanced Visual Interfaces, AVI 94 ( Bari, Italy, June 1-4, 1994) 1-11. Previous version referenced as CAR-TR-685 CS-TR-3133, ISR-TR-93-73. [Published Version] HCIL-93-16, CS-TR-3287, CAR-TR-715, ISR-TR-94-47. [Link to Report] Ahlberg, C., Shneiderman, B. (Sept. 1993)The Alphaslider: A Compact and Rapid SelectorACM CHI '94 Conference Proc. (Boston, MA, April 24-28, 1994) 365-371. [Published Version] HCIL-93-15, CS-TR-3132, CAR-TR-684, SRC-TR-93-72. [HTML] [Link to Report] Ahlberg, C., Shneiderman, B. (Sept. 1993)Visual Information Seeking: Tight Coupling of Dynamic Query Filters with Starfield DisplaysACM CHI '94 Conference Proc. (Boston, MA, April 24-28, 1994) 313-317. Also appears in Readings in Human-Computer Interaction: Toward the Year 2000, Baecker, R.M., Grudin, J. , Buxton, W.A.S. & Greenberg, S., Eds., Morgan Kaufmann Pubs., Inc ., (1995) 450-456, inside back cover. [Published Version] HCIL-93-14, CS-TR-3131, CAR-TR-638, SRC-TR-93-71 [HTML] [Link to Report] Shneiderman, B. (1993)Preface to Sparks of Innovation in Human-Computer InteractionSparks of Innovation in Human-Computer Interaction, B. Shneiderman, Ed., Ablex Publ. (1993) 385 pages. ACM Interactions, vol. 1, 1 (Jan. 1994) 67-71. HCIL-93-13 [HTML]  [Link to Report] Shneiderman, B. (1993)Declaration in Apple vs. Microsoft/Hewlett-PackardSparks of Innovation in Human-Computer Interaction, B. Shneiderman, Ed., Ablex Publ. (1993) 355-363. HCIL-93-12 [HTML]  [Link to Report] Chimera, R., Shneiderman, B. (1993)User Interface Consistency: An Evaluation of Original and Revised Interfaces for a Videodisk LibrarySparks of Innovation in Human-Computer Interaction, B. Shneiderman, Ed., Ablex Publ. (1993) 259-273. HCIL-93-11 [HTML]  [Link to Report] Marchionini, G., Ashley, M., Korzendorfer, L. (1993)ACCESS at the Library of CongressSparks of Innovation in Human-Computer Interaction, B. Shneiderman, Ed., Ablex Publ., Norwood, NJ (1993) 251-258. HCIL-93-10 [HTML]  [Link to Report] Chimera, R. (March 1993)Evaluation of Platform Independent User Interface Builders Complete paper version available with 100 page statistical data section. HCIL-93-09 [HTML]  [Link to Report] Osada, M., Liao, H., Shneiderman, B. (April 1993)AlphaSlider: Development and Evaluation of Text Retrieval Method Using Sliders9th Symposium on Human Interface (Kobe, Japan, Oct. 18-20, 1993) 91-94. HCIL-93-08, CS-TR-3078, CAR-TR-673, ISR-93-52. [HTML] [Link to Report] Shneiderman, B. (March 1993)Education by Engagement and Construction: Experiences in the AT&T Teaching Theater AACE (Charlotesville, VA) Education Multimedia and Hypermedia Annual, Maurer, H., Ed., 1993, Ed-Media 93 (Orlando, FL, June 23-26, 1993) 471-479. HCIL-93-07 [HTML] [Link to Report] Lindwarm, D., Norman, K. (May 1993)Student Evaluation of the Software in the AT&T Teaching Theater HCIL-93-06, CS-TR-3069, CAR-TR-672 [HTML] [Link to Report] Shneiderman, B. (April 1993)Engagement and Construction: Education Strategies for the Post-TV EraComputer Assisted Learning, International Conference on Computers and Learning, (Wolfville, Nova Scotia, Canada, June 17-20, 1992) 39-45. Also Journal of Computing in Higher Education, vol. 4 (2) (Spring 1993) 106-116. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 345-350. HCIL-93-05 [HTML]  [Link to Report] Sears, A. (March 1993)Layout Appropriateness: Guiding User Interface Design with Simple Task Descriptions Dissertation (113p.), see 92-02 (CS-TR-2823) and 92-15(CS-TR-2997) for condensed version. HCIL-93-04 [Link to Report] Shneiderman, B. (Sept. 1993)Beyond Intelligent Machines: Just Do It!IEEE Software, vol. 10, 1 (Jan 1993) 100-103. HCIL-93-03 [HTML]  [Link to Report] Potter, R. (Jan. 1993)Triggers: Guiding Automation with Pixels to Achieve Data AccessWatch What I Do: Programming by Demonstration, Cypher, A., Ed., MIT Press (1993) 360-380. HCIL-93-02, CS-TR-3027, CAR-TR-658 [HTML] [Link to Report] Shneiderman, B. (Jan. 1993)Dynamic Queries: For Visual Information SeekingIEEE Software, vol. 11, 6 (Nov. 1994) 70-77. HCIL-93-01, CS-TR-3022, CAR-TR-655, SRC-TR-93-3. [HTML]  [Link to Report]1992 Carter, L. (October 2006)The Effects of Spatial Information Presentation on Human Decision-Making Ph.D. Dissertation from the Department of Psychology HCIL-92-19 [Link to Report] Shneiderman, B. (June 4, 1992)Socially Responsible Computing I: A Call to Action Following the L.A. Riot Shneiderman, B., Socially Responsible Computing I: A call to action following the L. A. Riots, ACM SIGCHI Bulletin 24, 3 (July 1992), 14-15. Socially Responsible Computing II: First steps on the path to positive contributions, ACM SIGCHI Bulletin 24, 3 (July 1992), 16-17. Reprinted in revised form as a Viewpoint, Communications of the ACM 36, 1 (January 1993), 15-16. HCIL-92-18 [HTML]  [Link to Report] Plaisant, C. (Editor) (June 1992)1992 Human-Computer Interaction Laboratory Video Reports HCIL-92-17, CS-TR-3529, CAR-TR-792 [HTML] [Link to Report] Kuah, B., Shneiderman, B. (Nov. 1992)Providing advisory notices for UNIX command users: design, implementation, and empirical evaluations HCIL-92-16, CS-TR-3007, CAR-TR-651 [Link to Report] Sears, A., Shneiderman, B. (Nov. 1992)Split menus: effectively using selection frequency to organize menusACM Transactions on Computer-Human Interaction, vol. 1, 1 (March 1994) 27-51. HCIL-92-15, CS-TR-2997, CAR-TR-649 [HTML]  [Link to Report] Jungmeister, W., Turo, D. (Nov. 1992)Adapting treemaps to stock portfolio visualization HCIL-92-14, CS-TR-2996, CAR-TR-648, SRC-TR-92-120. [Link to Report] Plaisant, C., Carr, D., Hasegawa, H. (Oct. 1992)When an intermediate view matters: A 2D browser experiment HCIL-92-13, CS-TR-2980, CAR-TR-645, SRC-TR-92-119. [HTML] [Link to Report] Plaisant, C. (May 1992)Touchscreen toggle design Video in CHI `92 Video Program (Monterey, CA, May 3-7, 1992) Available through ACM SIGGRAPH Video Review, issue 77, ACM, New York. A two page video summary appears in CHI' 92 Proceedings, 667-668. Video also available through HCIL as part of the  1991 HCIL Video Report. HCIL-92-12 [Video] [Link to Report] Chimera, R. (May 1992)Value bars: An information visualization and navigation tool for multi-attribute listings, demonstration summary appears in ACM CHI `92 Conference Proc. (Monterey, CA, May 3-7, 1992) 293-294.ACM CHI `92 Conference Proceedings, (Monterey, CA, May 3-7, 1992) 293-294. HCIL-92-11 [Link to Report] Johnson, B. (May 1992)TreeViz: Treemap visualization of hierarchically structured information Demonstration summary appears in ACM CHI `92 Conference Proc. (Monterey, CA, May 3-7, 1992) 369-370. HCIL-92-10 [Link to Report] Karl, L., Pettey, M., Shneiderman, B. (July 1992)Speech versus mouse commands for word processing: an empirical evaluationInternational Journal of Man-Machine Studies, vol. 39, 4 (Oct. 1993) 667-687. HCIL-92-09, CS-TR-2925, CAR-TR-630, SRC-TR-92-86. [HTML] [Link to Report] Norman, K., Carter, L. (May 1992)A preliminary evaluation of the electronic classroom: The AT&T Teaching Theater at the University of Maryland HCIL-92-08, CS-TR-2892, CAR-TR-621 [HTML] [Link to Report] Young, D., Shneiderman, B. (May 1992) A graphical filter/flow representation of boolean queries: a prototype implementation and evaluationJournal of American Society for Information Science, vol. 44, 6, (July 1993) 327-339. HCIL-92-07, CS-TR-2905, CAR-TR-627 [Video] [Link to Report] Turo, D., Johnson, B. (May 1992)Improving the visualization of hierarchies with treemaps: Design issues and experimentationProc. Visualization `92 (Boston, MA, Oct. 19-23,1992) 124-131. HCIL-92-06, CS-TR-2901, CAR-TR-626, SRC-TR-92-62. [HTML] [Link to Report] Carr, D., Hasegawa, H., Lemmon, D., Plaisant, C. (March 1992)The effects of time delays on a telepathology user interfaceProc. of the 16th Annual Symposium on Computer Applications in Medical Care , SCAMC (Baltimore, MD, Nov. 7-11, 1992) 256-260. HCIL-92-05, CS-TR-2874, CAR-TR-616, SRC-TR-92-49. [Link to Report] Rivlin, E., Botafogo, R., Shneiderman, B. (March 1992)Navigating in hyperspace: designing a structure based toolboxCommunications of the ACM, vol. 37, 2, (Feb. 1994) 87-96. HCIL-92-04, CS-TR-2861, CAR-TR-606 [Link to Report] Liao, H., Osada, M., Shneiderman, B. (Feb. 1992)Browsing Unix directories with Dynamic Queries: An evaluation of three information display techniques9th Symposium on Human Interface (Kobe, Japan, Oct. 18-20, 1993) 95-98. HCIL-92-03, CS-TR-2841, CAR-TR-605 [Link to Report] Sears, A. (Dec. 1992)Layout appropriateness: a metric for user interface evaluationIEEE Transactions on Software Engineering, vol. 19, 7 (July 1993) 707-719. HCIL-92-02, CS-TR-2823, CAR-TR-603 [Link to Report] Williamson, C., Shneiderman, B. (Jan. 1992)The dynamic HomeFinder: Evaluating dynamic queries in a real-estate information exploration systemProc. ACM SIGIR `92 (Copenhagen, June 21-24, 1992) 338-346. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 295-307. [Published Version] HCIL-92-01, CS-TR-2819, CAR-TR-602 [HTML] [Video] [Link to Report]1991 Plaisant, C. (Editor) (June 1991)1991 Human-Computer Interaction Laboratory Video Reports HCIL-91-16, CS-TR-3528, CAR-TR-791 [HTML] [Link to Report] Shneiderman, B. (1991)Education by engagement and construction: A strategic education initiative for a multimedia renewal of American education Sociomedia: Multimedia, Hypermedia, and the Social Construction of Knowledge, Barrett, E., Ed., MIT Press (1992) 13-26. HCIL-91-15 [Link to Report] Chimera, R. (Oct. 1991)Value bars: an information visualization and navigation tool for multi-attribute listings and tables HCIL-91-14, CS-TR-2773, CAR-TR-589 [Link to Report] Shneiderman, B., Williamson, C., Ahlberg, C. (Nov. 1991)Dynamic queries: Database searching by direct manipulation Video in CHI `92 Video Program (Monterey, CA, May 3-7, 1992) Available through ACM SIGGRAPH Video Review, issue 77, ACM, New York. A two page video summary appears in CHI' 92 Proceedings, 669-670. Video also available through HCIL as part of the  1992 HCIL Video Report. HCIL-91-13 [Link to Report] Plaisant, C., Sears, A. (Sept. 1991)Touchscreen interfaces for alphanumeric data entryProc. of the Human Factors Society - 36th Annual Meeting, vol. 1, (Atlanta, GA, Oct. 12-16, 1992) 293-297. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 195-204. Also Human Factors Perspectiv es on Human-Computer Interaction,Selections from Proc. of Human Factors and Ergonomics Society Annual Meetings 1983-1994, Perlman, G., Green, G.K., Wogalter, M.S., Eds. (1995) 261-265. HCIL-91-12, CS-TR-2764, CAR-TR-585 [Link to Report] Ahlberg, C., Williamson, C., Shneiderman, B. (Sept. 1991)Dynamic queries for information exploration: An implementation and evaluationACM CHI `92 Conference Proc. (Monterey, CA, May 3-7, 1992) 619-626. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 281-294. [Published Version] HCIL-91-11, CS-TR-2763, CAR-TR-584 [HTML] [Link to Report] Shneiderman, B. (July 1991)Visual user interfaces for information exploration 1991 ASIS Proc., 379-384. HCIL-91-10, CS-TR-2748, CAR-TR-577 [Link to Report] Weiland, W., Shneiderman, B. (July 1991)A graphical query interface based on aggregation/generalization hierarchiesInformation Systems, vol. 18, 4 (1993) 215-232. HCIL-91-09, CS-TR-2702, CAR-TR-562 [Link to Report] Norman, K. (1991)Models of mind and machines, information flow and control between humans and computersAdvances in Computers, vol. 32, M. Yovits, Ed., Academic Press Inc., NY, NY (1991) 201-254. HCIL-91-08 [Link to Report] Sears, A., Revis, D., Swatski, S., Crittenden, R., Shneiderman, B. (April 1991)Investigating touchscreen typing: The effect of keyboard size on typing speedBehavior & Information Technology, vol. 12, 1 (Jan-Feb 1993) 17-22. HCIL-91-07, CS-TR-2662, CAR-TR-553 [Link to Report] Johnson, B., Shneiderman, B. (April 1991)Treemaps: a space-filling approach to the visualization of hierarchical information structuresProc. of the 2nd International IEEE Visualization Conference (San Diego, Oct. 1991) 284-291. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 309-322. [Published Version] HCIL-91-06, CS-TR-2657, CAR-TR-552, SRC-92-62. [HTML] [Video] [Link to Report] Keil-Slawik, R., Plaisant, C., Shneiderman, B. (April 1991)Remote direct manipulation: A case study of a telemedicine workstationHuman Aspects in Computing: Design and Use of Interactive Systems and Information Management, 4th Int. Conf. on HCI (Stuttgart, Sept. 91) 1006-1011. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 51-61. HCIL-91-05, CS-TR-2655, CAR-TR-551 [Link to Report] Botafogo, R., Shneiderman, B. (April 1991)Identifying aggregates in hypertext structuresACM Proc.of Hypertext `91 (San Antonio, TX, Dec. 15-18) 63-74. [Published Version] HCIL-91-04, CS-TR-2650, CAR-TR-550 [Link to Report] Shneiderman, B. (March 1991)Tree visualization with treemaps: a 2-d space-filling approachACM Transactions on Graphics, vol. 11, 1 (Jan. 1992) 92-99. HCIL-91-03, CS-TR-2645, CAR-TR-548 [HTML] [Link to Report] Shneiderman, B. (March 1991)Touch screens now offer compelling usesIEEE Software 8, 2, (March 1991) 93-94, 107. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 187-193. HCIL-91-02  [Link to Report] Chimera, R., Wolman, K., Mark, S., Shneiderman, B. (revised Sept. 1993)An exploratory evaluation of three interfaces for browsing large hierarchical tables of contentsACM Transactions on Information Systems, vol.12., 4 (Oct. 94) 383-406. HCIL-91-01, CS-TR-2620, CAR-TR-539 [Link to Report]1990 Norman, K. (June 1990)The electronic teaching theater: interactive hypermedia and mental models of the classroomCurrent Psychology: Research & Reviews, Summer 90, vol. 9, 2, 141-161. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 133-151. HCIL-90-13 [Link to Report] Shneiderman, B. (Oct. 1990)Protecting rights in user interface designsACM SIGCHI Bulletin, Oct. 1990. Excerpt of this paper also appeared as: Intellectual protection for user interfaces?, Communications of the ACM, 34, 4, (April 1991) 13-14. Also Sparks of Innovation in Human-Computer Interaction, Shneider man, B., Ed., Ablex (June 1993) 351-354. HCIL-90-12 [HTML]  [Link to Report] Botafogo, R., Rivlin, E., Shneiderman, B. (Dec. 1990)Structural analysis of hypertexts: identifying hierarchies and useful metricsACM Transactions on Information Systems, vol. 10, 2, April 1992, 142-180. HCIL-90-11, CS-TR-2574, CAR-TR-526 [Link to Report] Plaisant, C., Shneiderman, B., Battaglia, J. (1990)Scheduling home-control devices: a case study of the transition from the research project to a productHuman Factors in Practice, Computer Systems Technical Group, Human Factors Society (Santa Monica, CA, Dec. 1990) 7-13. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 205-215. HCIL-90-10 [Link to Report] Plaisant, C. (Nov. 1990)Guide to opportunities in volunteer archaeology - case study of the use of a hypertext system in a museum exhibit Hypertext/Hypermedia Handbook, Berk E. & Devlin, J., Eds., McGraw-Hill (1991) 498-505. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 223-229. HCIL-90-09, CS-TR-2559, CAR-TR-523 [Link to Report] Plaisant, C., Wallace, D. (Nov. 1990)Touchscreen toggle switches: push or slide? Design issues and usability study HCIL-90-08, CS-TR-2557, CAR-TR-521 [Video] [Link to Report] Shneiderman, B. (Sept. 1990)Human values and the future of technology: a declaration of responsibility Keynote address for the ACM SIGCAS 90 Conference: Computers and the Quality of Life. Also in the ACM SIGCHI Bulletin (Jan. 1991). Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 337-343. HCIL-90-07 [Link to Report] Sears, A. (revised March 1991)Improving touchscreen keyboards: design issues and a comparison with other devicesInteracting with Computers, vol. 3, 3 (1991) 253-269. HCIL-90-06, CS-TR-2536, CAR-TR-515 [Link to Report] Jones, T., Shneiderman, B. (July 1990)Examining usability for a training oriented hypertext: can hyper-activity be good?Electronic Publishing, vol. 3 (4) (Nov. 1990) 207-225. HCIL-90-05, CS-TR-2499, CAR-TR-509 [Link to Report] Butler, S. (June 1990)The effect of method of instruction and spatial visualization ability on the subsequent navigation of a hierarchical database HCIL-90-04, CS-TR-2398, CAR-TR-488 [Link to Report] Lifshitz, J., Shneiderman, B. (March 1990)Window control strategies for hypertext traversal: an empirical study Proc. 29th Annual ACM DC Technical Symposium (June 1991). HCIL-90-03, CS-TR-2356, CAR-TR-475 [Link to Report] Shneiderman, B., Plaisant, C., Botafogo, R., Hopkins, D., Weiland, W. (revised May 1991)Designing to facilitate browsing: a look back at the Hyperties work station browserHypermedia, vol. 3, 2 (1991)101-117. Based on Visual engagement and low cognitive load in browsing hypertext. HCIL-90-02, CS-TR-2433, CAR-TR-494 [Link to Report] Sears, A., Plaisant, C., Shneiderman, B. (June 1990)A new era for high-precision touchscreensAdvances in Human-Computer Interaction, vol. 3, Hartson, R. & Hix, D. Eds., Ablex (1992) 1-33. HCIL-90-01, CS-TR-2487, CAR-TR-506 [Link to Report]1989 Furuta, R., Plaisant, C., Shneiderman, B. (Dec. 1989)Automatically transforming regularly structured linear documents into hypertextElectronic Publishing - Origination, Dissemination and Design, vol. 2, 4 (1990) 211-229. HCIL-89-20 [Link to Report] Koivunen, M. (Sept. 1989)WSE: an environment for exploring window strategiesProc. of Eurographics'90 (North-Holland, 1990) 495-506. HCIL-89-19, CS-TR-2353, CAR-TR-473 [Link to Report] Plaisant, C., Shneiderman, B. (revised Feb. 1991)Scheduling home control devices: design issues and usability evaluation of four touchscreen interfacesInternational Journal of Man-Machine Studies (1992) 36, 375-393. [Published Version] HCIL-89-18, CS-TR-2352, CAR-TR-472 [Video] [Link to Report] Sears, A., Shneiderman, B. (June 1989)High precision touchscreens: design strategies and comparisons with a mouseInternational Journal of Man-Machine Studies, (1991) 34, 4, 593-613. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 171-185. HCIL-89-17, CS-TR-2268, CAR-TR-450 [Link to Report] Faloustos, C., Lee, R., Plaisant, C., Shneiderman, B. (June 1989)Incorporating string search in a hypertext system: user interface and signature file design issuesHypermedia, vol. 2, 3 (1991). HCIL-89-16, CS-TR-2266, CAR-TR-448 [Link to Report] Furuta, R., Plaisant, C., Shneiderman, B. (May 1989)A spectrum of automatic hypertext constructionsHypermedia, vol. 1, 2 (1989) 179-195. HCIL-89-15, CS-TR-2253, CAR-TR-443 [Link to Report] Plaisant, C. (May 1989)Semi-automatic conversion to a hypertext database, the case study of the NCR management college course catalog HCIL-89-14 [Link to Report] Shneiderman, B. (Sept. 1989)Future directions for human-computer interactionProc. Human-Computer Interaction '89 (Boston, Sept. 18-22, 1989). Also Designing and Using Human-Computer Interfaces and Knowledge Based Systems, Salvendy, G. & Smith, M. J. Eds., Elsevier Science B.V. Also International Journal of Huma n-Computer Interaction (1990) 2 (1) 73-90. [Published Version] HCIL-89-13, CS-TR-2235, CAR-TR-436 [HTML] [Link to Report] Weiland, W., Shneiderman, B. (Aug. 1989)Interactive graphics interfaces in hypertext systemsProc. 28th Annual ACM DC Technical Symposium, 23-28. HCIL-89-12, CS-TR-2267, CAR-TR-449 [Link to Report] Shneiderman, B. (1989)Intelligent interfaces: from fantasy to factProc. IFIP 11th World Computer Congress, (San Francisco, CA, Aug. 28-Sept. 1, 1989). HCIL-89-11 [Link to Report] Hobbs, J., Shneiderman, B. (1989)Design, implementation, and evaluation of automatic spelling correction for UNIX commands HCIL-89-10, CS-TR-2243, CAR-TR-440 [Link to Report] Jones, T. (May 1989)Incidental learning during information retrieval: a hypertext experimentProc. International Conference on Computer-Assisted Learning, Springer Verlag (Berlin) 235-253. HCIL-89-09 [Link to Report] Shneiderman, B., Brethauer, D., Plaisant, C., Potter, R. (May 1989)Evaluating three museum installations of a hypertextJournal of the American Society for Information Science, 40(3) 172-182. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 231-250. HCIL-89-08 [Link to Report] Mitchell, J., Shneiderman, B. (April 1989)Dynamic versus static menus: an exploratory comparisonACM SIGCHI Bulletin, 20(4) (1989) 33-37. HCIL-89-07 [Link to Report] Shneiderman, B., Kearsley, G. (1989)Hypertext Hands-On! Available through Addison-Wesley,192 pages + 2 PC disks. HCIL-89-06 [Link to Report] Shneiderman, B. (April 1989)A nonanthropomorphic style guide: overcoming the humpty dumpty syndrome The Computing Teacher, 16(7), (1989) 5. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 331-335. HCIL-89-05 [HTML]  [Link to Report] Seabrook, R., Shneiderman, B. (April 1989)The user interface in a hypertext, multi-window program browserInteracting with Computers, 1(3) (1989) 299-337. HCIL-89-04, CS-TR-2237, CAR-TR-437 [Link to Report] Sears, A., Kochavy, Y., Shneiderman, B. (1989)Touchscreen field specification for public access database queries: let your fingers do the walkingProc. of the ACM Computer Science Conference `90 (Feb. 1990) 1-7. HCIL-89-03 [Link to Report] Norman, K., Butler, S. (Jan. 1989)Search by uncertainty: menu selection by target probability HCIL-89-02, CS-TR-2230, CAR-TR-432 [Link to Report] Shneiderman, B. (1989)Reflections on authoring, editing, and managing hypertextThe Society of Text, Barrett, Ed., MIT Press (1989) 115-131. HCIL-89-01, CS-TR-2160, CAR-TR-410 [Link to Report]1988 Wallace, D., Norman, K., Plaisant, C. (Sept. 1988)The american voice and robotics "guardian" system: a case study in user interface usability evaluation HCIL-88-10, CS-TR-2113, CAR-TR-392 [Link to Report] Potter, R., Berman, M., Shneiderman, B. (Nov. 1988)An experimental evaluation of three touchscreen strategies within a hypertext databaseInternational Journal of Human-Computer Interaction,1(1) (1989) 41-52. HCIL-88-09, CS-TR-2141, CAR-TR-405 [Link to Report] Chin, J. (Oct. 1988)A dynamic user adaptable menu system: linking it all together HCIL-88-08, CS-TR-2120, CAR-TR-396 [Link to Report] Norman, K., Mantel, W., Wallace, D. (Oct. 1988)User's guide to the menu selection prototyping system HCIL-88-07, CS-TR-2114, CAR-TR-393 [Link to Report] Shneiderman, B. (1988)We can design better user interfaces: a review of human-computer interaction stylesProc. International Ergonomics Association 10th Congress 31, vol. 5 (Sydney, Australia, Aug. 1-5, 1988) 699-710. HCIL-88-06 [Link to Report] Kreitzberg, C., Shneiderman, B. (1988)Restructuring knowledge for an electronic encyclopediaProc. International Ergonomics Association 10th Congress 31, vol. 2, (Sydney, Australia, Aug. 1-5, 1988) 615-620. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed. , Ablex (June 1993) 123-131. HCIL-88-05 [Link to Report] Potter, R., Weldon, L., Shneiderman, B. (May 1988)Improving the accuracy of touch screens: an experimental evaluation of three strategiesProc. of the Conference on Human Factors in Computing Systems, CHI `88 (Washington, DC) 27-32. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 161-169. [Published Version] HCIL-88-04 [Link to Report] Chin, J., Norman, K. (June 1988)Declarative and procedural knowledge in menu systems: diagramming cognitive maps of phone and ATM commands HCIL-88-03, CS-TR-2053, CAR-TR-366 [Link to Report] Wang, X., Liebscher, P., Marchionini, G. (Jan. 1988)Improving information seeking performance in hypertext: roles of display format and search strategy HCIL-88-02, CS-TR-2006, CAR-TR-353 [Link to Report] Marchionini, G., Shneiderman, B. (Jan. 1988)Finding facts vs. browsing knowledge in hypertext systemsIEEE Computer, 21, 1, 70-80. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 103-121. HCIL-88-01 [Link to Report]1987 Norman, K., Schwartz, J. (1987)Memory for hierarchical menus: effects of study modeBulletin of the Psychonomic Society, 25, 163-166. HCIL-87-14 [Link to Report] Wallace, D., Anderson, N., Shneiderman, B. (Oct. 1987)Time stress effects on two menu selection systemsProc. of the 31st Annual Meeting - Human Factors Society, (NY, NY) 727-731. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 89-97. Also Human Factors Perspectives on Human-Computer Interaction,Selections from Proc. of Human Factors and Ergonomics Society Annual Meetings 1983-1994, Perlman, G., Green, G.K., Wogalter, M.S., Eds. (1995) 105-109. HCIL-87-13 [Link to Report] Norman, K., Chin, J. (Oct. 1987)The menu metaphor: food for thoughtBehavior and Information Technology, 8, 125-134. HCIL-87-12, CS-TR-1944, CAR-TR-334 [Link to Report] Chin, J., Diehl, V., Norman, K. (Sept. 1987)Development of an instrument measuring user satisfaction of the human-computer interfaceProc. ACM CHI '88 (Washington, DC) 213-218. [Published Version] HCIL-87-11, CS-TR-1926, CAR-TR-328 [Link to Report] Callahan, J., Hopkins, D., Weiser, M., Shneiderman, B. (Sept. 1987)An empirical comparison of pie vs. linear menusProc. ACM CHI '88 (Washingotn, DC) 95-100. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 79-88. HCIL-87-10, CS-TR-1919 [Video] [Link to Report] Ostroff, D., Shneiderman, B. (Sept. 1987)Selection devices for users of an electronic encyclopedia: an empirical comparison of four possibilitiesInformation Processing & Management, vol. 24, 6, 665-680. HCIL-87-09, CS-TR-1910, CAR-TR-321 [Link to Report] Shneiderman, B. (Aug. 1987)User interface design and evaluation for an electronic encyclopediaProc. of the 2nd International Conference on Human-Computer Interaction, (Honolulu, HI, Aug. 1987). Cognitive Engineering in the Design of Human-Computer Interaction and Expert Systems, G. Salvendy, Ed., Elsevier (1987) 207-223. HCIL-87-08, CS-TR-1819, CAR-TR-280 [Link to Report] Lifshitz, J., Shneiderman, B. (July 1987)Window control strategies for on-line text traversal Computer Science Internal Report. HCIL-87-07 [Link to Report] Chin, J., Norman, K., Shneiderman, B. (July 1987)Subjective user evaluation of CF Pascal programming tools HCIL-87-06, CS-TR-1880, CAR-TR-304 [Link to Report] Chin, J. (July 1987)Top-down and bottom-up processes in sorting computer menu system commands HCIL-87-05, CS-TR-1879, CAR-TR-303 [Link to Report] Margono, S., Shneiderman, B. (June 1987)A study of file manipulation by novices using commands vs. direct manipulation26th Annual Technical Symposium Washington DC Chapter of the ACM, (Gaithersburg, MD, June 11, 1987) 154-159. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 39-50. HCIL-87-04, CS-TR-1775, CAR-TR-264 [Link to Report] Shneiderman, B. (Feb. 1987)A taxonomy and rule-base for the selection of interaction stylesHuman Factors for Informatics Usability, Shackel, B. & Richardson, S., Eds., Cambridge University Press, 325-342. Also Readings in Human-Computer Interaction: Toward the Year 2000, Baecker, R.M., Grudin, J. , Buxton, W.A.S. & Greenberg, S., Eds., Morgan Kaufmann Pubs., Inc. (1995) 401-410. HCIL-87-03, CS-TR-1776, CAR-TR-265 [Link to Report] Mills, C., Weldon, L. (1987)Reading text from computer screensACM Computing Surveys, 19 (4), 329-358. HCIL-87-02, CS-TR-1449, CAR-TR-94 [Link to Report] Shneiderman, B. (1987)User interface design for the Hyperties electronic encyclopediaProc. Hypertext '87, 199-205. See 86-09 for previous version. [Published Version] HCIL-87-01 [Link to Report]1986 Laverson, A., Norman, K., Shneiderman, B. (1986)An evaluation of jump-ahead techniques for frequent menu usersBehaviour and Information Technology, 6, 2 (1987) 97-108. HCIL-86-11, CS-TR-1591, CAR-TR-168 [Link to Report] Baroff, J., Simon, R., Gilman, F., Shneiderman, B. (Dec. 1986)Direct manipulation user interfaces for expert systemsExpert Systems: The User Interface, J. Hendler, Ed., Ablex (1987) 101-127. HCIL-86-10, CS-TR-1745, CAR-TR-244 [Link to Report] Morariu, J., Shneiderman, B. (Nov. 1986)Design and research on The Interactive Encyclopedia System (TIES)Proc. 29th Conference of the Association for the Development of Computer Based Instructional Systems, 19-21. See 87-01 for revised version. HCIL-86-09 [Link to Report] Reisel, J., Shneiderman, B. (Oct. 1986)Is bigger better? The effects of display size on program readingErgonomic and Stress Aspects of Work with Computers, G. Salvendy, S. L. Sauter, & J. J. Hurrell, Jr., Eds., Elsevier (Aug. 1987) 113-122. HCIL-86-08, CS-TR-1722, CAR-TR-231 [Link to Report] Norman, K., Weldon, L., Shneiderman, B. (Aug. 1986)Cognitive layouts of windows and multiple screens for user interfacesInternational Journal of Man-Machine Studies, 25, 229-248. HCIL-86-07, CS-TR-1498, CAR-TR-123 [Link to Report] Shneiderman, B., Shafer, P., Simon, R., Weldon, L. (May 1986)Display strategies for program browsing: concepts and an experimentIEEE Software 3, 3 (May 1986) 7-15. HCIL-86-06, CS-TR-1635, CAR-TR-192 [Link to Report] Ostroff, D. (May 1986)Selection systems: interactive devices and strategies 161 page masters thesis, see 87-09 for condensed published version. HCIL-86-05 [Link to Report] Koved, L., Shneiderman, B. (April 1986)Embedded menus: selecting items in contextCommunications of the ACM 29, 4, 312-318. Also (Aug. 13, 1985), IBM Research Report RC 11310. Reprinted in Hebrew in Maaseh-Hoshev. Also Sparks of Innovation in Human-Computer Interaction, Shneiderman, B., Ed., Ablex (June 1993) 67-77. HCIL-86-04, CS-TR-1562, CAR-TR-153 [Link to Report] Shneiderman, B. (March 1986)Designing menu selection systemsJournal of American Society for Information Science, 37, 2, 57-70. HCIL-86-03 [Link to Report] Shneiderman, B. (Feb. 1986)Empirical studies of programmers: the territory, paths, and destinations Keynote address for workshop, Empirical Studies of Programmers, E. Soloway & R. Iyengar, Eds., Ablex (June 1986) 1-12. HCIL-86-02, CS-TR-1623, CAR-TR-187 [HTML] [Link to Report] Ewing, J., Mehrabanzad, S., Sheck, S., Ostroff, D., Shneiderman, B. (Jan. 1986)An experimental comparison of a mouse and arrow-jump keys for an interactive encyclopediaInternational Journal of Man-Machine Studies, 24, 1, 29-45. HCIL-86-01 [Link to Report]1985 Koved, L. (July 1985)Restructuring textual information for online retrieval masters thesis HCIL-85-04, CS-TR-1529, CAR-TR-133 [Link to Report] Schwartz, J., Norman, K., Shneiderman, B. (March 1985)Performance on content free menus as a function of study method HCIL-85-03, CS-TR-1477, CAR-TR-110 [Link to Report] Parton, D., Huffman, K., Pridgen, P., Norman, K., Shneiderman, B. (1985)Learning a menu selection tree: training methods comparedBehaviour and Information Technology, 4, 2, 81-91. HCIL-85-02 [Link to Report] Weldon, L., Mills, C., Koved, L., Shneiderman, B. (1985)The structure of information in online and paper technical manualsProc. Human Factors Society - 29th Annual Conference (Santa Monica, CA) 1110-1113. HCIL-85-01 [Link to Report]1984 Norman, K., Schwartz, J., Shneiderman, B. (May 1984)Memory for menus: effects of study mode (revised 1987, see 87-14 Memory for hierarchical menus: effects of study mode). HCIL-84-01, CS-TR-1412, CAR-TR-69 [Link to Report]1983 Shneiderman, B. (August 1983)Direct Manipulation: A Step Beyond Programming LanguagesIEEE Computer 9, 4 (August 1983), 57-69. Please contact Ben Shneiderman (ben@cs.umd.edu) for a copy of this paper. HCIL-83-01 [Link to Report]With the newly developed enthusiasm for RDF as the basis for library bibliographic data we are seeing a number of efforts to transform library data into this modern, web-friendly format. This is a positive development in many ways, but we need to be careful to make this transition cleanly without bringing along baggage from our past.Recent efforts have focused on translating library record formats into RDF with the result that we now have: ISBD in RDF FRBR in RDF RDA in RDFand will soon have MODS in RDFIn addition there are various applications that convert MARC21 to RDF, although none is "official." That is, none has been endorsed by an appropriate standards body.Each of these efforts takes a single library standard and, using RDF as its underlying technology, creates a full metadata schema that defines each element of the standard in RDF. The result is that we now have a series of RDF silos, each defining data elements as if they belong uniquely to that standard. We have, for example, at least four different declarations of "place of publication": in ISBD, RDA, FRBR and MODS, each with its own URI. There are some differences between them (e.g. RDA separates place of publication, manufacture, production while ISBD does not) but clearly they should descend from a common ancestor: RDA: place of publicationRDA: place of distributionRDA: place of manufactureFRBRer: has place of publication or distributionISBD: has place of publication, production, distribution This would be annoying, but not unworkable, if these different instances of "place of publication" could be treated as having some meaning in common such that one could link a FRBRer element to an ISBD element, but they cannot. The reason they cannot is that each of these constrains the elements in a particular way that defines its relationship to a single data context (what we generally think of as a "record structure"). The elements are not independent of that context, and this means that each can only be used within that particular context. This is the antithesis of the linked data concept, where data sets from diverse sources share metadata elements. It is this re-use of elements that creates the "link" in linked data. To achieve this, metadata elements need to be unconstrained by a particular context.Linking can also be achieved through vertical relationships, similar to "broader" and "narrower" in thesauri. This is less direct, but makes it possible to mix data sets that have differing levels of granularity. In our case, the ISBD "place of publication, production, distribution" could be defined as broader to the three RDA elements that treat those separately. Unfortunately that is not possible because of the way that ISBD and RDA have been defined in RDF. (I'll post more detail about this later for those who want more.)The result is that we now have a series of RDF silos, expressions of our data in RDF that lack the linking capabilities of linked data because they are bound to specific data structures. Clearly we gain little in terms of linked data by creating mutually incompatible bibliographic views. Not only are these RDF schemes not compatible with each other, none will be linkable to bibliographic data from communities outside of libraries who published their data on the Web. That means no linking to Amazon, to Wikipedia, to citations within documents.Given where we are in the development of linked data for libraries, we now have two options:1) Define 'super-elements' that float above the record formats and that are not bound by the constraints of the RDF-defined records. In this case there would be a general "place of publication" that is super- to all of the "place of publication" elements in the various records, and would be subordinate to a general concept of "place" that is widely used (possibly a property of GeoNames). To implement linking, each record element would be extrapolated to its super elements.2) Define our data elements outside of any particular record format first, then use these in the record schemas. In this case there would be only one instance of "place of publication" and it would be used throughout the various bibliographic records whenever an element with that meaning is needed. Those records would be interchangeable as linked data using their component data elements, and would interact with other bibliographic data on the Web using the RDF-defined elements and their relationships.My message here is that we need to be creating data, not records, and that we need to create the data first, then build records with it for those applications where records are needed. Those records will operate internally to library systems, while the data has the potential to make connections in linked data space. I would also suggest that we cease creating silo'd RDF record formats, as these will not move us forward. Instead, we should concentrate on discovering and defining the elements of our data, and begin looking outward at all of the data we want to link to in the vast information universe. _____ * Note on RDA: RDA in RDF includes two "versions" of each data element: one bound to FRBR and one not. The latter has potential for re-use outside of a FRBR environment, and was designed for this purpose by the DCMI/RDA task force. Its relationship to "official" RDA is somewhat unclear at this time but hopefully will gain support as the linked data concept is absorbed into the bibliographic framework.Infrastructures are installations and services that function as “mediating interfaces” or “structures ‘in between’ that allow things, people and signs to travel across space by means of more or less standardized paths and protocols for conversion or translation.” A digital research infrastructure is no different: it’s a mediating set of technologies for research and resource discovery, collaboration, sharing and dissemination of scientific output.Infrastructures, however, are also strong cultural and political symbols.From electricity systems in the 1920s, to coal trains in the 1950s, through to the gateways and bridges on Euro notes in the present decade, infrastructures have been mobilized repeatedly in broader spheres as symbols and metaphors for broader forms of modernization, integration and co-operation. (Badenoch and Fickers, 2)Infrastructure projects have always been seen as motors of change propelling society into a better and brighter future. Yet precisely because those “human-built material links between nations and across borders in Europe… predated, accompanied and transcended the ‘offical’ processes of political and economic integration” (ibid., 1), it would be all too tempting — and all too easy — to approach the question of digital research infrastructures uncritically by getting caught in the moment and embracing the master narratives of efficiency and progress without discussing the larger and more complex implications of institutionalizing networked research. A digital infrastructure is not only a tool that needs to be built: it is also a tool that needs to be understood.The entire field of digital humanities is evolving against the backdrop of global capitalism in its electronic mode, the so-called “eEmpire”, which is sustained by “a loose assemblage of relations characterized by… flexibility, functionality, mobility, programmability, and automation.” It would be naive to think that our field is immune to economic and ideological tensions that characterize information capitalism. It would even more naive to think that we can build expensive, transnational digital research infrastructures that will function in some abstract networked space unburdened by politics and ideology.Digital research networks à la DARIAH are part of a transnational history of materializing Europe. Which means that their importance extends beyond strictly scholarly work. I would therefore like to suggest some general, non-technical questions that those of us who are in one way or the other involved in the creation of digital infrastructures should try to keep in the back (and, occasionally, the front) of our minds.1. What is the political capital of a digital infrastructure? What is the extent of its sovereignty? And how can we make sure that the digital infrastructure — not even the one one we are trying to build now, for ours are baby steps, but the future one, the one we hope to see built one day — does not turn from being a power grid into a grid of (hegemonic) power?2. Infrastructures, in general, have a tendency to disappear out of sight: once the novelty of their implementation wears off, they tend to become invisible or self-evident, inscribed as “a kind of objective unconscious in our lives.” As we build our digital infrastructures today, we need to prepare for their “disappearance” tomorrow. We need to think about what type of inherent cultural values and what type of control mechanisms we are programming into digital infrastructures as public institutions before we start taking them for granted.3. The logic of infrastructures is the logic of industrial society: it is based on normativity, mass production, serialization and, ultimately, social discipline. As we build a digital infrastructure for the humanities, how do we make sure we don’t end up locking ourselves in, disciplining ourselves to the point that technical protocols become our only destiny, the limits of our intellectual horizons?4. When infrastructures remain visible, they usually do so by their absence: in places where they do not exist and where their lack is a very clear indicator of large-scale social inequalities and injustices. We should ask ourselves about the implications of digital infrastructure projects for the dynamics between those who are in and who are out. Can we create a truly European infrastructure? When will be a good time to start thinking beyond Europe? What are the actual, physical limits of a scientific infrastructure?I do not mean to imply that we should engage in collective navel-gazing to the point at which we can’t actually build anything. I just think we shouldn’t shy away from making things slightly more difficult for ourselves.A few months ago, Science published a Thanksgiving article on what scientists can be grateful for. It’s got a lot of good points, like being thankful for family members who accept the crazy hours we work, or for those really useful research projects that make science cool enough for us to get funding for the merely really interesting. It does have one unfortunate reference to humanists:We are thankful that Ph.D. programs in the sciences, as much as we complain about them, aren’t nearly as horrifying as, say, Ph.D. programs in the humanities. I just heard today from a friend in his ninth year of a comparative literature Ph.D. who thinks he might finish “in a year and a half.” At least the job market for comp lit Ph.D. awardees is thriving, right?Ouch. I suppose the truth hurts. The particularly interesting point that inspired this post, however, was:We are thankful for that one colleague who knows statistics. There’s always one.A Scientist's Thanksgiving. (Image from the above Science article)The above quote about statisticians is so true it hurts, as (we just discovered) the truth is wont to do. It’s even more true in the humanities than it is in the more natural and quantitative sciences. When we talk about a colleague who knows statistics, we generally don’t mean someone down the hall; usually, we mean that one statistician who we met in the pub that one night and has a bizarre interest in the humanities. That’s not to say humanist statisticians don’t exist, but I doubt you’re likely to find one in any given humanities department.This unfortunately is not only true of statistics, but also of GIS, network science, computer science, textual analysis, and many other disciplines we digital humanists love to borrow from. Thankfully, the NEH ODH’s Institutes for Advanced Topics in the Humanities, UVic’s Digital Humanities Summer Institutes, and other programs out there are improving our collective expertise, but a quick look for GIS/Stats/SNA/etc. courses in most humanities departments still produces slim pickings.Math is scary. (I can't find attribution, sorry. Anybody know who drew this?)One of the best things to come out of the #hacker movement in the Digital Humanities has been the spirit to get our collective hands dirty and learn the techniques ourselves. It’s been a long time coming, and happier days are sure to follow, but one skill still seems underrepresented from the DH purview: statistics.In a recent post by Elijah Meeks, he called Text Analysis, Spatial Analysis, and Network Analysis the “three pillars” of DH research, with a sneaking suspicion that Image Analysis should fit somewhere in there as well. This seems to be the converging sentiment in most DH circles, and although when asked most would say statistics is also important, it still doesn’t seem to be among the first subjects named.With another round of Digging Into Data winners chosen, and a bevy of panels and presentations dedicating themselves to Big Data in the Humanities, the first direction we should point is statistics. Statistics is a tool uniquely built for understanding lots of data, and it was developed with full knowledge that the data may be incomplete, biased, or otherwise imperfect, and has legitimate work-arounds for most such occasions. Of course, all the caveats in my first Networks Demystified post apply here: don’t use it without fully understanding it, and changing it where necessary.http://vadlo.com/cartoons.php?id=71Many Humanists, even digital ones, frequently seem to have a (justifiably) knee-jerk reaction to statistics. If you’ve been following the Twitter and blog conversations about AHA 2012, you probably caught a flurry of discussion over Google Ngrams. Conversation tended toward horrified screams of the dangers of correlation vs. causation (or at least references to xkcd), and the ease with which one might lie via statistics or omission. These are all valid cautions, especially where ngrams is concerned, but I sometimes fear we get so caught up in bad examples that we spend more time apologizing for them than fixing them. Ted Underwood has a great post about just this, which I will touch on again shortly. (And, to Ted and Allen specifically, I’m guessing you both will enjoy this post.)In short: statistics is useful. To quote the above-linked xkcd comic:Correlation doesn’t imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing ‘look over there’.So how do we go about using statistics? In a comment on Ted’s recent post about statistics, Trevor Owens wrote:if you just start signing up for statistics courses you are going to end up getting a rundown on using t-tests and ANOVAs as tools for hypothesis testing. The entire hypothesis testing idea remains a core part of how a lot of folks in the social sciences think about things and it is deeply at odds with what humanists want to do.The key is not appropriation but adaption. We must learn statistics, even the hypothesis testing, so that we might find what methods are useful, what might be changed, and how we can get it to work for us. We’re humanists. We’re really good at methodological critique.One of the areas of statistics most likely to bear fruit for humanists is Bayesian statistics. Some of us already use it in our text mining algorithms, although the math involved remains occult to most. It basically builds uncertainty and belief directly into statistics. Instead of coming up with one correct answer, Bayesian analysis often yields a range of more or less probable answers depending what seems to be the case from prior evidence, and can update and improve that range as more is learned.The one XKCD comic nobody seems to have linked to. (http://xkcd.com/892/)For humanists, this importance is (at least) two-fold. Ted Underwood sums up the first reason nicely:[Bayesian inference] is amazingly, almost bizarrely willing to incorporate subjective belief into its definition of knowledge. It insists that definitions of probability have to depend not only on observed evidence, but on the “prior probabilities” that we expected before we saw the evidence. If humanists were more familiar with Bayesian statistics, I think it would blow a lot of minds.The second and more specific reason worth mentioning here deals with the ranges I discussed above. If a historian, for example, is trying to understand how and why some historical event happened, Bayesian analysis could yield which set of occurrences were more or less likely, and which were so far off as to not be worth considering. By trying to find reasonable boundary conditions rather than exact explanations to answer our questions, humanists can retain that core knowledge that humans and human situations are not wholly deterministic machines, who all act the same and reproduce the same results in every situation.We are intrinsically and inextricably inexact, and until we get computers that see and remember everything, and model it all perfectly, we should avoid looking for exact answers. Bayesian statistics, instead, can help us find a range of reasonable answers, with full awareness and use of the beliefs and evidence we have going in.After I read that post about a scientist’s thanksgiving, I realized I didn’t want to have to rely on that one colleague who knows statistics. Nobody should. That’s why I decided to enroll in a Bayesian Data Analysis course this semester, taught by and using the book of John K. Kruschke. It’s a very readable book, directed toward people with no prior knowledge in statistics or programming, and takes you through the basics of both. Kruschke’s got a blog worth reading, as does Andrew Gelman, an author of the book Bayesian Data Analysis. I’m sure a basic Google search can point you to video lectures, if that’s your thing. I’ll also try to blog about it over the coming months as I learn more.There are several (occasionally apocryphal) anecdotes about the great theoretical physicists of the early 20th century needing to go back to school to learn basic statistics. Some still weren’t terribly happy about it (“God does not play dice with the universe”), but in the end, pressures from the changing nature of their theories required a thorough understanding of statistics. As humanists begin to deal with a glut of information we never before had access to, it’s time we adapt in a similar fashion.The wide angle, the distant reading, the longue durée will all benefit from a deeper understanding of statistics. That knowledge, in tandem with traditional close reading skills, will surely become one of the pillars of humanities research as Big Data becomes ever-more common. Welcome to the scottbot irregular. My name’s Scott, and the US Government has for some reason seen fit to give me money to study Science. It’s ‘Science’ with a capital ‘S’ because I’m not studying individual aspects of the world using science, but rather studying Science in general as a social, historical, philosophical, and intellectual phenomenon. What’s worse, I’m attempting to do it scientifically. This blog is my attempt at giving the country its money’s worth. Also, I kinda would love feedback on my eventual dissertation. See? Everybody wins.scott b. weingartis pretty clueless about a lot of things. This is his attempt to be less so.I pledge to be a good scholarly citizen. This includes:Opening all data generated by me for the purpose of a publication at the time of publication.Opening all code generated by me for the purpose of a publication at the time of publication.Freely distributing all published material for which I have the right, and fighting to retain those rights in situations where that is not the case.Fighting for open access of all materials worked on as a co-author, participant in a grant, or consultant on a project.I pledge to support open access by:Only reviewing for journals which plan to release their publications openly.Donating to free open source software initiatives where I would otherwise have paid for proprietary software.Citing open publications if there is a choice between two otherwise equivalent sources.I pledge never to let work get in the way of play.I pledge to give people chocolate occasionally if I think they’re awesome._[This is somewhat out of date. Please stand by for new information!]Hello World!Student of History & Philosophy of Science and Information Science at Indiana University.You’ve managed to stumble across my little corner of the internet. I’m currently a student and researcher in the HPS and SLIS departments at IUB under two of the most interesting and capable professors I’ve had the fortune to meet: Colin Allen and Katy Börner. I studied history of science and computer engineering at UF, where I slaved researched for the infinitely patient Robert A. Hatch, who taught me more in four short years than I’d yet learned in aggregate over my entire life.Early InPhO Concept MapThese days, I split my time between classes, the Indiana Philosophy Ontology Project (InPhO) and the Cyberinfrastructure for Network Science Center (CNS). At InPhO I program and design visual, navigable representations of our dynamically generated taxonomy of ideas; analyze relational networks (influenced, disagreed with, etc.) from our Thinkers database; and map and compare philosophical ontologies. The CNS keeps me busy with all sorts of scientometric analyses, and I am also involved in the development of large scale network analysis software such as the NWB, creating workflows, providing software feedback, writing documentation and teaching workshops.Co-authorship network created using the Network Workbench ToolResearchHow do changes in communication structures and technologies affect scientific discourse and collaboration?Science is totally rad. So I study it.There are all sorts of ways to study science, of course, and you can’t leave out even one if you want to understand Science as a whole. That means taking a look at its philosophy, history, anthropology, culture and all sorts of other things as well (perhaps even sociology!). It also means looking at (gasp) the science itself, because no self-respecting scholar should claim to understand physics and physicists without being able to calculate the distance the bullet travels before it falls.My overarching research is in modeling and mapping the growth of science on a large scale – thematically, geographically and temporally – hoping eventually to reveal what conditions yield the most rapid rate of discovery and innovation. Looking back, we see times when scientific progress lurches forward at alarming rates, times when studies come to a halt, times when great minds exposit to deaf ears. Sometimes the reasons are obvious: burned libraries, overthrown empires, new sources of funding, technological breakthroughs, wars that need to be won. But these are heavy brush-strokes painted across the canvas of history.If we could somehow view the whole of scientific endeavors for the last thousand years, across every topic and in every city, with the same fine granularity used to research modern-day science, imagine how much we could learn. By zooming out and looking for “hot spots” of innovation in the history of science, and by understanding the environment in which these hot spots formed, we can learn how to induce those same ideal conditions in modern day research.If the synthesis of new ideas in physics tends to come from young researchers working on their own and with backgrounds in other fields, funds can be allotted to make sure more of those exist. If medical innovations come fastest when small groups of experts collaborate, or if science in general runs smoother in small-world type collaborative networks rather than completely connected networks, that information can be used to focus funding in just the right way to improve the rate of innovation.The closest we can come to that fine granularity, to understanding science across contexts, is by using as many research tools as we can find. We must be comfortable working in whatever discipline with whatever methodology is necessary to find the answers sought. Huge historical data sets will be a must. Scientometricians and others in related fields do an amazing job of learning the structure of modern science, but that structure is necessarily bound to the mediums it inhabits. Modern science is a beast of national laboratories, e-mails, universities, cited journals, click-throughs, conferences and page hits.Marshall McLuhan may or may not have been correct when he claimed “the medium is the message,” but there is no doubt that the medium plays a large role in how science is adopted, disseminated and studied. That role cannot be understood without stepping back and viewing all of the alternatives – correspondences, scientific societies, book transcriptions, etc.Dutch Republic of Letters created in collaboration with The Huygens InstituutThe task, then, is to collect as much data as possible, as far back as we can. We should track where books traveled within Medieval Europe and Asia; who corresponded with whom, how often, and about what during the Early Modern period; who taught whom and where scientists studied; how many books were published in what languages; what universities had copies of which journals; where shared resources traveled.This is an impossible amount of data, of course, and can only exist if created collaboratively and in the spirit of openness. These are not ideas to be copyrighted – they are numbers and data points, and they should be accessible and compatible and aggregated in one place. A History of Science Data Commons, so to speak. More on that project coming soon.Trying to understand all of it at once is a big task… and absolutely impossible.  I’ve sliced myself two pieces of the pie that are hopefully manageable and definitely inseparable:Periods of rapid scientific production and progress.Inflection points in scientific communication and collaboration.Changes in communication structures and technologies obviously affect scientific progress deeply, and it is exactly what those effects are that I hope to uncover. Scientific revolutions and media revolutions, what a tired subject! Well, perhaps, but there are two very good reasons they’re overstudied: they’re terribly important, and nobody’s got them right yet.InterestsCourtney and I contact jugglingThankfully for my friends and family I do not work 24/7. When not working, I can often be found juggling, attending renaissance festivals, geocaching, camping, campaigning for rationality, and reading science fiction & fantasy novels. When I feel guilty about not working, but not enough to actually get back to work, I read about physics, cognitive science and linguistics. I am also perpetually writing a history of the obscure art of contact juggling.Juggling has been a big part of my life for nearly a decade now; I was president of Objects in Motion (UF Juggling Club) for a few years and brought the club from 3 to 30 active members, taught lessons at Groovolution dance studio, and performed with Circle & Spice in Bloomington. I’m now involved in the IU Juggling Club and juggle irregularly at the Bloomington Farmer’s Market. I have performed as far north as Calgary, as far east as Amsterdam, as far west as Los Angeles, all the way south in Miami, and all sorts of places in between.None of that would have been possible without my good friends and co-performers in the Spherocity contact juggling troupe: Matt, Jay, Cory, Courtney, Steve, and Leighanna. Thanks to Nick, Nicole, Leah, Ian and the rest of the crew, Objects in Motion keeps growing larger and better and I miss them terribly. And if you’re reading this, Sierra, you should start juggling again.Juggling knives in CalgaryAs if there’s not enough on my plate already, I’m also involved in two wonderful pseudo-academic organizations. I co-founded Sophosessions with Warren C. Moore, the coolest cat I know, in my junior year at UF. The group still meets a little more than monthly and allows its two-dozen members to present talks on whatever they feel like, from Chinese calligraphy to Zen Buddhism to advanced fractal mathematics to building robots. Then everyone goes to Ben & Jerry’s. I still webcast into meetings whenever I can, but it’s just not the same without the ice-cream.The Venerable IU Beer & Algorithms Club fills two Monday nights a month, and I get to listen to a bunch of Computer Science and Math graduates present their favorite algorithms in gory detail, all while eating a tasty meal and enjoying an equally tasty beverage. What could be better?Comments OffEditors’ Note: The following talks, panel websites, blog posts, and public documents all came from the 2012 meetings of the Modern Language Association and American Historical Association and associated THATCamp the past weekend. *updated 1/26/12*Papers and PanelsPanels: #alt-ac: The Future of ‘Alternative Academic’ Careers and Alternative Paths, Pitfalls, and Jobs in the Digital HumanitiesPanel: Archivists, Historians, and the Future of Authority in ArchivesPanel: Building Digital Humanities in the Undergraduate ClassroomKathryn E. Crowther, Brian Croxall, Maureen Engel, Paul Fyfe, Kathi Inman Berens, Janelle A. Jenstad, Charlotte Nunes, and Heather Zwicker, Electronic ShowcaseRoger T. Whitson, My Storify of Digital Pedagogy at #mla12Kathi Inman Berens, Mapping Occupy: Digital Pedagogy on a DeadlinePanel: Close Playing: Literary Methods and Video Game StudiesPanel: Composing New Partnerships in the Digital HumanitiesPanel: Crowdsourcing History: Collaborative Online Transcription and ArchivesPanel: The Cultural Place of Nineteenth-Century PoetryPanel: Debates in the Digital HumanitiesWorkshop: Digital Humanities: A Hands-On WorkshopPanel:  E-Roundtable on Digital PedagogyPanel: The Future is Here: Pioneers Discuss the Future of Digital HumanitiesPanel: Gertrude Stein and MusicPanel: How to Get Started in Digital HumanitiesPanel: Language, Literature, LearningPanel: Learned Journals and Libraries: Knowledge Economies and Economics of KnowledgePanel: The Literary Archive in an Age of Quantification: Evidence, Method, ImaginationPanel: Networks, Maps, and Words: Digital-Humanities Approaches to the Archive of American SlaveryPanel: Of King’s Treasuries, (GIGO) Wiki, (anti-) Google, and the E-Protean Invasion: The Evolving Nature of Scholarly ResearchPanel: Old Books and New ToolsPanel: Presenting Historical Research Using Digital MediaPanel: Racial Silences in the Archive and the Historiography of Race in Postcolonial Latin AmericaPanel: Reading Writing Interfaces: E-Literature’s Past and PresentPanel: Rhetorical Historiography and the Digital HumanitiesPanel: Text:Image – Visual Studies in the English MajorPanel: What Works? Integrating Culture into First-Year English and Foreign Language CoursesKeynote Address for the Council of Editors of Learned JournalsAdditional MaterialsExhibit: Electronic Literature ExhibitWorkshop: Evaluating Digital Work for Tenure and Promotion: A Workshop for Evaluators and Candidates at the 2012 MLA ConventionShared DocumentsReflections on the Digital Humanities at AHA and MLAComments[This is not what I'll be saying at the AHA on Sunday morning, since I'm participating in a panel discussion with Stefan Sinclair, Tim Sherrat, and Fred Gibbs, chaired by Bill Turkel. Do come! But if I were to toss something off today to show how text mining can contribute to historical questions and what sort of issues we can answer, now, using simple tools and big data, this might be the story I'd start with to show how much data we have, and how little things can have different meanings at big scales...] Spelling variations are not a bread-and-butter historical question, and with good reason. There is nothing at stake in whether someone writes "Pittsburgh" or "Pittsburg." But precisely because spelling is so arbitrary, we only change it for good reason. And so it can give insights into power, center and periphery, and transmission. One of the insights of cultural history is that the history of practices, however mundane, can be deeply rooted in the history of power and its use. So bear with me through some real arcana here; there's a bit of a payoff. Plus a map. The set-up: until 1911, the proper spelling of Pittsburg/Pittsburgh was in flux. Wikipedia (always my go-to source for legalistic minutia) has an exhaustive blow-by-blow, but basically, it has to do with decisions in Washington DC, not Pittsburgh itself (which has usually used the 'h'). The city was supposedly mostly "Pittsburgh" to 1891, when the new US Board on Geographic Names made it firmly "Pittsburg;" then they changed their minds, and made it and once again and forevermore "Pittsburgh" from 1911 on. This is kind of odd, when you think about it: the government changed the name of the eighth-largest city in the country twice in twenty years. (Harrison and Taft are not the presidents you usually think of as kings of over-reach). But it happened; people seem to have changed the addresses on their envelopes, the names on their baseball uniforms, and everything else right on cue. Thanks to about 500,000 books from the Open Library, though, we don't have to accept this prescriptive account as the whole story; what did people actually do when they had to write about Pittsburgh? Here's the usage in American books: What does this tell us about how practices change? This is telling us that usage was fairly confused up to about 1890; there seems to have been a push for 'Pittsburg' in the 1860s, but it somehow lost steam. When the Feds appeared in 1890, the 'h' appeared in about 80% as many books as the solo-G--not such a bad decision to go with the 'G' as some imply. But the rule made a difference: once the state intervened, 'Pittsburg' became about three times as popular as 'Pittsburgh'; and after it changed tacks in 1911, usage quickly tracked back towards 'gh.'* *[Quick guide for the perplexed: since we're comparing ratios, we have to use a log chart to get reasonable data. The dotted line at 1 shows where "Pittsburgh" and Pittsburg are used in equal numbers of books; equal distances above and below show proportionally more use of "Pittsburgh" and "Pittsburg" respectively. Each dot is a year, with a trend line superimposed. And for all of these, I use number of books using the words, not word counts; we're interested in what people are doing, and letting how many times they do it into the data will just muddy it up. The color just reinforces that scale on a blue-to-white-to-red distribution. And remember those colors, we're using them again later.] Now, I'm interested in not the fact of linguistic change, but its dynamics. What happens when the government tries to implement changes of spelling; does everyone respond equally? This is where library metadata starts to get useful. The first thing I checked here was the usage by age groups. A lot of language changes by cohort displacement; is that the case for spelling reforms? Here's the same sort of chart I made in the spring with author age on the y-axis, and year on the x. (Longer explanation here; I'm using a moving average to smooth instead of loess, for sanity reasons). Once again, blue is with an h, and red without one: This one is quick: there's almost no age effect. We're missing some data from the early period here, but pretty clearly the shifts in 1891 and 1911 happen across all generations, except maybe the very old. (And there, we expect to see some reprints skewing the data more.) The next place I looked for this was by stack location. Taking the headline LC classifications, do any patterns jump out? This one is more suggestive... K, the law, is curiously unresponsive to most shifts; and some areas (J, political science, and S, agriculture) seem more attached to the 'gh' spelling before 1891, perhaps because of individual institutions (the University of Pittsburgh, etc.); and C and E, both history genres, take their time accomodating to changes. But nothing seems overwhelming. Publication country, however, has a dramatic difference: (still blue for Pittsburgh, red for Pittsburg) Only one of these countries flies the red, white and blue. The British don't get the memo--according to ngrams, it's not until the 1930s they make the switch for good. (The Bookworm database only goes to 1922, so we can't tell here). The Canadians, with many fewer books, show less of a pattern, but might be in between. I find this interesting; it says something about either the ability of information like lexicographic reform to travel across international boundaries, or about the ability of states to impose these constraints on others. Britons kept moving along in their own community of practice for two decades; why bother changing spelling? It's not as though they had to address many envelopes. But the most compelling for me lies in the patterns across US states, by location of publisher (only including the top 15 states, since there aren't many books from elsewhere): This reveals two separate, interesting things: 1) This points toward the possibility of an event that I didn't know about before. 1891 may not actually be the first time the government changed its spelling of 'Pittsburgh'; something happened quickly and decisively to shift it in Washington DC in 1871 as well. Looking at book titles, I'm pretty confident this is a real pattern, albeit one the Pittsburgh sources don't mention. That mysterious plateau in the first chart from 1871 to 1891 towards 'Pittsburgh,' I suspect, can be directly attributed to whatever this is. Anyone want to put together a grant proposal to the Carnegie foundation? There's an opportunity for a major contribution to Pittsburghiana here! 2) Of much broader interest; the spread seems to have a geographical pattern. Most states lack the quick and clear definition of Washington DC in their shifts, but if we squint for the white squares and try to group them by the date they cross over to using the 'gh' spelling, a pattern emerges: 1911: DC,CT 1912: PA,MD,OH 1913: IN, IL,NY 1914: WI, IA, MA, MI 1916: MN 1918: MO Post-1922: CA (A special case, along with Kansas and Oklahoma, since it has a 'Pittsburg' of its own. Still, they all are dwarfed by the real one; California's had 6,000 people in 1920.) With the exception of Connecticut (which already had a strange predilection for the 'h'), what do we see? I would say: a path that traces concentric circles away from Washington, DC (or maybe Pittsburgh itself). I trust this enough to make a map. If we add every state with at least 20 books using either spelling after 1910, and bump up the smoothing window to six years, we can see this on a map: when do they switch over to the newly mandated spelling?Year majority of books published in state switched from "Pittsburg" to "Pittsburgh" (Anything that crosses later than 1922, I just count as 1923 here. And sorry for the legend; I don't know how to tell GoogleVis about years). Now the DC story is slightly less strong visually--Pennsylvania seems to be on top of the trend as well. (The story from the ground seems to be that Pittsburgundians were only too happy to go along with change). Knowing what we do, it's not unreasonable to see the practice as a joint effort of the city and the federal government. And indeed, the change does seem to radiate out from that central point through space--the farther out, the longer it takes the new practice to come into effect. (And the weaker it is when it does). Data for the 1891 transition work less well--in large part, the Western states seem to just like the simpler spelling no matter what, and likewise Pennsylvania the 'gh'. But we can make a less easy to read, but more accurate, map as well. It shows how the (log of the ratio of the) ratio of 'gh' to 'g' use changed from the aughts to the teens; bright blue (like Maryland) means it followed the federal mandates more strictly, purples show foot-dragging, and more red means that it departed from them.Strength of change in "Pittsburg(h)" spelling, 1900-1908 compared to 1913-1921 I find it somewhat compelling that the two poles are Washington DC (not pictured on account of size), which uses the new spelling 20x more frequently after 1911; and Utah, which managed to use the federally mandated spelling proportionally less in 1917 than it did before the government demanded it. (Of course it's Utah--and just after statehood, too! One wishes South Carolina, in all its fire-eating glory, would have published enough books to show up on the chart.) Perhaps this one is less clear than the straight transition, but there's still quite a noticeable trend from metropole to periphery. The influence of the government wanes as we get farther from its native sphere. (Not shown; the pattern is somewhat similar, with less data, for the 1880s to 1890s transition.) This, I'd argue, is pretty close to what you'd expect to see if the normative power of the federal government declines as you get further from its seat. Historians have well overplayed the center-periphery card in recent years, but they do exist; even in America, maybe the field of power declines with distance. (Can anyone think of some more terms that allow exactly this sort of application? British titles come to mind--Who says 'Sir David', and who merely "David Cannadine"?- but I don't have extensive geographic information for the UK.) Certainly, it seems like this one normative pattern does. Am I saying that this one orthographic example tells us basic things about the nature of the fin-de-siecle American state? Well, not really. (Although I probably would push it a bit farther than you'd like). This data is suggestive at best, misleading at worst; Enough for a hunch, and not much more. But, quite seriously, the accumulation of practices like this, when we can figure out how to group them together, has the potential to tell us enormously compelling things about sources of power and the patterns of imitation in all sorts of cultural spheres. Not all practices are driven by government actors; some are generational or disciplinary. There are vast numbers of subtle linguistic ticks--like the spelling of Pittsburgh--that exist in the textual record; we can use them to see how practices reproduce, how influences spread. And they only work because they're so hard to spot that even the people using them may not think about what they're doing. An author may remind himself about the up-to-date spelling of Pittsburgh; but it's very rare that he'd think about his relationship to the federal government before deciding which version to use. And yet, what he does reflects his participation in those fields nonetheless. These sorts of changes are like the dark matter of the historical universe; weak, tricky to spot, and not worth much in isolation; but they're everywhere. Cast a wide enough net, and we can use them for our ends."Culturomics" — linking cultural trends to quantitative analysis — is a nifty tool, but caution should be exercised when proponents of digital data encourage claims that it could make “much of what [historians] do trivially easy.” (Hemera Technologies)For more stories about all things Google, see the links at the end of this article.Earlier this year, a group of scientists — mostly in mathematics and evolutionary psychology — published an article in Science titled “Quantitative Analysis of Culture Using Millions of Digitized Books.” The authors’ technique, called “culturomics,” would, they said, “extend the boundaries of rigorous quantitative inquiry to a wide array of new phenomena spanning the social sciences and the humanities.” The authors employed a “corpus” of more than 5 million books — 500 billion words — that have been scanned by Google as part of the Google Books project. These books, the authors assert, represent about 4 percent of all the books ever published, and will allow the kind of statistically significant analysis common to many sciences.This sounds impressive. The authors point out that 500 billion words are more than any human could reasonably read in a lifetime. Their main method of analysis is to count the number of times a particular word or phrase (referred to as an n-gram) occurs over time in this corpus. (Try your own hand at n-grams here.) Their full data set includes over 2 billion such “culturomic trajectories.” One of the examples the authors give is to trace the usage of the year “1951.” They note that “1951” was not discussed much before the actual year 1951, that it appeared a lot in 1951, and that its usage dropped off after 1951. They call this evidence of collective memory.I initially reacted to this article with skepticism. As I read more — including a recent piece (one might call it a puff piece) in Nature on one of the co-authors, Erez Lieberman Aiden, in which he was dubbed “the prophet of digital humanities” — my skepticism became stronger. I think culturomics is a nifty tool, but we need to be cautious and critical about this kind of digital data and about claims that culturomics could make “much of what [historians] do trivially easy.” Historians do much more than follow trajectories, so I am not so sure that culturomics will lead to a new way of doing historical work. It’s not the game-changer it’s been claimed to be.I would not call myself a Luddite — I use digital resources all the time, in my research and my teaching. I have hundreds of PDFs of books I have downloaded from a variety of online sources — Early English Books Online, Eighteenth Century Collections Online, Gallica (the digital service of the French National Library), and yes, Google Books — that I use in my research.But when I read the Science article, I was immediately struck by what seems to me to be a fundamental flaw in its methodology: its reliance on Google Books for its sample. Google Books has focused on digitizing academic libraries. I would argue that books found in academic libraries are not necessarily representative of cultural trends across society. As any historian knows, every scholarly library is different and every library has its biases. And surely I am not the only historian who has noticed that the digitizing policy of Google Books does not, and perhaps cannot, result in anything like a uniform, or a uniformly random, sample of all books in a given period. Google’s ability to digitize books is dependent on a number of factors: the willingness of libraries to open their collections for digitization; the condition of the books being digitized; copyright regulations, which allow only “snippets” of many 20th-century books; and the quality of the digitization process itself.The authors further narrow their range by admitting only publications for which they have “metadata” — that is, author, title, year, immediately confining the range of publications to books, and not periodicals or other more ephemeral literature — and to the period after 1800. The article itself gives no clue as to how the authors obtained this metadata. But surely it skews their data set even more toward a certain kind of book, while treating books as interchangeable pieces of data. In this universe, one book is much like another.The authors equate size with representativeness and quantity of data with rigor. I am not sure that is true. I do not deny that some of their results are interesting, particularly the tracing of linguistic and grammatical changes over time, which is like watching a speeded-up newsreel. But some of the results are simply banal. The year “1951” appears most often in 1951. The word “slavery” appears more often during the U.S. Civil War. The word “influenza” appears more often during pandemics. Duh. Are these even historical questions?Perhaps most disturbing to me is the underlying assumptions of such work about the humanities and about what scholars in the humanities do. One assumption is that the humanities need to be more like science and that we need to be more like scientists — that quantitative knowledge is the only legitimate knowledge and that humanities scholars are therefore not “rigorous.” For well over a century, historians and their critics have debated whether their discipline is a science or an art. When the journal Past and Present was founded by a group of Marxist historians in the early 1950s, it was billed as “a journal of scientific history.” By the mid-1960s this had changed to simply “a journal of historical studies.” On the one hand, there are plenty of examples of humanities scholars who have been using sophisticated digital tools and quantification for years. The Cambridge population survey, with birth and death information gleaned from thousands of parish record books all over England, revolutionized social history when it began in the 1960s. When I was in graduate school in the 1980s, the SPSS statistical package could be mastered as an alternative to a second language. As cultural history became more prominent, quantitative history became less fashionable, but it never disappeared.On the other hand, as these examples indicate, there is not just one kind of historical or, more broadly, humanities scholarship as the Science authors seem to think. Not all of us trace ideas over time. Some of us look at the people who had those ideas and the places they lived and worked, and the people they knew, and how they lived. Not all of this can be found in books but must be traced across a variety of published, manuscript and material media. Although the culturomics people are confident that they can apply their methods to manuscripts and maps, I’m not going to wait for that possibility.Much like the digital versus the long-lost card catalog, such a sweeping tool leaves out the chance juxtapositions and serendipities that often tell us much more than the texts themselves. I spent many years off and on at the British Library reading advertisements in the microfilmed Burney collection of 18th-century newspapers. Now these have been digitized, and I can search for “anatomy lectures” and come up with dozens of hits that took me many eye-straining hours to find. But it cannot tell me that on the previous page, or in the previous issue, there was an ad for a patent medicine, or a live animal combat, or another fascinating bit of 18th-century London life that lends meaning and context to the bare entry.It is revealing of another kind of bias that the long list of authors of the Science article includes no historians, in fact no one from the humanities (Louis Menand also pointed this out in an interview in The New York Times). To be fair, “R. Darnton” and “C. Rosenberg” (presumably the Harvard historians Robert Darnton and Charles Rosenberg) are thanked at the end. The Nature article goes out of its way to point out that Erez Lieberman Aiden studied history and philosophy and even creative writing, which is something like saying I took physics in college, and therefore I can publish on quantum mechanics in Nature. Both articles show a nearly complete lack of understanding of what historians and other humanities scholars actually do.When Lieberman Aiden and his co-authors presented their findings at the meeting of the American Historical Association in January, AHA President Tony Grafton expressed cautious praise of this new tool. In the Nature article he sounds decidedly more anxious: “You can’t help but worry that this is going to sweep the deck of all money for the humanities everywhere else.”Indeed.More Stories About GoogleHow Google Disrespected Mexican HistoryDear Google: Do Not Track MeThe Government, Google and Lady GagaGoogle Street View Ruffles European FeathersSign up for the free Miller-McCune.com e-newsletter.“Like” Miller-McCune on Facebook.Follow Miller-McCune on Twitter.Add Miller-McCune.com news to your site.Names can shape fields. In the proposal for a panel to be held at the MLA this week, Lori Emerson argued that the introduction of the term “electronic literature” by the founding of the Electronic Literature Organization in 1999, in fact founded the field by creating “a name, a concept, even a brand with which a remarkably diverse range of digital writing practices could identity: electronic literature,” as Lori explains in a blog post. Seen in this perspective, the first book on electronic literature is Loss Glazier’s Digital Poetics in 2001. This renders invisible the very rich theory and practice of electronic literature before 2001 (as Mark Bernstein has pointed out), which I dislike, but Lori is correct in that a name that a field agrees upon is important in terms of establishing a creative and scholarly field.The discussion did lead me to wonder when the term electronic literature first became common. Did the ELO really invent it? Fortunately we live in an age where this sort of question can easily be answered, at least partially, so I asked Google.Google has digitised around 4% of all books ever published (as described by the Culturomics research team in Michel et.al. 2011), and Google’s Ngram viewer allows us to graph the occurrence of terms over time in all those books. Here is a graph showing the rise and fall of “hypertext fiction” (the red line) and the rises of “electronic literature” (the blue line) and “digital poetry” (the yellow line). “Digital literature” (green) is also fairly common, but “e-poetry” just gives us a flat purple line along the bottom of the graph.Google's ngram viewer allows us to graph the frequency with which different terms for electronic literature were used in books published between 1985 and 2008. The terms are "hypertext fiction", "electronic literature", "digital literature", "digital poetry" and "e-poetry".As expected, hypertext fiction (the blue line) was the more popular term in the 1990s, but it also retained its dominance for several years into the 2000s. This could show that the new term “electronic literature” took time to gain general acceptance, or it could also simply be a by-product of the slow pace of scholarship and book publishing. By 2008, the term “electronic literature” is still not as popular as “hypertext fiction” was at its peak, although the combined use of all these terms is growing steadily. It is interesting to see how high the use of “hypertext fiction” remains, even after the dominance of “electronic literature, and the rapid rise of “digital poetry” is particularly striking.A problem with this graph is that the term “electronic literature” was frequently used in the 1980s and 1990s to describe research literature that is in electronic form, and so there are a lot of false positives, especially in this period. In fact, almost all uses of the term before the late 1990s are in this non-literary sense of the word. However, by clicking through and looking at the individual hits for each year I did find a few notable exceptions.Jay Bolter published an article in 1985 called “The Idea of Literature in the Electronic Medium” (in the journal Topic: Computers in the Libreral Arts, vol. 39, p 23-34) which consistently uses the term electronic literature exactly as we do today. However, he writes as though there are no works of electronic literature yet. In fact, there were works by Roy Ascott (1983), bp nichols (1984), Robert Pinsky (1984) and no doubt others, but these works were not easily accessible or gathered by a shared community. In the article Bolter imagines a future electronic literature, writing that the adventure games of 1985 hold promise, but need better writers:Electronic literature will never attract serious notice, if it remains at the level of the current adventure game. By the same token, no one would snow consider the motion picture an important art form, if it had remained at the artistic level of the nickelodeon. (page 25)Bolter goes on to describe a system for allowing writers to write in a word processor (rather than typing into the program code itself) and a children’s version of the Odyssey that “enlists interactive participation”, much as the adventure game does. The described work would require readers to attempt to solve the problems Odysseus faces, changing the course of the story in doing so. He also proposes detective stories and quest literature as suitable genres for treatment in electronic literature.I was surprised at how clearly Bolter states that “such literature is growing out of th computer games that are so popular today” (page 32), given how strongly authors of the late 1980s and early 1990s worked to distance themselves from games. But he also notes that the Oulipo’s works and concrete poetry are natural allies to the computer, and the possibility of generative electronic literature.Bolter also uses the term “electronic literature” in his seminal book Writing Space (1991), although by this time, hypertext fiction had become the dominant genre and therefore the more common term.A 1992 article in ThePrint Collector’s Newslettermentions that Eastgate publishes electronic literature. In 1995, Robert Kendall uses the term in an article that presents an overview of electronic literature at the time, presciently titled “Writing for the New Millennium: The Birth of Electronic Literature.” So the term “electronic literature” was in use well before 1999 when the ELO was founded.A few other interesting finds included a notice in Billboard 14 April 1979 of a conference at UCLA:I would love to know what kind of electronic literature this was. It’s both exciting and frustrating to have such easy access to such a wealth of partial information!I’ve written an essay on the early days of electronic literature that will be published in an upcoming special issue of Dichtung Digital that will include a lot of great papers on communities of electronic literature. The graph and some of the discussion are from that essay.Related posts:anthology on archiving electronic literaturearchiving and documenting electronic literatureCall for Papers and Works: Seminar on Electronic Literature in Europe03. January 2012 by Jill Categories: Electronic literature | 3 commentsSorry, but comments from before December 2010 are lost in the database and I've not yet figured out how to display them properly.Related posts:anthology on archiving electronic literaturearchiving and documenting electronic literatureCall for Papers and Works: Seminar on Electronic Literature in EuropeI’m watching a presentation by Albert László Barabási on his book BURSTS: The Hidden Pattern Behind Everything We Do, and he’s telling some fascinating stories.For instance, had you heard of Hasan Elahi, a media artist who after being investigated by the FBI for suspected terrorism (due to unusual travel patterns, among other things) put his entire life online at Tracking Transcience? Here’s a piece Elahi wrote in the New York Times about this project, or you can watch him talk about the project for TED. Actually the information is not very easy for a viewer to put together (he calls it user-unfriendly), but he has fed in a lot of images, his bank records, location, and so forth. The images are very sparse, void of people, factual. There are photos of every toilet he uses, for instance, or every taco he eats near a railway station, all time, location and date-stamped. But apparently, in BURST, Barabási analyses Elahi’s data to see how typical his patterns of movement are.The reason for the title of BURSTS is that it turns out our behaviour is conglomerated in bursts (around 16 mins into the talk). We have periods where we send lots of emails, or talk on the phone a lot, or visit the library a lot, or even have sex a lot, and then there are gaps where we do these things much less. This actually follows power laws, and is not purely random.Barabasi also analysed anonymised mobile phone data to analyse how people move around (around 24 mins into the video). He found that if you know the past movements of a person, you can predict their next location with a 93% accuracy. Another interesting point is that there is nobody whose predictability (in terms of location) is less than 80%. (That might actually explain why Foursquare and Gowalla and so on get a little boring after a while.) But ultimately, Barabási argues, with enough data, we might be able to predict all kinds of human behaviour – collapses of stock markets, wars, major historical events, and so on.BURST has rather poor reviews on Amazon, where it is criticised for having too much story and too little science. Regardless of whether or not you read the book, his 30 minute talk on it is interesting.Related posts:facebook protects us from having our data scraped – but that also stops us from MOVING our datapersonal data visualisation: dopplr, dailybooth, flickr and trixietrackerno human intervention04. January 2012 by Jill Categories: Uncategorized | 3 commentsSorry, but comments from before December 2010 are lost in the database and I've not yet figured out how to display them properly.Related posts:facebook protects us from having our data scraped – but that also stops us from MOVING our datapersonal data visualisation: dopplr, dailybooth, flickr and trixietrackerno human interventionIn those feverishly exciting early years of blogging Liz Lawley (a.k.a. mamamusings) was one of my favorite blogging buddies, and I’m excited to see she’s decided to blog more again, rather than leaving ideas and conversations on Facebook or Twitter. Some obvious problems with having most of our conversations on Facebook or Twitter is the forced brevity, that conversations happen in a non-public forum (good in some cases, bad for research though) and that its hard to search or access old conversations, even one’s own, and certainly other peoples’.I’m going to blog more often, too. When I make time to blog, I not only feel more engaged with my research and teaching and have more interesting ideas, I also quite simply enjoy my job a lot more. Not too surprising, really…Related posts:tweeting vs blogging – and the prompts of social network sitesresearch blogging as writing in the momentanother way of making money blogging04. January 2012 by Jill Categories: Uncategorized | 5 commentsSorry, but comments from before December 2010 are lost in the database and I've not yet figured out how to display them properly.Related posts:tweeting vs blogging – and the prompts of social network sitesresearch blogging as writing in the momentanother way of making money bloggingI have the unusual luxury of two whole weeks at the office before teaching begins and while I do have quite a few other things to do, I’m happy to have time to really think about how I want to teach this semester’s course.Benedikte Irgens, who teaches Japanese here at UiB, linked on Facebook to this article about the use of clickers in teaching medicine at the University of Oslo. An immediate discussion broke out about whether or not clickers can be useful in teaching in the humanities, where we often want critical thinking and reflection rather than true/false answers, and so I remembered that Liz Lawley uses clickers, and she writes thatinstead of using this for multiple-choice quizzing, I use this for things like “Choose Your Own Lecture,” in which students pick which path I take through the lecture material, or for polling the students on what they thought about a required reading or video, or for letting them vote on whether we should end class early on a beautiful day and go outside. It’s not perfect, but it’s a way to discourage passivity.Next I found a whole book, by Derek Bruff, about creative ways of using them precisely to foster classroom discussion and higher-order thinking. So now of course I want to try out clickers in the classroom, being a nut for trying out new and interesting teaching tricks. But I think not this semester.Reading about clickers led me to another post on Derek Bruff’s blog about structured Twitter assignments, where he gives a couple of examples of ways teachers have used Twitter in teaching. It’s key, Bruff argues, that students have clear deliverables and aren’t just asked to Tweet whatever they feel like, and also that “student tweets are aggregated and analysed” and not just ignored. An examples is Mark Sample’s use of Storify to gather student tweets about watching Blade Runner, or using Poll Everywhere, as Corbette Doyle has done, to have students vote on which questions (previously posed on Twitter) should be addressed in an upcoming class.I like the idea of requiring questions on the readings before class and gathering them somehow. Corbette Doyle used HootSuite, which seems to gather Tweets from your students and let you (and the students) easily read them. I’m not sure how much added value this really gives you in comparison to simply using a Twitter hash tag or a Facebook group for class discussions.Related posts:teaching nightmarestudents teaching with blogsstealing from dr. crazy: teaching close reading and using grading sheets05. January 2012 by Jill Categories: Teaching | 2 commentsSorry, but comments from before December 2010 are lost in the database and I've not yet figured out how to display them properly.Related posts:teaching nightmarestudents teaching with blogsstealing from dr. crazy: teaching close reading and using grading sheetsOur research librarian just sent a list of all the books the University Library bought for Digital Culture in 2011. There are 97 books on the list, which says something about how fast this field is growing, and browsing the titles I really wish I had time to read them all. Sorry about the plain text and truncated titles – if you’re interested you’ll easily find more info about the books at your library or bookstore. I <3 our library!Most of these books are new, some are older, and of course we already had a lot of books before 2011. Leave a comment if you see glaring omissions and I’ll suggest the library buy those books as well.Peer-to-Peer video : the economics, policy, and culture ofEthnographies of the videogame : gender,/ Thornham, HelenComputer-Mediated Communication for Ling/ Bodomo, Adams B.SpecLab : digital aesthetics and project/ Drucker, JohannaRemixthebook / Amerika, MarkGames and gaming : an introduction to ne/ Hjorth, LarissaHow to do things with videogames / Bogost, IanThe Reconstruction of space and time : mobile communicatioThe Digital condition : class and cultur/ Wilkie, RobHello Avatar : rise of the networked gen/ Coleman, B.Programmed visions : software and memory/ Chun, Wendy HuiThe digital origins of dictatorship and / Howard, Philip NUncreative writing : managing language i/ Goldsmith, KenneDigital Media : Technological and Social Challenges of theContext providers : conditions of meaning in media artsRace after the internetDigital folklore : to computer users, with love and respec@ is for activism : dissent, resistance / Hands, JossAgainst expression : an anthology of conceptual writingCybernetic revolutionaries : technology / Medina, EdenReading Machines : Toward and Algorithmi/ Ramsay, StephenOnline gaming in context : the social and cultural signifiThe new digital storytelling : creating / Alexander, BryanText and Genre in Reconstruction : Effects of DigitalizatiCyber_reader : critical writings for the digital eraGender-Technology Relations : Exploring / Corneliussen, HiNetworks Without a Cause : A Critique of/ Lovink, GeertMobile interface theory : embodied space/ Farman, JasonBetween Page and Screen : Remaking Liter/ Wurth, Kiene BriUnderstanding Digital Humanities / Berry, David M.Gender and sexuality in online game cult/ Sundén, JennyDigital storytelling : capturing lives, / Lambert, JoeMemory bytes : history, technology, and digital cultureNew Directions in Digital Poetry : Poetr/ Funkhouser, C. THypertext and the Female Imaginary / Odin, JanishreeThe Virtual representation of the pastThe Principles of Beautiful Web Design / Beaird, JasonArtistic bedfellows : histories, theories, and conversatioLegend-Tripping Online Supernatural Folk/ Kinsella, MichaeA Networked self : identity, community, and culture on socUnderstanding video games : the essentia/ Egenfeldt-NielseCybertext poetics : the critical landsca/ Eskelinen, MarkkThroughout : Art and Culture Emerging with Ubiquitous CompAesthetics : a comprehensive anthologyhuman readable messages / Breeze, MezCollaborative Research in the Digital HumanitiesLeet Noobs : The Life and Death of an Ex/ Chen, MarkDet mørke nettet / Strømmen, ØyvindInterface criticism : aesthetics beyond buttonsNew Media artArt of the digital age / Wands, BruceCollaborative circles : friendship dynam/ Farrell, MichaelInternet and surveillance : the challenges of Web 2.0 andHyperworks : on digital literature and c/ Gunder, AnnaThe fundamentals of digital art / Colson, RichardThe Ethics of emerging media : information, social norms,Technology, literature and culture / Goody, AlexGaming matters : art, science, magic, an/ Ruggill, Judd EtArt practice in a digital cultureLiteratures in the digital era : theory and praxisDivining a digital future : mess and myt/ Dourish, PaulMedia archaeology : approaches, applications, and implicatMobile communication : bringing us together and tearing usThe Handbook of science and technology studiesThe Video game theory reader. 2The secret war between downloading and u/ Lunenfeld, PeterThe American literature scholar in the digital ageThe Aesthetics of net literature : writing, reading and plDigital art and meaning : reading kineti/ Simanowski, RobeBeyond Barbie and Mortal Kombat : new perspectives on gendAlone together : why we expect more from/ Turkle, SherryBeyond the screen : transformations of literary structuresDigital poetics : the making of e-poetri/ Glazier, Loss PeCode/space : software and everyday life / Kitchin, RobDigital content creation : perceptions, practices, and perCastells and the media : theory and medi/ Howard, Philip NLocal transcendence : essays on postmode/ Liu, AlanPremediation : affect and mediality afte/ Grusin, RichardNet locality : why location matters in a/ Gordon, EricCognitive surplus : creativity and gener/ Shirky, ClayMaking is connecting : the social meanin/ Gauntlett, DavidTales from Facebook / Miller, Daniel,Digital cultures / Doueihi, MiladDigital planet : tomorrows’s technology / Beekman, GeorgeThe philosophy of software : code and me/ Berry, David M.A Networked self : identity, community, and culture on socA small world : smart houses and the dre/ Heckman, DavinSwitching codes : thinking through digital technology in tRegards croisés : perspectives on digital literaturePost-cinematic affect / Shaviro, StevenThe Handbook of internet studiesHandbook of research on effective electronic gaming in eduArchivierung von digitaler Literatur : Probleme-Tendenzen-Teaching narrative theoryRelated posts:the nordic digital culture networkapply to do a PhD in digital culture at our department!don’t let em google our books!06. January 2012 by Jill Categories: Uncategorized | 6 commentsSorry, but comments from before December 2010 are lost in the database and I've not yet figured out how to display them properly.Related posts:the nordic digital culture networkapply to do a PhD in digital culture at our department!don’t let em google our books!I read Mark Andrejevic‘s article on “Social Network Exploitation” yesterday (in Paparcharissi (ed) A Networked Self, Routledge 2011), and its marxist critique of the ways companies might and/or do exploit our use of social media is excellently chilling. I’m going to be teaching this article this semester, and have a half-written blog post on it, so more of that later.But anyway, inspired by Andrejevic’s article, I poked around a bit and found two interesting conferences this spring:Critique, Democracy, and Philosophy in 21st Century Information Society: Towards Critical Theories of Social Media is an academic conference to be held in Uppsala near Stockholm on May 2-4. Andrejevic is on the list of speakers, as are many other interesting thinkers, including Christian Fuchs, Trebor Scholz, Ursula Huws, Andrew Feenberg and Charles Ess. I’m not impressed that only two of the listed 14 speakers are women – you’d think balanced representation would be key in a critique of social media, and I certainly hope they improve upon that before the program is final. They do also have an open call for papers, with abstracts due Feb 29. Here’s some of the topic description:This conference deals with the question of what kind of society and what kind of Internet are desirable, what steps need to be taken for advancing a good Internet in a sustainable information society, how capitalism, power structures and social media are connected, what the main problems, risks, opportunities and challenges are for the current and future development of Internet and society, how struggles are connected to social media, what the role, problems and opportunities of social media, web 2.0, the mobile Internet and the ubiquitous Internet are today and in the future, what current developments of the Internet and society tell us about potential futures, how an alternative Internet can look like, and how a participatory, commons-based Internet and a co-operative, participatory, sustainable information society can be achieved.Unlike Us 2 is an event to be held in Amsterdam on March 8-10 which is hosted by a “research network of artists, designers, scholars, activists and programmers who work on ‘alternatives in social media’”, with the subtitle “Understanding Social Media Monopolies and their Alternatives.” David Berrytweeted this one to me after I mentioned the Uppsala conference, and he is one of the confirmed speakers. So is Anne Helmond, whose research blog I’ve been following for years. There are also some interesting videos from the first Unlike Us event on their blog.Andrejevic’s chapter, and in fact the whole of Papacharissi’s anthology, is available as a PDF on Scribd, which is presumably not even remotely legal, so I won’t link.Related posts:social media study groupfrench discussions about social mediaconnecting students working on social media at uib06. January 2012 by Jill Categories: social media | Tags: conferences, critical studies | 3 commentsSorry, but comments from before December 2010 are lost in the database and I've not yet figured out how to display them properly.Related posts:social media study groupfrench discussions about social mediaconnecting students working on social media at uibThis post is in Norwegian, asking readers to contribute their thoughts on the challenges to digital content and services in Norway on behalf of DIGITutvalget.DIGITutvalget er nedsatt av Fornyingsdepartementet for å identifisere hindringer og barrierer for vekst i den norske digitaløkonomien, og i dag er jeg på vei til Oslo for å delta i utvalgets fjerde møte. På vei til det første møtet skrev jeg litt om hvem som er med i utvalget og om mandatet,Mandatet til DIGITutvalget er bredt, og vi er interessert i å høre dine meninger både om hva som er det viktigste i mandatet og om hva du mener er de største hindringene og utfordringene for de som vil skape verdier i den digitale sektoren i Norge. Har du innspill, tar vi gjerne imot dem – ikke som kommentar her eller på Twitter, men først og fremst i dette skjemaet.Related posts:launch for digitale fortellingerdigitale fortellingerÅpen høring10. January 2012 by Jill Categories: Uncategorized | Leave a commentSorry, but comments from before December 2010 are lost in the database and I've not yet figured out how to display them properly.Related posts:launch for digitale fortellingerdigitale fortellingerÅpen høringI love Jenny Holzer’s truisms, but I think I like @JennyHolzerMom even better!I’ve decided to drastically lower my threshold for what I think of as blog worthy. Just because I already retweeted one of these tweets doesn’t mean it’s not also useful to put it up on my blog. It might entertain you–and I will almost certainly forget about it if I leave it in the immediate yet forgettable Twitter-verse.How much substance do you think a blog post needs?Related posts:some respect, i’m your mother!kate hayles’ mother was a computerthe game system as mother10. January 2012 by Jill Categories: Digital Art | Tags: fake, truisms, twitter | Leave a commentSorry, but comments from before December 2010 are lost in the database and I've not yet figured out how to display them properly.Related posts:some respect, i’m your mother!kate hayles’ mother was a computerthe game system as motherThis semester just properly started with our program’s orientation meeting today. It’s great to see so many new and familiar faces!I’m teaching a new to me course this semester: DIKULT251: Critical Perspectives on Technology and Society with Bachelor Thesis, and I’ve just completed the schedule for the semester. There are still some gaps to fill in, though. Luckily, it’s a perfectly-sized seminar class, with around 12-15 students, we’ll be able to adapt our discussions to the students’ interests.I’m combining general social media theory with digital methods workshops from the Digital Methods Initiative in Amsterdam, and am excited about this practical aspect to the course. More details on that will follow.I’m sure a lot of ideas from the course will leak into this blog, and vice versa. If you have any ideas or suggestions, let me know!Related posts:new semesterplanning next semesterstage-fright / glow12. January 2012 by Jill Categories: Teaching | Leave a commentSorry, but comments from before December 2010 are lost in the database and I've not yet figured out how to display them properly.Related posts:new semesterplanning next semesterstage-fright / glowIn an effort to teach less next semester (since I’m administrating far, far more) I’ve outsourced 2/3 of the course I usually teach on digital media aesthetics to Rune Klevjer, videogame expert and Scott Rettberg, electronic literature guru. Then I’m picking up three weeks (so six lectures) in HUIN106, an undergraduate course on “Digital culture and norms”, which is co-taught by Hilde, who does a section on gender and technology and Magne, who does history of computing and open source. In total it means I might actually get away with really only spending 18.75 hours a week on teaching and administration, the way I’m supposed to. Hooray!I’m looking forward to teaching into HUIN106. It was really rather nice figuring out what goes into the dossier, which had to get to the bookshop today for copyright clearance and copying. I’m going to do online communities, using the very pedagogical-looking overviews in Gunnar Liest¯l and Terje Rasmussen’s book. I figure we’ll look at MUDs and Usenet and chatrooms and all the rest of it. Then we’ll use Steven Johnson’s bit about the desktop metaphor to talk about how interfaces affect us, and we’ll look at Unix, Windows and Mac and compare that and then relate it back to the different interfaces for online communities. Bolter’s chapter on “Writing the Self”, along with Angela Thomas on how teenaged girls represent themselves online will take us into the self-expression side of things (I really must read Goffman, I promise I will this summer), which will lead on into blogs, and then on into ideas about democracy and power in online communities, and for that we’ll read the second chapter in Liest¯l and Rasmussen’s book, and my article on links and power, and Susan Herring et. al. on blogs.Here are the articles and book chapters going into the dossier:Liest¯l, Gunnar og Terje Rasmussen. Kapittel 8 og 9 (ìVirtuelle fellesskapî og ìDigitalt demokratiî) fra Digitale medier: en innf¯ring. Oslo: Universitetsforlaget, 2003. 97-125.Johnson, Steven. ìThe Desktopî, fra Interface Culture: How New Technology Transforms the Way We Create and Communicate. New York: HarperEdge, 1997. 42-75.Bolter, Jay David. Kapittel 9 og 10 (ìWriting the Selfî og ìWriting Cultureî) fra Writing Space: Computers, Hypertext, and the Remediation of Print. Mahwah, NJ: Lawrence Erlbaum, 2001.Thomas, Angela. “Digital Literacies of the Cybergirl.” E-Learning 1.3 (2004): 358-82.Walker, Jill. “Links and Power: The Political Economy of Linking on the Web.” Library Trends 53.4 (2005): 524-29.Herring, Susan C., Inna Kouper, Lois Ann Scheidt og Elijah Wright. ìWomen and Children Last: The Discursive Construction of Weblogs.î Laura J. Gurak, Smiljana Antonijevic, Laurie Johnson, Clancy Ratliff, og Jessica Reyman (red.) Juni 2004..There’s no social software as such in there, or social network stuff. I might slip an online text in somewhere, I guess, and I’ll certainly show some of this in class.Related posts:planning imagenew semesterwhat norwegians do online02. June 2005 by JillLeave a commentSorry, but comments from before December 2010 are lost in the database and I've not yet figured out how to display them properly.Related posts:planning imagenew semesterwhat norwegians do onlineContinue reading at http://jilltxt.net/?p=1424 .Comments OffEditors’ Note: Over the last week, the position of Digital Humanities within literary studies has been discussed by a number of scholars in advance of the annual MLA meeting. Several posts are linked below, and please tweet or email us with any suggestions.Ted Underwood, “Why digital humanities isn’t actually “the next thing in literary studies,” December 27, 2011DH is not the kind of trend humanists are used to, which starts with a specific methodological insight and promises to revive a discipline (or two) by generalizing that insight. It’s something more diffuse, and the diffuseness matters….. I suppose, if pressed, I would say “digital humanities” is the name of an opportunity. Technological change has made some of the embodiments of humanistic work — media, archives, institutions, perhaps curricula — a lot more plastic than they used to be. That could turn out to be a good thing or a bad thing. But it’s neither of those just yet: the meaning of the opportunity is going to depend on what we make of it.” Read Full Post Here.Feisal G. Mohamed,”Can There Be a Digital Humanism?” December 28, 2011Here’s where I get troubled: digital humanities projects often say that they are innovating the way we investigate texts and cultures, though that innovation, as Ted notes, arises from a set of technological tools rather than an intellectual position. And when at their most excited, “digital humanists” can sometimes claim to be transforming humanistic study itself. Read Full Post Here.Marc Santos,”Digital Humanities,” December 29, 2011I think there’s two basic genealogies to digital humanities/technology studies. Reductive? Sure. But helpful. The first traces back to Heidegger’s “Question Concerning Technology.”….The second traces back to McLuhan. Read Full Post Here.Multiple Authors, “Digital Humanities: Whence, Wherefore, and Why?,” December 29-30, 2011Alex Reid, “I do think that that the core definition of DH is dissolving. The Fish piece, for good or bad, shows how an outsider to DH imagines it as this expansive business. I think this is commonplace. The folks at the DH conference may have a more narrow view, but elsewhere that it less and less the case. Those of us who have been doing computers and writing, digital rhetoric, technical communication, etc. need to decide whether we want to adopt the DH term and expand it to include us or insist that we are separate but equal.” Read Full Conversation Here.Ted Underwood,”A Brief Outburst About Numbers,” January 3, 2012None of this is to say that we can simply borrow tools or methods from scientists unchanged. The humanities have a lot to add — especially when it comes to the social and historical character of human behavior. I think there are fascinating advances taking place in data science right now. But when you take apart the analytic tools that computer scientists have designed, you often find that they’re based on specific mistaken assumptions about the social character of language. Read Full Post Here.CommentsComments OffFrom all of us at Digital Humanities Now, happy holidays and best wishes for the new year!We will return in January to bring you more digital humanities scholarship, conversations, news, and events.In the meantime, we invite you to participate in this experiment in digital publishing. Please tell us how we can improve Digital Humanities Now in the upcoming year.CommentsDigital Humanities Now showcases the scholarship and news of interest to the digital humanities community, through a process of aggregation, discovery, curation, and review. Learn more about our community-driven identification and evaluation system, which determines the content of the DHNow site and its quarterly journal, the Journal of Digital Humanities.The following is a guest post by Kalev H. Leetaru, University of Illinois, who presented these ideas at the 2012 General Assembly of the IIPC. This post is the third in a three-part series. Read the first and second posts or the full paper on netpreserve.org.As web archives mature and expand, a growing question revolves around the role of these archives in society. What should their primary mission(s) be and how can they best fulfill those roles? At their most basic level, I believe web archives fulfill three primary roles: Preservation, Research, and Authentication, in that order.• Preservation. First and foremost, web archives preserve the web. They act as the web equivalent of the archive or library, constantly monitoring for new content, requesting a copy of that content, and keeping a copy of it for posterity. In this role, their mission is to acquire and preserve the web for future generations, with access being primarily through basic browsing and retrieval. Some archives, for legal purposes, may not even be able to provide access to their holdings during the lifetime of the organizations providing them content, instead holding that material under embargo for a certain number of years, but ensuring its continued survival for future generations.Library of Congress Main Reading Room, Carol M. Highsmith Archive.• Research. A unique and emerging use of archives is as a research service for scholars. Very few academics, especially in the social sciences and humanities, have the computational expertise or resources to crawl and download large portions of the web for research. Commercial web crawling companies like Google do not provide their data for research, and thus web archives provide a fundamentally unique and enabling resource for the study of the web that scholars can turn to. Even more critically, many key humanities and social science questions revolve around how ideas and communication change over time, and web archives capture the only view of change on the web. In this role, the secondary mission of archives is to provide access to their holdings that goes beyond the basic browsing needed for casual use or deep scholarly reading of a small number of works, towards providing programmatic tools and access policies that support computational data mining of large portions of their holdings.• Authentication. A final emerging use of archives is as an authentication service. Web data is highly mutable, changing constantly, and there is no way to authenticate whether the page I see today is the same as what I saw yesterday, especially if the change is a small one. It took more than five years for changes to White House press releases to be spotted via copies held in the Internet Archive, and even then the discovery was entirely by accident. Third party archives allow “authentication” of what a page looked like at a given moment. One could even imagine someday a browser plugin that, as a user browsed certain sites on the web (such as government pages, perhaps medical or other pages), would compare each page with the most recent copy stored by a network of web archives, and display an indicator to the user as to whether the page has changed since it was last archived, as well as highlight those changes. In this role, the third peripheral mission of the web archive is to act as a “disinterested third party” that can authenticate and verify the contents of a given web page at a given moment in time.Wikipedia offers an intriguing vision of what the ultimate web archive might look like. Every edit to every page since the inception of the site has been archived and is available at a mouse click, allowing a visitor or scholar to trace the entire history of every word. Every operation taken on the site and the complete source code to every algorithm used for various automated processes are fully documented and make available, offering complete technical transparently. Finally, a dedicated bulk download page is maintained in which researchers may download a ZIP file containing the entirety of the site and every edit ever performed, which has made Wikipedia a mainstay of considerable social and computer science research.As our digital world continues to grow at a breathtaking pace and more and more of our daily live occurs within its digital boundaries, we must ensure that web archives are there to preserve our collective global consciousness for future generations.When data exploration produces Christmas-themed charts, that's a sign it's time to post again. So here's a chart and a problem. First, the problem. One of the things I like about the posts I did on author age and vocabularychange in the spring is that they have two nice dimensions we can watch changes happening in. This captures the fact that language as a whole doesn't just up and change--things happen among particular groups of people, and the change that results has shape not just in time (it grows, it shrinks) but across those other dimensions as well. There's nothing fundamental about author age for this--in fact, I think it probably captures what, at least at first, I would have thought were the least interestingtypes of vocabulary change. But author age has two nice characteristics. 1) It's straightforwardly linear, and so can be set against publication year cleanly. 2) Librarians have been keeping track of it, pretty much accidentally, by noting the birth year of every book's author. Neither of these attributes are that remarkable; but the combination is. There are plenty of linear variables out there: I'd love to be able to see how vocabulary changes lie in time by linear variables like author income, years of schooling, or annual sales figures for books; but no one has been collecting that data. The stuff that has been collected, on the other hand, is essential categorical--a book can be fiction, published in Philadelphia, about set theory, in English. Nobody keeps track of any of these as linear variables, though they could (it just barely mentions set theory, it has a lot of French words, etc.) The trick is to make this categorical data more ordinal. Given something reasonably good at turning publication location into real life places, for instance, you could turn geographical data into latitude-longitude pairs, or into any number of mildly interesting one-dimensional series. (Maybe the adoption of some vocabulary can be modeled well by miles from Muncie, or by city population at date of publication). But book data just isn't strongly geographical enough to make those sorts of comparisons worth coding. (Newspaper data, on the other hand...) And I'm particularly interested in genre. What I'd really like is some way to make genre information univariate. One way to do this is to create new ordinal genre information through principal components analysis or something. But that doesn't use metadata, just the text, which seems somewhat wasteful. The best genre information we have is probably LC classification numbers; and they are frustratingly almost-ordinal. Q-R-S-T is all science-math-technology type stuff; D-E-F is history leading into the social sciences in G-H-K; and so on. But there's not really a continuous scale from A to Z. Right? I wanted to get a quick-ish handle on this, and just how similar or dissimilar the various LC classes are, and how that maps to the order they're shelved in. This is where the chart comes in. The easiest way to compare genres seemed to be comparing their word usage using cosine similarity. (To keep the data size manageable, I actually compared only words preceding the word 'are.' Good enough, hopefully; it shouldn't seriously compromise the data, but does mean that the variations are mostly about noun-usage, not word usage in general.) 1 is perfect similarity, and anything below about .85 is 'not very close'--I've lumped those together. Every point is colored to show the similarity score of the genre immediately to the left against the genre below. Green is very similar, white is averagely similar, red is not very similar. You'll see a green line running through the middle--that's because every genre is identical to itself. The chart, in accidentally Christmas colors (click on the chart to enlarge, and to Wikipedia for a refresher on LC classifications): This is not one of those charts where the meaning just jumps out. But a few notes: 1) There are roughly three big groupings that are relatively coherent: the social sciences and humanities, let's call them, A to PN; fiction, PQ-PZ; and the sciences, Q to Z. These map on to the LC classification scheme relatively well, so it's not a completely arbitrary mapping. 2) Some genres are mostly red, meaning they're entirely sui generis. Most notable is fiction, PZ, which is also the largest one in the collection, and the other P-categories; QA, math; and TK, electrical engineering. 3) Some genres have green bands running all the way up and down. (Or left and right, since the chart's symmetric). Q, R, T, F, and G are like this; notably, those are all general classes. So the books classed as 'general science' or 'general technology' actually do have some lack of specificity, either individually or when averaged out, that makes them closer to random other books. That's sort of interesting. Still, it doesn't exactly look like the genres are placed in the best of all possible orders. AE (encyclopedias) looks more like science than like its nearest neighbors in the B category, psychology-philosophy-religion (although that category, which has always felt too much like a grab-bag to me, actually coheres very nicely in a sea of green. The early Ps, which are world literature and literary studies, look more like world history (the Ds) than they do like the bulk of fiction in PR, PS, and PZ. And so on. So, can we create a single best linear ordering? No, not really. The data is too dimensional for that. That would be like trying to create a single ordering of the cities in North America from the distance grid in the corner of a AAA map. You could run a spectrum from San Diego to St John's Newfoundland, or from Vancouver to Miami; either would make sense, but neither would work, because the data is fundamentally two-dimensional. (I actually just tried this using lat-long coordinates; principal components analysis runs a spectrum from Providence RI to Eugene Oregon, that Bangor and Vancouver end up in the inside of.) Here, the data has many more than 2 dimensions, which makes a single useful ordering all the less likely. What we can do, though, is create any number of somewhat useful orderings; to extend the analogy, the best ranking of the cities in North America for me is going to be their distance from Somerville, MA. So we can rearrange this chart by showing the distance of various genres from QH, natural history: Reading down from the left, QH is identical to itself; next closest is Q (general science, then QL (zoology), QP (physiology), and so on. On the face of it, this doesn't look much better, or much worse, than the original one. We still get some nice groupings, but outside of a few helpful rearrangements close to QH (anthropology is like natural history!) it's more arbitrary than the original LC ordering, and certainly not as good as the hierarchy I built using textual data a while back. What's potentially interesting, though, about that sort of ordering is that it lets us look at how transmission moves--or doesn't--across those similarity lines. We know that Q is statically similar to S, and not so to PR; when language changes, how do those similarities affect the changes that happens? So that's what's next.I like this essay by John Jones about search algorithms, which he compares to “mechanical Turk” automatons of the 18th Century.It’s a point that’s well-understood in some circles and completely not in others. Witness the degree to which users continue to express some preference for couching search queries to Google and Siri in the form of natural-language questions: according to Bo Pang and Ravi Kumar, that tendency seems to be steadily increasing as users become more familiar with the functioning of search engines rather than decreasing. Users sometimes relate to Google as if it were an oracle, a non-human being with its own personality and knowledge.Understanding search algorithms as Jones describes them means understanding that however you phrase your query, you’re really asking us, not a creature named Google or Siri. It’s not quite garbage in, garbage out, but it is “what the set of all users and producers of online information know in, what the set of all users and producers of online information know out”. The really tricky thing is to understand how extensive use of that process both changes and expands that set: not just that we put more information online, but that information begets information.When I started research on the content of children’s television for a co-authored book that was published in 1999, I had three principal sources of information to draw upon. First, my memories and my brother’s memories of watching TV. Second, the memories of contemporaries gathered from real-world conversations and in online discussions on Usenet and other early forums. (Hooray for alt.society.generation-x!) Third, published resources of various kinds, both old and new. Online information about children’s television, independent of message board conversation, was fairly sparse.Only a few years later, Wikipedia, YouTube and so on came into existence, and at the same time, owners of media libraries began to much more comprehensively push their content out the door in various formats. Today if I want to see every episode of Jabberjaw, know every voice actor’s casting on the show, get comprehensive information about its production and broadcasting, the title character’s appearances in other Hanna-Barbera shows, and the lyrics to a song about the show by the band Pain, I can.The general implications of this shift are constantly, incessantly discussed. But what I’m not so sure we fully appreciate are the specific implications of online information as a mirror of what we know and how knowing what we know is something that we’ve never really known before.It’s true that there are still many things that people know, many kinds of information, which are not strongly represented in online repositories. It’s also true, as Eli Pariser has eloquently explained, that both the deliberate infrastructure of online information and the unintended practices arising from our collective use of it, is actively excluding or hiding some information through a progressively tighter series of feedback loops. Even if the “filter bubbles” were popped in some fashion, there would be human ways of knowing and interpreting that could never be adequately included in the most capacious digital informational space imaginable.Those cautions noted, there is still a huge unused potential for generative changes to the nature of knowledge production that requires making the intellectual paradigm shift that Jones describes, to understanding the mirror of online information for what it is and looking closely at the never-before-seen reflection it provides. Just to cite one example that I have harped on so constantly that I’m sure my Swarthmore colleagues are tempted to punch me in the face every time I say it, suppose that every professor in every institution in the United States published every syllabus they taught in a form where the materials for the course (texts, images, films, etc.) were easily stripped and aggregated as metadata.Suddenly the canon in a particular field of study would not be a matter of folk knowledge within a discipline, or would not be knowledge residing in four or five highly fragmented and proprietary archives (publishers, disciplinary associations, bookstores, etcetera). We’d know at any one moment what professionals in a particular field of study deemed to be the most teachable, useful or authoritative material. We’d know over time how that judgment had changed. We’d know if what scholars represented as authoritative through citations was significantly different from what they chose to teach.Notice all the things that this knowledge doesn’t resolve in and of itself. It doesn’t tell us what to teach. It doesn’t tell us why or how to teach it. It doesn’t tell us if there’s a very large missing set of materials that professors would prefer to teach but cannot obtain (either out-of-print materials or things which have never been written or created). It doesn’t tell us what students did with this material, or how and whether they learned from it.What does it tell us, then? It tells us what mirrors always tell us, if we look at them without flinching: the gap between how we look and how we imagine and claim we look. The mirror of information, our multitudinous automaton, shows us hidden depths we’ve never noticed and blemishes we’d rather not see.Some of what we see makes clear what a mirror will never show us (whip out your Zen koans here: your face before you were born and all that).Some of what we see puts older just-so stories and tall tales in their place, and that’s no small feat. Think about the way that academics have traditionally represented (and deconstructed) canons to each other. A comprehensive picture of pedagogical usage might surprise us in all sorts of ways, change our sense of what we think our practices are. Yes, with some potential for perverse or unintended effects, as in the case of comprehensively tracking citations and using citations as a metric of scholarly value. But mostly I think it is fantastically generative to be able to put aside a massive swamp of arguments and studies that never get beyond an initial attempt to answer the question of “what is it that people actually do“, whether or not the answer is what we expected it to be. Whether we’re scraping data from World of Warcraft to find out what the distribution of character choice is, compiling the totality of all print publication in world history, or learning what it is that we actually all use in our classrooms, what we see isn’t just the end of some fumbling-in-the-dark, it is the beginning of some more interesting conversations.The mirror of information clears out the dead brush from the undergrowth. If we know, really know, that some high-culture canons are an infinitesimal fraction of the totality of global cultural production over the last five hundred years, it sharpens our conversation about why that happened, whether we should be studying all of the occluded culture that was lost in the light of a thin crescent of publication or creation, or whether there’s some reason to stay focused largely on that fraction. If we really know what we’re all teaching, what we value in that context of usage, we might have a far clearer view of what we’re trying to accomplish in creating scholarship, of how we read and interpret knowledge, of what works out in usage.Understanding that search algorithms are a mechanical Turk–that it’s just us hiding inside–is, if we choose to see it as such, another chance to step towards wisdom through self-knowledge.Fred and I got some fantastic comments on our Hermeneutics of Data and Historical Writing paper through the Writing History in the Digital Age open peer review. We are currently working on revising the manuscript. At this point I have worked on a range of book chapters and articles and I can say that doing this chapter has been a real pleasure. I thought the open review process went great and working with a coauthor has also been great. Both are things that don’t happen that much in the humanities. I think the work is much stronger for Fred and I having pooled our forces to put this together. Now, one the comments we got sent me on another tangent. One that is too big of a thing to shoe horn into the revised paper.On the Relationship Between Data and EvidenceWe were asked to clarify what we saw as the difference between data and evidence. We will help to clarify this in the paper, but it has also sparked a much longer conversation in my mind that I wanted to share here and invite comments on. As I said, this is too big of a can of worms to fit into that paper, but I wanted to take a few moments to sketch this out and see what others think about it.What Data Is to a Humanist?I think we have a few different ways to think about what data actually is to a humanist. I feel like thinking about this and being reflexive about what we do with data is a really important thing to engage in and here is my first pass at some tools for thought about data for humanists. First, as constructed things data are a species of artifact. Second, as authored objects created for particular audiences, data can be interpreted as texts. Third, as computer processable information data can be computed in a whole host of ways to generate novel artifacts and texts which themselves open to interpretation and analysis. This gets us to evidence. Each of these approaches, data as text, artifact, and processable information, allow one to produce/uncover evidence that can support particular claims and arguments. I would suggest that data is not a kind of evidence but is a thing in which evidence can be found.Data are Constructed ArtifactsData is always manufactured. It is created. More specifically, data sets are always, at least indirectly, created by people. In this sense, the idea of “raw data” is a bit misleading. The production of a data set requires a set of assumptions about what is to be collected, how it is to be collected, how it is to be encoded. Each of those decisions is itself of potential interest for analysis.In the sciences, there are some agreed upon stances on what assumptions are OK and given those assumptions a set of statistical tests exist for helping ensure the validity of interpretations. These kinds of statistical instruments are also great tools for humanists to use. However, they are not the only way to look at data. For example, most of the statistics one is likely to learn have to do with attempting to make generalizations from a sample of things to a bigger population. Now, if you don’t want to generalize, if you want to instead get into the gritty details of a particular individual set of data, you probably shouldn’t use statistical tests that are intended to see if trends in a sample are trends in some larger population.Data are Interpretable TextsAs a species of human made artifact, we can think of datasets as having the characteristics of texts. Data is created for an audience. Humanists can, and should interpret data as an authored work and the intentions of the author are worth consideration and exploration. At the same time, the audience of data is also relevant, it is worth thinking about how a given set of data is actually used, understood and how data is interpreted by audiences that it makes its way to. That could well include audiences of other scientists, the general public, government officials, etc. In light of this, one can take a reader response theory approach to data.Data are Processable InformationData can be processed by computers. We can visualize it. We can manipulate it. We can pivot and change our perspective on it. Doing so can help us see things differently. You can process data in a stats package like R to run a range of statistical tests, you can do like Mark Sample and use N+7 on a text. In both cases, you can process information, numerical or textual information, to change your frame of understanding a particular set of data.Data can Hold Evidentiary ValueAs a species of human artifact, as a cultural object, as a kind of text, and as processable information data is open to a range of hermeneutic processes of interpretation. In much the same way that encoding a text is an interpretive act creating, manipulating, transferring, exploring and otherwise making use of a data set is also an interpretive act. In this case, data as an artifact or a text can be thought of as having the same potential evidentiary value of any kind of artifact. That is, analysis, interpretation, exploration and engagement with data can allow one to uncover information, facts, figures, perspectives, meanings, and traces which can be deployed as evidence to support all manner of claims and arguments. I would suggest that data is not a kind of evidence; it is a potential source of information which could hold evidentiary value.Comments OffEditors’ Note: For those interested in networks and network visualization, a series of posts from Scott Weingart and Elijah Meeks that introduce, explain, and provide examples and instructions for the analysis of networks are linked below. *updated 1/4/12*Scott Weingart, Networks Demystified 2: Degree, December 17, 2011Today I’ll cover the deceptively simple concept of node degree. I say “deceptive” because, on the one hand, network degree can tell you quite a lot. On the other hand, degree can often lead one astray, especially as networks become larger and more complicated. A node’s degree is, simply, how many edges it is connected to. Generally, this also correlates to how many neighbors a node has, where a node’s neighborhood is those other nodes connected directly to it by an edge. In the network below, each node is labeled by its degree. Read Full Post Here.Scott Weingart, Demystifying Networks, December 14, 2011A bunchofmyrecentposts have mentioned networks. Elijah Meeks not-so-subtly hinted that it might be a good idea to discuss some of the basics of networks on this blog, and I’m happy to oblige. He already introduced network visualizations on his own blog, and did a fantastic job of it, so I’m going to try to stick to more of the conceptual issues here, gearing the discussion generally toward people with little-to-no background in networks or math, and specifically to digital humanists interested in applying network analysis to their own work. This will be part of an ongoing series, so if you have any requests, please feel free to mention them in the comments below (I’ve already been asked to discuss how social networks apply to fictional worlds, which is probably next on the list). Read Full Post Here.Elijah Meeks, More Networks in the Humanities or Did books have DNA?, 12/6/11I thought, though, that I might post the slides I used to describe networks in general and the examples using network analysis and representation based on the literature network that Matt has produced for his research. I’m never sure about who’s in a digital humanities audience and whether they need to have the most basic aspects of a network explained. As I said during the presentation yesterday, I think there are three pillars to DH research: Text Analysis, Spatial Analysis and Network Analysis. The network is not a social network or geographic network or logical network but rather a primitive object capable of and useful for the modeling and analysis of relationships between a wide variety of objects. I continue to have a sneaking suspicion that Image Analysis is something else that sits with the aforementioned three, especially after witnessing the presentations at HASTAC. Read Full Post Here.Scott Weingart, Contextualizing Networks with Maps, 11/22/11Just as networks can be used to contextualize text (and vice-versa), the same can be said of networks and maps (or texts and maps for that matter, or all three, but I’ll leave those for later posts). Now, instead of starting with the maps we all know and love, we’ll start by jumping into the deep end by discussing maps as any sort of representative landscape in which a network can be situated. In fact, I’m going to start off by using the network as a map against which certain relational properties can be overlaid. That is, I’m starting by using a map to contextualize a network, rather than the more intuitive other way around. Read Full Post Here.Scott Weingart, Topic Modeling and Network Analysis, 11/15/11Since shortly after Blei’s first publication, researchers have been looking into the interplay between networks and topic models. This post will be about that interplay, looking at how they’ve been combined, what sorts of research those combinations can drive, and a few pitfalls to watch out for. I’ll bracket the big elephant in the room until a later discussion, whether these sorts of models capture the semantic meaning for which they’re often used. This post also attempts to introduce topic modeling to those not yet fully converted aware of its potential. Read Full Post Here.Elijah Meeks, Visualization of Network Distance, 11/11/11I’ve just finished my first Gephi plugin, which distorts a geographically laid out network to emphasize network distance. The NBM can be found here and the source code is on GitHub. The layout takes the XY coordinates of the node and maintains their angle from a defined central node but increases the distance to match network distance. This is better shown than written. It was created for Walter Scheidel, who wanted to see how the transportation network of Imperial Rome would look if subject to the same transformations he’d seen applied to the London Subway network. Read Full Post Here.CommentsHas been a long time since I started working (with others) on building a new model of (Mozilla) community here in Barcelona and around (and I know I have to write more about this experience).“What is the future of learning, freedom and the web? It’s a slate of ongoing projects. It’s a percolating of new ideas. It’s a crossbreeding of old categories. It’s a building of new relationships. It’s a founding of new organizations. It’s the construction of new systems. It’s the coining of new words. It’s the creation of a new reality.Together. [..] What really keeps a community going? Shared work, shared goals, shared fun, shared vocabulary, and shared rituals. There doesn’t have to be one ultimate unified vision. The idea of what learning will mostly look like in ten years, 50 years, or 100 years remains fuzzy, and that’s by design, because one definition of an improved future is one that has a greater diversity of choices than in the past. [..] In many ways, the medium speaks louder than the message” – From the book Learning, Freedom and the Web.When I refer to a new model of community, I mean designing new processes, creating new event frameworks that can invite others to participate and adopting new practices for community development. That can mean interacting with people you’ve never did before, start conversations with other communities of practice, delegate responsibility and act as a coach for the new, future community leaders. This may seem uncomfortable, but after starting doing it you’ll have a lot of fun.Early this year, I wrote a post about computer users groups (which evolved into specific operating systems / programing languages / technologies groups) and how those groups kept alive the whole tech. community, the hacking spirit. Then, I asked myself if Web(maker) local groups/clubs could nurture a movement around the Open Web? And I think they could. There are a lot of good things to learn from Users Groups: their self-organizing / decentralized culture, their focus on sharing knowledge and help others to understand software.The various user groups and communities of practice I belonged in the past years helped me not only to learn more about software and tech. platforms, but also nurtured my skills and, most importantly, made me adhere to a set of ethics of software/technology development. However, I believe we could innovate more in this space, from the community building perspective. We now have the tools and the platform to do this. The ethics stance has been one of the dominant values of groups around free software / GNU Linux.Bellow, I’m highlighting some outputs and insights I got from some events I facilitated in the last six months. In concrete, I’ll take Hackasaurus program – because it demonstrates that can offer local groups and communities the building blocks needed to tinker, create and learn, and potentially helping build this kind of mixed, different local groups/communities.Photo by Samuel HuronMozilla’s Hackasaurus is a great tool that helps simplifying web making and learning, so a broader audience could at least understand what the Web is and how it works. But, it also opens a way to a lot more and engages curious users into building the web and take a more active part into a community.Here are three events where we brought Hackasaurus to different audiences:June, HackJam Barcelona, CitilabOutput: In June we organized the first HackJam in Barcelona. We spent one hour with 16 kids (10 – 12 years old), introducing them to the first phase of web making (understanding the remixable/buildable nature of the web). We started with a small, simple example of a “remixed paper magazine” (collage), so participants could see how they can change a story, remix a content just by experimenting and playing around. Then, we dived into playing with X-Rays Googles and remixing some webpages.In this session we didn’t have enough time to deep into HTML/CSS and , but still, I learned a lot from it. And it was a great achievement to help kids discover that the web is something you can play with it, remix, build it, use it to express your feelings, vision, etc.Although by the end of the sessions I had the feeling of “we should have done more… (dive more into making and the understanding HTML)”, after seeing the enthusiasm of participants presenting their “hacks” I realized that we actually unlocked a critical part of the web building blocks, opening the way to more. And most importantly, part of the participants wanted to learn more and play more with the tool.October, facilitators HackJam, Televall TelcentreOutput: This was a second HackJam Enric and I facilitated, this time in a village by the Pyrenees range, at Televall – the first telecenter built in Catalonia. The audience was very different from the first one: four facilitators from the very Telecenter. We spent almost two hours demoing with Hackasaurus X-Googles Rays and remixing content on webpages. The aim was to understand the potential that today’s web has: you can build almost everything on it and there are also tools as Hackasaurus that simplify the understanding and learning process and engage in a creative way.Before the session, I had a conversation with people there about the activities and life at the telecentre. “Now, teens and kids have their own computers and access to Internet. They go straight to home after school”. It was somehow sad to see how those spaces that years ago pioneered the access to Internet and technology are at now risk of losing their role and impact in the local digital society. But, at the same time, I was thinking about the opportunity we, local community of practice as Mozilla, could offer to those spaces, making them again be at the heart of digital transformations.How using Mozilla’s tools and learning programs such as Hackasaurus, Popcornmaker or Hive (a learning network concept) could make Telecentres think and be more like the Web, transform them from the simple “access to computers / Internet points” to community learning centers – building local communities and help local youth become web literates.November, demoing Hackasaurus and Popcornmaker, Digital Humanities event, CCCBOutput: Last month we were invited to join the Center of Contemporany Culture of Barcelona and Institute of Innovation and Research Center at George Pompidou from Paris to participate at the annual Digital Humanities event. More than 30 mediators, facilitators, educators and representatives of public, cultural and research institutions participated. Enric, Toni and I helped with a one hour hands-on session demoing Popcornmaker, Hackasaurus and Universal Subtitles. Then we participated in a broader conversation – “Education and contributions in the future of Digital Humanities”, where we explored the impact that informal, community-learning education has in shaping the future of Digital Humanities. The debate around education and Digital Humanities was very insightful for me. One of the starting points of this debate was about the decrease of attention coming as a natural effect of new technologies (as Internet, Web, Social networks), especially among teens and kids. This raised a series of questions and answers around how we can use the same technology to transform those effects into something good to society (adapt to technological/connected life and not remain indifferent, make efforts to understand how technology works instead of taking it for granted, start using technology to create not just consume it).And in a way, Hackasaurus or Popcorn can transform those effects and focus the attention to creating/building/remixing/experimenting.Humanities, a critical part of our education system, need to adapt much more to new technologies. Around the topics brought into discussion were: thinking about the creation of a new school, how community based learning models and programs could reshape the education in a digital, connected age.One of the most interesting parts was when discussing about the need of building new communities of practice (communities that could drive innovation, initiate new processes and, above all, be driven by a desire to create and make things) and build relationships between local communities of practice and cultural/research public institutions (museums, public libraries etc.).There were more events this year that influenced part of my thinking about communities in general (and Mozilla Community in a first place) – the GlobalMelt workshop, Design Jam Barcelona, the small and informal Barcelona UX/WebDev community meet-ups, MoJo Hackfest. And this year I also focused more on working with local civic centers and public spaces – which I consider critical for building new healthy communities (more on this in the following blogposts). Mozilla started two years ago to explore new ways to advance in its mission, grow and rejuvenate the community, diversify our interest domains and expand focus (go beyond Firefox). Now, with programs and tools as Hackasaurus and Popcorn that are getting stronger, the work on Identity and Apps Ecosystem is a huge opportunity to put efforts on building a new kind of community (both local and global), inspire others and promote a new way of working and building relationships.Building a community of Webmakers with an ethics stance at its core (build web using native web technologies, work in open, share, respect the user) is one of the long-term goals Mozilla has. And there are lot of things to do to achieve this, a lot of things to change and some exciting upcoming years for designing this community. In the next blog posts I want to highlight some of my experiences and share some concrete steps on how we might do this, at least at the local level. This is the first post from a series through I’ll try to express my personal thinking and vision on Community Building and Development.December 13, 2011 · PodcastsInstitute of the Humanities and Global Cultures: Humanities in a Digital Age SymposiumOn November 11th, the University’s new Institute of the Humanities and Global Cultures hosted a daylong symposium on “The Humanities in a Digital Age.” The symposium included two panels—one on Access & Ownership and the other on Research & Teaching—and two keynote talks.The first keynote was given by Stephen Ramsay, Associate Professor in the Department of English and Fellow in the Center for Digital Research in the Humanities at the University of Nebraska – Lincoln.The second keynote was given by Dan Cohen, Associate Professor in the Department of History and Director of the Roy Rosenzweig Center for History in New Media at George Mason University.Panel 1: Access and OwnershipJeremy Boggs, Humanities Design Architect, UVa Library Scholars’ LabAnn Houston, Director of Humanities and Social Sciences, UVa LibraryKeynote: Stephen Ramsay, “Textual Behavior in the Human Male”Panel 2: Research and TeachingAlison Booth, Professor, Department of EnglishMitch Green, Horace W. Goldsmith Distinguished Teaching Professor of Philosophy, Department of PhilosophyKeynote: Dan Cohen, “Humanities Scholars and the Web: Past, Present, and Future”As always, you can listen to (or subscribe to) our podcasts on the Scholars’ Lab blog, or on iTunesU.Ronda GrizzleRonda is the outreach specialist for the SLab; a librarian by both training and inclination; fascinated by organizational and personal development; personal coach; owned by a bossy dog.Starts with results of an informal poll conducted among museum technology professionals at MCN 2011. Continues with 4 more in-depth case studies: the Indianapolis Museum of Art (IMA), Tate, MoMA, and t...Starts with results of an informal poll conducted among museum technology professionals at MCN 2011. Continues with 4 more in-depth case studies: the Indianapolis Museum of Art (IMA), Tate, MoMA, and the Walker Art Center (WAC). What are the impacts of digital publishing on organizational structures, workflows, and institutional voice? Presented in Rotterdam at DISH 2011.I have a confession to make: I've never cared much about museums on the Web. I'm focused on the onsite, in-person experience. When smart people talk about digital museums and virtual experiences, I nod and compartmentalize it as someone else's bailiwick. I understand the value of having a web presence that is reflective of institutional brand, makes content available for people to use in a variety of ways, and enables new connections with community members. But I've never really understood what it could mean for a museum to create a website that has a complementary function to the physical institution--an entity in its own right that expands beyond the scope of the physical institution, programs, and collection. Now, I think I'm starting to get it. Last week, the Walker Art Center launched a major website redesign, which museum geeks are hailing as "a potential paradigm shift for institutional websites," (Seb Chan) and an "earth-shaking game changer" (Museumnerd). Here's what I see: a website as a unique core offering--alongside, but not subservient to, the physical institution. Walkerart.org is not about the Walker Art Center. It is the Walker Art Center, in digital form. The new site resembles an online newspaper, featuring articles written by Walker staff alongside stories from the greater world of art reporting on the web. While there is a tight menu of Walker Art Center offerings at the top (Visit, Exhibitions & Events, Media, Collections, Join), the rest of the website is a digitally-based panoply of content broadly related to the Walker's mission. It is an online experience about contemporary art that goes beyond the Walker's walls. And it breaks a lot of conventional rules about museum homepages. Such as:It organizes the content primarily by "stories"--a news lexicon that I've never seen used in a museum context.There's lots of content everywhere, including little things you wouldn't usually see on a museum website--like the current weather in Minneapolis, where the Walker resides.It features many stories ("Art from Elsewhere") that were not produced by the museum and are not about the museum.The "Art from Elsewhere" stories all link to sites that are not associated with the Walker. No more lock 'em in and keep 'em here--the theory is that the value of the site is in the curation of links across the web.The name "Walker Art Center" is abbreviated to "WALKER" at the top of the homepage. It's not 100% clear that this thing called walkerart.org represents a museum or facility, though there are ample opportunities to discover that.Is this the future of all museum websites? Probably not. The care and feeding required for a site like this is tremendous. The Walker employs a four-person editorial team (one of whom is completely dedicated to the website), along with a five-person new media initiative group. That's more people than work in my museum total--and a lot more who are dedicated to digital experiences and content than at even the largest museums around the world. But the biggest reason that the Walker site is so unusual is its clarity of purpose. Not only did the Walker have the resources to create a major online project, they had the institutional coherence and focus to make it their primary website. Many, many museums have created superlative online experiences--from the IMA's ArtBabble to the Exploratorium's educational resources to the V&A's collections site any number of exhibition microsites--but these are all offshoots in the museum website universe. What the Walker has done is commit to a unique online approach--not just for one program or microsite, but for their homepage. They took their vision of the institution as an idea hub, looked at comparable sites online that achieve that vision, and adopted and adapted the journalistic approach to their goals. An institution of any size with enough mission-coherence and courage could create a website that is comparably unique. It comes down to articulating your mission in a digital space. Not every museum would choose a journalistic approach. Maybe the metaphor for your institution is a restaurant with a simple set of consistent offerings or a music venue with a constantly rotating program of events. Maybe some museum websites would look like online schools, or community bulletin boards, or shopping sites. But I suspect that most of them would continue being a little of this, a little of that, with a brochure for visiting slapped on top. And I think that's ok too if your goal is to have a physical museum with a website that supports it. But if you want to create a digital museum as a partner to the physical, take note. Thank you, Walker Art Center, for showing me one version of what this can look like.KEYNOTE VIDEO: The Future of Digital Publishing - Phil Pochoda (chair), University of Michigan; Tara McPherson, University of Southern California; Dan Cohen, George Mason University; Richard Eoin Nash: Founder of Cursor, a site to coordinate a portfolio of onlineA common problem in search and exploration interfaces is the vocabulary problem. This refers to the great variety of words with which different people can use to describe the same concept. For people exploring a text collection, this makes search difficult. There are only a limited number different queries they can think of to describe that concept, but they may be missing many other instances that use different words. This is an important issue for humanities scholars. Often, the very first step of a literature analysis is to comb through text, trying to find thought-provoking examples to study later.In this post, I give an example of how our project WordSeer, a text analysis environment for humanities scholars, can be used to overcome this problem. In this example, I’ll using an instance of WordSeer running on the complete works of Shakespeare from the Internet Shakespeare Editions. It’s live, so you can follow along with this example on the web at wordseer.berkeley.edu/shakespeare.You can read the post after the jump, or just watch this video.I began with a simple question, “What are some things that are ‘beautiful’ in Shakespeare?”. Normally, this would be a challenging question by itself. WordSeer, however, uses grammatical search. This is a search feature that goes beyond keyword search and instead also searches over grammatical relationships between words. These are relationships such as subject-object and modifier-subject. For example, in the sentence “The good God has given every man intellect”, there is a relationship between “good” and “God” — “good” is an adjective that modifies ”God”. There is also a verb-agency relationship between “God” and “given”, and a verb-object relationship between “given” and “man”. For more than a decade, it has been possible to automatically extract such relationships from text using computational linguistics algorithms. WordSeer uses these well-known algorithms to analyze the works of Shakespeare (in this case) and allow users to search over them.Figure 1: The query "____ described as beautiful", which retrieves all words to which the adjective "beautiful" has been applied.In the case of my question, “What are some things that are ‘beautiful’ in Shakespeare?” I can use grammatical search to good effect, as shown in Figure 1 above. I leave the left-hand-side box blank to retrieve all matches, select the “described as” grammatical relationship, and put in “beautiful” to create the fill-in-the blanks query “_____ described-as beautiful”. I press “Go” to search, and WordSeer returns all the matching sentences. These are sentences containing the adjective “beautiful”, applied to some other word. I get the results in Figure 2.Results for the query _____ described-as beautiful. To my alarm, there was only one match!To my alarm, there was only one match: the sentence “His youngest daughter, beautiful Bianca”, from The Taming of the Shrew. Concerned that my algorithms had malfunctioned, I did a simple search for the word beautiful, without any grammatical relationships. Lo and behold, there were only 16 results (Figure 3).Figure 3. Search results for the word "beautiful" in Shakespeare. There were only 16 results.I had encountered the vocabulary problem. Not being a Shakespeare scholar, I couldn’t think of any other words that could have been used instead of “beautiful”, and it seemed preposterous that these were the only results in all of Shakespeare. There must be other words.Figure 4. Clicking on the "book" icon next to a search result opens a new window with the full text of the play, and automatically scrolls to, and highlights, the matching sentence.To investigate further, I decided to read some of the context around the word “beautiful”. To do this, I clicked on the “book” icon to the left of my “beautiful Bianca” search result. This brought up a new window (Figure 4) with the full text of The Taming of The Shrew, opened up to the exact line matching the search result.Figure 5. Right-clicking on "beautiful" (or any other word) brings up synonyms of that word -- computed based on similar usage to the query word.After convincing myself that “beautiful” did actually mean what I thought it did in Shakespeare, I decided that I needed to see synonyms. WordSeer supports this need. Using the contexts of words, it computes synonyms based on other words that “behave” in the same way — that are used in the same contexts, that have grammatical relationships to similar words, and so on. Right-clicking on a word while reading (Figure 5) brings up synonyms. These are computed based on being used in a similar way to “beautiful” in Shakespeare, and not based on some external measure of similarity, such as a dictionary or thesaurus. Therefore, they reflect the particular idiosyncrasies of just the Shakespeare collection.Figure 6. Adding some interesting synonyms to the heat map query -- click a word to add it to the heat map query.This list of synonyms seemed promising. It contained words such as “tractable”, “fair”, and “gentle”, that I would never have thought of including in my initial search. To investigate whether these were more widespread, I decided to investigate their prevalence in the collection using WordSeer’s heat map tool. I clicked some interesting words and added them to my query (Figure 6). This took me to the heat map view.Figure 7. The collection-wide occurrence of some of the synonyms of "beautiful". It is easy to see that these are much more prevalent.WordSeer heat maps can be confusing if you have never seen one before, so I’ll explain them here. If you know what this means, you can skip this section.Heat MapsWordSeer uses heat maps to visualize collection-wide occurrence patterns of words and phrases. In this example, I’ll use the word “fair” to illustrate.Figure 8. The column corresponding to "Macbeth"Typing in “fair” creates the pretty picture in Figure 8 above. Each vertical column is a single document — in the picture, I am hovering over the column corresponding to “Macbeth”. The documents are lined up side by side in long vertical columns.Figure 9. The column corresponding to "A Midsummer Night's Dream"All of Shakespeare’s works are here . Figure 9 shows the column corresponding to “A midsummer night’s dream”. The blue highlights show occurrences of the query. In each vertical colum, blue blocks indicate that the query word, in this case “fair”, has occurred in that location. Blocks higher up in the column mean that the word occurred near the beginning, and blocks lower down in the column mean that the word occurred towards the end of the document. The documents all “appear” the same length, so shorter documents are “stretched” (a few taller blocks) and longer documents are squeezed (many squat blocks).Figure 10. The blue highlights show where the query word has occurred in each document.Hovering over a highlighted block brings up a window showing the matched sentence. In this case, I’ve hovered over a line containing “fair” from “The Tempest”. The popup shows the line, with “fair” highlighted in the same color, and a book icon. Clicking the icon opens up a new window, in which I can more of the text if I wish to.But back to the the vocabulary of beautyFigure 11. Heat map showing synonyms of the word "beautiful"The heat map I got from my synonym query (Figure 11. above) showed that, although “beautiful” was quite a rare word, other synonyms for it seemed much more prevalent. “Fair”, in particular, seemed to be used a lot — the whole map was purple. Hovering over individual instances of “fair” showed that it did seem to be used the way “beautiful” is in today’s english.The word tree for "fair". This is a visualization of the contexts surrounding the word "fair".For further verification, I looked at the word tree for beautiful (Figure 12), which was displayed on the same page just below the heat map. It showed that “fair” was used in constructions like “fair and virtuous”, “fair and happy”, “fair and good”.Figure 13. A revised query: "_______ described-as fair"All this evidence convinced me that I might be better to use the word “fair” instead of “beautiful” to investigate the concept of beauty in Shakespeare. Returning to the search page, I typed in a new grammatical search query – ”_________ described-as fair”. The results (Figure 14) were much more informative:Figure 14. Search results for the grammatical search query "_________ described-as fair"Because there was more than one result this time, WordSeer showed a bar graph summarizing the matches. At a glance, I could see that I was on the right track. There were a lot of women’s names, interspersed with other words like ”queen”, “daughter”, and “day”. It seemed that I had successfully overcome the vocabulary problem, at least this time. I had a starting answer to my original question, “what are some things that are ‘beautiful’ in Shakespeare?”The goal of text analysis interfaces like WordSeer is to properly combine powerful language processing algorithms with easy user interfaces. Neither is be enough by itself, but together, they allow users to progress naturally from step to step, assisting them through an analysis.Like this:Be the first to like this.THATCamp ranks as my favorite conference experience, mostly because it blows apart the passivity and formality of a traditional conference to get to the essence, bringing people together to share ideas. After attending Startup Weekend Houston a few weeks ago, I now have another event to add to my list of favorite conferencey experiences. Just as THATCamp challenges attendees to set and steer the agenda, Startup Weekend leaves a lot up to the participants, who have 54 hours to pitch a product idea (typically tech-related), form teams, validate their idea, develop a business model, and put together a demo and a longer pitch.Houston Startup Weekend Saturday Nov 2011-4 by sarahmworthyLike THATCamps, Startup Weekends are low-cost ($99 or less), community-driven events that take place around the world and are run primarily by volunteers, who receive help from the Startup Weekend’s central office in staging the event. Startup Weekend provides the key challenge, overarching structure, and access to many of the resources you need to build your project, such as excellent mentors, tools that help you to flesh out your ideas, coffee, good food (including a nirvana-inducing chocolate malt cupcake from Houston’s fabulous Kitchen Incubator), and meeting space.Kitchen Incubator feeds Houston Startup Weekend Nov 2011-2, by sarahmworthySome might wonder what entrepreneurship training has to do with the digital humanities (DH), but I believe that the two communities have much in common and can learn from each other. As startup guru Steve Blank suggests, startups exist to “search for a repeatable and scalable business model,” which itself is “how your company creates, delivers and captures value.” While DH projects typically don’t form companies and don’t aim to make a profit, most do need to consider how to define their value, find users and sustain themselves. To get off the ground, DH projects go through a process similar to a start-up: identifying a need and potential solution, drafting project plans, putting together a team, building a prototype, iterating on that prototype, and disseminating the product (whether a tool, collection, model, publication, or large-scale research). Both the DH and lean startup communities have embraced similar principles, such as agile development, user-focused design, open source software, and iteration. In a broader sense, I believe that DH brings the spirit of entrepreneurship–taking risks, experimenting, building something that serves a need, innovating, tolerating failure–to the humanities. We can see this spirit manifested in the NEH’s Digital Humanities Start-Up grants, the many digital humanities “Labs” (a term also used frequently by startups and tech companies), and One Week One Tool, which was inspired by “crash ‘startup’ or ‘blitz weekends’.” In a sense, many DH centers serve as startup incubators, providing the know-how and support to help get an idea off the ground.Events like Startup Weekend could address a need in the DH community for more training in successfully launching projects. Often graduate training in the humanities does not prepare people for the complexities of getting a major DH project started and keeping it going. Such training is now being offered at the Digital Humanities Summer Institute (taught by Lynne Siemens, a professor in U Victoria’s school of public administration who does research in entrepreneurship and academic team development), at THATCamp workshops (such as Sharon Leon’s Introduction to Project Management in Digital Humanities), as part of DH educational programs such as UVA’s Praxis Program, and in publications such as Sharon Leon’s Project Management for Humanists:Preparing Future Primary Investigators. I think StartUp Weekend offers another compelling model for providing training in a fast, fun and experiential way.Here some ideas from Startup Weekend that I think have relevance for the DH community:challenge & competition: At Startup Weekend, your team competes against others to persuade a panel of expert judges that your product is the best. Competition adds energy and intensity to the weekend (and a little stress).criteria: In evaluating projects, judges assess customer validation, business model, and execution, as well as overall effectiveness. These criteria help to structure the challenge and give teams concrete elements on which to spend their limited time. If you look at grant guidelines, different terms are used, but the criteria are similar. Have you conducted needs analysis to determine whether there is an audience for your project? Have you validated whether your ideas will meet those needs? Do you have a model for sustaining the project?collaboration: Like many DH projects, Startup Weekend requires collaboration among a range of people, including developers, designers, and business development specialists. Not only do you create a better product, but you also learn from each other–and have more fun in the process.mentors: Startup Weekend Houston recruited a great group of mentors who gave up part or all of their weekend to work with project teams. Mentors asked tough questions, suggested ways to approach problems, connected us with people who could help us test or advance our ideas, and provided feedback on startup ideas and business plans. The DH community also offers mentors, such as through the ACH mentoring program.communication: As you try to explain your project to a range of people, from a provost to your next-door neighbor, it helps to know how to pitch an idea succinctly and persuasively. At Startup Weekend, the first big event (following noshing and networking) is the pitch, where you have one minute to describe your project idea and persuade others to join your team. Startup Weekend culminates in a pitch contest, where teams make four-minute pitches to convince the judges that their project is the best. Mentors can help you to develop an effective pitch, and you learn by doing.tools: The startup community has created some handy templates that help teams crystallize the core elements of their startup idea, particularly the Lean Canvas or Business Model Canvas. Developing such a model forces you to think through key questions and gives you a handy reference as you share your ideas with others.Startup Weekend has recently begun sponsoring events focused on education, as documented by Audrey Watters’ great series of posts on gatherings in DC, Seattle and San Francisco. Why not Startup Weekend Digital Humanities? Of course, the digital humanities community already offers some events that serve a similar purpose. For example, at this year’s MLA Convention the DH Commons is hosting a workshop in which veteran digital humanists will share tips on succeeding in the digital humanities and lead small group sessions on topics such as project management, community building, and topic modeling. Although the workshop is now full, DH Commons is also sponsoring a project mixer where people can learn about DH projects that they can help out with. If you have a DH project and want to recruit volunteers or spread the word about it at the mixer, please sign up. (I’m a member of the DH Commons team and would be happy to answer any questions.) In a broader sense, I believe that entrepreneurial thinking can help higher education tackle some thorny challenges, such as improving learning, reducing costs while maintaining quality, and reforming scholarly communication. Thus I’m exploring how to bring entrepreneurial thinking to the liberal arts community through my work at NITLE.Like this:2 bloggers like this.performs an epistemological function by structuring and influencing the ways we acquire knowledge.I’ve created this blog to provide a forum for exploring these issues, as well as discussing how anthropology might most effectively and fruitfully approach the study of algorithms as cultural objects.As a starting point, I am posting below the full text of a paper I recently presented at the annual meetings of the American Anthropological Association, which outlines some of my first thoughts on these subjects.Search Magic: Discovering How Undergraduates Find InformationIntroductionSearching for information might seem like one of the most routine and commonplace activities of university life. However, as students work within an information environment that is increasingly open and dynamically changing, research assignments also represent a complex and potentially daunting task, and one that is fraught with embedded social and cultural processes and relationships.The Ethnographic Research in Illinois Academic Libraries (ERIAL) Project was a two-year study of student research practices involving a collaborative effort of five Illinois universities: DePaul University, Illinois Wesleyan University (IWU), Northeastern Illinois University (NEIU), the University of Illinois at Chicago (UIC) and the University of Illinois at Springfield (UIS). Using a mixed-methods approach that integrated nine qualitative research techniques and included over 600 participants, the ERIAL project sought to gain a better understanding of undergraduates’ research processes based on first-hand accounts of how they obtained, evaluated, and managed information for their assignments.In this presentation, I will focus only on the subset of this data related directly to the search itself. This data is principally drawn from two research methods: 156 semi-structured ethnographic interviews, in which students were asked to demonstrate searches they had conducted for a recent research assignment, and 60 research process interviews, in which a project anthropologist accompanied and observed students as they located resources.Algorithmic CultureMy goal in this paper is not only to describe some of the results from the ERIAL study, but also to illustrate how an ethnography of students’ search practices might demonstrate and contribute to the concept of “algorithmic culture.” Following Ted Striphas (2011a), I use the term “algorithmic culture” to describe how some aspects of the work of culture–“the sorting, classifying, hierarchizing, and curating of people, places, objects, and ideas”– are becoming the purview of “machine-based information processing systems.” Furthermore, “some of our most basic habits of thought, conduct, and expression. . .are coming to be affected by algorithms, too. It’s not only that cultural work is becoming algorithmic; cultural life is as well” (Striphas 2011a).Algorithms are also cultural artifacts themselves, and can be understood as embodying a set of socially and culturally embedded negotiations, decisions, judgments, biases, politics, and ideologies. For example, PageRank, the ranking and relevancy algorithm that comprises the core of Google search, is fundamentally premised on a concept of aggregated social judgment, that is, the assumption that a mathematical calculation based on the number of links to a website combined with an evaluation of the relative importance of the websites from which those links originate, can be used as a proxy for evaluating the quality or value of a site in a way that is analogous to how citations are used to evaluate academic papers (Brin & Page 1998; Page et. al. 1999; see also Battelle 2005:75-76). In addition to PageRank, the Google search algorithm uses a total of more than 200 “signals”[1] to rank its search results–including measures related to localization, personalization, timeliness, and quality (e.g. spam/content farms)–each of which represents a specific decision about the relative value of information. Because of these embedded judgments, “algorithmic culture encourages us to see computational process not as a window onto the world but as an instrument of order and authoritative decision making. The point of algorithmic culture, both terminologically and methodologically, is to help us understand the politics of algorithms and to approach them and the work they do more circumspectly, even critically” (Striphas 2011b).One central characteristic of the politics of algorithms is their secrecy. Secrecy is fundamental to the algorithmic culture expressed both by Google and by other search tools. While the general parameters of Google’s search algorithm are publicly available, its details are proprietary and closely guarded corporate secrets. One justification for Google’s secrecy is the argument that it helps ensure the efficacy and impartiality of search results by preventing websites from gaming the algorithm to artificially inflate their ranking (for example, see Battelle 2005:159-163). Indeed, an entire industry–search engine optimization–has materialized to reverse engineer Google’s algorithm. It is, of course, also in Google’s economic interest to carefully control the availability of information about its algorithms. In this way, the algorithmic culture established by Google, as well as other proprietary search algorithms, ultimately rests on a tension between proprietary knowledge on the part of the corporation and trust on the part of the user (Battelle 2005:183-185). Because search systems can not be properly interrogated by their users, (except, perhaps, by a few with very sophisticated technical skills), these users must simply put their faith and trust in the algorithm and the people who designed it.By shaping the processes through which information is found, and by extension, becomes known, search algorithms perform an epistemological function. By structuring the discovery of information, search algorithms express a form of Foucaultian disciplinary power that provides the scaffolding for how students complete their academic work and profoundly structures the way students acquire knowledge. The secrecy inherent to these search processes and tools should therefore be critically addressed by educators and students alike.Google’s Simple SearchGoogle has become the primary starting point for students for both everyday and academic research, as evidenced not only by the ERIAL project, but also by almost every recent study of student search habits (Connaway and Dickey 2010:28–29; Head and Eisenberg 2009:15; De Rosa 2006:1–7; Prabha, Connaway, and Dickey 2006:13–14,16–18; Griffiths and Brophy 2005:550, 545). For this reason, understanding the embedded cultural politics of Google’s search algorithm is critical to understanding students’ search practicesNot surprisingly, for the students interviewed in ERIAL project, Google was by far the most prevalent search tool used. 88% of the students interviewed discussed Google, and Google was mentioned over three times more frequently than the next most popular search tool (JSTOR). Google’s dominance may also be affecting academic search in other subtle, yet critical, ways. Google’s simplicity and single search box seems to have created the expectation among students of a specific search experience for academic search systems: in particular, a single search box that quickly accesses many resources, and an overreliance on simple keyword search (see Hampton-Reeves et. al. 2009:45; CIBER 2008:14).During the 60 research process interviews conducted for the ERIAL study, the research team observed 161 unique searches.[2] 80% (128) of these searches were for unknown items (e.g., when a student was attempting to discover sources related to a research question, rather than a specific book title or journal article). The vast majority of these searches were simple searches. Students generally treated all search boxes as the equivalent of a Google search box, and searched “Google-style,” using the “any word anywhere” keyword search as a default, even when it was not appropriate or effective to do so. In total, 202 of the 238 (85%) observed sets of search terms used this approach (see also CIBER 2008,14; Hampton-Reeves et. al. 2009, 45). A junior in nursing explained, “I basically throw whatever I want into the search box and hope it comes up. . . . But it’s like Google and I use it like Google. I don’t know how to use it any other way.”Students’ overuse of the simple search leads directly to the problems of obtaining too many or too few search results. These twin problems of “too little” and “too much” information are really one and the same, as both issues stem from a lack of sufficient conceptual understanding of how information is organized and how to build an effective search query. Almost all of the students interviewed by the ERIAL Project exhibited difficulties evaluating and narrowing down (or expanding) search results. When faced with unsatisfactory results, students usually changed the search, either by entering new search terms or trying a different database altogether, rather than using more advanced search tools to expand or refine the search. Perhaps because of their experience with Google, students often appeared to believe that if they could only find the magic words or phrase, whatever piece of information they were looking for would be revealed to them and a manageable list of results would be returned. In this way, the search experience is thus iterative rather than determinative. This practice can lead to students using lower quality or less accurate search terms because they return fewer results (especially in a typical full text search).This is another important expression of the algorithmic culture of search: the Google search experience has conditioned students that search should be simple. Academic libraries and information providers have likewise embraced simplicity as a positive value for search, and have sought to reduce the “cognitive load” of using the various and fragmented catalogs, databases, and interfaces contained on a typical library website (CIBER 2008:30 see also Wong et. al. 2009:6). Much effort has recently gone into making the experience of searching library databases more like Google by the creation of “discovery tools” that can simultaneously search across a library’s many platforms, catalogs, and databases to provide aggregated results. Like Google, these search systems provide a single search box that is easily queried with natural language, and like Google they rely on complex and proprietary algorithms to produce relevancy rankings.However, by making search easier for students these tools can counterintuitively decrease the quality of the search. By enabling students to get to information faster and easier, these systems can also reinforce unreflective research habits that contribute little to the overall synthesis of a research paper or academic argument.Database ChoicesThis process can be illustrated by the usage of academic research databases observed in the ERIAL project. While students routinely began with Google, the ERIAL project and other studies (Gabridge, Gaskell, and Stout 2008:516–17; Head and Eisenberg 2009:3) have observed that students working on academic assignments do eventually consult library databases, particularly when seeking reliable or scholarly sources.When choosing a database to conduct a search, students typically returned repeatedly to a resource that had worked in the past, even if it was not the best or most appropriate for the task (see also Head and Eisenberg 2009:3). Usage statistics gathered during to ERIAL project at the Ames Library at Illinois Wesleyan University reflected this. Of the 101 databases that had one year of comparable search data, the top three databases (JSTOR , PsycINFO, and Academic Search Premier) accounted for 38.7 percent of searches. Usage then fell off quickly, with the next seven databases accounting for 20.4 percent of searches. The remaining 91 databases accounted for 40.9 percent of searches, with 76 of these databases holding a less than 1 percent share of total searches (see Table 1).[3]DatabaseNumber of SearchesPercentage of Total SearchesJSTOR32,11614.78%PsycINFO27,90612.84%Academic Search Premier24,08211.08%Top 384,10438.7%CINAHL Plus with full text94814.36%Hoover’s Online65012.99%MLA International Bibliography61902.85%Science Citation Index57892.66%LexisNexis55152.54%Social Sciences Citation Index54782.52%Arts & Humanities Citation Index54002.49%Top Ten128,45859.11%Remaining 91 Databases88,84040.88%Total434,596100%Table: 1 Database use at IWU, AY 2008-2009.The popularity of the JSTOR database is perhaps illustrative of the privileged place simplicity takes in the algorithmic culture of search. JSTOR is a multidisciplinary content provider of full-text academic journals, and allows students (and scholars) to easily search across up to 1400 journals via a single search interface. JSTOR was extremely popular among the students who were interviewed by the ERIAL project. JSTOR was referenced more than five times as often and by twice as many participants than then next most popular database, Academic Search Premiere. Students appeared to rely on JSTOR disproportionately compared to other academic databases, to an extent that surprised the librarians working on the project, who often did not view JSTOR as the optimum resource for students’ research assignments. However, for students, JSTOR was usually sufficiently robust to meet the minimum requirements of a particular assignment—typically around five sources. JSTOR simply works for a wide range of assignments across a wide range of disciplines, providing fast access to full-text and reliable resources.Students generally did not realize—and had not investigated—the limitations of the database that might make it inappropriate for a given task. For example, students regularly used JSTOR to search for current information, not realizing that JSTOR often does not provide access to the most recently published articles (articles typically only appear in JSTOR after 3–5 years, depending on publisher).[4] Nor did students think to investigate whether or not there was a database that would be more focused on their topic of choice. Students found JSTOR effective because it fit in well with their established work practices. Unfortunately, because it provides access to full-text materials, as well as its flexibility and wide coverage of topics, JSTOR also enabled students to succeed using subpar search strategies because it worked well enough.Trust Bias: “I Never Go Past the First Page”In an information environment where the retrieval of information is increasingly trivial, students’ ability to effectively evaluate information becomes preeminently important. Unfortunately, in the searches observed by the ERIAL project, students’ evaluation of potential sources appeared cursory (see also CIBER 2008:10). Students typically made rapid appraisals of a source’s usefulness, often based only on its title or a superficial scan of its abstract. When evaluating search results, students seldom examined citations past the first page of results, an observation that is supported by Griffiths and Brophy’s recent study of search engine use (2005:551).These practices are also an expression of the disciplining effects of algorithmic culture. Through the act of ordering and ranking, search systems’ relevancy algorithms impart (and reinforce) a sense of authority and credibility in the results. Users regularly assume that information that is objectively “best” will be ranked first. This “trust bias” is well documented in the literature on search engines (Vaidhyanathan 2011:59; Hargittati et. al. 2010; Hargittai 2007; Pan et. al. 2007). Because it holds the power to create a list of results, the search engine self-validates the quality of its results. In this recursive loop, users depend solely on their trust in the search algorithms brand, be it Google, or JSTOR, or something else.This belief that credible and quality resources should appear on the first page or two of search results caused many students observed by the ERIAL Project to assume that if they cannot quickly find information on a topic, then the information must not exist and they should give up on that topic. Only rarely did students conclude that a lack of search results might, in fact, reflect incorrect search terms or an ineffective search strategy.For example, when discussing a recent research paper, a sophomore international studies major observed, “Originally I had a different topic. I was thinking about something that had to do [with] the discrimination of Jews in sixteenth-century London, and I realized that finding information on that would be almost impossible. ’Cause I’m interested in the really obscure topics that you would be like, “that’s really interesting.” But no one really has done anything on that, so it’s really hard to find. So, [crime in nineteenth century London] seemed like it would be easy to find information on, so I decided on that one. . .”Students regularly overestimated how “obscure” a particular topic actually is, and demonstrated remarkable ease in changing topics to fit easily found information. Because of this, students often passed up unique or interesting topics in favor of topics with widespread coverage.Search MagicSearch algorithms can thus reveal or conceal information depending on the skills of the user. Unfortunately, the students who participated in the ERIAL project did not appear to adequately understand conceptually how information is organized or how search works. Of all the students who were asked, none could correctly explain how a search in Google (or any other search engine) works or organizes results. Search results were returned “as if by magic.”Arthur C. Clarke’s observation “That any technology, sufficiently advanced, is indistinguishable from magic” (1973[1962]:21) has been used in relation to Google perhaps to the point of cliché (see Battelle 2005:129; Vaidhyanathan 2011:53). However, we should still attend to the reason why search feels like a magical experience.Siva Vaidhyanathan argues that Google seems magical because of its usefulness for helping its users find meaning by providing a managed and ordered set of actionable choices for a query (2011:53). Coupled with a speed that seems near-instantaneous (see Vaidhyanathan 2011:53-54), this experience makes it easy for users to forget that Google (and other search algorithms) are simply tools, especially since their workings are made intentionally opaque. This secrecy makes it difficult for students to fully understand the embedded politics of how information is organized and retrieved. This lack of “algorithmic literacy” potentially renders students vulnerable to the disciplinary power contained in search systems, as well as subjects, rather than agents, of algorithmic culture.Indeed, and students described experiences of anxiety and confusion as they searched for resources. A senior in women’s studies described her difficulties conducting a search, “. . .finding ways to narrow down, there was just so much information. . . how do I weed out what my specific topic is from the general larger topic? . . . How do I find specifically my information when there’s not a book titled [on] this topic? So, I guess just being overwhelmed with the amount of literature out there [that] doesn’t really relate to my topic and how do I pull my stuff out of it? ’Cause I feel like I was very much kind of blindly branching out and a lot of times by chance finding things and then going on from there.One challenge for educators and librarians is to balance facilitating ease of use with a conceptual understanding of how search works. Search shouldn’t be magic; it’s only when its processes and algorithmic culture are demystified that our students become empowered to use it effectively.Works CitedBattelle, J. 2005. The search: How Google and its rivals rewrote the rules of business and transformed our culture. New York: Portfolio.Brin, S., and L. Page. 1998. “The anatomy of a large-scale hypertextual Web search engine* 1.” Computer networks and ISDN systems 30 (1-7): 107–117.CIBER (Centre for Information Behaviour and Evaluation of Research). 2007. Information Behaviour of the Researcher of the Future: A CIBER Briefing Paper. London: CIBER.Clarke, Arthur C. 1973[1962] Profiles of the Future: An Inquiry into the Limits of the Possible. Revised edition. New York: Harper & Row.Connaway, Lynn Silipigni, and Dickey, Thomas. 2010. The Digital Information Seeker: Report on Findings from Selected OCLC, RIN and JISC User Behaviour Projects. OCLC Research.Griffiths, J. R, and P. Brophy. 2005. Student Searching Behavior and the Web: Use of Academic Resources and Google. Trends 53(4): 539.Hampton-Reeves, S., C. Mashiter, J. Westaway, P. Lumsden, H. Day, and H. Hewertson. 2009. Students’ Use of Research Content in Teaching and Learning. A report for the Joint Information Systems Council (JISC). Centre for Research-informed Teaching, University of Central Lancashire.Hargittai, E. 2007. “The social, political, economic, and cultural dimensions of search engines: An introduction.” Journal of Computer-Mediated Communication 12 (3): 769–777.Head, Alison J., and Michael Eisenberg. 2009. How College Students Seek Information in the Digital Age. Project Information Literacy Progress Report. University of Washington.Hargittai, E., L. Fullerton, E. Menchen-Trevino, and K.Y. Thomas. 2010. “Trust online: young adults’ evaluation of Web content.” International Journal of Communication 4: 468–494.Page, L., S. Brin, R. Motwani, and T. Winograd. 1999. “The PageRank citation ranking: Bringing order to the web.”Pan, B., H. Hembrooke, T. Joachims, L. Lorigo, G. Gay, and L. Granka. 2007. “In Google we trust: Users’ decisions on rank, position, and relevance.” Journal of Computer-Mediated Communication 12 (3): 801–823.Prabha, C., L. S Connaway, and T. J Dickey. 2006 The Whys and Hows of College and University User Satisficing of Information Needs. Phase IV Report: Semi-Structured Interview Study. Report on National Leadership Grant LG-02-03-0062-03, to Institute of Museum and Library Services, Washington, D.C. Columbus, Ohio: School of Communication, The Ohio State University. http://imlsproject.comm.ohio-state.edu/imls_reports/imls_PH_IV_report_list.html.Striphas, Ted. 2011a. “Who Speaks for Culture?” posted Sept. 26, 2011, http://www.thelateageofprint.org/2011/09/26/who-speaks-for-culture/2011b. “Culturomics,” posted April 5, 2011, http://www.thelateageofprint.org/2011/04/05/culturomics/Vaidhyanathan, Siva. 2011. The Googlization of Everything (and Why We should Worry).  Berkeley: University of California Press.Wong, William, Stelmaszewska, Hanna, Bhimani,Nazlin, Barn, Sukhbinder, and Barn, Balbir. 2009. User Behaviour in Resource Discovery: Final Report. JISC.[1] http://www.google.com/about/corporate/company/tech.html[2] For the purposes of our analysis, we defined a search as any time a student opened a new resource to search for information. If the student changed his search terms within a resource, we did not count this as a new search. Therefore we observed 161 searches encompassing 238 separate sets of search terms.[3] These statistics encompass usage by the entire university. Unfortunately, it is impossible to differentiate student searches from other users. However, given that IWU students vastly outnumber faculty, it is reasonably safe to assume that this usage is student-driven.[This is a guest post by Augusta Rohrbach and David Tagnani. Augusta Rohrbach is an Associate Professor of English at Washington State University and Editor of ESQ: A Journal of the American Renaissance. She's working on The Gallows Diary of Mary Surratt, Presidential Assassin, a book that uses this case history to examine the conditions of subjectivity when accessed exclusively through secondary archival sources only. David Tagnani is a PhD student in the Department of English at Washington State University. He studies ecology, mysticism, and the coincidence of the two in British and American Literature.--@jbj]There are lots of tools out there that aggregate existing information and even organize it for users to interpret. Since the early Hypercities, GIS tools, for instance, have been very much the rage among humanists who wish to add geographical and census data to enhance the “lived experience” of a text. But there are fewer tools that actually build an archive of live interpretation—as opposed to facts layered and ready for interpretation–around a stable text. And that’s where what I call “Reading with the Stars” comes in.Last year I attended a presentation by Reinhard Engels (Harvard University Libraries) in which he demonstrated a deep zoom widget he was working on called “HIGHBROW.” Using important texts such as the Bible, the Divine Comedy, and Shakespeare (First Folio), Reinhard’s widget brought these texts together with some of their more famous commentaries. A spike graph at the top of the screen showed viewers where the text had received more (or less) comment, and scrolling down into the text allowed viewers to see specific comments from a range of well known thinkers such as St. Augustine and Sir Thomas Moore on the Bible. Highbrow offered viewers a snapshot of the text’s reading history through the lens of established experts. As a teacher of ENGL 372, a large undergraduate lecture class required for our major at WSU, that focuses on the Transatlantic 19th century, I wondered if the static archive Highbrow could create might be transformed into a dynamic archiving tool for student comments around major texts.Several conversations and many emails later, Reinhard and I conspired to load the texts of Ralph Waldo Emerson. Why Emerson? Well, in part because he urged his readers to write their own books and using Highbrow to engage students in his text would provide them a way to “speak (their) latent convictions,” as he adjured in “The American Scholar,” as a way for them to discover “the universal truth.” There were other reasons, too. Emerson was a good fit because he is a major figure of the transatlantic 19th century, because I am the Editor of ESQ: A Journal of the American Renaissance, because David Tagnani (my co-teacher for ENGL 372) and his fellow graduate students in ENGL 529 are building Digital Emerson: a collective archive, and (perhaps the best reason of all) because it’s fun to have a rocking good time in the classroom.Click for full-size But, as we all know, having a good time in a large readings course of 60 people that is part of a series of four readings courses required by the major takes a level of orchestration that isn’t all that fun. Reinhard’s widget promised to be just the thing. By mid-August, David and I were figuring out possible assignments to help students use Highbrow as a means to “read with the stars.” On the theory that more is merrier, I also posted an invitation to join my class’s experience on the C19 list; several people responded and decided to pilot the Emerson archive along with us. We shared our assignment, got some helpful feedback and geared up for the semester.Unlike traditional wikis, “Reading with the Stars” allows students to see, using the spike graph at the top of the screen, and the heat map of the text on the screen, where activity is in the text; reader comments have a life alongside the text. (View Reinhard’s screencast for easy to follow instructions.) And using the “Reading with the Stars” approach in our rather large classroom helped to mitigate the limitations of the seminar format. A truly dynamic and participatory seminar is not practicable in a room of over four dozen students. There is simply not time for everyone to contribute every class period, and the less confident and more introverted students find it easy to just hide in the crowd. This technology extended the seminar dynamic beyond the physical class space.Click for full sizeThat’s important because it established a relationship between reader and text and proved mighty useful in supporting students in their efforts to engage the text.We found that students often wrote annotations in response to other annotations – debating, disputing, supporting, or otherwise engaging with the ideas of other students. In a questionnaire designed to gauge student reaction to the technology, students consistently identified the interaction with their peers as the most rewarding feature. Students said that seeing what other students thought exposed them to new ideas and enhanced their experience of the text. This space functioned as a virtual conversation that helped the students think more critically about the text and about their own ideas. One even went so far as to say, “I didn’t feel alone in my thoughts.”Click for full sizeClearly the sense of community and collegiality was transported outside of the classroom for that student. A few students even observed that Highbrow exposed them to a greater diversity of voices and ideas than actual class discussions due to the reticence of some students in class and the compulsory nature of the assignment. As this interactivity is the keystone of the seminar, Highbrow can be said to have been a success in this regard. We noticed that aside from the interactive dynamic of the annotations, students were interested in simply seeing which passages solicited the most attention from their classmates. This led students to think more deeply about those passages, passages that they may have initially ‘took at surface level,’ in the words of one. Other students said they were impelled to think more deeply and focus more closely in order to write annotations. Some even revealed that the Highbrow assignment provided additional pressure to actually complete the reading before coming to class. All of these factors contributed to an increased sense of comprehension. Many students wrote that the annotations helped them understand other ideas about the text and to reveal layers of meaning that had eluded them. As it turned out, only one other colleague ended up having her class annotate an Emerson text. Laura Saltz’s Colby College course in American Studies, took a shot at “Nature”. “My class is small,” Saltz explained, “so for my students, the excitement was less about communicating with each other than about the fact that their comments would be public, read by people beyond the classroom. One student even developed a kind of performance anxiety–she couldn’t decide which passage to write about or what to say. When we later talked about it as a group, we agreed that having a real (though virtual) audience changed they way they thought about their responses to the text. They were also fascinated by the fact that a few select passages drew so much commentary from their fellow classmates.”Indeed, when our students at WSU found out that they could read comments from a group of students approaching the text from a different context, the excitement was palpable: they wanted to see what students from another school and another kind of class thought about the text. I couldn’t fit in another reading on the syllabus, alas, but students were eager to see future iterations of the archive. This is an open invitation for you to add to this archive by bringing it into your classroom.Photo “claude levi-strauss — annotated” by Flickr user uair01 / Creative Commons licensedLast year, I dwelled briefly on the implications of the Google Books settlement and Dan Cohen’s critique of all pre-Google history as merely “anecdotal.” In the mean time, the “Culturomics” project has burst onto the scene, offering a new way of doing the kind of total history envisioned by Cohen. Culturomics, which uses the Google data set to trace how different words and phrases change over time, has inspired cautious optimism among historians and other groups. I think the project’s claim to represent all of human culture is potentially dangerous, and I will explain why below. But first, to show that I’m not just another luddite crank, I’d like to demonstrate how truly valuable Google Books has been for my work.Historians, in general, do not like to foreground their methodology. Narrative historians like myself, especially, tend to bury our research strategies and theoretical scaffolding in footnotes and appendices and prospectuses. This helps create a more seamless reading experience, but is not always a good thing. So, in the spirit of open source and to make up for missing the HASTAC Conference this weekend, I will share part of the digital methodology from one of my recent articles.The article (which you can check out here, if you’re lucky enough to have the right institutional subscription) examines the story of a murder committed by a South American man just south of Chiloé Island in the mid-1700s. I argue that the sole witness to the murder is unreliable and use contextual analysis, manuscripts, printed narratives, and oral histories to back my claim. The murder story appealed to Charles Darwin, who used it at two key moments in his career, and unfortunately it has been mindlessly repeated by historians ever since. Thanks in part to Darwin, the story is now falsely associated with the Yahgan people he encountered on the Cape Horn Archipelago – a group hundreds of miles away and very different from that of the original alleged murderer.Although I don’t really talk about it in the article, I used anti-plagiarism software (designed to catch student cheats) to track the copying of certain quotes and phrases across texts. The fuzzy logic employed by some of these programs, meant to catch students who alter a word here or a phrase there, is especially helpful in identifying “borrowed” passages in historical documents. I used The Complete Work of Charles Darwin Online to mine for certain phrases. Their large collection of foreign language material is really cool – there is a strong argument to be made for these kinds of subject-specific digital repositories as separate entities from the big universal search engines. I also used Google Ngram and related platforms to chart the use of the murder story by various authors over time.The results (summarized in the chart at left) confirmed my thesis. Use of the original murder story (the blue line) dropped precipitously in the middle of the nineteenth century. Meanwhile, Darwin’s version of the murder (the red line) shot way up. Texts that falsely attribute the murder to the Yahgan people (the green line) correlate more or less directly with the popularization of Darwin’s version of the murder. You can view my original data set here (it’s not the final version, since I stopped using Google Docs at some point, but it gives you the idea).The graph cannot, however, establish that the murder story is a lie. It can only replicate the lie as it develops over time. Without the broader context established by more traditional historical research, these results would be meaningless. And this brings me to the danger inherent in Culturomics. First, machine-readable texts do not, and will never, represent the totality of the human experience. What about paintings, illustrations, and photographs, statues and figurative art, architecture, music, material culture, and ecology? What about oral history? What about economic, statistical and demographic evidence? Although there are millions upon millions of books, magazines, newspapers, and other printed material, they represent only the visible, privileged, literate tip of a vast store of human culture.Even more troubling, texts lie. “There is no document of civilization,” said Walter Benjamin, “which is not at the same time a document of barbarism.” One of the great insights of the “New Social History” was the need to rub documents against the grain. Text mining usually rubs with the grain, merely reproducing the endemic biases and structured incompleteness of the written past. The graph can only replicate the lie.This is not to say that Culturomics is hopelessly biased and needs to be discarded. On the contrary, it is precisely this kind of utopian enthusiasm – the dream that we can actually develop a more total vision of human culture – that is needed to keep history afloat. Large scale text mining is simply wonderful. Like all great inventions, though, it can be used for good or for ill. And it makes sense, I think, to guard against the naive assumption that all of human culture or history can be reduced to a computational algorithm.Cross-posted at HASTACPlease join us on Thursday, May 31, 2012, when three leading Digital Humanists will take part in a panel discussion that addresses DH-related efforts to archive and preserve materials after catastrophic events.Thursday, May 31, 2012, 6:30pm-8:30pmRoom 6496, CUNY Graduate CenterArchiving Catastrophe: Digital Humanities & Times of DisasterPaul Millar (University of Canterbury, New Zealand), Tom Scheinfeldt (George Mason University), and Steve Brier (CUNY Graduate Center)Please RSVP hereIn the months since a 7.1 magnitude earthquake hit New Zealand’s Canterbury province in September 2010, the region has experienced over ten thousand aftershocks, 430 above magnitude 4.0. The most devastating aftershock, a 6.2 earthquake under the centre of Christchurch on 22 February 2011, had one of the highest peak ground acceleration rates ever recorded. This event claimed 185 lives, damaged 80% of the central city beyond repair, and forced the abandonment of 6,000 homes. It is the third costliest insurance event in history. Paul Millar, project leader of the CEISMIC Canterbury Earthquakes Digital Archive, will discuss the role of Digital Humanities in developing an international resource to preserve the digital record of the earthquakes’ impacts and the long-term process of recovery.The Hurricane Digital Memory Bank, based at the Roy Rosenzweig Center for History and New Media (RRCHNM) at George Mason University, uses electronic media to collect, preserve, and present the stories and digital record of Hurricanes Katrina, Rita, and Wilma. The project contributes to the ongoing effort by historians and archivists to preserve the record of these storms by collecting first-hand accounts, on-scene images, blog postings, and podcasts. Tom Scheinfeldt, Managing Director of RRCHNM, will discuss both this project and, with CUNY Grad Center’s Steve Brier, the September 11 Digital Archive.The September 11 Digital Archive uses electronic media to collect, preserve, and present the history of the September 11, 2001 attacks in New York, Virginia, and Pennsylvania and the public responses to them. Funded by a major grant from the Alfred P. Sloan Foundation and organized by the American Social History Project at the Graduate Center and at RRCHNM, the work of the Archive is not only to gather digital materials related to the attacks but also to assess how history is being recorded and preserved in the twenty-first century, and to develop free software tools to help historians do a better job of collecting, preserving, and writing history. To these ends the Archive has partnered with the Library of Congress, which in September 2003 accepted the Archive into its permanent collections – an event that both ensured the Archive’s long-term preservation and marked the Library’s first major digital acquisition.All three projects seek to foster positive legacies of terrible events by allowing the people affected to tell their stories in their own words, which as part of the historical record will remain accessible to a wide audience for generations to come.About Paul Millar: Associate Professor Paul Millar is the Head of the Department of English, Cinema and Digital Humanities at the University of Canterbury, New Zealand. His research interests include New Zealand and Pacific literature, literary biography, digital textual scholarship and Australasian attitudes to China. In 2001 he co-founded the New Zealand Electronic Text Centre, and he is currently focused on adding functionality to the CEISMIC Canterbury Earthquakes federated digital archive.About Tom Scheinfeldt: Tom Scheinfeldt is Managing Director of the Roy Rosenzweig Center for History and New Media and Research Assistant Professor of History in the Department of History and Art History at George Mason University. He has lectured and written extensively on the history of popular science, the history of museums, history and new media, and the changing role of history in society, and has worked on traditional exhibitions and digital projects at the Colorado Historical Society, the Museum of the History of Science in Oxford, The Louisiana State Museum, the National Museum of American History, and the Library of Congress. In addition to managing general operations at RRCHNM, Scheinfeldt directs several of its online history projects, including Omeka, THATCamp, One Week | One Tool, the September 11 Digital Archive, the Hurricane Digital Memory Bank, the Papers of the War Department, 1784-1800, and Gulag: Many Days, Many Lives. He gave a memorable talk here at CUNY DHI in December 2010.About Steve Brier: Dr. Stephen Brier founded the Interactive Technology and Pedagogy Certificate Program at The Graduate Center in 2002 and serves as its Coordinator. He is a historian and a member of the doctoral faculty in Urban Education who has published widely in text, video, and various forms of multimedia on issues from U.S. history to the uses of interactive technology to improve teaching and learning. He was the founding director of The Graduate Center’s American Social History Project/Center for Media and Learning and was the executive producer of the award-winning “Who Built America?” multimedia curriculum, including textbooks, videos, and CD-ROMs. He has co-produced other award-winning websites, including “History Matters” and the “September 11 Digital Archive”. Brier, who previously served for eleven years as a senior administrator at The Graduate Center, is also the institution’s Senior Academic Technology Officer and the co-director of its New Media Lab.This is the full version of a paper I presented at the National Digital Forum, 30 November 2011.In 1901, one of the first acts of the Commonwealth of Australia was to create a system of exclusion and control designed to keep the newly-formed nation ‘white’. But White Australia was always a myth. As well as the Indigenous population, there were already many thousands of people classified as ‘non-white‘ living in Australia — most were Chinese, but there were also Japanese, Indians, Syrians and Indonesians.Here are some of them…The real face of White AustraliaThe administration of what became known as the White Australia Policy created a huge volume of records, much of which is still preserved within the National Archives of Australia. These photographs are attached to certificates that non-white residents needed to get back into the country if they decided to travel overseas. There are thousands upon thousands of these certificates in the Archives. Thousands of certificates representing thousands of lives — all monitored and controlled.But is is too easy to see these people as the powerless victims of a repressive system. There were many acts of resistance. Some argued against the need to be identified ‘just like a criminal’. Others exercised control over their representation, submitting formal studio portraits instead of mug shots.Most commonly and most powerfully, people resisted the policy simply by going ahead and living rich and productive lives.My partner, Kate Bagnall, is helping to rewrite Australian-Chinese history by overthrowing the stereotype of the culturally isolated Chinese man living a lonely, meagre existence surrounded by gambling and opium dens. By mining the available records, by reading against the grain of contemporary reports and by working with family historians, Kate is documenting their intimate lives — their wives, their lovers, their families and descendants — the sorts of relationships that sent a shudder through the edifice of White Australia. Power can be reclaimed in many subtle and subversive ways.‘The real face of White Australia’ is an experiment. It uses facial detection to technology to find and extract the photographs from digital copies of the original certificates made available through the National Archives of Australia’s collection database. The photographs you see here come from just one series, ST84/1. There’s no API to the collection so I reverse-engineered the web interface to create a script that would harvest the item metadata and download copies of all the digitised images. There are 2,756 files in this series. On the day I harvested the metadata, 347 of those files had been digitised, comprising 12,502 images. It took a few hours, but I just ran my script and soon I had a copy of all of this in my local database.Then came the exciting part. Using a facial detection script I found through Google and an open source computer vision library, I started experimenting with ways of extracting the photos. After a few tweaks I had something that worked pretty well, so I pointed my aging laptop at the 12,502 images and watched anxiously as the CPU temperature rose and rose. It took a few emergency cooling measures, but the laptop survived and I had a folder containing 11,170 cropped images. About a third of these weren’t actually faces, but it was easy to manually remove the false positives, leaving 7,247 photos.These photos. These people.With my database fully primed and loaded it was just a matter of creating a simple web interface using Django for the backend and Isotope (a jQuery plugin) at the front. Both are open source projects. All together, from idea to interface, it took a bit more than a weekend to create, and most of that was waiting for the harvesting and facial detection scripts to complete. It would be silly to say it was easy, but I would say that it wasn’t hard.What we ended up with was a new way of seeing and understanding the records — not as the remnants of bureaucratic processes, but as windows onto the lives of people. All the faces are linked to copies of the original certificates and back to the collection database of the National Archives. So this is also a finding aid. A finding aid that brings the people to the front.According to Margaret Hedstrom the archival interface ‘is a site where power is negotiated and exercised’.[] Whether in a reading room or online, finding aids or collection databases are ‘neither neutral nor transparent’, but the product of ‘conscious design decisions’. We would like to think that this interface gives some power back to the people within the records. Their photographs challenge us to do something, to think something, to feel something. We cannot escape their discomfiting gaze.But this interface represents another subtle shift in power. We could create it without any explicit assistance or involvement by the National Archives itself. Simply by putting part of the collection online, they provided us with the opportunity to develop a resource that both extends and critiques the existing collection database. Interfaces to cultural heritage collections are no longer controlled solely by cultural heritage institutions.It’s these two aspects of the power of interfaces that I want to focus on today.There are a growing number of examples where the records created by repressive or discriminatory regimes have, in Eric Ketelaar’s words, ‘become instruments of empowerment and liberation, salvation and freedom’.[] Nazi records of assets confiscated during the Holocaust have been used to inform processes of restitution and reparation. Government records have helped members of Australia’s Stolen Generations trace family members. Descendants of inmates incarcerated by American colonial authorities in what was the world’s largest leprosy colony in the Philippines, have embraced the administrative record as an affirmation of their own heritage and survival.[] Records can find new meanings. Power can be reclaimed.Technology can help. Tim Hitchcock has described how something as simple as keyword searching can turn archives on their heads. Recordkeeping systems tend to reflect the structures and power relations of the organisations that create them. The ‘hierarchical and institutional nature of most archives’, Hitchcock argues, ‘contains an ideological component which is sucked in with every dust-filled breath’.[] But digitisation and keyword searching free us from having to follow the well-worn paths of institutional power. We can find people and follow their lives against the flow of bureaucratic convenience. We can gain a wholly new perspective on the workings of society. ‘What changes’, Hitchcock asks, ‘when we examine the world through the collected fragments of knowledge that we can recover about a single person, reorganised as a biographical narrative, rather than as part of an archival system?’[]Projects such as Unknown no longer may help us answer that question.Unknown no longerIt’s aiming to extract the names and biographical details of slaves from the 8 million manuscript documents held by the Virginia Historical Society. The documents include court records, receipts, wills and inventories. Here is a page from the ‘Inventory of Negroes at Berry Plain Plantation, King George County, Virginia’ for 1855, listing names, occupations and valuations.Tim Hitchcock is one of the directors of London Lives a project that similarly seeks to find the people in 240,000 manuscript pages documenting the lives of plebeian Londoners in the 17th century.London LivesMore than three million names have already been extracted from the records of courts, workhouses, hospitals and other institutions. Work is continuing to link these names together, to merge these various shards of identity and piece together the experiences of London’s poorest inhabitants.Remember me from the US Holocaust Memorial Museum is working with photographs taken by relief agencies in the aftermath of World War Two. The photographs are of displaced children who survived the Holocaust but were separated from families. What happened to them? The project is seeking public help to identify and trace the children.Remember meThese are all projects about finding people. Finding the oppressed, the vulnerable, the displaced, the marginalized and the poor and giving them their place in history. This is what Kate and I hope to do with Invisible Australians, the broader project of which our faces experiment is part.Invisible Australians‘Invisible Australians’ aims to extract more than just photographs. We want to record and aggregate the biographical data contained within the records of the White Australia Policy — to extract the data and rebuild identities.But we want to do more, we want to link these identities up with with other records, with the research of family and local historians, with cemetery registers and family trees, with newspaper articles and databases we don’t even know about yet. We want to find people, families and communities.It’s ridiculously ambitious and totally unfunded. But it is possible.The most exciting part of online technology is the power it gives to people to pursue their passions. As with the faces, we don’t need the help of the National Archives. We need the records to be digitized, but that’s happening anyway and we can afford to be patient. Most of the tools we need already exist, and are free. In the past 12 months, for example, there have been a number of open source tools released for crowd-sourced transcription of manuscript records.People with passions, people with dreams, people who are just annoyed and impatient, don’t have to wait for cultural institutions to create exactly what they need. They can take what’s on offer and change it.Interfaces can be modified. It is amazingly easy to write a script that will change the way a web page looks and behaves in your browser. I was frustrated by the standard interface to digitized files in the National Archives of Australia’s Recordsearch database — so I changed it.Before and afterNot only did make it look a bit nicer, I added new functions. My script lets you print a whole file or a range of pages and display the entire contents of the file on a pretty cool 3d wall.I’ve shared this script, and a few other Recordsearch enhancements. Anyone can install them with a click and use them.Wragge Labs EmporiumInterfaces are sites of power and we can claim some of that power for ourselves. Online technologies not only free us from the having to brave the physical intimidation of the reading room, they free us up to engage with the records in new ways. The archivist-on-duty would probably not be pleased if I pulled out some scissors and started snipping photos out of certificates. Or if I pulled a file apart and pasted it’s contents on the wall. But online we are free to experiment.The power of cultural heritage organisations is perhaps expressed most forcefully in their ability to control the arrangement and description of their collections. ‘Every representation, every model of description, is biased’, note Verne Harris and Wendy Duff, ‘because it reflects a particular world-view and is constructed to meet specific purposes’.[] Archives, libraries and museums are already starting to share this power, by allowing tagging, or seeking public assistance with description through crowd sourcing projects. But most of the these activities still happen within spaces created and curated by the institutions themselves. Our cathedrals of culture might be opening their doors and inviting the public to participate in their ceremonies, but that doesn’t make them bazaars. The architecture stills speaks of authority.In any case, people already have a space where they can explore and enrich collections — it’s called the internet.It would be great to see cultural institutions doing more to watch, understand and support what people are doing with collections in their own spaces — following them as they pursue their passions, rather than thinking of ways to motivate them.A quick example… You might have heard of Zotero, it’s an open source project that lets you capture, annotate and organize your research materials.ZoteroOne cool thing about Zotero is that you can build and contribute little screen scrapers, called translators, that let Zotero extract structured data from any old collection database. You might not be surprised to learn that I’ve created a translator for Recordsearch. Another cool thing about Zotero is that you can share the stuff that you collect in public groups.Invisible Australians Zotero groupPut those two cool things together and what do you have? Well to me they spell out user generated finding aids — parallel collection databases created by researchers simply pursuing their own passions.Linked Open Data greatly increases opportunities for collection description to leak into the wider web. If objects and documents are identified with a unique URL, then anyone can can make and publish statements about them in machine-readable form. These statements can then be aggregated and explored. Initiatives such as the Open Annotation Collaboration will hasten the development of these shared descriptive and interpretative layers around our cultural collections.And of course all this descriptive and interpretative work can be harvested back to enhance existing collection databases. We could start doing it now — though I will spare you today my rant about the possibilities of mining footnotes.As well as exploring the possibilities of user-generated content, cultural institutions are starting to open up their collection data for re-use. APIs are great (though Linked Open Data is better), and New Zealand is lucky to have an organisation like DigitalNZ which just gets it. People can and will make cool things with your stuff.But again, we don’t have to wait for everything to be delivered in a convenient, machine-readable form. If it’s on the web anybody can scrape, harvest and experiment.You probably all know about the National Library of Australia’s newspaper digitisation project — it’s building a magnificent resource. But I wanted to do more than just find articles. I wanted to explore and analyze their content on a large scale. So I built a screen scraper to extract structured data from search results, and then used the scraper to power a series of tools. I have a harvester that lets you download an entire results set — hundreds or thousands of articles — with metadata neatly packaged for further analysis.HarvesterOr what about a script that graphs the occurrence of search terms over time, and allows you to ask questions like When did the Great War become the First World War?.When did the Great War become the First World War?In the end I got a bit carried away and built my own public API to the Trove newspaper database.Unofficial Trove newspapers APII think it’s important to note that the tools I developed were guided by the types of questions I wanted to ask. While we should welcome APIs and celebrate their possibilities, we should also remain critical. APIs are interfaces, they too embed power relations. Every API has an argument. What questions do they let us ask? What questions do they prevent us from asking?Even as we move from the age of lumbering, slow-witted data silos into the rapidly-evolving realms of Linked Open Data, we have to constantly question the models we make of the world. Ontologies and vocabularies are culturally determined and historically specific. Yes, they too are interfaces, complete with their own distributions of power and authority. But we can revisit and change them. And we can relate our new models to our old models, capturing complex, long-term shifts in the way we think about the world. That’s incredibly exciting.All of this hacking, harvesting, questioning, enriching and meaning-making makes me think about the possibilities of grassroots leadership. Online technologies enable people to take cultural institutions into unexpected realms. They can build their own interfaces, ask their own questions, determine their own needs — they can point the way instead of simply waiting to be served.You might wonder what the National Library of Australia thinks of my various scrapers and harvesters. I can’t speak for them, but I can say that they’ve awarded me a fellowship to explore further the possibilities of text-mining in their newspaper database.The idea of grassroots leadership brings me back to the title of this talk — ‘It’s all about the stuff’. It seems to me that we tend to model the interactions between cultural institutions and the public as transactions. The public are ‘clients’, ‘patrons’, ‘users’ or ‘visitors’. But the sorts of things I’ve been talking about today give us a chance to put the collections themselves squarely at the centre of our thoughts and actions. Instead of concentrating on the relationship between the institution and the public, we can can focus on the relationship we both have with the collections.It’s all about the stuff.It’s all about the respect and responsibility we both have for our collections.It’s all about the respect and responsibility we both have for people like this.Comments OffEditors’ Note: Many scholars working in the Digital Humanities are thinking about the theory, design, and social and pedagogical impact of games. The posts below cover some of the variety of issues within this field. Further discussion will occur at THATCamp Games, January 20-22, 2012 at the University of Maryland-College Park. Please Tweet @dhnow or email dhnow [at] pressforward [dot] org if you have more to suggest. *updated 12/1/11*Shawn Graham, Lies & Gamification, November 29, 2011 Gamification - love it or hate it, any time you use some sort of game mechanic, you’ve done it. Which makes ‘being a student’ the ultimate game of all. Write an essay, do a mid term, ace the final, level up to the next course, forget the previous course’s content…I’ve written before about ‘gamifying my historian’s craft‘, about why I gamified the course website, what my ‘achievements’ were, and how they tied to the course content, and my larger paedegogical goals…. I’m just finishing up a second round of my gamified HIST2809, ‘The Historian’s Craft’ course….But I wanted more. Perhaps what I needed, in addition to gamification of the practical hands-on practice part of being an historian, was some game based learning. Read Full Post Here.Gabe Zichermann, The Six Rules of Gamification, November 29, 2011While every experience is (and should be) experientially different, there are six new rules that I’ve distilled from my work. We can use these as an excellent jumping off point for the gamification design process:Understand what constitutes a “win” for the organization/sponsorUnpack the player’s intrinsic motivation and progress to masteryDesign for the emotional human, not the rational human.Develop scalable, meaningful intrinsic and extrinsic rewardsUse one of the leading platform vendors to scale your project (don’t roll your own)Most interactions are boring: make everything a little more funRead Full Post Here.Geoff Kaufman, Mind/Games #1: Reducing Implicit Bias with Games, November 23, 2011Given that one of the major goals of Tiltfactor’s current research is to design games aimed at reducing implicit bias held toward (or by) women in science, technology, engineering, and math (STEM), I thought it would be worthwhile to take a step back and discuss what psychologists have discovered about implicit bias – and how games might be an especially powerful means of reducing or combating it. Read Full Post Here.Gabe Zichermann, Kids, Brains and Games: a Ted Talk, November 21, 2011Because gamified design relies heavily on behavioral economics and psychology (as well as game design and loyalty), I’ve found myself spending a great deal of time in familiar (but substantially updated) territory: thinking about the inherent skills and abilities of people and how to motivate them to change. Much of the science has been radically rethought (including brain plasticity, the extent of which has only recently been revealed), but much of it is fundamentally the same. If we see the complex interplay of hereditary and environmental (or intrinsic and extrinsic) factors on a continuum, we will be best able to design gamified experiences that motivate the change we want to see. Read Full Post Here.Maryland Institute for Technology in the Humanities, “Archive Ahoy!”: A Dive into the World of Game Preservation, November 4, 2011While the preservation process of digital games up to now has been mostly ad-hoc, currently there is a huge interest among libraries to build an archive of digital games. By asking what the artifact is, and what aspects of the game must be documented, [Preserving Virtual Worlds 2] is coming up with a set of best practices for the preservation of digital games for those institutions that seek to archive and collect these significant digital materials. Read Full Post Here.Geoffrey Rockwell, Ritsumeikan Center for Game Studies, October 26, 2011Michael Douma, What is Gamification?, October 20, 2011Gameplay has a lot to teach us about motivating participation through joy. ‘Gamification’ is a new term, coined in 2008, for adapting game mechanics into non-game setting — such as building online communities, education and outreach, marketing, or building educational apps. Here are some ideas for how to do it. Read Full Post Here.CommentsTed Underwood has been talking up the advantages of the Mann-Whitney test over Dunning's Log-likelihood, which is currently more widely used. I'm having trouble getting M-W running on large numbers of texts as quickly as I'd like, but I'd say that his basic contention--that Dunning log-likelihood is frequently not the best method--is definitely true, and there's a lot to like about rank-ordering tests. Before I say anything about the specifics, though, I want to make a more general point first, about how we think about comparing groups of texts.The most important difference between these two tests rests on a much bigger question about how to treat the two corpuses we want to compare. Are they a single long text? Or are they a collection of shorter texts, which have common elements we wish to uncover? This is a central concern for anyone who wants to algorithmically look at texts: how far can we can ignore the traditional limits between texts and create what are, essentially, new documents to be analyzed? There are extremely strong reasons to think of texts in each of these ways. The reason to think of many shorter texts are more obvious. In general, that seems to correspond better with the real world; in my case, for example, I am looking a hundreds of books, not simply two corpuses; any divisions I introduce are imperfect. If one book is a novel with a character named Jack, "Jack" may appear hundreds of times in the second corpus; it would be vastly over-represented. That knowledge, though, doesn't lead us to any useful knowledge about the second corpus--there's nothing distinctively 'Jack'-y about all the other books in it. Now, the presumption that every document in a set actually stands on its own is quite frequently misplaced. Books frequently have chapters with separate authors, introductions, extended quotations. For instance: Dubliners will be more strongly characterized by the 30 times the word "Henchy" appears than the 29 times "Dublin" appears, even though Henchy appears only in "Ivy Day in the Committee Room" and "Dublin" is much more evenly distributed across the set, since "Dublin" is a more common word in general. Even if we could create text-bins of just the right size, we wouldn't always want to do so, though. There are lots of occasions where it makes a lot of intuitive sense to group texts together as one large group. Year-ratio counts is one. Over the summer, I was talking about the best way to graph ratios per year, and settled on something that looked roughly like this: This is roughly the same as the ngrams or bookworm charts, but with a smoothing curve. If you think about it, though, what's presented as a trend line in all these charts is actually linking data points that consist of massively concatenated texts for each of the years. That spike for evolution in 1877 is possibly due to just one book--it makes us think something's happening in the culture, when it's really just one book again--the Jack/Henchy problem all over again? Instead of umping the individual years together, why don't we just create smoothing lines over all the books published as individual data points? Well, partly, it's just too much information: Giving each book as a point doesn't really communicate anything more, and we get _less_ sense of the year to year variability than with the more abstract chart. That's not to say there aren't advantages to be gained here (it's good to get a reminder of just how many more books there are from 1890-1922, for instance), and I do think some distributional statistics might be better than the moving average for a lot of the work we're doing. But it also takes an order of magnitude or two longer to calculate, which is a real problem; and it's much easier to communicate what a concept like 'percentage of all words in that year' means than 'expected value from a fitted beta distribution of counts for all books in that year', even if the latter might be closer to what we're actually trying to measure. There are other cases where the idea of a text breaks down--with rare words, for instance, or extremely short texts. I'd love to just be able to use the median ratio percentage instead of the mean from the chart above; but since 3/4 of all books never use the word evolution at all, we won't get much use out of that. With much rarer words or terms--which we are often very interested in--lots of useful tools will break down entirely. How does this connect to these specific comparison algorithms? The Dunning log-likelihood test treats a corpus exactly the same as it treats a document. There is both computational and philosophical simplicity in this approach. It lets us radically simplify texts across multiple dimensions, and think abstractly about corporate authorship across whatever dimension we choose. I could compare Howells to Dickens on exactly the same metric I could compare LC classification E to LC classification F; the document is longer, but the comparison is the same. But it suffers that Jack- The Mann-Whitney test, on the other hand, requires both corpus and document levels. By virtue of this, it can be much more sophisticated; it can also, though, be much more restricted in its application. It also takes considerably more computing power to calculate. It *might* be possible to include some sort of Dunning comparisons in Bookworm using the current infrastructure, but Mann-Whitney tests are almost certainly a bridge too far. (Not to get too technical--but basically, Mann-Whitney requires you to load up an manipulate the entire list of books and counts for each word, sort it, and look at those lists, while Dunning lets you just scan through and add up the words as you find them; this is a lot simpler. I don't have too much trouble doing Dunning Tests on tens of thousands of books in a few minutes, but the Mann-Whitney tests get very cumbersome beyond a few hundred.) Mann-Whitney also requires that you have a lot of texts, and that your words appear across a lot of books; if you compared all the copies of Time magazine from 1950 to the present to all the phone books in that period, it would conclude that "Clinton" was a phone book word, not a news word, since Bill and Hillary don't show up until late in the picture. So when is each one appropriate? This strikes me as just the sort of rule of thumb that we need and don't have to make corpus comparison more useful for humanists. Maybe I can get into this more later, but it seems like the relative strengths are something like this:Dunning: Very large corpuses (because of memory limitations) Very small corpuses (only a few documents) Rare words expected to distinguish corpuses (for instance, key names that may appear in a minority of documents in the distinguishing corpus). Very short documents Limited Computational resourcesMann-Whitney: Medium-sized corpuses (hundreds of documents) Fairly common distinguishing words (appearing in most books in the corpus they are supposed to distinguish) Fairly long documents Also worth noting:TF-IDF: Similar strengths to Dunning, but works with a single baseline set composed of many (probably hundreds, at least) documents used with multiple comparison sets.Update 2/23/12:This post was acknowledged as Editor’s Choice (11/28/11) for Digital Humanities NowSee Pollyanna Macchiano talk about this project at THATCamp Pedagogy (video).The Project submitted a research proposal (pdf) to the CSU Student Research Competition and were chosen as one of four projects to represent the College of Arts & Humanities; they will now compete for one of four spots to represent SJSU at the CSU-wide competition. Wish them luck! [Update 3/28: They didn't advance in the competition but did receive some very valuable feedback, especially from the director of digital studies at our School of Library and Information Sciences]The group will represent the project as a poster at the Re:Humanities Conference, a gathering run entirely by and for undergraduates who perform Digital Humanities research and scholarship, held at Swarthmore College in March. See Video from the ReHumanities 2012 conference & Storify of tweetsIn my recent tenure dossier, to my university’s administrators, at conferences, in coffee meetings, over lunch, across Twitter, in webinars, within Day in the Life of Digital Humanities, and in an occasional article I’ve been discussing the efficacy of bringing students into Digital Humanities. I’ve accomplished this (somewhat) by inviting them to use digital tools to collaborate on assignments (TechnoRomantic Timeline) or to simply to expose their ideas by posting to class-public fora. I’ve moved beyond PowerPoint in the classroom, not because PowerPoint is an inadequate tool, but because we have other ways of generating and demonstrating their mastery of information and knowledge. My latest experiment, a la Cathy Davidson, was a collaborative mid-term for the Gothic Novel and Horror Fiction course. Of course, inherent to all of these pedagogical experiments is a sense of productive failure — for both me and the students. (The difference, of course, is that when I fail in an experiment, I often give extra points to overcome the shock of failure for the students.) All of this falls under the catch-phrase of student-centered learning. What we would like students to become are lifelong learners. Does this type of classroom activity inspire that?Collaboration is the lynchpin to supporting all of this productivity, learning, experimenting, and knowledge acquisition. This unwritten goal was reinforced by a few tech industry magnates at Stanford’s BiblioTech Symposium last year: the CEOs want liberal arts and humanities doctoral students who can command language, interpret technical jargon into metaphor and narrative, and work collaboratively in team situations. Humanities scholars often think of themselves as the lonely bibliophiles in the library stacks, quietly slaving over monographs. But, Digital Humanities has altered that paradigm — even required that Humanists consider exposing their collaborative work, even if it isn’t digitally-inclined. Paul Fyfe even proposes that teaching can assume the tenets of Digital Pedagogy without pushing an ON button. Adding to that conversation, I propose that undergraduates and master’s students can offer intriguing, if not altogether unique, perspectives to work in Digital Humanities — beyond the limitations of classroom-specific assignments. That life-long learning that could translate so well to economic/employment success.That’s what we’ve done here, in the Beard-stair Project. Or, rather, this is what four intrepid, interested, passionate students have decided to do. There’s a story to the beginning of this project. Bear with me:ONCEupon a time in early September, Jesus found himself in possession of five slim volumes that weren’t the property of the library where he was working. Someone had dropped them into the outside library return bin for some odd reason. According to Jesus, this happens all the time. The library staff usually sends the books to Friends of the Library for sale to the general public. Jesus, having already taken my book-history-infused Digital Humanities course, brought them to me. We chatted for a bit, ogled the gorgeous illustrations, and wondered about this rag-tag collection of disparate artists’ books. There was no doubt that they were of some value. The handmade paper and uncut pages in all of them signaled a potential research moment.After sorting through WorldCat, I discovered that two of the books were extremely valuable. (Egads! What else could be in that store then?) Immediately, it was very clear that these were not books that I was to own. With the rarity of at least one, I felt it incumbent upon me to create a digital edition — which would be easy enough considering that there were only a few pages in each volume. But the art history value coupled with the provenance, book history, literary history, and Victorian/Modernist specifics meant that the topics were out of my realm of expertise. And, I didn’t have time to work on them if I wanted to finish my book projects by the end of my sabbatical in August 2012.I tweeted about it, discussed it in my grad course on Romanticism, and emailed Jesus and another digitally-inclined student. Colette jumped on board from Twitter and also happens to be a student at SJSU’s School of Library and Information Science. Doll is an engaged MA student in our English Department and gleefully took the opportunity. Jesus, an English major and our link to the books, stepped in with a passion. And, Pollyanna, another English major, has a penchant for figuring out, theorizing, and operating digital tools in addition to a passion for design. All four demonstrated an immediate enthusiasm for the initial, exploratory meeting. I set the first gathering: my apartment, sit-down dinner, peruse the books. At that meeting, we ate (panko-encrusted chicken cutlets seared in brown butter), then fondled the books. I took notes on a whiteboard while they talked. Because we were wildly traversing disciplines and historical moments, I turned my television into a makeshift display with my laptop. We couldn’t keep up with the ideas and the questions!We determined a two-fold approach to the project:Fall semester goals: exhibit in Special Collections and research time to figure out the connections and contexts; andSpring semester goals: construct a digital edition supported and maintained by the library and to be peer-reviewed by NINES, if at all possible.My goal: create a digital scholarly edition that would become a resource for scholars. This means that their research and writing would have to match scholarly requirements. It also means that we need an out-of-the-box platform. The closest we could come up with was Omeka with a WordPress plug-in, but the team isn’t necessarily satisfied with that.After describing archival research and the exploratory impetus behind doing this kind of work, the team committed to follow any path or avenue that was compelling. And, more importantly, they agreed to update each other over our Google Group and to exchange books at each meeting. This way, each person would spend four weeks with a single volume. They were committed to going down the rabbit hole. (To tell you the truth, I think this is what drives them: not knowing what questions to ask but knowing that there’s more out there to discover. Derrida’s mal d’archive is written all over them.)This Fall, we’ve met once each month, at my tiny apartment, where all I can do is encourage and feed them while they chat. I take notes, set goals for the month, post interesting/relevant links to our Google Group, look for funding, and send out reminders for the next meeting. I’m a project manager. They are the scholars. Each month, I keep expecting someone to fall away or become overwhelmed with the work (because they’re all taking a full course load+). But each month, they return energized about their discoveries and inspiring each other to dig deeper into histories. They’ve recently decided that they will consult other researchers and scholars, but they would like to maintain the sanctity of the group and write this material themselves. They don’t want to be scooped! (Ah ha! they are indeed scholars now!)Two of them submitted a proposal to the Re:Humanities conference; next semester, the four of them will submit a proposal to CSU’s research competition with the hope that they will be selected to demonstrate their project at a CSU-wide conference. One, Pollyanna, co-presented a bootcamp with me at THATCamp Pedagogy and described the project towards the conclusion of the presentation (see video in link above).But, there’s a hitch with this incredible project-centered course. No one is getting credit for it. It became too complicated to involve the SJSU administration and various disciplines. We would have to request independent study for everyone, and with the budget crunch, my department frowns on that solution. Additionally, independent study shows up on their transcripts under the associate chair, not me. Consequently, it didn’t make sense. Quite frankly, this frees me from assessing their work — because assessing digital projects requires a different framework than assessing course work. I’m not focusing on the outcome, the product. Instead, we are engaged in a process; one that will take a year to come to fruition. They consider this project and our meetings their fun time. FUN TIME!So, we gather, talk, eat, sometimes drink, touch books, exchange stories, make progress, ask questions, laugh, celebrate. But, most importantly, we collaborate. Correction. They collaborate.I can’t wait to see what they come up with next, what they solve, what they query, where this project takes them.This is the kind of teaching that I’d like to do: project-centered courses that resolve real-time issues.Now, how can I get this written into the curriculum here at SJSU? Guess I’ll have to work on that one m’self. In the meantime, I’m preparing the menu for our next meeting. Lamb Tagine? or Braised short ribs?Like this:Be the first to like this.One of the most prevalent debates within the Digital Humanities (DH) is the idea that practitioners should just go about doing rather than talking, or to practice “more hack, less yack.” In other words, instead of pontificating and problematizing, DH scholars should be more concerned with making stuff, and making stuff happen. The “more hack, less yack” mantra has been going on for a while now, and has brushed up against some challenges; notably Natalia Cecire’s (@ncecire) argument for the need for a ThatCamp Theory to uncover the theoretical leanings of the digital humanities, in Alan Liu’s call for the need to integrate cultural studies into dh approaches, and in the recent TransformDH collective, set up by Anne Cong-Huyen (@anitaconchita), Moya Bailey (@moyazb) and M. Rivera Monclova (@phdeviate) to bring race/gender/class/disability criticism to the digital humanities.** In many of these debates, it seems as though the “theoretification” of DH is viewed with suspicion as it disturbs the implicit good nature of much of the DH community. Roger Whitson (@rogerwhitson), for example, mused on whether the digital humanities “really needs to be transformed,” arguing that: “It seems to me that the word “guerilla” reappropriates the collaborative good will of the digital humanities, making it safe for traditional academic consumption and inserting it into the scheme Stanley Fish and William Pannapacker (@pannapacker) highlight.”I’ve been musing on the “more hack, less yack” issue recently, and it seems that Tara McPherson’s (@tmcphers) essay “U.S. Operating Systems at Mid-Century: The Intertwining of Race and UNIX” in Lisa Nakamura (@lnakamur) and Peter Chow-White’s (@pachowwhite) recent collectionRace After the Internet may offer some important insights into this ideological impasse. In her essay, McPherson argues that in the mid-twentieth century, a common structural logic developed due to computerization, one which argued for the importance of “modular thinking”, “common-sense” and disciplinary hyperspecialization. By focusing on processes which work via the modular form—simple blocks by which a complex system is broken down and analyzed by individual groups—the rationale of this system appears “common-sensical”, thereby obscuring the actual political and social moment from which it emerges.McPherson sees this modular logic manifest in both the development of UNIX as well as racial formations in the United States, and expands this to argue that this might be a hallmark of the Fordist moment of capitalist production in the United States, and finds its manifestation in the hyperspecialization of late capitalism, extending to the specialization of disciplines in higher education such as Area Studies departments. This mode of modular thinking, she argues, is a type of “lenticular logic” which undergirds both the structures of UNIX as well as the covert racism of color blindness:“A lenticular logic is a covert racial logic, a logic for the post-Civil Rights era. We might contrast the lenticular postcard to that wildly popular artifact of the industrial era, the stereoscope card. The stereoscope melds two different images into an imagined whole, privileging the whole; the lenticular image partitions and divides, privileging fragmentation. A lenticular logic is a logic of the fragment or the chunk, a way of seeing the world as discrete modules or nodes, a mode that suppresses relation and context. As such, the lenticular also manages and controls complexity.” (25)Reading McPherson makes me think: to what degree does lenticular logic underlie the DH imperative for “more hack, less yack?” How much does digital humanities work, through the way it is processed and organized through computational models, actually follow the Fordist logic of modularity? In the same way that UNIX engineers extolled programmers to “common sense and notions about simplicity to justify their innovations in code,” (28), neglecting how his common-sense is similarly constituted by their historical specificity as a class of workers in the 1970s, how has this sentiment actually provided the language behind “more hack, less yack?”In other words, common sense is never simply “common sense.” What is “common sense” comes out of a particular socio-historical moment, just as “hacking” has derived from a very specific social context. And, just as UNIX programmers relied, in McPherson’s argument, on a common-sense modular “lenticular logic” to avoid speaking about the socio-political origins and conditions that allowed for their “common sense” to come into being, perhaps the same logic has underwritten our resistance to theory within the digital humanities. Where does our “common sense” in the digital humanities come from? How is it implicated in structures of privilege which remain invisible to us? Why are we so resistant to speaking about it, and how does the language of modularity aid us in this silence?It appears to me that much of the “more hack, less yack” issue circles around the problem of modularity and common-sensical “form” that McPherson outlines in this essay. I see this in Bethany Nowviskie’s (@nowviskie) recent post, Don’t Circle the Wagons:“Software development functions in relative silence within the larger conversation of the digital humanities, in a sub-culture suffused — in my experience — not with locker-room towel-snaps and construction-worker catcalls, but with lovely stuff that’s un-voiced: what Bill Turkel and Devon Elliott have calledtacit understanding, and with journeyman learning experiences. And that’s no surprise. To my mind, coding itself has more in common with traditional arts-and-crafts practice than with academic discourse.Too often, the things developers know — know and value, because they enact them every day — go entirely unspoken. These include the philosophy and ethos of software craftsmanship and, by extension, the intellectual underpinnings of digital humanities practice. (If you use their tools, that goes for your DH practice, too.)”Nowviskie’s elaboration of a “tacit understanding” that derives from “journeyman learning experiences” makes me wonder how much of these learning experiences dovetail with McPherson’s notion of modular, lenticular logic that structures UNIX and other mid-century structuralist Fordist systems. This “tacit understanding” creates a common-sense notion of simplicity, but one whose structure and “common-sensical” nature similarly allows for a significant amnesia towards its own socio-political origins and context. In the same way that UNIX engineers extolled programmers to “common sense and notions about simplicity to justify their innovations in code,” (McPherson 28), neglecting how his common-sense is similarly constituted by their historical specificity as a class of workers in the 1970s, how has this mode of thought provided the language for the “more hack, less yack” sentiment?McPherson’s argument recalls Paul De Man’s Blindness and Insight, where De Man asserted that all critical readings are ultimately predicated upon a “negative movement that animates the critic’s thought, an unstated principle that leads his language away from its asserted stand… as if the very possibility of assertion had been put into question.” De Man argued that we needed to return to engaging how a certain type of form made certain readings possible. At the same time, he asserted that the blindness to that very form was critical to structuring our insights. While De Man’s metaphor is problematically ableist***, it still makes a critical point: that we need to interrogate how the logic of form tends to erase the perspective of its own creation. As literary theory given critics insights that hide their own foundations, the logics of computation have given us a certain type of structure, a type of tacit understanding, a sort of visible logic and knowing that have simultaneously obscured their own foundational assumptions.I do not mean to suggest that tacit understanding equates to a certain type of blindness. That said, I do mean to recognize that all forms of shared, cultural understandings, whether they come under the umbrella terms “common sense,” “tradition” or “ritual,” are founded upon an important obscuring of their own particular socio-political specificity, and that to ignore this specificity is troubling. As Pierre Bourdieu observed, all cultural practices exist as habitus, a set of learned dispositions, skills and ways of acting that appear simply natural, but which are rooted in specific social-cultural contexts. My call, then, is for us to interrogate the habitus that makes up the Digital Humanities community.Let me be clear. I get annoyed by jargon and obfuscation as much as the next person, which is why I am so attracted to the digital humanities community. But I do think that we need to invest in the creation of a metalanguage that will allow us to see the ideological foundations that undergird our “common sense.” And sometimes that comes hand in hand with theory. Also, theory doesn’t always need to be annoyingly grating, especially if it allows us to understand how our implicit systems invisibly privilege and disenfranchise certain groups of people. We need to question the forms that make us see “common-sense”, and to see value in the converse “less hack, more yack” proposition.If computation is, as Cathy N. Davidson (@cathyndavidson) and Dan Rowinski have been arguing, the fourth “R” of 21st century literacy, we very much need to treat it the way we already do existing human languages: as modes of knowledge which unwittingly create cultural valences and systems of privilege and oppression. Frantz Fanon wrote inBlack Skin, White Masks: “To speak a language is to take on a world, a civilization.” As Digital Humanists, we have the responsibility to interrogate and to understand what kind of world, and what kind of civilization, our computational languages and forms create for us. Critical Code Studies is an important step in this direction. But it’s important not to stop there, but to continue to try to expand upon how computation and the digital humanities are underwritten by theoretical suppositions which still remain invisible.** Alexis Lothian’s article, “Marked Bodies, Transformative Scholarship and the Question of Theory in the Digital Humanities.” Journal of Digital Humanities 1:1, November 4, 2011, gives an excellent history of the #TransformDH group, and the call towards Theory within the Digital Humanities. Thanks Alexis (@alothian) for pointing me to this!***Thanks to Natalia Cecire (@ncecire) for reminding me of this.Image CreditEdited to Add: Some interesting responses to this post[This is a guest post by Peter Bradley, associate professor of philosophy at McDaniel College. He works primarily in philosophy of mind and cognitive science, but most recently has been developing digital tools to support critical thinking instruction across the curriculum. You can find him online at his website and on Twitter at @pbradl42.--@jbj]A few weeks ago, I attended THATCamp Pedagogy, where I met loads of lovely humanists, each of whom is doing fascinating things with digital tools to study humanistic questions or asking humanistic questions about digital content.There was one core humanistic discipline largely absent from this unconference: my own, philosophy. This is not new, nor surprising. It is, however, deeply regrettable.A quick, though not exhaustive, search of the various THATCamp participants on *.thatcamp.org found only one participant, other than me, officially affiliated with philosophy–a director of information technology who used to be a philosophy professor. There are a handful of excellent information technologists with backgrounds in philosophy involved in the THATCamp / Digital Humanities movement, but I know of only one other actual philosophy professor: Chris Sula, of the Pratt Institute and phylo.info.My point here is not that there are no philosophers developing digital content or using information technology to further philosophical research: David Bourget’s PhilPapers.org, John Immerwahr’s teachphilosophy101.org, and Andy Cullison’s sympoze.com are notable examples of excellent and innovative uses of informational technology to advance philosophy. At the same time, there are a number of notable philosophers thinking about the interface of technology and ourselves- David Chalmers, Luciano Floridi and Andy Clark spring to mind.There are not, however, numerous examples of philosophers using techniques of the digital humanities to _do_ philosophy or using digital tools to teach philosophy.On an incredibly basic, overly simplified level, philosophy is interested in the discovery, development, classification and analysis of human concepts and reasoning. We teach texts, concepts, arguments, and the historical and social development and influence of such texts, concepts and arguments.All of these tasks are amenable to the digital humanities. The concepts and reasoning structures common in digital environments are accessible for philosophic analysis, and the tools developed to analyze and archive literature and language can clearly be adapted to philosophic work.Here I’ll suggest, in broad outlines, a number of areas in which I believe philosophy can, and should, contribute to the digital humanities. These suggestions are by no means exhaustive.First, our concepts of the digital, and the concepts that are accentuated by digital technology, are open to philosophic analysis. Obvious examples include space, personal identity, textuality, social networks, experience, intellectual property, etc. But there is also a set of concepts and problems that appear because of the ubiquity of information systems that should be subject to philosophical analysis, including interesting new forms of reasoning in multimedia socially-networked information systems.Second, the tools of the digital humanities can be extended to philosophy, and especially the teaching of philosophy. Here are a three simple examples, just off the top of my head: Relative influence of philosophers may be approximated by word-frequency analysis of their mentions and/or citations. For example, Google ngrams can quickly produce a graph of the relative importance of Locke, Hobbes and Rousseau during the 20th century:click for full-sizeNgrams are easy to generate and adaptable to classroom exercises. They can quickly illustrate grand historical trends in concept development. Terms that originate in philosophy–such as ‘utilitarianism’ or ‘emotivism’–are particularly accessible for undergaduates to investigate. In a recent writing class, I had students use ngrams, along with twitter, to research the instances of word use in order to test their proposed conceptual analyses.With respect to argumentation, I often tell my students that reducto ad absurdum is, if not the most common argumentation form inphilosophy, one of the most common. It would be very useful to have actual evidence to support that claim. With a bit of textual markup of historical work in philosophy, we could query the relative frequency of argumentative form and establish relative frequencies of argumentation form.And for current work in philosophy as a discipline- one can use digital humanist techniques to map trends in the discipline. The opposite approach is typified by a certain well-known blogger in philosophy who has been known to prognosticate on ‘trends’ in philosophy, both continental and analytic. None of his claims are ever been backed up with data. Now that many journals provide their abstracts via RSS feed, it’s easy enough to pull abstracts together and run word-frequency analysis on those abstracts. This technique will not, of course, provide a definitive answer, but it can provide be a basic heuristic that could indicate a trend. I actually set up an automatic script to do just that some time ago. The resulting wordles, along with all the data, are available here: http://inquiry.mcdaniel.edu/trends/There is so much potential for philosophy generally, and teaching philosophy in particular, in the THATCamp and digital humanities movement – we need, as a discipline, to engage with other humanists and participate with new and interesting research tools.Photo plato by Flickr user DJ Hoogerdijk / Creative Commons licensedI’m home again after a very interesting session about building a National Digital Public Library at the Los Angeles Public Library and I’d like to recap it while offering a few of my own opinions for the readers. Public librarians reading this post: a national digital library movement is most certainly underway, and quite frankly I think it may be the only thing that will prevent our libraries as we know them from simply fading from relevance. I’m aware that my opinions on these matters may be a bit controversial for those libraries and librarians kicking back out there, those that believe providing access to books for public consumption will remain a useful, supported service that your community will tax themselves to keep around.Before going any further, I think it’s worth stating for clarification that this #NDPL conference was an IMLS-funded gathering that was the product of a concurrent vision overlapping with another project that’s been the talk of the town lately, that being the Digital Public Library of America (#DPLA) initiative. I’d like to thank Martin Gomez for having the foresight to put this meeting into the works a year ago, and IMLS and the Library Foundation of Los Angeles for supporting it. I’d also like to thank him for closing the conference with a statement endorsing the DPLA movement as something vital and of imminent importance, something that public libraries must become deeply involved with in order to steer it in a direction that serves their users’ needs. With that, I will stop switching between the NDPL / DPLA nametags and return to DPLA consistently when I refer to this movement; I hope others will too because clear messaging and good communication are important components of this (or any) movement.It was excellent to have so many strong voices from public libraries pondering the difficult predicament libraries find themselves in as content lenders in the digital environment. There were library directors and state librarians as well as public library technologists present, not to mention representatives from OCLC, The Sloan Foundation, the Gates Foundation and many, many more. That said, as much as the room was brimming with public library leadership talent and as encouraging as it was to see those representatives, I was reminded of the reason why I rushed to become involved in the DPLA immediately upon hearing about the idea not quite one year ago. I saw the DPLA as a chance to inform, influence and leverage change in my profession from the outside, rather than change it from the inside as I’ve been trying to do for years as a public employee. After working for public libraries for 13 years now I don’t believe that the type of fundamental change that we need to make to the very definition of what a public library IS could ever happen from the inside. That’s regrettable, but true.So, indeed, I believe that our era of digital content replacing print content requires a much larger re-visioning of what public libraries can and should be in their communities. I’d describe public libraries’ current approach to the future as ‘people want their books as eBooks now, so lets fight an uphill battle to provide eBooks to patrons as a new media format the same way we’ve always done so’. That is not going to happen, and if that’s what the goal of the DPLA is then I’m sorely disappointed and I’m sure it’ll fail. Instead, let me suggest for public libraries and the DPLA a new mission and vision, one that taxpayers WILL support for many years to come because no other competitor does it, and because if it is explained and implemented properly (see: nationally) it will build stronger, smarter communities, and ultimately build a stronger, smarter country. In one sentence: public libraries need to support information production with the same level of commitment that they’ve always treated information consumption. For a while now I’ve been preaching this like a scratched record on an old turntable, but it’s important to continue to reiterate it. Libraries have always been read-only institutions, offering access to media for consumption. Card holders can read library-lent books, watch library-lent movies, listen to library-lent music. If the public library is to succeed in this new digital era, card holders (hopefully meaning everyone in your community) need to be given access and education so they can create and contribute to culture, and gain the skills to produce things like the new media creations that will be the successors to the things we call eBooks now. Library cardholders should be equipped to participate in the incremental construction of end-product multimedia packages (eBooks, web platforms and services, or others) through conversation and single-channel media production, and public libraries CAN and SHOULD be the intermediaries making that possible.The meeting in Los Angeles was not about this path. This meeting focused on the preservation of culture, on digitizing our past rather than supporting the creation of new culture. Now I hope someone will correct me if I’m wrong here, but in my experience this revelation that our unique, historically significant holdings hidden in public library basements and attics are our most important materials is actually quite new. My impression is that only five years ago the trend was to try to sell or gift these materials to other agencies who think of ‘preservation’ as a piece of their mission. I can remember quite a few meetings in quite a few different contexts where librarians of all levels agreed that the mission of the contemporary public library has very little to do with ‘preservation’, and that the only responsible thing we can do is get these materials into the hands of others who can deal with them responsibly. I had an excellent conversation with Helene Blowers of the Columbus Metropolitan Library on this topic. We both agreed that while preservation of existing cultural artifacts is important, there’s an immediate need to imagine a framework that supports both the creation and archiving of current and future knowledge. We both hoped that the DPLA could carry a torch along that road, rather than merely shine a light on the dusty corners of our past.For me, the highlights from the meeting in Los Angeles came from Ben Vershbow of New York Public Library’s Labs team and Peter Brantley of the Internet Archive. Peter asked the group to think further ahead, and to consider the future of digital reading and what eBook packages will look like in 2-5 years. This is something I’ve been deeply concerned about lately as well. For example, with Amazon now acting as publisher, platform, and device they’ve completely monopolized the eMedia life cycle. There’s no reason Amazon couldn’t work directly with the author of an upcoming ‘novel’ and provide them with the tools create a rich, multimedia work that would only ‘read’ or ‘play’ on the Kindle Fire, or perhaps would have enhanced features that are only available when using a Kindle Fire. Peter retweeted a statement from Michael Colford, Director of the Boston Public Library at one point, saying “if we don’t deal with access to contemporary books, we risk losing relevance for libraries.” I appreciate that, yet I remain completely unconvinced that the future of libraries rests solely or even largely on awkward content lending arrangements. Librarians have made it about that, but it doesn’t have to be about that. I think it’s time to move away from a model that places value in consumptive transactions, and toward an experiential learning model that supports creativity and contribution. I’m reminded of the final sentence in an old blog post by Aaron Schmidt from 2009 called Libraries Might Not Provide Content in the Future & it’s Okay, where he says “If anything, we should consider books, movies, music and computers loss leaders and show people what we can really do for them once we’re lucky enough to have them in our buildings.”Ben Vershbow did more than just show off the jaw-droppingly amazing digital projects he manages at NYPL Labs; he was the one speaker who offered a glimpse of a realistic vision for the future of a DPLA working in harmony with our existing public library infrastructure. Ben briefly moved conversation away from digitize, digitize, digitize and brought up the Make Magazine article from earlier this year about public libraries as hacker spaces or fablabs where people can create projects for their own personal growth, for the benefit of the community, or both. He also pointed to the project in Fayetteville NY, where the librarians have actually set up the first public fablab. Friends, it’s not an accident or a weird coincidence that writers, technologists, entrepreneurs, futurists, hackers, artists, and intellectuals of all sorts have been either expressing interest in a ‘new kind of public library’ or even trying to make such things themselves. It’s not a coincidence that there’s a group of hackers voluntarily building a mobile app for the Tulsa Public Library in Oklahoma, while there’s also a group of hackers doing work in the Washington DC Public Library, or while a ‘TechRaising’ event in Santa Cruz brought together developers who were happy to help the library realize a new digital project. It’s a sign of the times that efforts like the Read/Write Library in Chicago will be featured in a talk at SXSW this year, and that popup portable reading rooms like the Uni project run successful Kickstarter campaigns and have mass appeal.What I’d really like to hear at future meetings are some ideas about how the DPLA movement can incorporate the ideas and the energy that all of these other independent projects have, and how that kind of work can be supported on a national scale without losing the local flavor that remains so important in communities. I’d like to hear less about digitization, which is not to say it is unimportant, but it is to say that preserving the past is probably the least imaginative step forward public libraries can take into the digital future right now. So, in conclusion, here’s what I believe this movement needs next: the DPLA needs a public-facing laboratory; an experimental beta space where we can prototype ideas, curricula, interfaces, strategies, and experiences. I know I’m not alone in wanting this, I’ve had many conversations in which this has come up. Look for future posts describing this beta space.This special post was co-written with David Bloom, VertNet Coordinator and crossposted (with some minor mods) at the Vertnet Blog.http://www.loc.gov/exhibits/treasures/images/1831s.jpgFirst and foremost, digitization of natural history collections and tools to make these digitized records available, such as VertNet, support global biodiversity research. We suspect that the majority of use of digitized records will be to generate products such as species distribution models and change assessments, and to answer questions about what is in any given museum collection. However, in the broader context of academic endeavor, these data could also serve as a unique link between the digital sciences and the digital humanities. Work in the digital humanities includes everything from crowdsourcing manuscript transcription to humanistic fabrication to data mining — work that is not so dissimilar in method, description, or data type from that in the digital sciences.Biological collections aren’t the only organizations engaged in massive digitization efforts; libraries and archives have been digitizing and making their materials discoverable and interoperable for decades as well. As a result of these efforts, an unprecedented number of research materials from a wide range of domains are now available for free on the Web. Just as VertNet does for biodiversity data, the University of Illinois’ Digital Collections and Content project does for cultural heritage records, the Australia National Library’s Trove for newspapers, articles, and music. The Hathi Trust makes more than 9 million books available — and the list goes on. Digitization allows these materials to be recombined and analyzed quickly and (relatively) easily in new ways.Our question is a simple one: Where do the digital humanities and e-science overlap and interconnect? One method of digital investigation that caught our attention is the mapping of novels and other historic texts; researchers take prose text and mine it for mappable units. Erin Sells and her students, for instance, have used this method to create dynamic maps of Virginia Woolf’s Mrs. Dalloway, which incorporate “pictures, sounds, videos, and the text itself into the map.” Similarly, in the Google Ancient Places project, researchers mine archaeological and historical texts to create databases of georeferenced ancient locales which can then be mapped. Though these researchers are working with novels, they’re producing data in formats similar to those used for species occurrence records in databases such as VertNet.This made us think: what sorts of questions could we ask of a data set composed of all kinds of georeferences — not just species occurrence records, but locations from history or works of fiction as well? If students of the humanities can create maps with such texture using similarly organized data sets, could they build on this richness by including analysis of the natural world as it existed at the time described in the novel? Perhaps searching on the VertNet portal (or GBIF or ALA) could provide a detailed list of vertebrate species and, with a little more work, the associated ranges of these species. Suddenly, the map of Mrs. Dalloway’s world, and the atmosphere of Clarissa’s party, can be enriched not only with human influence and creation, but by the natural environment, too. Conversely, data from diaries or other digitized sources could be mined for data about distributions of now-extinct species. Could these data be used as observations and published as records along with those from natural history collections?From Lewis and Clark's journals - http://www.smithsonianeducation.org/images/educators/lesson_plan/lewis_and_clark/si_ci_bird_lg.jpgWe hope that VertNet will support interdisciplinary research in the science and the humanities by providing new avenues for deeper readings, and new ways to reconstruct real and imagined worlds. Where are the specimens that Lewis and Clark found on their expeditions and how do those link up with their journals (online already!!)? What about whale species described by Melville? How accurate are James Fenimore Cooper’s depictions of the animals Hawkeye and Cora encountered as they traveled through the Great Lakes? What does this accuracy or inaccuracy tell you about Cooper as an author? What about Thoreau’s notebooks of life at Walden Pond, and how have this iconic landscape and its animals and plants changed since his stay?We also hope that other folks have more ideas about what new combinations of data and domains of inquiry are possible now that so many different sources of knowledge have been digitized. How can eScience support and enrich the digital humanities and vice-versa? What happens when images of specimens* mix with drawings from the literature? Point-radius georeferences, for example, are easy enough to pull together from different sources — what further visualizations could be created with the combination of journals, books, and catalog ledgers? What further ways can we use data and smarts to bridge gaps between the sciences and the humanities?SYTYCD is offering the inaugural Thinky People’s Digitizaton Challenge (THIPDIC). This first THIPDIC will go to the person or people who provide our favorite comment showing how digital science and the digital humanities intersect. Any cool examples? Any deeper thoughts about how this happens? Any cute pictures of animals reading book? Winners will be celebrated the world over and will be eligible for a (modest) prize, offered by Rob (don’t worry, it’ll be something interesting and of actual value). You may now talk amongst yourselves.* gigapan snakes in jar!Like this:2 bloggers like this. This fall, Andie will be starting the PhD program in Library and Information Science at the University of Illinois at Urbana-Champaign. Her research interests include data curation, natural history museum work and biodiversity informatics. This summer she is interning at the National Center for Atmospheric Research in Boulder, CO; she used to excavate at the La Brea Tar Pits; and spent last summer doing neat stuff at the Petrified Forest National Park. She still wishes there was a shorter way of saying all that.According to Google Scholar, David Blei’s first topic modeling paper has received 3,540 citations since 2003. Everybody’s talking about topic models. Seriously, I’m afraid of visiting my parents this Hanukkah and hearing them ask “Scott… what’s this topic modeling I keep hearing all about?” They’re powerful, widely applicable, easy to use, and difficult to understand — a dangerous combination.Since shortly after Blei’s first publication, researchers have been looking into the interplay between networks and topic models. This post will be about that interplay, looking at how they’ve been combined, what sorts of research those combinations can drive, and a few pitfalls to watch out for. I’ll bracket the big elephant in the room until a later discussion, whether these sorts of models capture the semantic meaning for which they’re often used. This post also attempts to introduce topic modeling to those not yet fully converted aware of its potential.Citations to Blei (2003) from ISI Web of Science. There are even two citations already from 2012; where can I get my time machine?In my recent post on IU’s awesome alchemy project, I briefly mentioned Latent Semantic Analysis (LSA) and Latent Dirichlit Allocation (LDA) during the discussion of topic models. They’re intimately related, though LSA has been around for quite a bit longer. Without getting into too much technical detail, we should start with a brief history of LSA/LDA.The story starts, more or less, with a tf-idf matrix. Basically, tf-idf ranks words based on how important they are to a document within a larger corpus. Let’s say we want a list of the most important words for each article in an encyclopedia.Our first pass is obvious. For each article, just attach a list of words sorted by how frequently they’re used. The problem with this is immediately obvious to anyone who has looked at word frequencies; the top words in the entry on the History of Computing would be “the,” “and,” “is,” and so forth, rather than “turing,” “computer,” “machines,” etc. The problem is solved by tf-idf, which scores the words based on how special they are to a particular document within the larger corpus. Turing is rarely used elsewhere, but used exceptionally frequently in our computer history article, so it bubbles up to the top.LSA and pLSALSA utilizes these tf-idf scores  within a larger term-document matrix. Every word in the corpus is a different row in the matrix, each document has its own column, and the tf-idf score lies at the intersection of every document and word. Our computing history document will probably have a lot of zeroes next to words like “cow,” “shakespeare,” and “saucer,” and high marks next to words like “computation,” “artificial,” and “digital.” This is called a sparse matrix because it’s mostly filled with zeroes; most documents use very few words related to the entire corpus.With this matrix, LSA uses singular value decomposition to figure out how each word is related to every other word. Basically, the more often words are used together within a document, the more related they are to one another.  It’s worth noting that a “document” is defined somewhat flexibly. For example, we can call every paragraph in a book its own “document,” and run LSA over the individual paragraphs.To get an idea of the sort of fantastic outputs you can get with LSA, do check out the implementation over at The Chymistry of Isaac Newton.Newton Project LSAThe method was significantly improved by Puzicha and Hofmann (1999), who did away with the linear algebra approach of LSA in favor of a more statistically sound probabilistic model, called probabilistic latent semantic analysis (pLSA). Now is the part of the blog post where I start getting hand-wavy, because explaining the math is more trouble than I care to take on in this introduction.Essentially, pLSA imagines an additional layer between words and documents: topics. What if every document isn’t just a set of words, but a set of topics? In this model, our encyclopedia article about computing history might be drawn from several topics. It primarily draws from the big platonic computing topic in the sky, but it also draws from the topics of history, cryptography, lambda calculus, and all sorts of other topics to a greater or lesser degree.Now, these topics don’t actually exist anywhere. Nobody sat down with the encyclopedia, read every entry, and decided to come up with the 200 topics from which every article draws. pLSA infers topics based on what will hereafter be referred to as black magic. Using the dark arts, pLSA “discovers” a bunch of topics, attaches them to a list of words, and classifies the documents based on those topics.LDABlei et al. (2003) vastly improved upon this idea by turning it into a generative model of documents, calling the model Latent Dirichlet allocation (LDA). By this time, as well, some sounder assumptions were being made about the distribution of words and document length — but we won’t get into that. What’s important here is the generative model.Imagine you wanted to write a new encyclopedia entry, let’s say about digital humanities. Well, we now know there are three elements that make up that process, right? Words, topics, and documents. Using these elements, how would you go about writing this new article on digital humanities?First off, let’s figure out what topics our article will consist of. It probably draws heavily from topics about history, digitization, text analysis, and so forth. It also probably draws more weakly from a slew of other topics, concerning interdisciplinarity, the academy, and all sorts of other subjects. Let’s go a bit further and assign weights to these topics; 22% of the document will be about digitization, 19% about history, 5% about the academy, and so on. Okay, the first step is done!Now it’s time to pull out the topics and start writing. It’s an easy process; each topic is a bag filled with words. Lots of words. All sorts of words. Let’s look in the “digitization” topic bag. It includes words like “israel” and “cheese” and “favoritism,” but they only appear once or twice, and mostly by accident. More importantly, the bag also contains 157 appearances of the word “TEI,” 210 of “OCR,” and 73 of “scanner.”LDA Model from Blei (2011)So here you are, you’ve dragged out your digitization bag and your history bag and your academy bag and all sorts of other bags as well. You start writing the digital humanities article by reaching into the digitization bag (remember, you’re going to reach into that bag for 22% of your words), and you pull out “OCR.” You put it on the page. You then reach for the academy bag and reach for a word in there (it happens to be “teaching,”) and you throw that on the page as well. Keep doing that. By the end, you’ve got a document that’s all about the digital humanities. It’s beautiful. Send it in for publication.Alright, what now?So why is the generative nature of the model so important? One of the key reasons is the ability to work backwards. If I can generate an (admittedly nonsensical) document using this model, I can also reverse the process an infer, given any new document and a topic model I’ve already generated, what the topics are that the new document draws from.Another factor contributing to the success of LDA is the ability to extend the model. In this case, we assume there are only documents, topics, and words, but we could also make a model that assumes authors who like particular topics, or assumes that certain documents are influenced by previous documents, or that topics change over time. The possibilities are endless, as evidenced by the absurd number of topic modeling variations that have appeared in the past decade. David Mimno has compiled a wonderful bibliography of many such models.While the generative model introduced by Blei might seem simplistic, it has been shown to be extremely powerful. When a newcomer sees the results of LDA for the first time, they are immediately taken by how intuitive they seem. People sometimes ask me “but didn’t it take forever to sit down and make all the topics?” thinking that some of the magic had to be done by hand. It wasn’t. Topic modeling yields intuitive results, generating what really feels like topics as we know them , with virtually no effort on the human side. Perhaps it is the intuitive utility that appeals so much to humanists.Topic models can interact with networks in multiple ways. While a lot of the recent interest in digital humanities has surrounded using networks to visualize how documents or topics relate to one another, the interfacing of networks and topic modeling initially worked in the other direction. Instead of inferring networks from topic models, many early (and recent) papers attempt to infer topic models from networks.Topic Models from NetworksThe first research I’m aware of in this niche was from McCallum et al. (2005). Their model is itself an extension of an earlier LDA-based model called the Author-Topic Model (Steyvers et al., 2004), which assumes topics are formed based on the mixtures of authors writing a paper. McCallum et al. extended that model for directed messages in their Author-Recipient-Topic (ART) Model. In ART, it is assumed that topics of letters, e-mails or direct messages between people can be inferred from knowledge of both the author and the recipient. Thus, ART takes into account the social structure of a communication network in order to generate topics. In a later paper (McCallum et al., 2007), they extend this model to one that infers the roles of authors within the social network.Dietz et al. (2007) created a model that looks at citation networks, where documents are generated by topical innovation and topical inheritance via citations. Nallapati et al. (2008) similarly creates a model that finds topical similarity in citing and cited documents, with the added ability of being able to predict citations that are not present. Blei himself joined the fray in 2009, creating the Relational Topic Model (RTM) with Jonathan Chang, which itself could summarize a network of documents, predict links between them, and predict words within them. Wang et al. (2011) created a model that allows for “the joint analysis of text and links between [people] in a time-evolving social network.” Their model is able to handle situations where links exist even when there is no similarity between the associated texts.Networks from Topic ModelsSome models have been made that infer networks from non-networked text. Broniatowski and Magee (2010 & 2011) extended the Author-Topic Model, building a model that would infer social networks from meeting transcripts. They later added temporal information, which allowed them to infer status hierarchies and individual influence within those social networks.Many times, however, rather than creating new models, researchers create networks out of topic models that have already been run over a set of data. There are a lot of benefits to this approach, as exemplified by the Newton’s Chymistry project highlighted earlier. Using networks, we can see how documents relate to one another, how they relate to topics, how topics are related to each other, and how all of those are related to words.Elijah Meeks created a wonderful example combining topic models with networks in Comprehending the Digital Humanities. Using fifty texts that discuss humanities computing, Elijah created a topic model of those documents and used networks to show how documents, topics, and words interacted with one another within the context of the digital humanities.Network generated by Elijah Meeks to show how digital humanities documents relate to one another via the topics they share.Elijah Jeff Drouin has also created networks of topic models in Proust, as reported by Elijah.Peter Leonard recently directed me to TopicNets, a project that combines topic modeling and network analysis in order to create an intuitive and informative navigation interface for documents and topics. This is a great example of an interface that turns topic modeling into a useful scholarly tool, even for those who know little-to-nothing about networks or topic models.If you want to do something like this yourself, Shawn Graham recently posted a great tutorial on how to create networks using MALLET and Gephi quickly and easily. Prepare your corpus of text, get topics with MALLET, prune the CSV, make a network, visualize it! Easy as pie.Networks can be a great way to represent topic models. Beyond simple uses of navigation and relatedness as were just displayed, combining the two will put the whole battalion of network analysis tools at the researcher’s disposal. We can use them to find communities of similar documents, pinpoint those documents that were most influential to the rest, or perform any of a number of other workflows designed for network analysis.As with anything, however, there are a few setbacks. Topic models are rich with data. Every document is related to every other document, if some only barely. Similarly, every topic is related to every other topic. By deciding to represent document similarity over a network, you must make the decision of precisely how similar you want a set of documents to be if they are to be linked. Having a network with every document connected to every other document is scarcely useful, so generally we’ll make our decision such that each document is linked to only a handful of others. This allows for easier visualization and analysis, but it also destroys much of the rich data that went into the topic model to begin with. This information can be more fully preserved using other techniques, such as multidimensional scaling.A somewhat more theoretical complication makes these network representations useful as a tool for navigation, discovery, and exploration, but not necessarily as evidentiary support. Creating a network of a topic model of a set of documents piles on abstractions. Each of these systems comes with very different assumptions, and it is unclear what complications arise when combining these methods ad hoc.Although there may be issues with the process, the combination of topic models and networks is sure to yield much fruitful research in the digital humanities. There are some fantastic tutorials out there for getting started with topic modeling in the humanities, such as Shawn Graham’s post on Getting Started with MALLET and Topic Modeling, as well as on combining them with networks, such as this post from the same blog. Shawn is right to point out MALLET, a great tool for starting out, but you can also find the code used for various models on many of the model-makers’ academic websites. One code package that stands out is Chang’s implementation of LDA and related models in R.Blei, D. M. (2011). Introduction to Probabilistic Topic Models. Communications of the ACM.Blei, D. M., & Lafferty, J. D. (2006). Dynamic topic models. Proceedings of the 23rd international conference on Machine learning, ICML ’06 (pp. 113–120). New York, NY, USA: ACM. doi:10.1145/1143844.1143859Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77. doi:10.1145/2133806.2133826Boyd-Graber, J., & Blei, D. M. (2009). Multilingual topic models for unaligned text. Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09 (pp. 75–82). Arlington, Virginia, United States: AUAI Press. Retrieved from http://dl.acm.org/citation.cfm?id=1795114.1795124Broniatowski, D. A., & Magee, C. L. (2010). Analysis of Social Dynamics on FDA Panels Using Social Networks Extracted from Meeting Transcripts. 2010 IEEE Second International Conference on Social Computing (SocialCom) (pp. 329–334). Presented at the 2010 IEEE Second International Conference on Social Computing (SocialCom), IEEE. doi:10.1109/SocialCom.2010.54AlSumait, L., Barbará, D., Gentle, J., & Domeniconi, C. (2009). Topic Significance Ranking of LDA Generative Models. In W. Buntine, M. Grobelnik, D. Mladenić, & J. Shawe-Taylor (Eds.), Machine Learning and Knowledge Discovery in Databases (Vol. 5781, pp. 67–82). Berlin, Heidelberg: Springer Berlin Heidelberg. Retrieved from http://www.springerlink.com/content/v3jth868647716kg/Chaney, A. J. B., & Blei, D. M. (2012). Visualizing Topic Models. Presented at the Internatoinal AAAI Conference on Weblogs and Social Media, Dublin, Ireland.Chang, J., & Blei, D. M. (2010). Hierarchical relational models for document networks. The Annals of Applied Statistics, 4(1), 124–150. doi:10.1214/09-AOAS309Dietz, L., Bickel, S., & Scheffer, T. (2007). Unsupervised prediction of citation influences. Proceedings of the 24th international conference on Machine learning, ICML ’07 (pp. 233–240). New York, NY, USA: ACM. doi:10.1145/1273496.1273526Erosheva, E., Fienberg, S. E., & Lafferty, J. D. (2004). Mixed-membership models of scientific publications. Proceedings of the National Academy of Sciences, 101, 5220–5227. doi:10.1073/pnas.0307760101Gardner, M. J., Lutes, J., Lund, J., Hansen, J., Walker, D., Ringger, E., & Seppi, K. (2010). The Topic Browser: An Interactive Tool for Browsing Topic Models. Presented at the NIPS Workshop on Challenges of Data Visualization.Gerrish, S., & Blei, D. M. (2009). Modeling Influence in Text Corpora. Presented at the NIPS Workshop on Applications for Topic Models: Text and Beyond., Whistler, Canada.Gerrish, S., & Blei, D. M. (2010). A language-based approach to measuring scholarly impact. Proceedings of the 26th International Conference on Machine Learning. Presented at the International Conference on Machine Learning, Haifa, Israael. Retrieved from http://www.cs.princeton.edu/ blei/papers/GerrishBlei2010.pdfGirolami, M., & Kabán, A. (2003). On an equivalence between PLSI and LDA. Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR ’03 (pp. 433–434). Presented at the SIGIR ’03, New York, NY, USA: ACM. doi:10.1145/860435.860537Gretarsson, B., O’Donovan, J., Bostandjiev, S., Hollerer, T., Asuncion, A., Newman, D., & Smyth, P. (2011). TopicNets: Visual Analysis of Large Text Corpora with Topic Modeling. ACM Transactions on Intelligent Systems and Technology (Vol. 5, pp. 1–26).Hall, D., Jurafsky, D., & Manning, C. D. (2008). Studying the history of ideas using topic models. Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08 (pp. 363–371). Stroudsburg, PA, USA: Association for Computational Linguistics. Retrieved from http://dl.acm.org/citation.cfm?id=1613715.1613763McCallum, A., Wang, X., & Corrada-Emmanuel, A. (2007). Topic and role discovery in social networks with experiments on enron and academic email. Journal of Artificial Intelligence Research, 30(1), 249–272. Retrieved from http://dl.acm.org/citation.cfm?id=1622637.1622644McCallum, A., Corrada-Emmanuel, A., & Wang, X. (2005). Topic and role discovery in social networks. Proceedings of the 19th international joint conference on Artificial intelligence, IJCAI’05 (pp. 786–791). San Francisco, CA, USA: Morgan Kaufmann Publishers Inc. Retrieved from http://dl.acm.org/citation.cfm?id=1642293.1642419Mei, Q., Cai, D., Zhang, D., & Zhai, C. (2008). Topic modeling with network regularization. Proceeding of the 17th international conference on World Wide Web, WWW ’08 (pp. 101–110). New York, NY, USA: ACM. doi:10.1145/1367497.1367512Mimno, D., & McCallum, A. (2007). Mining a digital library for influential authors. Proceedings of the 7th ACM/IEEE-CS joint conference on Digital libraries, JCDL ’07 (pp. 105–106). New York, NY, USA: ACM. doi:10.1145/1255175.1255196Nallapati, R. M., Ahmed, A., Xing, E. P., & Cohen, W. W. (2008). Joint latent topic models for text and citations. Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’08 (pp. 542–550). New York, NY, USA: ACM. doi:10.1145/1401890.1401957Rosen-Zvi, M., Griffiths, T., Steyvers, M., & Smyth, P. (2004). The author-topic model for authors and documents. Proceedings of the 20th conference on Uncertainty in artificial intelligence, UAI ’04 (pp. 487–494). Arlington, Virginia, United States: AUAI Press. Retrieved from http://dl.acm.org/citation.cfm?id=1036843.1036902Broniatowski, D. A., & Magee, C. L. (2011). Towards a Computational Analysis of Status and Leadership Styles on FDA Panels. In J. Salerno, S. J. Yang, D. Nau, & S.-K. Chai (Eds.), Social Computing, Behavioral-Cultural Modeling and Prediction (Vol. 6589, pp. 212–218). Berlin, Heidelberg: Springer Berlin Heidelberg. Retrieved from http://www.springerlink.com/content/w655v786lp583660/Steyvers, M., & Griffiths, T. (2006). Probabilistic topic models. In T. Landauer, D. McNamara, S. Dennis, & W. Kintsch (Eds.), Latent Semantic Analysis: A Road to Meaning (Vol. 427, pp. 424–440).Wang, E., Silva, J., Willett, R., & Carin, C. (2011). Dynamic relational topic model for social network analysis with noisy links. 2011 IEEE Statistical Signal Processing Workshop (SSP) (pp. 497–500). Presented at the 2011 IEEE Statistical Signal Processing Workshop (SSP), IEEE. doi:10.1109/SSP.2011.5967741Welcome to the scottbot irregular. My name’s Scott, and the US Government has for some reason seen fit to give me money to study Science. It’s ‘Science’ with a capital ‘S’ because I’m not studying individual aspects of the world using science, but rather studying Science in general as a social, historical, philosophical, and intellectual phenomenon. What’s worse, I’m attempting to do it scientifically. This blog is my attempt at giving the country its money’s worth. Also, I kinda would love feedback on my eventual dissertation. See? Everybody wins.scott b. weingartis pretty clueless about a lot of things. This is his attempt to be less so.I pledge to be a good scholarly citizen. This includes:Opening all data generated by me for the purpose of a publication at the time of publication.Opening all code generated by me for the purpose of a publication at the time of publication.Freely distributing all published material for which I have the right, and fighting to retain those rights in situations where that is not the case.Fighting for open access of all materials worked on as a co-author, participant in a grant, or consultant on a project.I pledge to support open access by:Only reviewing for journals which plan to release their publications openly.Donating to free open source software initiatives where I would otherwise have paid for proprietary software.Citing open publications if there is a choice between two otherwise equivalent sources.I pledge never to let work get in the way of play.I pledge to give people chocolate occasionally if I think they’re awesome._[This is somewhat out of date. Please stand by for new information!]Hello World!Student of History & Philosophy of Science and Information Science at Indiana University.You’ve managed to stumble across my little corner of the internet. I’m currently a student and researcher in the HPS and SLIS departments at IUB under two of the most interesting and capable professors I’ve had the fortune to meet: Colin Allen and Katy Börner. I studied history of science and computer engineering at UF, where I slaved researched for the infinitely patient Robert A. Hatch, who taught me more in four short years than I’d yet learned in aggregate over my entire life.Early InPhO Concept MapThese days, I split my time between classes, the Indiana Philosophy Ontology Project (InPhO) and the Cyberinfrastructure for Network Science Center (CNS). At InPhO I program and design visual, navigable representations of our dynamically generated taxonomy of ideas; analyze relational networks (influenced, disagreed with, etc.) from our Thinkers database; and map and compare philosophical ontologies. The CNS keeps me busy with all sorts of scientometric analyses, and I am also involved in the development of large scale network analysis software such as the NWB, creating workflows, providing software feedback, writing documentation and teaching workshops.Co-authorship network created using the Network Workbench ToolResearchHow do changes in communication structures and technologies affect scientific discourse and collaboration?Science is totally rad. So I study it.There are all sorts of ways to study science, of course, and you can’t leave out even one if you want to understand Science as a whole. That means taking a look at its philosophy, history, anthropology, culture and all sorts of other things as well (perhaps even sociology!). It also means looking at (gasp) the science itself, because no self-respecting scholar should claim to understand physics and physicists without being able to calculate the distance the bullet travels before it falls.My overarching research is in modeling and mapping the growth of science on a large scale – thematically, geographically and temporally – hoping eventually to reveal what conditions yield the most rapid rate of discovery and innovation. Looking back, we see times when scientific progress lurches forward at alarming rates, times when studies come to a halt, times when great minds exposit to deaf ears. Sometimes the reasons are obvious: burned libraries, overthrown empires, new sources of funding, technological breakthroughs, wars that need to be won. But these are heavy brush-strokes painted across the canvas of history.If we could somehow view the whole of scientific endeavors for the last thousand years, across every topic and in every city, with the same fine granularity used to research modern-day science, imagine how much we could learn. By zooming out and looking for “hot spots” of innovation in the history of science, and by understanding the environment in which these hot spots formed, we can learn how to induce those same ideal conditions in modern day research.If the synthesis of new ideas in physics tends to come from young researchers working on their own and with backgrounds in other fields, funds can be allotted to make sure more of those exist. If medical innovations come fastest when small groups of experts collaborate, or if science in general runs smoother in small-world type collaborative networks rather than completely connected networks, that information can be used to focus funding in just the right way to improve the rate of innovation.The closest we can come to that fine granularity, to understanding science across contexts, is by using as many research tools as we can find. We must be comfortable working in whatever discipline with whatever methodology is necessary to find the answers sought. Huge historical data sets will be a must. Scientometricians and others in related fields do an amazing job of learning the structure of modern science, but that structure is necessarily bound to the mediums it inhabits. Modern science is a beast of national laboratories, e-mails, universities, cited journals, click-throughs, conferences and page hits.Marshall McLuhan may or may not have been correct when he claimed “the medium is the message,” but there is no doubt that the medium plays a large role in how science is adopted, disseminated and studied. That role cannot be understood without stepping back and viewing all of the alternatives – correspondences, scientific societies, book transcriptions, etc.Dutch Republic of Letters created in collaboration with The Huygens InstituutThe task, then, is to collect as much data as possible, as far back as we can. We should track where books traveled within Medieval Europe and Asia; who corresponded with whom, how often, and about what during the Early Modern period; who taught whom and where scientists studied; how many books were published in what languages; what universities had copies of which journals; where shared resources traveled.This is an impossible amount of data, of course, and can only exist if created collaboratively and in the spirit of openness. These are not ideas to be copyrighted – they are numbers and data points, and they should be accessible and compatible and aggregated in one place. A History of Science Data Commons, so to speak. More on that project coming soon.Trying to understand all of it at once is a big task… and absolutely impossible.  I’ve sliced myself two pieces of the pie that are hopefully manageable and definitely inseparable:Periods of rapid scientific production and progress.Inflection points in scientific communication and collaboration.Changes in communication structures and technologies obviously affect scientific progress deeply, and it is exactly what those effects are that I hope to uncover. Scientific revolutions and media revolutions, what a tired subject! Well, perhaps, but there are two very good reasons they’re overstudied: they’re terribly important, and nobody’s got them right yet.InterestsCourtney and I contact jugglingThankfully for my friends and family I do not work 24/7. When not working, I can often be found juggling, attending renaissance festivals, geocaching, camping, campaigning for rationality, and reading science fiction & fantasy novels. When I feel guilty about not working, but not enough to actually get back to work, I read about physics, cognitive science and linguistics. I am also perpetually writing a history of the obscure art of contact juggling.Juggling has been a big part of my life for nearly a decade now; I was president of Objects in Motion (UF Juggling Club) for a few years and brought the club from 3 to 30 active members, taught lessons at Groovolution dance studio, and performed with Circle & Spice in Bloomington. I’m now involved in the IU Juggling Club and juggle irregularly at the Bloomington Farmer’s Market. I have performed as far north as Calgary, as far east as Amsterdam, as far west as Los Angeles, all the way south in Miami, and all sorts of places in between.None of that would have been possible without my good friends and co-performers in the Spherocity contact juggling troupe: Matt, Jay, Cory, Courtney, Steve, and Leighanna. Thanks to Nick, Nicole, Leah, Ian and the rest of the crew, Objects in Motion keeps growing larger and better and I miss them terribly. And if you’re reading this, Sierra, you should start juggling again.Juggling knives in CalgaryAs if there’s not enough on my plate already, I’m also involved in two wonderful pseudo-academic organizations. I co-founded Sophosessions with Warren C. Moore, the coolest cat I know, in my junior year at UF. The group still meets a little more than monthly and allows its two-dozen members to present talks on whatever they feel like, from Chinese calligraphy to Zen Buddhism to advanced fractal mathematics to building robots. Then everyone goes to Ben & Jerry’s. I still webcast into meetings whenever I can, but it’s just not the same without the ice-cream.The Venerable IU Beer & Algorithms Club fills two Monday nights a month, and I get to listen to a bunch of Computer Science and Math graduates present their favorite algorithms in gory detail, all while eating a tasty meal and enjoying an equally tasty beverage. What could be better?You can see further maps examining the relationship between population density and Wikipedia article density here.Such is the nature of the modern university that a sudden spark of inspiration can lead to a quick and radical dive into data that, once upon a time, would have taken supercomputers and manpower far beyond the reach of humanities scholars. When Jon Christensen proposed we explore the possibilities of mapping culture in urban areas, I immediately thought of Eric Fischer’s work mapping Twitter and Flickr users in an attempt to describe how we talk about certain things but photograph others. In a stroke of good timing, I’d recently seen Claudia Engel, academic technology specialist for Anthropology, demonstrate using the Drupal Feeds module to pull in geolocated articles from DBpedia, the database version of Wikipedia. I thought, why not map the places that had Wikipedia articles associated with them, to see what patterns emerged. The results of this excursion are presented below:The world of Wikipedia, with brighter areas having a higher density of geolocated articles.DBpedia is the ongoing attempt to transform Wikipedia into a semantically rich and queryable database of human knowledge. It stores much of the categorical information found in Wikipedia articles using RDF triples–simple links for every snippet of data, from the death date of a famous (and sometimes even real) person to the season number of every Simpsons episode, to the latitude and longitude of over half a million articles on a wide variety of subjects.The individual points represented by each article are aggregated using a common spatial analytical technique known as kernel density to show sparse and dense regions.Jon is the director of the Bill Lane Center for the American West, and he brought with him two undergraduate research assistants, Jenny Rempel & Judee Burr. I showed them how to perform simple spatial queries to get Wikipedia articles located in San Francisco and we discussed what this data may mean and the thorny issues that we may need to account for in its use.One of the problems of using Wikipedia as a proxy for anything is that it is subject to censorship by various states. Here we see that the Great Firewall of China, while impeding Wikipedia contributions, reduces but does not stop the creation of articles about Chinese subjects.Naturally, I made a fool out of myself trying to show off my limited SPARQL skills. SPARQL is the query language used to access DBpedia, and while I consider myself quite adept at MySQL (and, as a result of recent forays into PostGIS, somewhat skilled with Postgres), SPARQL still seems like a bit of a Lovecraftian nightmare to me. There are no set fields in a data store like this–how could there be when you’re storing biographies alongside episodes of Battlestar Gallactica and land wars in Asia and, well, other stuff–and even worse there are a variety of ontologies at play (or at war, I suppose it’s a matter of perspective).Among other issues, the geodata associated with articles is not in the same format, and so the maps presented here rely soley on the easier www.w3.org/2003/01/geo/wgs84_pos type geodata and need to be combined with any other significant sources of DBpedia geodata.The first significant problem we ran into was trying to find particular types of articles, which required that we better understand the various ontologies and how they may categorize locations or structures differently. Are you looking for places or features or structures? What’s the difference? The post-modern data problems depend on your point of view and which categorization of data you’re using. Beyond that, while DBpedia offers more the possibility to write more nuanced spatial queries, such as finding non-geolocated articles that are designated as having happened at geolocated articles, we decided to perform this first run with the most readily available data.While the United Kingdom is the most dense region from a global scale, on closer examination it would seem that there is a sort of Hadrian's Firewall as far as Wikipedia is concerned.Rather than build individual queries with bounding boxes for each city, I decided that the best thing to do was to grab every article that has geodata (with some exceptions, as noted above) and get a lay of the land. The result was some 492,000 points of data, which we transformed into a shapefile and then ran kernel density on using ArcGIS at various scales. Below are the top ten most populous cities in the United States, each with their local density of articles and, in some occasions, an exploration of issues we came upon.To put this in perspective, there are around 10 million Wikipedia articles in its various languages, so these maps are only 5% of Wikipedia and, upon more detailed examination, likely oversample along linguistic and community lines (a language map of Wikipedia would be almost as easy to produce using SPARQL to pull the language of the abstracts). Of course, those dealing with Twitter know that half a million data points is just five minutes of twitter or a month of a relatively popular hashtag, so big data, like the Peng Bird of Daoist lore, is really only big in comparison.The blue-to-red maps were created by me, while the orange-to-green maps represent the same data using a different color ramp and were created by Jenny Rempel. Please keep in mind that these are drafts.New York Metropolitan Area – 5677 ArticlesManhattan – 2200 ArticlesGreater Los Angeles – 1539 ArticlesChicago – 1933 ArticlesHouston – 414 ArticlesOne factor affecting the use of this technique is the high sampling of centroids, whether city, county, state or nation.Philadelphia -1669 ArticlesPhoenix -391 ArticlesSan Antonio -180 ArticlesSan Diego -502 ArticlesDallas -634 ArticlesSan Jose -544 ArticlesAnd as a special bonus, below you can find San Francisco and London. London, because it is, apparently, the navel of Wikipedia, and San Francisco, because that’s the benefit to nearby cities for having universities with strong digital humanities programs.London – 7000 ArticlesSan Francisco – 568 ArticlesComments OffNote from the Editors: These posts are part of an ongoing conversation about text-mining and statistical analysis of language. To further investigate the methods used, please follow the links provided by the authors.Identifying diction that characterizes an author or genre: why Dunning’s may not be the best method.By Ted Underwood“The basic question is just this: if I want to know what words or phrases characterize an author or genre, how do I find out? As Ben Schmidt has shown in an elegantly visual way,simple mathematical operations won’t work. If you compare ratios (dividing word frequencies in the genre A that interests you by the frequencies in a corpus B used as a point of comparison), you’ll get a list of very rare words. But if you compare the absolute magnitude of the difference between frequencies (subtracting B from A), you’ll get a list of very common words.” Read Full Post Here.Dunning AmokBy Ben SchmidtAnyhow, I think this is what we need the Dunnings for: extracting a list of words that are worth analyzing a bit more by hand. With each of these, we know there’s a real difference: we can then plot the degree of over-representation over time. I’m going to do this for the top 96 words. (Why 96? Why not?) So for instance, here’s the plot for “smile.” (Including “smiling,” “smiles,” etc.)Comments[Brian goes to a dark place after reading Evgeny Morozov's The Net Delusion: The Dark Side of Internet Freedom and Jaron Lanier's "Digital Maoism: The Hazards of the New Online Collectivism" for UNL's Digital Humanities Seminar.]Evgeny Morozov examines the Internet’s relation to authoritarian states, arguing there is a Western misconception, rooted in the Cold War, that the Internet, and information systems more broadly, create political change. This “cyber-utopianism,” as Morozov dubs it, assumes that the Internet inherently favors democracy and works against oppressive governments. Morozov warns that cyber-utopianists, combined with a misguided strategy he calls “Internet-centrism” creates the “Net Delusion,” a misunderstanding of the Internet that does not promote democracy and freedom, but rather threatens to harm global efforts on their behalf.Jaron Lanier’s essay arguing against collective thinking, though already largely outdated, hits on a similar misguided notion of Utopianism. Lanier says, “A core belief of the wiki world is that whatever problems exist in the wiki will be incrementally corrected as the process unfolds.” Like the Net Delusion, this Wiki Delusion (he calls proponents “wikitopians”) promotes the idea that technology, or perhaps more accurately the use of technology, can solve any problem.Like Morozov and Lanier, I find a similar Delusion, though one more academically minded, let’s call it the “DH Delusion.” The DH Delusion begins with a similar sort of cyber-utopianism. I remember the excitement of my first Digital History course in which it seemed not only possible, but probable that in a matter of years most scholarship would be produced in the digital medium. The Internet seemed to be promote the sort of intellectual freedom and scholastic democracy that could topple an oppressive and outdated structure of academia.However, just as Morozov points out that photocopies and radio programming did not cause the collapse of Communist Europe (broader structural issues did) and the Internet will not overturn oppressive governments worldwide, the Internet will not revolutionize scholarship without structural changes. Though the Internet has been influencing scholarship for quite a long time (see the Valley of the Shadow’s 20th reunion panel at the AHA) academic structures remain largely the same. Academic credit for digital scholarship, particularly in terms of the tenure process, seems to have remained a contentious issue even at some of the most DH-friendly of institutions. The infrastructure for publishing and peer reviewing digital scholarship still remains largely undeveloped.Likewise, Lanier’s argument against wikitopians raises potential pitfalls the DH community should avoid. As Lanier says, “The illusion that what we already have is close to good enough, or that it is alive and will fix itself, is the most dangerous illusion of all.” Just because there is a community that identifies as DH, does not mean DH will change academia. More people accepting DH as an identifier does not necessarily correlate to DH’s power to transform scholarly structures. In fact, if people content to maintain or replicate the scholastic infrastructure identify as a DHer, the power of the digital to remake academia lessens. I do not mean to bring up the debate of who’s in or who’s out, but rather hope to translate Lanier’s warning against the hive mentality to academia. Just because collective thinking holds potential, that does not mean it will live up to that potential. Just because DH holds potential, does not mean it will live up to that potential.About Brian Sarnacki I am a graduate student in history at the University of Nebraska-Lincoln interested in digital history and the digital humanities. I focus on late 19th century American social and urban history. All blog posts are CC BY-NC-SA. If you would like to use any work found on the blog for commercial opportunities or remixes please contact me at bsarnacki@gmail.comComments OffEditors’ Note:The conversation about theory and the digital humanities highlighted in the Theory Round-up last Friday has continued into this week. We’ve added the most recent posts to this list. Please Tweet @dhnow if you have more to suggest. *updated at 3:30pm EST*Trevor Owens, Please Write it Down: Design and Research in the Digital Humanities, November 11, 2011“What I see as the key issue to think through here is not so much should Digital Humanists also need to “re-encode” their work in writing. Reflective designers of all stripes are already doing a lot of writing. They are creating documentation, making wireframes, etc. The question here is what kinds of writing should humanities scholars who design software and make things in code be doing.” Read Full Post Here.Multiple Participants, Translation, Communication, Code Twitter Conversation, November 11, 2011“Are coders obliged to explain themselves? If not, why not? And if so, is it an epistemological obligation, an ethical one, or merely a practical one? Tom Scheinfeldt initiated the Twitter discussion, following Patrick Murray-John’s post ‘Theory, DH, and Noticing.’” Read Twitter Conversation Here.Patrick Murray-John, Theory, DH, and Noticing, November 10, 2011“I’m not sure I’d go so far as to say that to do theory in/on DH one needs to learn to code or design a database. But one does need some training to be able to start noticing the difference between two data models that at surface appear to describe the same things. And, coders should be ready to learn what useful things theorists can offer that, despite a first appearance of scope creep, might just be valuable things to consider building into the code” Read Full Post Here.Joe Grobenly, On Theory and Blowing Open the Doors , November 9, 2011The beauty of the humanities is that they allow for a reversal of the scientific method, which as Benjamin Schmidt points out, can lead to charges of “confirmation bias,” which is only really a concern if “objectivity” was what you were shooting for in the first place. In reality, humanists are in a luxury position of being able to create and analyze texts in the same motion, and work best to examine all of the human underpinnings of them. I think this creates a large problem for humanist librarians, espcially as we try and operate in the field now called “library science.” Read Full Post Here.Elijah Meeks, The Digital Humanities as Thunderdome, November 5, 2011“I love Gephi, that’s obvious, but it isn’t built for humanists because nothing is truly built for humanists, the closest we can get is something built by humanists. If you don’t know what a tool is doing and that your work is being extruded through it, then you’re in real trouble. I’m a bit concerned at how humanities scholars show a willingness to defer to tools, but I’m more concerned about how they can positively surrender to tool builders.” Read Full Post Here.Alexis Lothian, Mixed metaphors, marked bodies, and the question of “theory”, November 4, 2011“Part of the conversation about how we make theory has to be a conversation about which forms of theory-rich making are recognized and institutionally supported and which are not; about whether there are clear cut lines between digital humanities scholarship, digital media art, and digital media everyday practice, other than the question of where the funding comes from.” Read Full Post Here.Roger Whitson, THATCamp Theory Bunnies, November 4, 2011“I think this back and forth about theory as a weapon versus theory as a bunny is precisely the conversation that needs to be happening. Is it possible to create an environment where theory is embraced in a collaborative and creative manner? Can library staff and developers not traditionally “trained” in theory but who nevertheless engage in political and cultural critique in their projects everyday teach devotees of Derrida and Foucault a thing or two about differance or disciplinary regimes? Could THATCamp Theory turn theoretical weapons into cute, fuzzy, hugging bunnies?” Read Full Post Here.Natalia Cecire: American Nerds Go to THATCamp, November 3, 2011“But one concern continues to resurface in all of these posts, as well as in the Twitter conversation around my initial post, namely that theory, too, can be a site of power, one that has played all too well with the academic star system in the past, leaving people who now greatly benefit from DH (junior academics, people at teaching-oriented institutions, geographically peripheral institutions) in the cold.” Read Full Post Here.Jean Bauer: Who You Calling Untheoretical?, November 3, 2011“When we create these systems we bring our theoretical understandings to bear on our digital projects including (but not limited to) decisions about: controlled vocabulary (or the lack thereof), search algorithms, interface design, color palettes, and data structure.” Read Full Post Here.Ben Schmidt: Theory First, November 3, 2011“The promise and danger of the digital is that it lets us displace these texts, even though though by only a hair’s breadth, out of the systems of the past. Where we want to put it: that’s the question. Digital humanities would be a disaster if it simply rewrote our cultural heritage to fit neatly into present categories. That’s why we need theory, which is all about reconfiguring the way we look at the world in terms of difficult to see structures that mask the truth: systems and lifeworld, doxa and habitus. There’s a powerful significance there, and we need it.” Read Full Post Here.Amanda Phillips: #transformDH – A Call to Action Following ASA 2011, October 26, 2011“It’s starting to feel like we’re reaching a critical mass of people who are ready to see the “Digital Humanities” (used here in the most expansive sense possible) begin to diversify itself in terms of inclusion, approaches, theorization, and application to social justice issues.” Read Full Post Here.Ted Underwood: On Transitive and Intransitive Uses of the Word ‘theorize’, October 25, 2011“Because digital approaches make it possible to ask and answer different kinds of questions, there’s going to be a reciprocal interaction between humanistic goals and digital methods, not, as Cecire puts it, a “merely paratactic, additive concatenation.” We’re going to need to theorize about methods and goals at the same time. Together. Intransitively.” Read Full Post Here.Roger Whitson: Hacking THATCamp Theory, October 23, 2011“I feel that we shouldn’t use THATCamp to create yet another philosophical or theoretical meditation on technology; we get that enough from academic conferences and (especially) books. THATCamp Theory should turn the theoretical texts we know and love into alien sandboxes for technological and collaborative creativity.” Read Full Post Here.Natalia Cecire: When DH was in Vogue; or THATCamp Theory, October 19, 2011“And so far, despite the best of intentions, DH has not done a good job of theorizing either that disciplinary shift or its political implications—let alone “what is an author.” That’s why I think we should probably get over that aversion to “yack.” It doesn’t have to replace “hack”; the two are not antithetical.” Read Full Post Here.CommentsThe Center for Digital Scholarship recently completed a NEH Digital Humanities Start-Up Grant to create a set of experimental tools for analyzing TEI texts using the SEASR framework.SEASR lets users arrange and manipulate small computational “components” in series to allow data to be ingested, analyzed, transformed, and visualized. CDS produced about three dozen of these components to explore how collections of literary texts can be studied using these tools. More information, downloads, and samples are available at http://teicomponents.wordpress.com/.Download the TEI components for SEASR either from the Brown Digital Repository or GitHub.Comments OffEditors’ Note:Below are links to several pieces about the effects of social media on open access scholarship. Also included is a bibliography focused on the impact of open access scholarship.Trevor Owens, Finding Scholarship and Scholarship Finding Us“Now aside from the fact that more downloads = more people seeing your paper I think Melissa’s example is all the more important because of the kind of diffuse way those people came to find her work.” Read Full Post Here.Melissa Terras, What happens when you tweet an Open Access Paper“Prior to me blogging and tweeting about the paper, it got downloaded twice (not by me). The day I tweeted and blogged it, it immediately got 140 downloads. This was on a friday: on the saturday and sunday it got downloaded, but by fewer people – on monday it was retweeted and it got a further 140 or so downloads. I have no idea what happened on the 24th October – someone must have linked to it? Posted it on a blog? Then there were a further 80 downloads. Then the traditional long tail, then it all goes quiet.” Read Full Post Here.Melissa Terras, On thumb twiddling“I’m currently waiting on over ten papers to go “live” in our institutional repository, since I have uploaded them. I’ve been waiting on them to go live for a month. I have no idea how the process works. I submit papers: I wait. I get no email to indicate progress. Sometimes the person (and it is a person, they make a note on the record) deletes the file, with no reason given. I upload it again. It gets deleted. I send emails. They are ignored. I send more emails. They get replies from an email address that doesnt give the person’s name, just the “institutional repository”. I reply to those emails. They are ignored.” Read Full Post Here.Amanda French, Your Twitter followers and Facebook friends won’t read your peer-reviewed article if they have to pay for it, and neither will strangers“The question before today’s panel is “Can social media help broaden the audience for academic work?” I’m going to talk about a more specific version of this question, namely, “Can Twitter and Facebook help earn more readers for peer-reviewed articles?” The answer is “Yes, but those readers will not pay to read peer-reviewed articles.”” Read Full Post Here.Adam Crymble, Do People Want Open Access to Research? In an experiment earlier this week I decided to post my recent article about Social Media use by Archives and Archivists (see my previous entry) and made a quick announcement on Twitter. The response was far greater than I expected, with over 700 people in just a few days taking the time to visit and read the abstract or access the PDF and I received messages from several archivists thanking me for sharing the article. Read Full Post Here.The Open Citation Project, The effect of open access and downloads (‘hits’) on citation impact: a bibliography of studiesDespite significant growth in the number of research papers available through open access, principally through author self-archiving in institutional archives, it is estimated that only c. 20% of the number of papers published annually are open access. Recent studies have begun to show that open access increases impact. This chronological bibliography is intended to describe progress in reporting these studies; it also lists the Web tools available to measure impact. It is a focused bibliography, on the relationship between impact and access. View Bibliography Here.Commentson developing critical and methodological frameworks for the digital humanities, or the digital humanities is the humanities.To be an equal partner—rather than, again, just a servant—at the table, digital humanists will need to find ways to show that thinking critically about metadata, for instance, scales into thinking critically about the power, finance, and other governance protocols of the world. — Alan LiuFred Gibbs has a typically perceptive new post continuing his thinking about developing a more defined critical discourse for digital humanities as a field. Drawing his inspiration from Alan Liu’s question, “Where is Cultural Criticism in the Digital Humanities?”, Gibbs argues that digital humanists should aim for three main goals: 1) more effective critical discourse around DH work; 2) better rubrics for evaluating projects; and 3) a different kind of peer review.These are good starting points chock full of provocative possibilities. However, I think it’s worth returning to one of Liu’s key ideas: that digital humanities is not a “servant” to the humanities, it is the humanities.What I have been discovering in my own DH work on folk music and archival study is that the digital takes us back to core disciplinary questions (in my case these are long-running methodological and interpretive concerns in cultural history, folklore, pop music studies, cultural studies).Too often, DH gets framed as something new, as a breakthrough, as a reinvention of the wheel. Witness Patricia Cohen’s breathless “Humanities 2.0″ articles in the New York Times. It seems to me that this is because of an unwise conflation between Digital Humanities as an intellectual and scholarly endeavor and the narrative we use in contemporary society for innovations in the private technology sector.This conflation has everything to do with the contemporary moment, which finds academicians jockeying for money in an increasingly corporatized and neoliberal university setting. The danger here is that we are not thinking carefully about the framework in which Digital Humanities might thrive and contribute to society beyond assumptions about technology solving all problems and creating financial wealth. This, it seems to me, is where Digital Humanities needs to continue to develop greater critical self-reflection built upon well-tested humanistic models. In what larger systems of power are the digital humanities complicit? What is producing this moment in which digital humanities is making such an impact? How does the cultural context shape everything from the code we are creating to the findings we are producing to the jobs that are available in the field?To be clear, there’s nothing wrong with being critically self-reflective an intersection between private sector work and more public intellectual and scholarly concerns; the problem is a total conflation between the two. And there’s nothing wrong with being excited about the fresh, unprecedented, and surprising places that the digital takes us, so long as those are not placed in direct opposition to the rich past of humanities scholarship that we can draw upon (critically of course, since those traditions come with their own troubling problems and historical contexts).I am not trying to stop DH in its tracks. We can be critically self-reflective and move forward. But perhaps we can only do so if we also move backwards too, recovering and remembering all that the analog humanities has to offer.In sum, there’s a whole lot of new in the Digital Humanities, including what I think is already an extremely sophisticated intellectual move to cut through stale assumptions about old disciplinary boundaries, approaches to evidence, understandings of authorship, and more. The bits and bytes of the critical theory that Gibbs calls for is already happening, in my opinion, on numerous Twitter feeds, countless blogs, and at various conferences and un-conferences.But even as we find ourselves experiencing the new, it’s just as worthwhile to locate Digital Humanities in relation to the old. For there is a return, a circling back, to pursue if we so choose. DH takes us back—in deeply illuminating ways—to age-old issues in various fields across the arts and sciences. It is not a revolution away from the humanities, but a turn more fully into the humanities.It is in this sense that the digital humanities should reinvent the wheel.Links: This entry was posted on Friday, November 4th, 2011 at 1:30 pm and is filed under Academia, Digital Historiography, Digital History, Digital Humanities, Digital Humanities Critical Discourse, Digital Humanities Methodologies, Digitizing Folk Music History. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site.I have problems with the idea of infrastructure, particularly that of the e-research variety. It seems like we always end up talking about huge amounts of money and multi-institutional partnerships. It just doesn’t seem like a great model for innovation. As I’ve previously argued, I’d like to see something more like the funding schemes offered by the NEH Office for Digital Humanities. Encourage people with ideas, don’t just reward the good networkers. Build tools and apis, not portals and platforms.Of course I’d still like to see the digital humanities well represented in the list of Virtual Laboratories and eResearch Tools currently under consideration by NeCTAR. It’s time the digital research needs of the humanities were properly recognised. There are lots of possibilities, most of which we can’t yet envisage, but as I was asked what I would like to see as part of a Virtual Laboratory I had a go at setting down a few brief ideas. For what it’s worth, here’s my e-research infrastructure wishlist…Grappling with abundanceTraditional historical research is often based on a presumed scarcity of resources — the skill is in tracking down the sources. But large digital collections, like the Trove newspapers database, change this — you now have to make sense of the sheer volume of material. Digital history, through techniques such as text-mining and visualisation, offer a way of using these new riches effectively. We need to ensure that investments in digitisation are accompanied by evolutions in scholarly practice.Understanding what’s not onlineAt the same time, it must be recognised that large quantities of our cultural heritage are not available in digital form. For example, only about 10% of the holdings of the National Archives of Australia are described in their collection database, and only a small proportion of these are digitised. Easy online access could foster a certain circularity in historical research where only ‘known’ resources are consulted. We need to develop tools and visualisations that reveal the valleys as well as the mountaintops — identifying the holes in our research fabric.Critical engagementMore generally, we need to foster critical engagement with the tools and assumptions of digital research. Federated searching sounds great, but as scholars we need to expose the assumptions implicit in any such tool. What is being federated, from where, how is relevance being determined etc? Humanities e-research infrastructure should have built-in levels of reflexivity that enable scholars to understand the limits and assumptions of their digital research. Every algorithm contains an argument.Documenting changeThe resources we build are arguments with are subject to change. The Trove newspapers database, for example, is constantly adding new titles and articles, while users are improving the text transcriptions. Any analysis based on the holdings of this database needs to explicitly recognise this. At the very least the tools we have need to be able to generate time-stamped citations. It would be even better if we could capture a snapshot of the data to accompany our analyses. Perhaps there are possibilities for using something like the Memento project to ensure that the temporal context of humanities research is adequately documented.Show your working outScholarly publication in history, and the humanities generally, tends to present a finished product. But as we delve further into digital research the research processes themselves will be equally important both for fostering critical engagement with tools and methods and for enabling others to reproduce or extend the research. We need easy ways for researchers to expose their working out (subject to whatever access controls they think appropriate). It should be possible to save a series of steps – search, analysis, visualisation etc as modules for sharing and re-use.Follow your noseSearch needs to be complemented by rich, exploratory environments that encourage browsing, enable you to follow relationships, and foster serendipitous discovery. The problem with many collections is knowing enough about what’s in them to frame a useful search. Browsing, though a variety of interfaces — people, maps, events, record types, physical proximity — overcomes this problem. As more cultural institutions make use of Linked Open Data and shared identifiers — such as People Australia, Geonames or the Powerhouse Object Thesaurus — the possibilities for navigating this rich contextual space will increase.CitationWe need to develop better models for embedding rich citations within scholarly research — citations that describe not only the resource in structured, machine-readable forms, but also relevant relationships. This will link research directly to resources, making scholarly outputs a means of resource discovery, and enabling resource databases to re-use the scholarly research to enhance their own descriptions and finding aids.Constructing narrativesMoving beyond simple citation, we need better ways of exposing the structures of people, events, places and things that are referenced in our narratives. Linked Open Data provides a model, but we need tools to make it simple and examples to make it obvious.The future of digital humanities in museums as a contribution to Neal Stimler's (@nealstimler) presentation at Museum Computer Network, 2011.This is the rough text of a short talk I am scheduled to deliver at a symposium on 'Future Directions in Book History' at Cambrdige on the 24th of November 2011.I am on the programme as talking briefly about the ‘OldBailey Online and other resources’ (by which I assume is meant London Lives, Connected Histories, and Locating London’s Past, and the other websites I have helped to create over the last ten or twelve years). But I am afraid I have no interest whatsoever in discussing the Old Bailey or the other websites. The hard intellectual work that went in to their creation was done between 1999 and 2010, and for the most part they have found an audience and a user base and will have their own impact, without me having to discuss them any further. We know how to do this stuff, and anyone can read the technical literature, and I very much encourage you to do so.Instead, I want to talk about how the evolution of the forms of delivery and analysis of text inherent in the creation of the online, problematizes and historicises the notion of the book as an object, and as a technology; and in the process problematizes the discipline of history itself as we practise it in the digital present.The project of putting billions of words of keyword searchable stuff out there is now nearing completion. We are within sight of that moment when all printed text produced between 1455 and 1923 (when the Disney Corporation has determined that the needs of modern corporate capitalism trumped the Enlightenment ideal), will be available online for you to search and read. The vast majority of that text is currently configured to pretend to be made up of ‘books’ and other print artefacts, But, of course, it is not. At some level it is just text – the difference between one book and the next a single line of metadata. The hard leather covers that used to divide one group of words from another are gone; and every time you choose to sit comfortably in your office reading a screen, instead of going to a library or an archive, while kidding yourself that you are still reading a ‘book’, you are in fact participating in a charade. We are swimming in deracinated, Google-ised, Wikipedia-ised text.In other words, and let’s face it: the book as a technology for packaging and delivery, storing and finding text is now redundant. The underpinning mechanics that determined its shape and form are as antiquated as moveable type. And in the process of moving beyond the book, we have also abandoned the whole post-enlightenment infrastructure of libraries and card catalogues (or even OPACS), of concordances, and indexes and tables of contents. They are all built around the book, and the book is dead.If this all sounds rather doom laden and apocalyptic – and no doubt we could argue about the rosy future and romantic appeal of the hard copy book – it shouldn’t. At least as far as the ‘history of the book’ is concerned these developments have been entirely positiveFirst, it has allowed us to begin to escape the intellectual shackles that the book as a form of delivery, imposed upon us. If we can escape the self-delusion that we are reading ‘books’, the development of the infinite archive, and the creation of a new technology of distribution, actually allows us to move beyond the linear and episodic structures the book demands, to something different and more complex. It also allows us to more effectively view the book as an historical artefact and now redundant form of controlling technology. The 'book' is newly available for analysis.The absence of books makes their study more important, more innovative, and more interesting. It also makes their study much more relevant to the present – a present in which we are confronted by a new, but equally controlling and limiting technology for transmitting ideas. By mentally escaping the ‘book’ as a normal form and format, we can see it more clearly for what it was. And to this extent, the death of the book is a fantastic and liberating thing – the fascism of the format is beaten.At the same time, I think we are confronted by a profound intellectual challenge that addresses the very nature of the historical discipline. This transition from the ‘book’, to something new, fundamentally undercuts what we do more generally as ‘historians’. When you start to unpick the nature of the historical discipline, it is tied up with the technologies of the printed page and the book in ways that are powerful and determining. Our footnotes, our post-Rankean cross referencing and practises of textual analysis are embedded within the technology of the book, and its library.Equally, our technology of authority – all the visual and textual clues that separate a CUP monograph from the irresponsible musings of a know-nothing prose merchant – are slipping away. While our professional identity – the titles, positions and honorifics – built again on the supposedly secure foundations of book publishing – is ever less compelling. So the question then becomes, is history – particularly in its post-Rankean, professional and academic form - dead? Are we losing that beautiful disciplinary character that allows us to think beyond the surface, and makes possible complex analyses that transcend mere cleverness?And on the face of it, the answer is yes – the renewed role of the popular block buster, and an every growing and insecure emphasis on readership over scholarship, would suggest that it is. In Britain we shy away from the metrics that would demonstrate ‘impact’ primarily because we fear that we may not have any.Collectively we have put our heads in the sands, and our arses in the air, and seemingly invited the world to take a shot. A single and self-evident instance that evidences a deeper malaise is our current failure to bother citing what we read. We read online journal articles, but cite the hard copy edition; we do keywords searches, while pretending to undertake immersive reading. We search 'Google Books', and pretend we are not.But even more importantly, we ignore the critical impact of digitisation on our intellectual praxis. Only 48% of the significant words in the Burney collection ofeighteenth-century newspapers are correctly transcribed as a result of poor OCR. This makes the other 52% completely un-findable. And of course, from the perspective of the relationship between scholarship and sources, it is always the same 52%. My colleague Bill Turkel, describes this as the Las Vegas effect – all bright lights, and an invitation to instant scholarly riches, but with no indication of the odds, and no exit signs. We use the Burney collection regardless – not even bothering to apply the kind of critical approach that historians have built their professional authority upon. This is roulette dressed up as scholarship.In other words, we have abandoned the rigour of traditional scholarship. Provenance, edition, transcription, editorial practise, readership, authorship, reception – the things we query issues in relation to books, are left unexplored in relation to the online text we actually read.And as importantly, the way we promulgate our ‘history’ has not kept up either. I want television programmes with footnotes, and graphs with underlying spreadsheets and sliders. Yes, I want narrative and analysis, structure, point and purpose. I want to continue to be able to engage in the grand conversation that is history; but it cannot continue to be produced as a ragged and impotent ghost of a fifteenth century technology; and if we don’t do something about it, we might as well all go off and figure out how to write titillating tales of eighteenth-century sex scandals, because at least they sell.The book had a wonderful 1200 odd year history, which is certainly worth exploring. Its form self-evidently controlled and informed significant aspects of cultural and intellectual change in the West (and through the impositions of Empire, the rest of the world as well); but if, as historians, we are to avoid going the way of the book, we need to separate out what we think history is designed to achieve, and to create a scholarly technology that delivers it.In a rather intemperate attack on the work of Jane Jacobs, published in 1962, Louis Mumford observed that:‘… minds unduly fascinated by computers carefully confine themselves to asking only the kind of question that computers can answer and are completely negligent of the human contents or the human results.’I am afraid that in the last couple of decades, historians who are unduly fascinated by books, have restricted themselves to asking only the kind of questions books can answer. Fifty years is a long time in computer science. It is about time we found out if a critical and self-consciously scholarly engagement with computers might not now allow us to more effectively address the ‘human contents’ of the past.Teaching historical empathy through gaming is an important area in digital media and learning, but collaborations between university professors and game designers aren’t always easy. Nonetheless, UC San Diego Theater and Dance Professor Emily Roxworthy, who leads a National Endowment for the Humanities funded project about Japanese American internment camps in the American South during World War II that also used resources from the San Diego Supercomputing Center to bring the action to life, argues that the challenges are well worth the rewards.In the prototype level of Drama in the Delta that is currently available, “Jane’s Favor,” the user plays as Akiko, a young Japanese-American girl at the Jerome internment camp who is preparing to move to another camp as the war progresses. The player has thirty minutes to roam through the camp in search of items like a dance card, a piece of wood for whittling, a martial arts belt, and a potato sack.One of the obvious design issues Drama in the Delta needed to grapple with immediately was the fact that a game about imprisonment in which movement is highly constrained might not be very appealing to players who have grown to expect being able to explore very large and visually rich game worlds. The Virtual Guantamo installation in Second Life attracts visitors who may understand the pedagogical function of constrictions that obviously limit freedom, but Roxworthy’s target audience of K-12 students might not have the patience to deal with an interface that conveys experiences about boredom, alienation and provides the player a very limited repertoire of actions. As Roxworthy explained in an interview for this post, Drama in the Delta is about “inhibiting play as it is usually understood.”Games and Teaching EmpathyRoxworthy described her primary motivation as tackling the preconception that “gaming doesn’t promote empathy” and that emotional engagement is necessarily “momentary and fleeting” in digital environments. Although Roxworthy characterized herself as “not a gamer” other than “Super Mario Bros. back in the day,” she described working with software designers and game interns who encouraged her to think about opportunities to incorporate not only “live action role playing" -- her area of expertise -- but also challenging game play, difficult puzzles, and testing and exploration activities such as driving or jumping on a military truck.In describing collaboration with game designers, who frequently discouraged her from inserting too much text in the game, Roxworthy said “I’ve gotten a lot more than they have out of it. It’s taught me a lot about how to teach. I feel humbled.” She has also thought a lot about the constructive criticism of fellow academics like Mark Sample, who has pointed out potential problems with using museum display of objects as a model for incorporating primary sources in games.Roxworthy’s game design experience has shaped her next project as well, since she plans to write a trans-medial book about role-play. She is also interested in how commercial games can be appropriated for progressive political ends, as Stephen Duncombe has done.Games and Teaching HistoryThe question of why history matters is one that I grapple with in my own teaching, where first-year students are expected to make sense of the legacies of the past. There are obvious answers about the value of facilitating critical thinking, promoting causal and comparative reasoning, teaching research and data analysis skills, and improving chronological and geographical literacy among student populations. But I find myself more and more interested in the importance of what Robert Moeller calls “historical empathy” as a way to involve students with exploring other identities and becoming more engaged with the narratives of the past. Moeller does a variety of role-playing exercises with his students, as does fellow historian of German culture Edith Sheffer, who has written about “Creating Lives in the Classroom” with an identity exercise adapted from Moeller.Although Roxworthy noted that sometimes “online gaming is more real to them than face-to-face contact” because it offers “more openness,” she understands the many problems of depicting a racially diverse cast of characters without perpetuating stereotypes. Furthermore, as other interviewees at DML Central have noted, questions of race and identity representation are important to foreground, and Roxworthy disputes “talk about the Internet as being a 'post-racial space.'"Despite the fact that Roxworthy has devoted a lot of time to building virtual spaces, she also emphasized the importance of reconstructing and preserving the actual physical sites of the internment camps, because “people made those structures home.” She also argued that public history projects should be conceptualized as being not just for schoolchildren.Roxworthy wants both students and members of the general public to use the game as a way to understand how “Jim Crow and internment policies are rigid systems” and how “segregational systems could be programmed” much as software programs control how “movement is constrained by game engines.” However, because “each of the missions culminates in a performance,” students can also learn about “subverting systems” of segregation as they experience “sites of interracial code-switching, where they are transgressing laws in some ways.”Image credits: Drama in the Delta http://dramainthedelta.org/[A revised and improved version of this essay appears in the inaugural issues of Journal of the Digital Humanities.]This [original] post is a moderately revised version of a talk I gave as part of MITH’s Digital Dialogues series, titled “Criticism in the Digital Humanities.” The original audio and slides have been posted; this version has benefitted from the thoughtful questions and comments that followed my presentation. Many thanks to MITH for both warm hospitality and provocative discussion.My interest in the role and nature of criticism in the Digital Humanities grows out of a question that Alan Liu has asked in a few places this year: Where is the cultural criticism in the digital humanities? Although i’m not convinced that DH needs its own brand of cultural criticism beyond what its constituents would normally do as humanists, the question resonated with me because it made me wonder (with only silence to follow): where is the criticism in the digital humanities?My ideas center around 3 main points that i’ll very briefly sketch out here.1) DHers have not created an effective critical discourse around their work. By this i don’t mean that we haven’t gone far enough in publicly trashing each others’ goals and results. Nor do i mean simply that there hasn’t been enough peer review (though it’s true). Rather, i suggest that DH criticism needs to go beyond typical peer review and inhabit a genre of its own—a critical discourse—that itself provides a valuable service both inside and outside the community. More importantly, criticism and the intellectual work that it does makes the value of our work clearer to those outside of the DH community.2) To achieve more effective criticism, we need more rubrics for evaluating DH work. Although scholarly communication and peer review have been highly active topics in DH circles for several years, especially this last one, i haven’t seen truly useful evaluative criteria emerge from these discussions that appreciate the differences between digital and traditional work (not that there is a strict dichotomy). If some are around, they’re not nearly as pervasive as they need to be. Everyone in the field knows that the most innovative DH projects cannot be fully evaluated through the traditional, critical, and theoretical lenses of the humanities. But what lenses do we have? How do we know when to use them? How can we help others outside the field use them?3) DH work requires a different kind of peer review to produce effective criticism. Effective single-author reviews are almost an impossible expectation given the complexity of most DH work. A new peer review that’s based on a new kind of collaborative, self-mediated peer review will make for much more effective criticism. Not only do we need a more crystallized rubric, but new models of publishing now require a fundamentally new kind of peer review—and i don’t mean simply online peer review or open peer review, two efforts that i think have gotten the lion’s share of (much-needed) attention when it comes to reforming antiquated review processes. The multifaceted nature of DH requires a different kind of critique than is typical in the humanities because it puts rather unique demands on both critics and criticism itself.We all know that disciplinary boundaries are notoriously difficult to define. Yet they do somehow exist beyond professional titles and departmental affiliations. This boundary problem also gives rise to the question of whether there is any real difference between the humanities and the digital humanities—an interminable debate that need not detain us now. It will suffice for present purposes to say that digital humanities is different enough from the analog humanities, at least at the moment. But allow me a brief moment of justification that will be important later on. The digital humanities are of course not fundamentally different in any larger epistemological or hermeneutic sense from the humanities at large. Both are fueled by humanistic inquiry about the human condition and good stuff like that. But there are lots of different methodologies, many objects of study, and many ways of writing about them. Why should the digital be any different?Part of what defines a discipline is its rhetoric and the aesthetics of its scholarly discourse. Philosophy texts sound different from history texts, which sound different from literary analysis. These differences become especially apparent during collaborative projects. As much as we champion cross-disciplinary work, there is an inherent unease to it, in no small part because it becomes harder to tell how to evaluate it. Given a particular piece of scholarship: How should one read it? Which criteria should be applied? Of course these lines in the sand are easily blurred and effectively dissolve if one looks too closely. But in the larger view, they’re there.If rhetoric and aesthetics can help characterize and delineate different kinds scholarly work, it’s manifest to no small degree by a critical discourse shaped by community consensus, convention, and by defining practical and theoretical ideals. One major way in which DH is in fact separate from the humanities (again, at least for now) is in that it requires new ways of evaluating very complex work in terms that are often unfamiliar to most humanists.One major way in which DH is in fact separate from the humanities (again, at least for now) is in that it requires new ways of evaluating very complex work in terms that are often unfamiliar to most humanists.One of my favorite illustrations of this is William Thomas’s article, “Writing a Digital History Journal Article from Scratch.” The article is from 2007, but describes events that seem ancient now, circa 2003. How did analog historians critique history scholarship in the form of a website? Despite the project’s many virtues, reviewers could only wonder what it did better than the standard practice, and whether “the rewards [of the electronic article] were simply not commensurate with the effort and confusion involved.” Well, it was a long time ago, you say. Agreed. But i’m not sure that a similar exercise today would yield significantly different criticism from a non-DH audience. There is indeed more sensitivity to digital work, but the work itself has gotten considerably more complex as well.This is not to criticize the average humanist for not knowing the value of normalized datasets, relational databases, or valid XML. This is to say that those who do know their value haven’t been particularly clear about why these things are useful in the context that they are employed. What we might perceive as ignorance on the part of reviewers is at least in part because the rhetoric and aesthetics of DH work is not particularly well established. In other words, the critical sphere has not yet materialized.Why might this be? One reason for difficulty in fostering a critical discourse might center around the nature of the DH community—a rallying point for many, if not most, self-proclaimed digital humanists. As a community, we’ve been encouraging and supportive, tending to include and welcome everyone with open arms. The Big Tent theme from the 2011 Digital Humanities Conference suggest it’s ongoing. Such an approach has been essential and ultimately very successful in terms of broadening the scope and influence of the field. This should, and hopefully will, continue.However, such strong community solidarity and support may inadvertently curtail public criticism. This is not to say that we should become rude, exclusionary, and inwardly hostile. But we can’t be unhappy that tradition-bound hiring committees, promotion and tenure committees, deans, and other humanists don’t appreciate the value of our work when we haven’t really outlined how it’s different and how it should be appreciated. In other words, we haven’t provided a public critical discourse that offers the traditional signals to those who are not expert as to what work is good and what is not—and thus serves as a compass for practitioners, critics, and outsiders alike.We haven’t provided a public critical discourse that offers the traditional signals to those who are not expert as to what work is good and what is not—and thus serves as a compass for practitioners, critics, and outsiders alike. In sum, DH needs more critical theory that grows out of its own work and also from farther afield, drawing on critical methods from those working in new media and history of technology, as well as platform, hardware, and software studies.Post-talk chatter via Twitter prompts me to clarify two important points: To argue for a critical discourse is not to suggest that DH projects are inherently flawed and must offer more tempered epistemological claims and greater transparency—and that they should be criticized when they don’t. Good criticism will, of course, address these issues, but that’s not really the point here. Criticism serves a much larger role beyond pointing out flaws, as i try to argue in the following section. I should also emphasize that to suggest that there is an insufficient critical discourse surrounding DH work does not inherently suggest that DHers are uncritical idiots. We’ve all criticized projects, approaches, results, and what-have-you behind closed doors. We always want learn from and improve upon past work; we all think carefully about how to do our best work and sound scholarship. But the most useful critical discourse is a public one. Exactly what constitutes the sound scholarship that we want to do (and actually do) is not nearly as apparent to others, especially those outside the DH community, as it should be. It befalls the producers of that good scholarship to explain what is and what is not considered good, and why.What is the function of the critical discourse? Certainly not for simply lambasting each other’s work. Criticism is fundamentally about interpretation. It outlines utility and value, blemishes and flaws; it identifies sources, commonalities, and missed opportunities. It points out true innovation when it’s perhaps not obvious that paint slopped onto a canvas is actually worth thinking about. It points out when success claims point to little more than—to adapt a phrase from Michael Joyce—technological frosting on a stale humanities cake.This is just one instance where a critical discourse for DH would be far more valuable than grant applications that sell potential work and post-facto white papers that champion whatever work happened to get completed. Have we not all not seen intriguing, if not jaw-dropping, visualizations that made virtually no sense? Of course the real thrill of these is to recognize the beauty in that some obscene amount of data could be viewed in a small space, possibly interactively. Anyone who’s even thought about trying it knows how difficult it is. But what does everyone else think? We need to discuss, for example, the value of being able to automate the creation of such visuals apart from the communication that happens as a result of their design. Is this a methodological triumph, or an interpretive one? How can an explanation of the creation of such visuals ease fears of black-box manipulation? This is just one instance where a critical discourse for DH would be far more valuable than grant applications that sell potential work and post-facto white papers that champion whatever work happened to get completed. We need more than traditional journal articles that describe the so-called “real” humanities research that came out of digital projects.Perhaps most importantly, as I’ve already suggested, criticism serves a crucial signaling function. Matthew Arnold in his The Function of Criticism at the Present Time defined it as “a disinterested endeavor to learn and propagate the best that is known as thought in the world.” (1864/5, 75) I think that this pretty well describes what we need to do. The staggering rate of DH project abandonment has caused some alarm of late. We’re painfully aware that most academics aren’t that good at marketing beyond their disciplinary peers. Criticism selects and propagates projects that deserve merit and to serve as models. To continue with the previous example, we need criticism that praises technological achievement of visualizing, while condemning poor design practices; we need criticism that lauds the interpretive potential while critiquing the extent to which anyone can use the methodology. Again, the point of such criticism isn’t to ridicule or minimize difficult work, but to advance the field.Of course criticism has to be good and original, not dogmatic. Irving Howe, the influential cultural critic from the mid-20th century, remarked that “power of insight counts far more than allegiance to a critical theory or position…No method can give the critic what he needs most: knowledge, disinterestedness, love, insight, style.” It’s not easy to have these! But when someone does, and can write clearly, the resulting criticism performs extremely valuable scholarly work—work that goes far beyond the original project, but makes it even more useful at the same time. Such criticism is especially good at establishing and debating terms of how to analyze a particular work. This discourse of critique where new standards get hammered out. It’s the connective tissue of projects that pronouncements from on-high simply cannot have.so what do we look for? This last year in particular has seen much energetic rethinking of scholarly publishing. Part of this discussion recognizes that what we’re publishing is different. There is the MLA site that outlines types of digital work; guidelines for evaluating digital work. To their credit, the MLA has been one of the most visible scholarly societies in facilitating these kinds of discussion. BUT, here and elsewhere, the focus has remained on getting non-print work recognized and promoting the value of process over results.These were important arguments to make (and to continue in some cases), but we have to go beyond that now as well. Even if digital work is more acceptable, we haven’t really created sufficient guidelines for evaluating digital work (broadly defined) on its own terms. As far as the MLA goes, the guidelines for evaluating digital work are not all that different from evaluating analog work. On one hand, that is exactly their point! On the other hand, it’s perhaps a bit counter-productive because it doesn’t consider what’s unique about digital work. More helpful, i think, are NINES guidelines for peer review. But they are at once too general to enable rigorous criticism, and too specific to NINES projects. They are, however, an excellent starting point.I’d like to outline a few very general criteria that might be broadly applicable to digital work, as disparate as it can be. Certainly this list is not comprehensive, but rather a starting point. I won’t dwell on them here; i don’t pretend to have all the answers. I only hope that this list can serve as a small step in furthering the discussion.Transparency“I used a certain proprietary tool to get this complicated visualization that took a gazillion hours to encode everything in my own personal schema–I won’t bore you with the details–but here’s what we learn…”Can we really understand what’s going on? If not, it’s not good scholarship. “I used a certain proprietary tool to get this complicated visualization that took a gazillion hours to encode everything in my own personal schema–I won’t bore you with the details–but here’s what I learned from the diagram…” This cannot be considered good scholarship, no matter what the conclusions are. It’s like not having footnotes. Even though we don’t check footnotes, generally, we like to think that we can. So it’s natural to expect resistance when the footnote resembles a black box. DHers have gained some traction in encouraging others to value process over product. Transparency helps us to evaluate whether a process is really innovative or helpful, or if it’s just frosting.ReusabilityDiscussion about what must be and what cannot be reusable will get worked out in a vibrant critical discourse about both concrete work and in abstract theoretical terms.Can people take away what you’ve done and apply it to existing or future projects? This embodies so much of what is central to the ethos of the community. We’re always looking for better ways of doing things. This applies to methodology, code, and data; it applies also to both process and product. It creates interesting gray area for generalized tools. Are these to be shared? For now, i would say: absolutely! It’s part of the effort, as Matthew Arnold put it, to propagate the best. Obviously, not everything is reusable, but discussion about what must be and what cannot be are important theoretical positions that will get worked out in a vibrant critical discourse about both concrete work and in abstract theoretical terms.DataIt’s not good enough to point people towards a raw data source only to say: “well, I cleaned this up, standardized it, reformatted it…but I’m going to keep that work invisible and hoard it.” It’s like footnotes without page numbers.Needless to say, most if not all DH projects rely on data. It must be available! Not just for retesting, but for use in other places. Exactly how data should look is far from obvious. If nothing else, discussing a project’s use of data will encourages conversations about ownership, copyright, the limits of what can be shared, and so on. Those issues are becoming more relevant than ever as we create new research corpora that bridge historically separate datasets. I think that a critical discourse about our projects is a fruitful venue for this, and perhaps more effective on the ground than more abstract, theoretical proclamations (like this essay…). It’s unacceptable to just simply keep our data hidden to avoid the difficult issues. In reality, it’s necessary sometimes. But it’s not good enough to point people towards a raw data source only to say: “well, I cleaned this up and standardized it, and reformatted it…but I’m going to keep this work invisible and hoard it.” It’s like footnotes without page numbers.DesignDesign is not only graphic in nature: it must also apply to the decisions behind database design, encoding, markup, code, etc.There is great value, I think, in representing humanities scholarship in non-traditional forms. We’re seeing more scholarship online, and new kinds of “publications”. However, academic convention dictates that—it least in terms of scholarly content—we privilege content over form. In reality there is a palpable tension between the separation of content and form. On one hand, especially in the context of new media, web design separates these; on the other hand, as McLuhan pointed out long ago: the medium is the massage. Even more broadly, by ‘design,’ i really mean organizing principles. Why is a particular design strategy the best one or not? DH projects might be more explicit about such choices, but certainly our critique of such work must address these issues as well. Design is not only graphic in nature: it must also apply to the decisions behind database design, encoding, markup, code, etc.Again, my point isn’t just that DH projects should embrace these values. Obviously, many already do. My point is that they need to get critiqued explicitly and publicly. But how do we really DO it?As everyone is well aware, the nature of publishing has changed; we now do many digital projects that are never really done or officially published (with an imprimatur of review and vetting and so on). This means that the typical review process has been turned on its head. Getting a grant is too often an end in itself, taken to justify even the completed work. But this signing-off by the scholarly community happens before any work gets done. While traditional scholarship (books and articles) is held accountable to its stated goals and methodologies (as far as the medium permits), digital projects have not had that accountability from the scholarly community. This is a grave disservice in two ways: Projects learn less from each other, and projects remain isolated from relevant scholarly discourse.It may sound as if i’m simply advocating for more peer review, and conversations about scholarly communication have often made similar suggestions. For example, in late spring of this year, Jennifer Howard wrote a very nice piece for the Chronicle titled “No Reviews of Digital Scholarship = No Respect.” The gist of the article is evident from the title. She ends by saying that scholarly societies and editors of traditional journals need to step up and encourage this work. I think that this has been a popular viewpoint.Obviously, i agree with Howard’s point that peer review is necessary for legitimization. However, while change on the part of societies and journals would be nice, i think that is entirely incumbent on the practitioners to set the terms. Let’s not wait for a few gatekeepers to dictate terms to us. More importantly, i’m not sure that getting a formal review and thus the imprimatur of serious scholarship is enough. Let’s be honest: we do need more peer review! But that’s not all. We need a fundamentally different kind of peer review. Just as the nature of publishing is changing, the nature of peer review must evolve, especially for large DH projects, but even individual ones as well. Digital humanities work requires a different kind of criticism than most academic criticism because of the very nature of the work. DH projects often serve much broader audiences, and embody interdisciplinary in a way that eludes traditional models of critique.As a way of fostering useful criticism, peer review needs be more collaborative than before. I mentioned earlier the unease of situating interdisciplinary work in professional pigeon holes. It makes for difficult reviews as well, which need to be collaborative in two ways:More people to review individual projects. How many people can really critique various facets of a digital humanities project, when they range from graphic design, interface design, code, encoding standards, etc. Even if one could, it’s a herculean task not befitting the typical lone reviewerCollaborate to organize these reviews. Given the nature of complex publishing models of DH projects, why not move away from editor-mediated peer review, which minimizes the public effect of the critique? What is the code like? What is the data like? What conversations is it trying to join? What work does it enable at either methodological or interpretive levels?Furthermore, it is incumbent upon the projects to build time for these critiques into grants and get this feedback during and after the project. They should get not just empty, laudatory critiques but ones that try to shape the project in productive, if challenging, ways. They should be published as part of the project, as it helps to foster a vibrant critical discourse is crucial. DH work is often iterative in nature, and the review process needs to be as well. Just as digital humanities projects are inherently more public than the typical humanities project, everyone benefits when their critiques are more public.A project without accountability, without connectedness, without critique, simply fills another plot in the DH project graveyard.Funders need to broaden expectations of sustainability beyond access and infrastructure, to include also how a project situates itself within the larger scholarly discourse. In other words, projects need a social contract to the broader scholarly community, not just funder. Funders must prioritize and encourage public critiques as a way of establishing scholarly value, rather than through grant selection alone. A project without accountability, without connectedness, without critique, simply fills another plot in the DH project graveyard.Not only do we need to do this ourselves, but it needs to be part of the DH curriculum. It seems like there’s a new slate of DH classes each year, if not each semester. These courses need to explicitly teach critical methods for the unique issues in confronting DH work. Both theory and practice is essential here. We must have more than gossipy complaints that don’t go beyond the classroom walls, or vapid reviews that fill the backs of most printed journals. Good criticism is very difficult. Students need practice pointing out what’s good and lacking in a project in a way that benefits both the project and the average humanist who needs to understand (and not just evaluate) it.The criteria i mentioned (transparency, reusability, data, design) operate in a larger theoretical framework as well. We can benefit, i think, from considering an adaptation of a well-known diagram of criticism from M. H. Abrams. With DH work at the center, four proximal spheres of criticism might guide our approach. The formalist critique examines the form of the work, examining how well its structure, form, and design serve its purpose in the context of similar works; didactic criticism focuses on the ability for the work to reach, inform, and educate an audience; mimetic criticism might evaluate how well the DH work is truly humanist work or facilitates it (this replaces “universe” in the original diagram); expressive critique discuss how the work reflects the unique characteristics and style of the creator(s). Here the “team” anchor replaces the original “artist” label, but only reluctantly, since DH projects require, as I have argued, a more complex kind of criticism than the typical scholarly work, perhaps one more akin to literary criticism. Of course these spheres are not entirely separate. Throughout each of them, for instance, we must remember that code and metadata, as well as data and whatever structures govern it, are not entirely objective entities but are informed, attacked, and defended by ideology and theory. Perhaps not to the same extent as a work of art, but they matter and they need to be discussed. These spheres of criticism are of course applicable to any humanities research; they are especially crucial to digital work when so much of it is misunderstood.In the end, I hope I’ve suggested how it would be useful for our projects to have their own kind of critical discourse that will do the essential work of outlining what’s good and why. My remarks and exhortation to criticism should absolutely not be taken as any kind of attack on the worth or value of the Digital Humanities or its practitioners. Obviously, i am offering criticism of existing practices (including my own), but only because I want the DH community to enjoy even greater, more efficient success and broader acceptance within the humanities at large. A more sophisticated critical discourse seems like one productive, if not essential, avenue towards those goals.Comments OffEditors’ Note:In the past month scholars have been writing more extensively about the intersections between digital humanities, hacking, and theory. Below are several pieces exploring the place of theory in digital humanities work, each with comments and links to earlier discussions. This conversation also has led to the creation of @THATCampTheory, being planned for 2012.Natalia Cecire: American Nerds Go to THATCamp, November 3, 2011“But one concern continues to resurface in all of these posts, as well as in the Twitter conversation around my initial post, namely that theory, too, can be a site of power, one that has played all too well with the academic star system in the past, leaving people who now greatly benefit from DH (junior academics, people at teaching-oriented institutions, geographically peripheral institutions) in the cold.” Read Full Post Here.Jean Bauer: Who You Calling Untheoretical?, November 3, 2011“When we create these systems we bring our theoretical understandings to bear on our digital projects including (but not limited to) decisions about: controlled vocabulary (or the lack thereof), search algorithms, interface design, color palettes, and data structure.” Read Full Post Here.Ben Schmidt: Theory First, November 3, 2011“The promise and danger of the digital is that it lets us displace these texts, even though though by only a hair’s breadth, out of the systems of the past. Where we want to put it: that’s the question. Digital humanities would be a disaster if it simply rewrote our cultural heritage to fit neatly into present categories. That’s why we need theory, which is all about reconfiguring the way we look at the world in terms of difficult to see structures that mask the truth: systems and lifeworld, doxa and habitus. There’s a powerful significance there, and we need it.” Read Full Post Here.Amanda Phillips: #transformDH – A Call to Action Following ASA 2011, October 26, 2011“It’s starting to feel like we’re reaching a critical mass of people who are ready to see the “Digital Humanities” (used here in the most expansive sense possible) begin to diversify itself in terms of inclusion, approaches, theorization, and application to social justice issues.” Read Full Post Here.Ted Underwood: On Transitive and Intransitive Uses of the Word ‘theorize’, October 25, 2011“Because digital approaches make it possible to ask and answer different kinds of questions, there’s going to be a reciprocal interaction between humanistic goals and digital methods, not, as Cecire puts it, a “merely paratactic, additive concatenation.” We’re going to need to theorize about methods and goals at the same time. Together. Intransitively.” Read Full Post Here.Roger Whitson: Hacking THATCamp Theory, October 23, 2011“I feel that we shouldn’t use THATCamp to create yet another philosophical or theoretical meditation on technology; we get that enough from academic conferences and (especially) books. THATCamp Theory should turn the theoretical texts we know and love into alien sandboxes for technological and collaborative creativity.” Read Full Post Here.Natalia Cecire: When DH was in Vogue; or THATCamp Theory, October 19, 2011“And so far, despite the best of intentions, DH has not done a good job of theorizing either that disciplinary shift or its political implications—let alone “what is an author.” That’s why I think we should probably get over that aversion to “yack.” It doesn’t have to replace “hack”; the two are not antithetical.” Read Full Post Here.CommentsFor me, the lines between digital humanities, libraries, and scholarly communication are so faint as to be insignificant. And my perception of the equivalences among these entities that often seem siloed to my colleagues presents a real challenge as I try to help people–both at my own institution and at other campuses–think about possible futures for higher education in our digital culture.The source of my perception lies in my having begun to learn about how digital innovations are changing libraries and publishing as a result of my first forays into digital humanities. In 2004, I participated in a series of workshops at Wheaton College that were sponsored by the National Institute for Technology in Liberal Education and funded by the Andrew W. Mellon Foundation. Those workshops focused on two sets of encoding standards that use extensible markup language (XML): the Encoded Archival Description Document Type Definition (EAD DTD) and the Text Encoding Initiative (TEI). The hands-on workshop sessions focused on TEI, and I attended the workshops out of interest in testing the use of TEI in teaching my undergraduate history students. But the EAD component of the initial workshops meant that librarians attended too, so perhaps I have found one source of my elision of digital humanities, libraries, and scholarly communication.Perhaps I have identified also a significant point about how these three often siloed entities are in fact connected. I don’t mean to claim originality here. Folks involved in digital humanities have been working on these questions for quite some time, as is clear from the discussion of the development of EAD at the Library of Congress website. EAD and TEI were both developed in the 1990s. Both began using Standardized General Markup Language (SGML), and both shifted to use of XML. And both are used by libraries.In fact according to the TEI website cited above, “Since 1994, the TEI Guidelines have been widely used by libraries, museums, publishers, and individual scholars to present texts for online research, teaching, and preservation.” A search of the TEI consortium’s website led me to slides from a talk by Susan Hockey of University College London, “Markup, TEI, Digital Libraries.” The talk was presented at the TEI Members Meeting in 2002, and it offers a good overview of issues about the relationships between changes digital innovations were bringing to libraries and digital scholarship at that time. The TEI has a Libraries special interest group (SIG), and they recently released an update to their recommendations for best practices for use of TEI by libraries.So TEI–the flavor of digital humanities that I practice–does have clear connections to libraries that can be traced back for at least two decades. I’m not making that up. What a relief!Scholarly communication, the third of my equivalences, belongs in the set as a result of the ways that digital innovations have affected communication in general, that is in the ongoing shift from print to digital formats. The most obvious example–the one that has received the most public outcry in the past couple of years–is the case of newspapers. Like many people, I no longer subscribe to print newspapers; I read them online. And I resented the introduction of a pay wall by my newspaper of choice, the New York Times, as the publisher sought a new way to make the newspaper profitable as a business. But eventually I gave in, and I pay my fifteen dollars every month.Like newspaper publishers, university presses have been changing their production practices for at least the past twenty years, as various word processing programs have become the tools of choice for scholars writing articles and books. I began to hear about changes in scholarly publication when I attended a NITLE meeting on scholarly communication that was held at Pomona College in January 2008. (I think that’s the right date.) Like all NITLE meetings, this one gave me plenty to think about, especially the idea of open peer review. And in the intervening years, I’ve had opportunities to sit in on discussions in which I’ve heard editors talk about workflows and publishing software. Now, I have an essay in a volume that is undergoing open peer review and that is under contract (the volume, not necessarily my essay) with the Digital Culture series at the University of Michigan Press.All of this seems perfectly transparent and logical to me, and I understand digital scholarship–which is the term I use to encompass my three equivalences–to be the future of scholarship and higher education. My greatest challenge lies in parsing out how that is the case for folks who haven’t had the advantages I have had over the past seven years as I’ve learned from my digital humanities colleagues.Like this:One blogger likes this.Rating: 4.5/5 (2 votes cast)Image via WikipediaA list of principles for designing on-line communities created by Dr. Sorin A. Matei and his students registered in various versions of the Online Interaction Graduate Seminar at Purdue UniversityOnline / virtual community design principlesDesign guidelines are not guarantees. Many are necessary for online community development but they don’t guarantee that communities will form. Good design principles are necessary but not sufficient. The following list provides some key principles to consider and implement when designing and online community:Policies and codes of conduct are necessary and should be shaped around the nature of the content and the community. E.g. A learning or educational community requires copyright policies. Write policy statements carefully – write for readability and use language, tone, and genre that stimulates the community feel that you desire to create.’Community Self-Rule: A system should be established to monitor and sanction members’ behavior carried out by the community members themselves rather than by an external authority. Let users resolve conflicts themselves whenever possible. Current examples of this include “terms of use” pages or agreements, and the option to have other users report violations of terms of use upon discovering them.Recognize that technology doesn’t make the community but use it intentionally to create a framework that supports, reinforces, and helps shape the community toward the intended goal. How you implement technology can either provide for, refine, or undermine your community goals. (To incorporate the note below — technology doesn’t make the community, but it must be dependable and functional so as not to disrupt the community’s flow.)Select appropriate barriers/boundaries. Barriers/boundaries are important and should be chosen intentionally. The level of barrier required depends on the nature of the community. One must ensure that barriers serve to frame the community members appropriately and not exclude unintentionally.Encourage your hosts and leaders to model the behavior you wish exemplified in the community. (Corollary: Unless you wish to have users dominate each other discourage over-control by leaders, moderators, and hosts).Front-load the community with members who are interesting, interested and engaging.Encourage interested community members to take leadership roles when appropriate. Outline a path by which they may do so.Hosting is about good hospitality – encouraging valuable, friendly, and open connections, handling disputes, answering questions, and helping find solutions to their needs within the community’s context. Remember that respect is essential to leadership.Owning One’s Space: Members must have the ability to change and modify their own environment. This includes bringing real-world behaviors online, such as allowing people to create private spaces. Users should still be able to connect to other peoples’ private spaces when invited.Reward–have a system of virtual rewards set up where users can praise each others’ contributions.Economic Exchange may be used to make the virtual world resemble the physical world. Consider allowing individuals to exchange goods and services (even virtual goods and services).Online communities require some persistence, institutional memory, and continuity in order to develop a strong identity.Cultivate a sense of accountability and continuity. Accountability and continuity include encouragement of the use of real names or persistent screen names whenever possible. Furthermore, continuity allows for the development of community norms. Encourage this through the use of groups.Diversity allows for or facilitates continuity — make the community interesting. Value the various ideas, personalities, and points of view. However, core groups (like the Dead Heads in the WELL) provide a solid base for community growth in the early phases.Provide room to grow. The need for balance and continuity must be balanced with a need to allow flexibility and dynamic development and interaction. Communities are organisms. They require room to grow.The community should facilitate flow. Those who visit the site should enjoy their stay and want to spend time in the site.Dissent is good as it illustrates that people care enough about the community to fight about it. Don’t be overly zealous in squashing disagreements between users.Translate offline community principles to online communities carefully. Thinking about good community offline can help formulate principles for online community so long as you translate the principles in regards to the advantages and limitations of the medium. Human beings are human beings regardless of their location.If at all possible, provide the community members with opportunities for offline interaction in addition to the virtual interaction.Purpose – it is vitally important to establish a correct understanding of the purpose. It must have a clearly defined scope so that the community design will not suffer scope creep. The purpose must be communicated clearly to the community audience, preferably on a welcome page.Legal disclaimers/policies – are necessary for some types of communities, such as copyright rules, privacy policy, and so on. Be sure these are easily accessible from the welcome page.Codes of conduct – a correct balance of “just enough” and “not too much” is required to foster cooperation and trust. They should specify acceptable behaviors and appropriate consequences for violating them. Accessibility to these should be very obvious throughout the community (e.g. a button on the upper left or right side). These should evolve when necessary along with the group and a mechanism for feedback provided.Active – communities must remain active to survive. New information must be generated and new conversations started. If the community falters, moderators must be ready to step in quickly to propose topics and post new items before participants begin to leave.Software – must be chosen with care, and with an eye to future requirements. Must be stable and reliable require little to no downtime.Interface – Must be reasonably easy to use, with main features obvious even to the computer semi-literate, and advanced features available to entertain the more experienced users. Tutorials aimed at different levels of users and different roles should be provided.Identity – how much self disclosure is required or allowed is an important decision especially because it affects trust building. Allowing users a measure of choice encourages self-disclosure reciprocity.Participant levels – recognize that all community members are not the same. They vary in technical expertise and personality. Some are active and others are lurkers. Provide for the variety of needs and target features for all of your main groups, not just some of them.Level of anonymity- whether or not members will be required to provide their real name and other demographic information should be based on the purpose of the community and the nature of the information provided/exchanged. This decision also relates to whether or not participants should be required to log-in.Promoting the community - making potential participants in your target group aware of your community is essential. Designers must continually promote the community in order to keep new members coming in, particularly in closed communities.Virtual communities also need:Rituals–as embodied in initiations, shared and sharing histories, and creation of community “myths” or narratives.Visuality–that allows members to connect specific, unique visual cues such as icons, avatars, or even simply color themes, to certain users or administrators. This aids in the building of group identity, while selecting a visual presence can serve as initiation for new members. While our readings don’t address this, the increasing ability to integrate images, video, and other new media applications make visual presence an important part of community building today and probably into the future.Derived from:Matei, S. A. and Britt, B. C. (2011). Virtual sociability: from community to communitas (Selected Papers from the Purdue Online Interaction Seminar)Wegner, E., McDermott, R., Snyder W. (2002). Cultivating Communities of Practice. Cambridge, MA: Harvard Business Press.Powazek, D. (2002). Design for Community. Chapters 5-9Preece, J. (2000). Online Communities: Designing Usability and Supporting Sociability. Chapter 9Williams, G.A. (1994) Online Community Building Concepts. Retrieved November 4, 2006 from [1]Godwin, M. (1994) Nine Principles for Making Virtual Communities Work. Retrieved November 4, 2006 from [2]Suler, J. (1996) Making Virtual Communities Work. Retrieved November 4, 2006 from [3]Kollack, P. (1996) Design Principles for Online Communities. Retrieved November 4, 2006 from [4]Browse other articles on the same topic from this site:Yesterday, my HeritageCrowd project website was annihilated. Gone. Kaput. Destroyed. Joined the choir.It is a dead parrot.This is what I think happened, what I now know and need to learn, and what I think the wider digital humanities community needs to think about/teach each other.HeritageCrowd was (may be again, if I can salvage from the wreckage) a project that tried to encourage the crowdsourcing of local cultural heritage knowledge for a community that does not have particularly good internet access or penetration. It was built on the Ushahidi platform, which allows folks to participate via cell phone text messages. We even had it set up so that a person could leave a voice message and software would automatically transcribe the message and submit it via email. It worked fairly well, and we wrote it up for Writing History in the Digital Age. I was looking forward to working more on it this summer.Problem #1: Poor record keeping of the process of getting things intalled, and the decisions taken.Now, originally, we were using the Crowdmap hosted version of Ushahidi, so we wouldn’t have to worry about things like security, updates, servers, that sort of thing. But… I wanted to customize the look, move the blocks around, and make some other cosmetic changes so that Ushahidi’s genesis in crisis-mapping wouldn’t be quite as evident. When you repurpose software meant for one domain to another, it’s the sort of thing you do. So, I set up a new domain, got some server space, downloaded Ushahidi and installed it. The installation tested my server skills. Unlike setting up WordPress or Omeka (which I’ve done several times), Ushahidi requires the concommitant set up of ‘Kohana‘. This was not easy. There are many levels of tacit knowledge in computing and especially in web-based applications that I, as an outsider, have not yet learned. It takes a lot of trial and error, and sometimes, just dumb luck. I kept poor records of this period – I was working to a tight deadline, and I wanted to just get the damned thing working. Today, I have no idea what I actually did to get Kohana and Ushahidi playing nice with one another. I think it actually boiled down to file structure.(It’s funny to think of myself as an outsider, when it comes to all this digital work. I am after all an official, card-carrying ‘digital humanist’. It’s worth remembering what that label actually means. At least one part of it is ‘humanist’. I spent well over a decade learning how to do that part. I’ve only been at the ‘digital’ part since about 2005… and my experience of ‘digital’, at least initially, is in social networks and simulation – things that don’t actually require me to mount materials on the internet. We forget sometimes that there’s more to the digital humanities than building flashy internet-based digital tools. Archaeologists have been using digital methods in their research since the 1960s; Classicists at least that long – and of course Father Busa).Problem #2: Computers talk to other computers, and persuade them to do things.I forget where I read it now (it was probably Stephen Ramsay or Geoffrey Rockwell), but digital humanists need to consider artificial intelligence. We do a humanities not just of other humans, but of humans’ creations that engage in their own goal-directed behaviours. As some one who has built a number of agent based models and simulations, I suppose I shouldn’t have forgotten this. But on the internet, there is a whole netherworld of computers corrupting and enslaving each other, for all sorts of purposes.HeritageCrowd was destroyed so that one computer could persuade another computer to send spam to gullible humans with erectile dsyfunction.It seems that Ushahidi was vulnerable to ‘Cross-site Request Forgery‘ and ‘Cross-site Scripting‘ attacks. I think what happened to HeritageCrowd was an instance of persistent XSS:The persistent (or stored) XSS vulnerability is a more devastating variant of a cross-site scripting flaw: it occurs when the data provided by the attacker is saved by the server, and then permanently displayed on “normal” pages returned to other users in the course of regular browsing, without proper HTML escaping.When I examine every php file on the site, there are all sorts of injected base64 code. So this is what killed my site. Once my site started flooding spam all over the place, the internet’s immune systems (my host’s own, and others), shut it all down. Now, I could just clean everything out, and reinstall, but the more devastating issue: it appears my sql database is gone. Destroyed. Erased. No longer present. I’ve asked my host to help confirm that, because at this point, I’m way out of my league. Hey all you lone digital humanists: how often does your computing services department help you out in this regard? Find someone at your institution who can handle this kind of thing. We can’t wear every hat. I’ve been a one-man band for so long, I’m a bit like the guy in Shawshank Redemption who asks his boss at the supermarket for permission to go to the bathroom. Old habits are hard to break.Problem #3: Security WarningsThere are many Ushahidi installations all over the world, and they deal with some pretty sensitive stuff. Security is therefore something Ushahidi takes seriously. I should’ve too. I was not subscribed to the Ushahidi Security Advisories. The hardest pill to swallow is when you know it’s your own damned fault. The warning was there; heed the warnings! Schedule time into every week to keep on top of security. If you’ve got a team, task someone to look after this. I have lots of excuses – it was end of term, things were due, meetings to be held, grades to get in – but it was my responsibility. And I dropped the ball.Problem #4: BackupsThis is the most embarrasing to admit. I did not back things up regularly. I am not ever making that mistake again. Over on Looted Heritage, I have an IFTTT recipe set up that sends every new report to BufferApp, which then tweets it. I’ve also got one that sends every report to Evernote. There are probably more elegant ways to do this. But the worst would be to remind myself to manually download things. That didn’t work the first time. It ain’t gonna work the next.So what do I do now?If I can get my database back, I’ll clean everything out and reinstall, and then progress onwards wiser for the experience. If I can’t… well, perhaps that’s the end of HeritageCrowd. It was always an experiment, and as Scott Weingart reminds us,The best we can do is not as much as we can, but as much as we need. There is a point of diminishing return for data collection; that point at which you can’t measure the coastline fast enough before the tides change it. We as humanists have to become comfortable with incompleteness and imperfection, and trust that in aggregate those data can still tell us something, even if they can’t reveal everything.The HeritageCrowd project taught me quite a lot about crowdsourcing cultural heritage, about building communities, about the problems, potentials, and perils of data management. Even in its (quite probable) death, I’ve learned some hard lessons. I share them here so that you don’t have to make the same mistakes. Make new ones! Share them! The next time I go to THATCamp, I know what I’ll be proposing. I want a session on the Black Hats, and the dark side of the force. I want to know what the resources are for learning how they work, what I can do to protect myself, and frankly, more about the social and cultural anthropology of their world. Perhaps there is space in the Digital Humanities for that.When I discovered what had happened, I tweeted about it. Thank you everyone who responded with help and advice. That’s the final lesson I think, about this episode. Don’t be afraid to share your failures, and ask for help. As Bethany wrote some time ago, we’re at that point where we’re building the new ways of knowing for the future, just like the Lunaticks in the 18th century. Embrace your inner Lunatick:Those 18th-century Lunaticks weren’t about the really big theories and breakthroughs – instead, their heroic work was to codify knowledge, found professional societies and journals, and build all the enabling infrastructure that benefited a succeeding generation of scholars and scientists.if you agree with me that there’s something remarkable about a generation of trained scholars ready to subsume themselves in collaborative endeavors, to do the grunt work, and to step back from the podium into roles only they can play – that is, to become systems-builders for the humanities — then we might also just pause to appreciate and celebrate, and to use “#alt-ac” as a safe place for people to say, “I’m a Lunatick, too.”Perhaps my role is to fail gloriously & often, so you don’t have to. I’m ok with that.Like this:Be the first to like this.The U.S. Office of the Register of Copyrights has released Legal Issues in Mass Digitization: A Preliminary Analysis and Discussion Document .Here's the announcement:The Copyright Office has published a Preliminary Analysis and Discussion Document that addresses the issues raised by the intersection between copyright law and the mass digitization of books. The purpose of the Analysis is to facilitate further discussions among the affected parties and the public discussions that may encompass a number of possible approaches, including voluntary initiatives, legislative options, or both. The Analysis also identifies questions to consider in determining an appropriate policy for the mass digitization of books.Public discourse on mass digitization is particularly timely. On March 22, 2011, the U.S. District Court for the Southern District of New York rejected a proposed settlement in the copyright infringement litigation regarding Google's mass book digitization project. The court found that the settlement would have redefined the relationship between copyright law and new technology, and it would have encroached upon Congress's ability to set copyright policy with respect to orphan works. Since then, a group of authors has filed a lawsuit against five university libraries that participated in Google's mass digitization project. These developments have sparked a public debate on the risks and opportunities that mass book digitization may create for authors, publishers, libraries, technology companies, and the general public. The Office's Analysis will serve as a basis for further policy discussions on this issue.| Google Books Bibliography | Digital Scholarship | This entry was posted on Tuesday, November 1st, 2011 at 10:01 pm and is filed under Copyright, Digital Copyright Wars, E-Books, Google and Other Search Engines, Mass Digitizaton, Publishing, Reports and White Papers. You can follow any responses to this entry through the RSS 2.0 feed. Both comments and pings are currently closed.| Digital Scholarship Overview | DigitalKoans | Categories | Date Index |By Roger Whitson, Mellon Postdoctoral Fellow, Digital Scholarship Commons (DiSC)One of the things that has already come out of my trip to THATCamp Pedagogy at Vassar College is a tension between a desire by individual teachers to create collaborative and digital assignments and institutional limitations: such as curriculum requirements and funding.For example, the oft-cited desire of DH teachers to see failure as an essential part of the pedagogical pr ocess is countered by this very thoughtful tweet from Rebecca Frost Davis:"how do you reconcile giving students freedom to fail with requirements of the curriculum #thatcamp #pdgy"Further, in the Gamification session, several campers expressed frustration at the difficulty of teaching and grading collaborative work. On the one hand, several campers talked about the pedagogical value in assigning group grades. On the other hand, a few noted distress when it became clear that no matter how much we want students to pass or fail as groups, they will always be seen as individuals in a University (and a culture) where individuals have GPAs and are made to craft individual careers.One of the greatest hinderances for collaborative digital work is a cultural obsession with individualism. This is why I feel the digital humanities should essentially be about activism and advocacy. We need to not only assign and engage in collaborative digital projects, but also by create an environment where such projects are valued. Kathleen Fitzpatrick already wrote an amazing article about the need for faculty to back graduate students who risk their graduate careers by engaging in digital work. I also really like what Natalia Cecire said about the political stakes of DH work in an already transforming academic labor market:[D]igital humanities and 'the job market' as it now manifests isn't a narrow, merely administrative sliver of life of interest solely to junior academics who are still gravely listening to advice about how to "tailor" the teaching paragraphs in their cover letters. Digital humanities has become important to 'the job market' exactly insofar as it is causing major shifts in the institutions of the profession. These shifts are political. And if you are in my profession, then they are your concern.Indeed, these are all of our concerns. Towards the end of the Gamification session, I replied to one of the undergrads who was anxious about collaborative grades by saying that we need to petition administration to change curriculum requirements so they more realistically reflect the needs of our students and the changing job market. I was countered by another camper who cited Universities that do experiment with alternative ways for assessing students. These Universities, this camper noted, rarely do well in terms of enrollment and retention rates. Parents and even students seem to want individual assessment.But, really, isn't the collaborative spirit of the digital humanities designed to combat this cult of the individual? We need to venture out of our classrooms and into public spaces, demonstrating the value of collaborative work to administrators, parents, and potential employers. We should partner with businesses to gauge how the workplace values collaboration and team building. We have to thread ourselves into libraries, community groups, and professional (not just academic) conferences.As a member of the Digital Scholarship Commons (DiSC) at Emory University, I see my role as enabling scholars and graduate students to advocate for the digital humanities. In fact, I created a Prezi back in July that discussed how librarians can contribute as activists.But librarians obviously can't do it alone. Faculty members, graduate students and undergraduates should use DH to create a common space for collaboration. Instead of lamenting the realities we see before us, we need to find ways of working together to change the culture of education. Digital humanists must first and foremost be digital activists.In a previous post I discussed and, hopefully, debunked some common assumptions on the next phase of the World Wide Web, or web 3.0. The general assumption is that in the 2.0 era the user was at the centre, the produser took control and the cult of the amateur was born. The web was being flooded with what seems an infinite amount of user generated content. Big platforms, such as Flickr and Facebook managed to centralize and collect some of these efforts effectively. The result of is a big, fragmented and messy dataset. Enter web 3.0; the iteration of the web which can be read and understood by machines, where the dots will be connected and contribute to an open sphere of knowledge, something that the current pragmatics of the web don’t easily allow for. The philosophy here is, bluntly put, that this connected sphere is more than the sum of its parts. Tim Berners Lee recognized the problematics of the messy web early on and proposed the Semantic Web to overcome messiness and apply a semantic structure to bring order (read: computer logic) in the chaos (read: human expression).Pierre Levy, French philosopher and leading expert on collective intelligence, is on a similar mission. While driven by, arguably, a similar set of goals, his approach takes a step further. The problem with Berners Lee semantic structure is that implies a universal ontology, which might prove out to be the Achilles heel of the protocol. Levy’s approach overcomes these problems.Levy is currently working on a research program, called IEML (Information Economy Meta Language). IEML is a metalanguage and proposes itself as the language of collective intelligence. As a metalanguage it differs fundamentally from natural languages we know. This can be best understood in the way it is conceived. Natural languages are, in the first place, the results of a process of documenting the spoken word. A metalanguage is artificial and is a result of formalizing ideas, instead of words. The practice of formalizing ideas in a universally adopted metalanguage is well established in the realm of natural sciences. For centuries now, ideas are being documented in terms of formulas, numbers, equations, molecules etc. There is a finite, well structured toolset at the hands of every natural scientist. In humanities the area of interest is infinite and not easily encapsulated in a formal manner. In humanities, knowledge is fuzzy, or as the IEML vision paper describes: “the knowledge and expertise accumulated by the humanities are diﬃcult to share in contexts that diﬀer from the initial environment in which they emerged” IEML offers the solution to this problem, offering humanities a language in which knowledge can be formally described. How this exactly will work pragmatically is to be determined, as the project is in a “fundamental research stage”, as Levy stressed out to me. On a hopeful note though: IEML also functions as a bridge between languages, the natural language of the end user is not relevant: “The IEML inter linguistic dictionary is precisely constructed to ensure that semantic networks can be automatically translated from one natural language to another”. Mr Levy was kind enough to answer a number of questions and concerns I had with this project.As Levy pointed out to me in order for an idea to contribute to the sphere of collective intelligence it should be described in a formal manner. This is where the digital humanities researcher comes in, who should master this new code to formalize his peers ideas, a crucial step in the process: “If ideas and concepts are not formalized, it is impossible to compute their semantic relationships automatically”.One of my concerns was with the IE in IEML, meaning Information Economy. In the vision paper it is noted that this sphere should be “observed”. I was curious what should be exactly observed in terms of meaningful data and the private sphere, Levy answered:There is currently an immense mass of public data on the World Wide Web that is not efficiently shared, analysed and used by humanities and social sciences. Considering the extraordinary range of these data and the computing power that is now at our disposal, a scientific revolution in the human sciences can be predicted for the 21st century. I can mention the areas of cultural heritage, health, education, economy, sociology, etc.One of the main reasons why this computational potential is not actualized today is the lack of semantic interoperability. A universal (interlinguistic and interdisciplinary) system of computable metadata, like IEML, could be the stepping stone leading us into a renewal of human sciences. Of course, every team or individual should be free to categorize and assess the data as he wishes. The common semantic code will allow for comparison and sharing.All this should be done while respecting existing laws and privacy of individuals. (There is enough work to be done on public data.) The dangers that you mention are not specifically linked to IEML and do exist for all digital data in general.The scientific revolution in the human sciences will culminate in collective intelligence, a common good which will throttle human development. In a recent book, Levy proposed a “loose IEML model” to monitor the coordination of human development. The axis of human development are defined by “education, health, sustainable economic prosperity, security, human rights, conservation and enrichment of cultural heritage, environmental balance, scientific and technical innovation” which are in accordance with the United Nations Development Program, Levy assures me. I wondered whether a metalanguage which positions itself functionally as neutral (as opposed to Berners Lee universal ontology) should contain assumptions on how western democratic society is structured to which Levy partly agrees that any metalanguage can’t be neutral:There can be a lot of disagreements about the right ways or methods to improve human development. IEML, as a universal semantic code, can accomodate any method. Above all, IEML provides a common semantic sphere where all disciplines of human sciences can compare their theories and methods and can coordinate their findings at the service of human development. (…) Now, you can say: “Okay, but what if I am against improving health and education because these are western values and / or it has been used to justify western imperialism”. My response is: “It’s up to you!” In general, I do not think that any theory or metalangage can be neutral. Every act, being practical or theoretical, occurs in a hypercomplex context and has an effect on this context. I do not claim any impossible neutrality or objectivity. The objection “you’re not neutral” is besides the point. I have a very precise goal. My aim is to improve human development, collective intelligence and knowledge management in the humanities.The creation of IEML is based on the explicit assumption that all human beings, and all cultures, have in common a basic linguistic-symbolic ability. The main limitation of artificial intelligence is the belief that logic and statistics are sufficent to model human intelligence. I don’t think that current techniques of automatic reasonning are enough to model the basic symbolic manipulation ability of the human species. In addition to the formal tools of artificial intelligence, we need a new kind of formalism to describe in a functional and computable manner our capacity to create and transform meaning (sense, signification). IEML provides precisely such a formalism. The main result of this scientific invention will be the expansion of a semantic sphere where any creative conversation on line will be able to observe its own processes of collective intelligence and to share methods and results with other conversations. IEML’s existing dictionary will be expanded. You’ll be able to build any kind of “universe of discourse” or semantic world by using IEML. Far from being “neutral”, the creation of IEML points toward a cultural leap forward: a perspectivist scientific reflexivity of human collective intelligence.My final concern had to do with cultural determination, as Levy had stressed in another interview; the attempts at creating a symbolic metalanguage can be found in many different cultures. Each of these cultures would produce a different “universal” language. How does one overcome this, can a symbolic metalanguage be universal?What is not culturally determined in the human realm, specially when it comes to language? IEML is first culturally conditionned by the technical environment of the 21st century : growing computing power (automation of symbol manipulation), growing memory power (availability of digital data), and growing communication power (ubiquity of data). It is also conditionned by the scientific method (which is of course a dated cultural institution), namely its insistence on functional computability and transparent (reproductible) procedures. I don’t see these determinations as limitations, but rather as very powerful engines that I have used in the service of human self-knowledge.Levi-Strauss being one of my favorite author, I am of course well aware of the dangers of “ethnocentrism” . As an inventor, I have been influenced by a wide range of disciplines and theories in the human and cognitive sciences. (All this is explained in my book The Semantic Sphere). I should also mention that I have dwelt on three continents (Africa, Europe, North Amercia) and learned a lot from different cultures, that I have studied traditional Greek, Indian and Chinese philosophy, that I have scrutinized Jewish, Christian, Muslim and Bouddhist metaphysics and that I am deeply involved into the amazing Brazilian cultural and economic development.Universal does neither mean “out of history” nor “out of culture”. The notation position system of numbers, including the zero, is universal. The decimal system is (almost) universal. The time zones system is universal. The meridian and parallel systems for geography are universal. The Internet Protocol and the HyperText Transfer Protocol are universal. However, all these symbolic systems have been invented somewhere, sometime.”Finally I enquired about the future of IEML, what can we expect from this project in the foreseeable future. First his team has to build technical tools and train “semantic engineers” the language. After that? Levy is not sure: “Beyond this, I have no precise idea of the actual development [will be]. I just feel that it will happen sooner or later. I think that some big university or some scientific endeavour will lead the way, followed by the companies operating in cloud computing and big data management. I foresee the development of ‘collective intelligence games’ looking like massive multiplayer online / real life games, or some sort of trans-platform smart social media.”Apart from the technicalities: Levy has a clear vision for the future. IEML can’t be regarded as merely a language or a tool, that wouldn’t do its intentions justice. IEML is a language with a mission and I can’t wait to find out how that will play out.I’m always surprised when i hear people attack open access publishing. There are no rational arguments as to why print based texts and journals are superior to open access texts, but there are plenty of rational reasons for open access publishing. Some seem to think that open access publications are less reputable and should count less towards tenure. Personally I think if you’re writing and speaking for tenure you might be in the wrong line of work, but that’s another matter. Such people seem to forget that Harvard went open access back in 2008. Perhaps Harvard is a second rate institution, but that seems like a difficult case to make. All that should matter is the peer review process. Are the editors qualified to peer review the material handed their way and do the directing editors only elect others to peer review articles who are qualified to do so? If so, there’s no difference or issues here. This is certainly a more rigorous process than the one involved in some of the vanity presses some academics publish their books with.Others argue that digital print can easily be lost. This is an odd argument, as books can be burned (Lucretius’s De Rerum Natura was reduced to one copy by Christians that sought to destroy the book), and 2) most open access texts are released in both print and digital formats.The arguments for open access publishing are obvious: open access books are ecologically friendly, reducing damage to trees and damage produced by carbon emissions due to shipping, they significantly reduce the cost of publishing, and they allow ideas to circulate freely, rather than be locked away in journals that are difficult for many to access either because they are extremely expensive or have small print runs. Opposition to open access publishing indicates both a lack of ecological awareness as well as an economic classism that approves those with little means (often graduate students, but also people outside the academy) being denied access to thought. In other words, the expensive price of print journals and articles is a material mechanism that re-produces certain class and social relations in knowledge production (those that have the means or a good library available get to participate, those that don’t don’t).From a sales angle, however, I’ve been surprised to discover that open access publishing actually seems to increase sales. The Speculative Turn has been a wild success. It crashed Re.Press’s server the night it was released, and has hovered around the 40-60 thousand sales rank on Amazon consistently since it was released a year ago. This is extraordinary for an academic text, especially given that anyone can access it for free. Graham’s Prince of Networks has done similarly well. It’s difficult to yet say how The Democracy of Objects will do in print form, but so far the internet traffic has been very promising.I’m very eager to see how O-Zone does once it is up and running over the next couple of years. Eileen Joy’s Postmedievalism, an open access journal, has been tremendously successful and is internationally recognized both in the field of medieval studies and outside of it. Some have griped about the advisory editors of O-Zone, expressing ire over the fact that the undergraduate Marisol Bate is on the team. First, they fail to realize that it was Marisol who first approached Kris and I with the idea of developing the journal. Second, the credentials of the editors both within the world of OOO and in academia as a whole are outstanding. All of the people involved in the journal are people who have made significant contributions to OOO in the form of publications, organizing conferences, and who have made significant contributions to “thingly” thought. In putting together the advisory board our considerations revolved around representing a number of different disciplines and practices, insuring good gender parity, and depth of accomplishment. We selected people with whom we have closely worked or whose work we are intimately familiar with.Third, our philosophy seeks to honor a variety of different perspectives both from within academia and from a variety of different disciplines in academia, and outside of academia. In many respects, this goes back to the original disciplinary attude of Graham, me, Morton, and Bogost. Graham and I are philosophers, Morton is a lit person, and Bogost researches technology, video games, and digital humanities. All of us have worked intimately at conferences and online from a variety of disciplines and practices ranging across artists, ethnographers, architects, novelists, musicians, geographers, historians, lit people, activists, poets, etc, etc, etc. Moreover, in the blogosphere we have cultivated a space that sidelines academic rank or belonging to academia at all, and that instead engages other in terms of the quality of their thought, work, and contributions. We have sought to capture that spirit in our editorial board, including people from a variety of disciplines as well as artists and activists. Marisol, an extraordinary thinker and person, falls into this category of activism, and is someone who fights human sex trafficing (in ways that have actually caused risk to her life and damage to her person), figts on behalf of her indigineous Hawaiian people against colonial invasion, and is involved in fighting capitalist exploitation with OWS. That’s a pretty qualified person to comment, with others, on certain political and activist submissions that come our way.As I’ve argued before, there’s a very nasty tendency among proponents of each discipline to treat their own discipline as a master-displine that is the foundation of all other disciplines and everything else. The rhetoricians cry that everything involves rhetoric and therefore rhetoric is “first philosophy”. The historians retort that everything involves history and therefore history is “first philosophy”. The philosophers claim that everything involves being and knowledge so therefore their discipline is first philosophy. And so it goes. Kris, Eileen and I are involved in trying to create something called post-disciplinarity where it is recognized that all of these disciplines are local knowledges, partial views on the world, where it is recognized that the artist, engineer, designer, and activist create knowledge and thought every bit as much as the scholar, and where a space can be opened where these divergent lenses can come to resonate with one another and generate new innovation in thought, art, design, and political engagement. We’re tired of talking with authoritarians that want their discipline or practice to be the master-science and who wish to subordinate everything else to their master narrative. Instead we want transversal forms of communication where delight and inspiration can be taken from the work, inventions, amd discoveries of others and where there’s no longer a question of foundational disciplines.Like this:2 bloggers like this.I’ve really enjoyed cruising through the Jack Dougherty and Kristen Nawrotzki open peer-review volume called Writing History in the Digital Age which is slated to be published by University of Michigan Press’s new Digital Humanities Series in their digitalculturebooks imprint. I commented on many of the contributions and mined them all for references and ideas. I’d encourage anyone interested or invested in the future of history in the digital age to check out the volume and to contribute to its open peer review. Since I have read all the articles in the volume and have been thinking a bit about history in the digital era myself lately, I thought I might offer some overarching comments on the volume (as is my wont).1. Coherence. One of the first things I noticed about the book is the wide range of contributions. These range from two recent Ph.D.s discussing how they used email to keep themselves motivated and sane while writing their dissertations to discussions on databases, GIS, visualization, and even non-linear digital editing. Articles on the use of Wikipedia and Social media in the classroom stand alongside more theoretical or research oriented papers. While such scope is commendable (and must reflect the “big tent” approach to digital humanities, in general), it caused me to wonder about the limits of a specific sub-field called “digital history” and how we plan to organize and reflect on the intersection of digital tools and history as the discipline becomes invested in digital technologies. For example, there were no articles celebrating the contribution of the so-called “personal computer” or “word processor” in the volume. These basic technologies clearly fell outside of what the authors and editors regarded as the discourse of digital history (although one can argue that these technologies had as big an impact on our field as Wikipedia or Facebook).Edited volumes always have ragged edges where the definitions and ideas of the contributors fail to line up precisely across the entire book or clash with those of the editors. This is part of the charm of the edited volume; it captures a snapshot of a particular topic in the minds of a group of scholars (as opposed to the carefully composed portrait that is a monograph). At the same time, recent discussions on the definition of the digital humanities might feature more prominently in a volume like this. Is there really enough theoretical, methodological, and topical coherence between all the papers here to justify their appearance in the same book?2. Institutions. One of the more interesting aspect of the volume was the subtle but (almost) ubiquitous mention of institutional support for the various initiative detailed. In some cases, the support came from powerful national organizations like National Endowment for the Humanities. In other cases, on campus labs or centers like Arts eResearch at the University of Sydney or MATRIX at Michigan State, provided the infrastructure necessary for a project’s development. Some initiatives were far more modest in scope and extended only slightly beyond the classroom’s walls or an immediate community. Few of the articles in this volume, however, problematized their work in terms of a formal research question framed in response to a pre-existing body of scholarship. (Few began with the ubiquitous phrases: “Scholars have argued…”)It appears, then, that the impetus for working in digital history derives as much from institutional pressures (and opportunities) as traditional appeals to the scholarly conversation. While this is hardly surprising for a recent development in the discipline, it may foreshadow an interesting shift in the structure of humanities scholarship. The pressure to collaborate and innovate is pushing scholars in the humanities away from well-trod arguments and to the brink of a kind of rupture in the discourse (in a Foucauldian sense). The external pressure and resources deployed by on campus and national institutions have insisted that historians (and other scholars in the humanities) shift their arguments from the small-picture debates that have long shaped these disciplines, to big picture, transdisciplinary, collaborative thinking. This is manifest in (some, but not all of) the scholarship that these projects produced: Writing History in the Digital Age recognizes a different audience and a different set of discursive rules than writing traditional history.3. Methods and Techniques. Traditional historical practice has been short on method. The so-called historical method is, in fact, a set of practices cobbled together from various other fields and epistemological systems. With the rise in digital history, however, a new interest in methods and practices has come to the fore and a number of the articles in Writing History in the Digital Age reflect this development. Digital historians are more willing to experiment with methods grounded in geography, the social sciences, media studies, and, even, computer programing and game studies.At the same time, this methodological growth requires critical attention to new techniques. Archaeology for example, has developed a robust methodological discourse over the past 40 years as the disciple embraced a “methodological turn” that sought to critically examine the tools, practices, and assumptions that shaped archaeological knowledge. The essays in this volume, in contrast, showed very little in the way of genuine methodology. Of course, some of the essays with a pedagogical bent, showed an awareness of and willingness to contribute to recent pedagogical developments, but few of the more research oriented pieces considered explicitly and critically the methodological assumptions of their use of digital tools.The absence of methodology extends to some extent to the techniques (for lack of a better word) used to generate the kind of digital analysis that their contributions celebrate. While software, programing and markup languages, and hardware appeared regularly in the pages, we were rarely invited to look behind the curtain to see how these aspects of digital history influenced the ways in which history could be written. (The notable exceptions to this were the several essays that discussed Wikipedia, but even these essays focused on the social, rather than technological aspects of this forum. For example, several of the essays mention the automated “bots” that crawl Wikipedia and can change entries systematically, but few essays explain how these bots work and why historian-trained bots couldn’t do the same things.) My feeling is that the next step in the study of digital history will involve a much more critical approach to the methods and tools used by digital historians to produce new knowledge.4. The Future. One of the most significant gaps in this small book were essays with an eye toward the future. Writing the future is always a risky game, especially for historians who are so accustomed to “looking backward“. At the same time, part of the writing digital history game is positioning history in a place not only to take advantage of digital tools created by other people, but also to shape how new technologies develop. I would have loved to hear how folks invested in digital history, as the contributors to this book clearly are, see the future of technology impacting our work as historians.Developments like the massive growth of computing power available to mobile devices, enhanced and augmented reality, MOOCs (Massive Online Open Courses), an endless stream of cloud services, the chaining notion of curation and the personal web, and the rapid mutation of social media communities, all offer new venues for presenting history, but also new spaces and tools for the analysis and interpretation of past events.Writing History in the Digital Age represents a moment in time in the discipline’s embrace of digital tools. At once it is possible to see ragged edge of the profession’s handling of digital media to communicate and interpret the past, as well as its growing confidence in embracing (if not fully engaging) new technologies.Like this:Be the first to like this.I’m a relative newcomer to digital humanities; I’ve been doing this for about a year now. The content of the field has been interesting, but in some ways even more interesting is the way it has transformed my perception of the academy as a social structure. There are clearly going to be debates over the next few years between more and less digitized humanists, and debate is probably a good thing for everyone. But the debate can be much more illuminating if we acknowledge up front that it’s also a tension between two different forms of social organization.Here’s what happens when that dimension of the issue goes unacknowledged: a tenured or tenure-track faculty member will give a talk or write a blog post about the digital humanities, saying essentially “you’ve got some great tools there, but before they can really matter, their social implications need to be theorized more self-consciously.” Said professor is then surprised when the librarians, or academic professionals, or grad students, who have in many cases designed and built those tools reply with a wry look.The reason for this, as Miriam Posner recently tweeted, is that “theory has been the province of scholars,” while “the work of DH has been done by staff.” So when you say “those tools need to be theorized,” you are in effect saying “those tools need to be appropriated or regulated by someone like me.” That’s, so to speak, the social implication.I hasten to add that I’ve got nothing against theories. I wouldn’t mind constructing a few myself. Literary theory, social theory, statistical theory — they’re all fun. But when the word “Theory” is used without adjective or explication, it does in my view deserve a wry look. When you take away all the adjectives, what’s left is essentially a status marker.So let’s not play that game. Nothing “needs to be theorized” in a vague transitive way; academics who use phrases like that need to realize what they’re saying. DH is an intensely interdisciplinary field that already juggles several different kinds of theory, and actively reflects on the social significance of its endeavors (e.g. in transforming scholarly communication). It is also, among other things, an insurgent challenge to academic hierarchy, organized and led by people who often hold staff positions — which means that the nature of the boundary between practice and theory is precisely one of the questions it seeks to contest.But as long as everyone understands that “theory” is not a determinate object belonging to a particular team, then I say, the more critique, debate, and intellectual exchange the better. For instance, I quite enjoyed Natalia Cecire’s recent blog post on ways DH could frame its engagement with literary theory more ambitiously. I don’t know whether it’s a good idea to have a “theory THATcamp”; I haven’t been to THATcamp, and don’t know whether its strengths (which seem to lie in collaboration) are compatible with that much yacking. But I think Cecire is absolutely right to insist that DH can and should change the way the humanities are practiced. Because digital approaches make it possible to ask and answer different kinds of questions, there’s going to be a reciprocal interaction between humanistic goals and digital methods, not, as Cecire puts it, a “merely paratactic, additive concatenation.” We’re going to need to theorize about methods and goals at the same time. Together. Intransitively. This post is slightly revised from the original version, mostly for clarity.]Like this:Be the first to like this.These are my notes “Building and Sharing (When You’re Supposed to be Teaching,” a lightning talk I gave on Tuesday as part of CUNY’s Digital Humanities Initiative. Shannon Mattern (The New School) and I were on a panel called “DH in the Classroom.” Shannon’s enormously inspirational lightning talk was titled Beyond the Seminar Paper, and mine too focused on alternative assignments for students. Our two talks were followed by a long Q&A session, in which I probably learned more from the audience than they did from me. I’ll intersperse my notes with my slides, though you might also want to view the full Prezi (embedded at the end of this post).I’d like to thank Matt for inviting me to talk tonight, and to all of you too, for coming out this gorgeous evening. I’m extremely flattered to be here—especially since I don’t think I have any earth-shattering thoughts about the digital humanities in the classroom. There are dozens and dozens of people who could be up here speaking, and I know some of them are here in this room right now.A lot of what I do in my classroom doesn’t necessary count as “digital humanities”—I certainly don’t frame it that way to my students. If anything, I simply say that we’ll be doing things in our classes they’ve never done before in college, let alone a literature class. And literature is mostly what I teach. Granted I teach literature classes that lend themselves to digital work—electronic literature classes, postmodern fiction, and media studies classes that likewise focus on close readings of texts, such as my videogame studies classes. But even in these classes, I think my students are surprised by how much our work focuses on building and sharing.If I change point of view of the title of my talk to my students’ perspectives, it might look something like this:Building and sharing when we’re supposed to be writing. And at the end of this sentence comes one of the greatest unspoken assumptions both students and faculty make regarding this writing:It’s writing for an audience of one—usually me, the instructor, us, the instructors. This is what counts as an audience to my students. They rarely think of themselves as writing for an audience beyond me. They rarely think of their own classmates as an audience. They often don’t even think of themselves as their own audience. They write for us, their professors and instructors.So the “sharing” part of my title comes from my ongoing effort—not always successful—to extend my students’ sense of audience. I’ll give some examples of this sharing in a few minutes, but before that I want to address the first part of my title: the idea of building.Those of you who know me are probably surprised that I’m emphasizing “building” as a way to integrate the digital humanities in the classroom. One of the most popular things I’ve written in the past year is a blog post decrying the hack versus yack split that routinely crops in debates about the definition of digital humanities.In this post, I argued that the various divides in the digital humanities, which often arise from institutional contexts and professional demands generally beyond our control—these divides are a distracting sideshow to the true power of the digital humanities, which has nothing to do with production of either tools or research. The heart of the digital humanities is not the production of knowledge; it’s the reproduction of knowledge.The promise of the digital is not in the way it allows us to ask new questions because of digital tools or because of new methodologies made possible by those tools. The promise is in the way the digital reshapes the representation, sharing, and discussion of knowledge.And I truly believe that this transformative power of the digital humanities belongs in the classroom. Classrooms were made for sharing. So, where does the “building” part of my pedagogy come up? How can I suddenly turn around and claim that building is important when I just said, in a blog post that has shown up on the syllabus of at least three different undergraduate introduction to the digital humanities courses?Well, let me explain what I mean by building. Building, for me, means to work. Let me explain that.In an issue of the PMLA from 2007 there’s a fantastic series of short essays by Ed Folsom, Jerry McGann, Peter Stallybrass, Kate Hayles, and others about the role of databases in literary studies. Folsom’s essay leads, and in it he describes what he calls the “epic transformation” of the online Walt Whitman Archive, which Folsom co-edits, along with Ken Price, into a database (1571). All of the other essays in some way respond to either the particulars of the digital Walt Whitman Archive, or more generally, to the impact of archival databases on research and knowledge production. It’s a great batch of essays, pre-dating by several years the prevalence of the term “digital humanities”—but that’s not why I mentioning these essays right now.I’m mentioning them because Peter Stallybrass’s essay has the provocative title “Against Thinking,” which helps to explain why I mean by working, which Stallybrass explicitly argues stands opposed to thinking.Thinking, according to Stallybrass is hard and painful. It’s boring, repetitious, and I love this—it’s indolent (1583).On the other hand, working is easy, exciting, a process of discovery. It’s challenging.This distinction between thinking and working informs Stallybrass’s undergraduate pedagogy, the way he trains his students to work with archival materials and the STC. In Stallybrass’s mind, students—and in fact, all need to do less thinking and more working. “When you’re thinking,” Stallybrass writes, “you’re usually staring at a blank sheet of paper or a blank screen, hoping that something will emerge from your head and magically fill that space. Even if something ‘comes to you,’ there’s no reason to believe that it is of interest, however painful the process has been” (1584).Stallybrass goes on to say that “the cure for the disease called thinking is work” (1584). In Stallybrass’s field of Renaissance and Early Modern literature, much of that work has to do with textual studies, discovering variants, paying attention to the material form of the book, and so on. In my own teaching, I’ve attempted to replace thinking with building—sometimes with words, sometimes without. And I want to run through a few examples right now.In general, these examples fall into two categories:[And here my planned comments dissolved into a brief tour of some of the ways I incorporate building and sharing into my classes. The collaborative construction category is more self-evident: group projects aimed at building exhibits or formulating knowledge, such as my Omeka-based Portal Exhibit and the current cross-campus Renetworking of House of Leaves. I described my creative analysis category as an antidote to critical thinking---a hazardous term with an all but meaningless definition. In this category I included mapping projects and game design projects that were alternatives to traditional papers. I concluded my lightning talk by noting that students who pursued these creative analysis projects spent far more time on their work than those who wrote papers, and while their end results were often modest, these students were far more engaged in their work than students who wrote papers.]Full PreziWorks CitedFolsom, Ed. “Database as Genre: The Epic Transformation of Archives.” PMLA 122.5 (2007): 1571-1579. Print.Stallybrass, Peter. “Against Thinking.” PMLA 122.5 (2007): 1580-1587. Print.Stroube, Sam. Radiohead Crowd. 2006. Flickr. Web. 20 Jan. 2009.I have written before about some issues relating to RDA and RDF. Today I want to actually consider some things we should consider that should cause us to question the concept of "RDA in RDF." For many decades we have been using relational databases to store our bibliographic data, bibliographic data that we create and exchange using the MARC format. Doing so was not by any means natural or intuitive because there is nothing about the structure or content of the MARC record that lends itself to being stored and managed in a relational database. The results were often awkward, inefficient, and unsatisfying. Part of the reason for this is the unitary and flat nature of MARC. In spite of the long history of creating separate authority files, each MARC record is a complete and closed document with no actual connections to data outside of itself. While some database implementations for MARC do create relational tables for headings, the degree to which a MARC record can be separated out into tables is minimal and gains us very little in terms of the functionality of an RDBMS. The underlying problem, however, is not in the structure of the MARC record but in the content of our catalog records. Moving from the card to a database for our data requires more than adding mark-up coding around the catalog data; to do so successfully requires re-thinking the data in terms of relational database principles. There are two basic principles to relational database design: repetition and combination. To design for relational databases you look at your data to see what elements will be repeated in many different records. Rather than carrying those data elements in multiple records, you create a separate database table for each repeating element, and you store that element once. For example, if you are creating a database of mailing addresses, you see quickly that elements like state and zip code will appear in multiple records. You therefore create a table of state names and one of zip codes, and perhaps even one that links zip codes to city names. In this way, your database carries the string "Mississippi" only once, and that string is replaced in the records with a database pointer that uses much less internal storage. Ditto the zip code. And if the zip code is associated in a table with a city name, you also only store city names once, and each address record needs only a pointer to the zip code, not a city name. In fact, with a zip code you can get the city and state, and your design might look like: In this way you have saved a huge amount of storage space. You have also made selection of your records on zip code, city and state much more efficient than if they were stored in every address record, because a search on a zip code, for example, retrieves a single entry in the zip code table, and that entry has database-managed links to the relevant records. In a database of customer orders that has your inventory information along with customer addresses, you use the tables in your database to search for things like "all customers in Mississippi who have ordered WidgetX in the last six months." Information about your inventory and information about purchases are all in appropriate sets of tables in your database and you can combine the data elements to develop different views of the data. Where the goal in relational database design is to identify and isolate data elements that are the same, the goal in library cataloging data is exactly the opposite: headings are developed primarily to differentiate at the data creation point rather than allow combination within the database management system. The goal is to have each data point be as unique as possible and to be assigned to as few records as possible. Thus, library cataloging creates headings whose purpose is to distinguish between entries: Shakespeare, William, 1564-1616. As you like it Shakespeare, William, 1564-1616. As you like it. 1905 Shakespeare, William, 1564-1616. As you like it. 1911. Shakespeare, William, 1564-1616. As you like it. 1919. Shakespeare, William, 1564-1616. As you like it. Czech Shakespeare, William, 1564-1616. As you like it. French These headings are counter to the functioning of a database management system. If moved to a database table to facilitate retrieval, they will each point to only one or a very small number of records. This negates both the space-saving aspect of database management and it also does not facilitate combination of data elements for retrieval. In the case of headings, the combination of elements is pre-coordinated in the data, rather than post-coordinated in the database retrieval function. A database approach might break this data into four tables: In this way one could search for this data by title, by title + author, date + language, or by any other combination of these four data elements. To search the library headings as anything but a single keyworded string, that is to use these headings to perform searches on title or date or language, would be incredibly inefficient. The upshot is that library headings are not "relational" and do not contribute to the functionality that database management systems can provide. Instead, database management systems make use of the separate coded elements, such as date and language, for combinatorial retrieval. Names and titles, because they are text strings and do not have an identified presence in the stored records, must be searched separately rather than being available for relational combination. The results of this type of searching are less than optimal in speed and accuracy. All of this may seem obvious to some of you, so you may be asking yourselves why I bring this up. I bring it up because even though RDA claims to have as its goal the creation of records in a relational design (see scenario one in this JSC document), it continues to instruct catalogers to create pre-coordinated headings like the ones above. Not only will these not be efficient or fruitful in a relational database, this brings into question whether RDA is truly modeled on the principles it claims to embrace. If it is not we have cause to worry: we cannot move forward with data that does not conform to a modern model. Note that in this post I have been emphasizing the use of relational database design for the data. The current plans for a new bibliographic framework appear to plan to create a data model for RDA that is based on semantic web principles. Those principles are yet another significant evolution following on the database model, which is now considered waning technology. Other communities, ones that have been designing for database management requirements for their data for decades, are now looking at ways to transform that data to RDF. It is possible that we can skip the relational database phase of our data development and move directly into a semantic web model. However, to think that data created following RDA instructions, which is not even suitable for a relational database, could be made usable on the semantic web without major modifications is simply wrong. If we create a bibliographic framework that takes RDA as it has been described and ports that, unchanged, to RDF we will create a data model that does not serve us, does not serve our users, and that cannot reasonably interact with other linked data on the web. What we need is an analysis of our data, not a transformation of it "as is" to a new technology. If we aren't ready to admit that some traditional practices, like headings, are no longer useful or usable in today's technological environment, we cannot have any hope that our data will be relevant in the future.(p.s. I anticipate that someone will state that headings are needed for alphabetical displays, to "collocate" the records. To that I reply: 1) you can do the same collocation using the data elements, and in fact you could devise multiple collocations by combining the elements in different ways and 2) a linear, alphabetic display is so anachronistic with today's technology, and so seldom used when available, that it is hard to justify the use of human catalogers to create these fields. If you still believe that library records must contain hand-crafted headings, all I can say is: you can believe what you want, but some of us will be exploring other solutions.)"More hack, less yack," they say. I understand the impulse, and to some degree admire the rough-and-tumble attitude of those in digital humanities whose first priority is Gettin' Shit Done. Hell, I like Gettin' Shit Done. But as I've mentioned before, I cannot agree with the distinction between theory* and practice that this sets up, nor the zero-sum logic that it implies (i.e. in order to do more you must speak less). I've long found the complete domination of THATCamp Bootcamps by technical skills from the CS side curious to the point of illogical. (It turns out that this post is an elaboration of my THATCamp SF post of about a year ago.) We seem to have a tendency to think that the "humanities" part of DH is stable, that we sort of already have it squared away, while the tech skills are what we need to gain. But the whole reason DH is theoretically consequential is that the use of technical methods and tools should be making us rethink the humanities. In The Big Sea, Langston Hughes retrospectively snarks on those at the center of the Harlem Renaissance who "thought the race problem had at last been solved through Art plus Gladys Bentley" (228). In the same way, "when DH was in vogue," there's a temptation to believe that the academia problem has at last been solved through the New Criticism plus TEI. It's the "plus" that makes Hughes's comment so snarky: he puts his finger on the merely paratactic, additive concatenation that we're tempted to make of what can and should be a much more paradigmatic change. In other words, we do not have the humanities part squared away. Nor, for that matter, can the digital be imported wholesale. And so I think it's time we insisted a little more strongly on theorizing all that hacking. There are some theoretical keywords for DH that get used in woefully unrigorous ways—keywords like "archive"; "labor"; "biopower"; "embodiment"; "disability" and "access"; "map"; "narrative"; "identity"; "author." You show up at a THATCamp and suddenly folks are talking about separating content and form as if that were, like, a real thing you could do. It makes the head spin. I don't mean to caricature, much less insult, DH scholarship. We all know of many DH scholars who do theoretically and historically rigorous work, and I think most DH scholars try to be fairly intentional, if not necessarily "theoretical," about their processes. And to be clear, I, too, routinely use Drupal content types with a field labeled "author." Sometimes you have to make a black box in order to build something bigger and more complicated on top of it; in fact, much of web programming now operates on that very principle ("modularity"). But—perhaps largely due to the recency of the field's entry into the mainstream—much of DH is still characterized by that "plus." Although it would be fair to object that there is undertheorized work in all fields, not just DH, I think the "more hack less yack" culture makes this tendency more widespread and more acceptable in DH than elsewhere; indeed, I occasionally get the sense that some see DH as a refuge from theory. The whole notion of "best practices," pervasive in tech and industry, lives uneasily with theoretical critique. And the pedagogical emphasis on quick entry into the field—and the incredible success with which THATCamps, DHSI, and other initiatives have brought huge numbers of humanities scholars meaningfully into the orbit of DH—is admirable, but comes with some costs that would bear mitigating. I'm writing this post in part because, after a long conversation with my sister (Maria Cecire) about her first THATCamp experience, these issues have been on my mind. (I'll leave it to Maria to add her own comments, if she chooses.) And then, yesterday morning, I read Alexis Lothian's smart post on the LA Queer Theory conference and her upcoming ASA roundtable, which issued some timely challenges to the way we've been allowing DH to develop. I was particularly struck by part of the ASA roundtable description, which, without accusing anyone of bad faith (and I agree; I don't think there is any), asks why the digital suddenly seems so congenial to the humanities just when ethnic studies departments and on-campus women's centers are getting axed (not to mention philosophy departments). The questions that roundtable poses get at what we stand to lose when we fail to theorize practice, or when we leave our theorizing implicit.In an era of widespread budget cuts at universities across the United States, scholars in the digital humanities are gaining recognition in the institution through significant grants, awards, new departments and cluster hires. At the same time, ethnic studies departments are losing ground, facing deep cuts and even disbandment. Though the apparent rise of one and retrenchment of the other may be the result of anti-affirmative action, post-racial, and neoliberal rhetoric of recent decades and not related to any effect of one field on the other, digital humanities discussions do often elide the difficult and complex work of talking about racial, gendered, and economic materialities, which are at the forefront of ethnic and gender studies. Suddenly, the (raceless, sexless, genderless) technological seems the only aspect of the humanities that has a viable future.**It is not so much that DH is gaining at the expense of these programs (there's no direct correlation) as that something is making it easier to fund DH just as it's getting harder to fund ethnic studies and queer studies. And so far, despite the best of intentions, DH has not done a good job of theorizing either that disciplinary shift or its political implications—let alone "what is an author." That's why I think we should probably get over that aversion to "yack." It doesn't have to replace "hack"; the two are not antithetical. So now, a few questions. First, what are the key theoretical ideas that DHers need to think about? I've proposed words like "narrative," "biopower," and "author." "Medium" seems like another obvious one. But I'm sure others would argue that different concepts lie at the heart of DH—or that, in fact, we need to be considering the non-obvious theoretical concepts. And second, what might a THATCamp Theory look like? (Besides the obligatory black turtlenecks, obviously.) I've often thought we needed humanities-based bootcamps on (e.g.) narrative, time, and surveillance. But I could also imagine sessions that look at different mapping projects in light of critical theories of space, or or that consider the interstitiality of iPhone apps and Twitter in light of queer and feminist theorizations of time. "Cecire," you might be thinking, "that sounds a hell of a lot like media studies, not DH." Fair. But perhaps that division itself is overdue for some repositioning. Perhaps a THATCamp Theory would take some of the theoretical questions posed by Alexis Lothian and her co-panelists, and lead to digital projects (the "building" that we are so fond of placing at the center of DH) shaped by those considerations. And as Maria observed to me backchannel, "this need not be for theory wonks only, but for anyone who can step back and get meta (which *should* be all of us - regardless of training)." Over the last several months, I've found myself returning to the Harlem Renaissance as a metaphor for DH. In part it's because DH seems to have the same sorts of identity crises that the Harlem Renaissance did. "What is DH?" is the question we still constantly ask ourselves—not in the "I know it when I see it" way that we ask "what is modernism?" but sincerely. Similar to the Harlem Renaissance, too, is the compulsive self-listing, self-mapping, self-visualizing, and general boosterism of (e.g.) totting up the number of DH panels at this year's MSA, MLA, ASA, AHA, etc., comparing this year's number of DH panels to last year's, comparing the MLA to the AHA, und so weiter. It reminds me of Alain Locke's lists of black writers—look how many we have! Have we not arrived? And apart from Hughes and a few others, we see in the Harlem Renaissance a good deal of the target of Hughes's satire, Art plus Gladys Bentley—painfully derivative capital-A Art, glued to some of that Harlem vogue. The comparison breaks down, of course. DH is not historically or substantively similar to the Harlem Renaissance, and in particular lacks the moral and political force of the Harlem Renaissance's sometimes misguided but deeply consequential efforts. But the way that the comparison breaks down is perhaps as important as the ways in which it holds. For one thing, it makes it all the more surprising when "the (raceless, sexless, genderless) technological" is rather unselfconsciously represented as somehow beleaguered in just the same way that women, the working class, and minorities have been.*** To note the internal tensions that the Harlem Renaissance and DH share is to raise the question: why does DH as a disciplinary formation—incongruously—seem to have so many tics in common with the Harlem Renaissance? What is the moral and political force of DH—what are its cultural and institutional consequences? Are we content to suppose that it has no such force, or ought we not inquire? Langston Hughes is right. Art plus Gladys Bentley is not going to get us where we're going, and the problem isn't Art, and it isn't (the queer black woman artist) Gladys Bentley; it's the plus. It's time for THATCamp Theory. UPDATE. There's nothing having a post retweeted to remind you that most conversation on the web does not happen via blog comments. Here are a number of related links: Via Miriam Posner, Boone Gorges's G+ post "Dude ranchin' at THATCamp" Matt Gold reminds us that his forthcoming edited collection Debates in the Digital Humanities takes up some of these concerns. Jentery Sayers observesthat THATCamp PNW (Social Justice) also seeks to address these issues. "Regarding DH convo about theory: #THATCamp PNW (Social Justice) will have 4 workshops blending cultural crit & tech practice. Details soon." @THATCamp also points out the Theorizing the Web conference. * * * Hughes, Langston. The Big Sea. Introd. Arnold Rampersad. 1940; New York: Hill and Wang, 1993. Print. Title: Hughes's "Art plus Gladys Bentley" line comes from a chapter in The Big Sea titled "When the Negro Was in Vogue." David Levering Lewis adapted the title for his 1989 history of the Harlem Renaissance, When Harlem Was in Vogue. *I'm going to use the word "theory" a lot in this post. I mean it as a catch-all term for thinking through the philosophical and cultural consequences of things, rather than the 1980s theory wars caricature known as capital-T "Theory." I love me some Derrida, but that's not really the point. **I, too, would rather be a cyborg than a goddess, but I can't help noticing from time to time that I am, in fact, a woman. ***The resonances here with what Tim Yu has called "the ethnicization of the avant-garde" are notable. Thanks to Maria for a productive conversation on these subjects the other night. Thanks to Aaron for comments on an earlier draft of this post.Perhaps this has occurred to others, but I was thinking this morning of the word object and the many other words it shares -ject with: project, deject, eject, reject, inject, interject ... perhaps you can think of others. Like "object," most of these words serve as both noun and verb (sometimes as adj.). In most of the etymologies, -ject comes from the Latin jacĕre meaning to throw. In the case of object, the inherited meaning is to throw something before the mind or senses. I suppose the etymology, in this case, could be viewed as unfortunate for OOO, which clearly presents objects as mind-independent, and certainly the evolved and contemporary understanding of object is mind-independent, though even being "objective" still requires a mind. However I don't exactly see it that way, particularly given my interested in an object-oriented or speculative rhetoric. While such a position begins with the premise that objects withdraw from one another, it is also a position that investigates how relations occur despite this premise. That is, while objects may withdraw from one another, what we know of each other and the world has to do with what is thrown before us, what we can encounter, which is mind-independent.With that in mind (sorry), my interest turns toward the vector that is implicit in the throw. Or perhaps I should say trajectory. To traject means to cross or also a point of crossing. Although we might say that a trajectory is a particular kind of -jection, so maybe we need a word with a different etymology, like vector. Either way, there is an energetic component to object, as is clear from the verb form. To object is to get in the way, to dissent. We could say that objects in OOO do this to us. They rise in our sensorium and resist. They object to our cognitive apprehension of them. In any case, the object is hardly static. We can't imagine that it just sits there. This understanding might contribute to how one would reconcile the differences between object-oriented positions and more process-oriented positions that arise from Deleuze (or in a very different way, in writing processes, which are of more specific interest to my field). An important point in recognizing objects have a vector or trajectory is that this does not mean that they have a destiny or destination. Instead, it is simply to state that objects have forces and these forces encounter one another. As I have suggested before, a minimal rhetoric might be understand as the study of forces, not, obviously, in the physics sense but rather as the study of forces that result in thought or agency.Now to think about an eject-oriented ontology. I don't mean this in the sense of ejection but rather in the way that the ubiquitous e appears before e-book, e-learning, e-mail, etc. In other words, I'm thinking about the ontological force relations pertaining to objects participating in digital networks. While we can speak of a general ontological philosophy, we can also recognize that certain objects have capacities (energies/forces) activated by their participation in particular networks, ecologies, etc. Obviously humans, for example, require certain conditions in order to remain human. As DeLanda remarks in A New Philosophy of Society, communication networks operate as a significant deterritorializing and decoding force for social assemblages. The e-object or e-ject participates in digital networks in a manner that alters its energetic profile: there is an intersection of forces.My research investigates such matters. Certainly there are objects that we would describe as digital but there are many more objects that are modified by the digital, e.g. digital pedagogy, digital scholarship, and so on. So the monograph was composed out of a particular assemblage to become its own withdrawing object as a printed book has its energetic profile altered by its encounter with the digital, which turns it into a PDF and distributes it online. The PDF is different from the book. Indeed each copy of the book is singular. And yet each book as an object, as that which is thrown before other objects, is altered by its encounter with the digital. That is, the existence of the PDF changes the book as encountered, speculated upon, object. This, in turn, alters the scholarship producing assemblages that compose future monographs.This is the rough text of a short talk I am scheduled to deliver at a symposium on 'Future Directions in Book History' at Cambrdige on the 24th of November 2011.I am on the programme as talking briefly about the ‘OldBailey Online and other resources’ (by which I assume is meant London Lives, Connected Histories, and Locating London’s Past, and the other websites I have helped to create over the last ten or twelve years). But I am afraid I have no interest whatsoever in discussing the Old Bailey or the other websites. The hard intellectual work that went in to their creation was done between 1999 and 2010, and for the most part they have found an audience and a user base and will have their own impact, without me having to discuss them any further. We know how to do this stuff, and anyone can read the technical literature, and I very much encourage you to do so.Instead, I want to talk about how the evolution of the forms of delivery and analysis of text inherent in the creation of the online, problematizes and historicises the notion of the book as an object, and as a technology; and in the process problematizes the discipline of history itself as we practise it in the digital present.The project of putting billions of words of keyword searchable stuff out there is now nearing completion. We are within sight of that moment when all printed text produced between 1455 and 1923 (when the Disney Corporation has determined that the needs of modern corporate capitalism trumped the Enlightenment ideal), will be available online for you to search and read. The vast majority of that text is currently configured to pretend to be made up of ‘books’ and other print artefacts, But, of course, it is not. At some level it is just text – the difference between one book and the next a single line of metadata. The hard leather covers that used to divide one group of words from another are gone; and every time you choose to sit comfortably in your office reading a screen, instead of going to a library or an archive, while kidding yourself that you are still reading a ‘book’, you are in fact participating in a charade. We are swimming in deracinated, Google-ised, Wikipedia-ised text.In other words, and let’s face it: the book as a technology for packaging and delivery, storing and finding text is now redundant. The underpinning mechanics that determined its shape and form are as antiquated as moveable type. And in the process of moving beyond the book, we have also abandoned the whole post-enlightenment infrastructure of libraries and card catalogues (or even OPACS), of concordances, and indexes and tables of contents. They are all built around the book, and the book is dead.If this all sounds rather doom laden and apocalyptic – and no doubt we could argue about the rosy future and romantic appeal of the hard copy book – it shouldn’t. At least as far as the ‘history of the book’ is concerned these developments have been entirely positiveFirst, it has allowed us to begin to escape the intellectual shackles that the book as a form of delivery, imposed upon us. If we can escape the self-delusion that we are reading ‘books’, the development of the infinite archive, and the creation of a new technology of distribution, actually allows us to move beyond the linear and episodic structures the book demands, to something different and more complex. It also allows us to more effectively view the book as an historical artefact and now redundant form of controlling technology. The 'book' is newly available for analysis.The absence of books makes their study more important, more innovative, and more interesting. It also makes their study much more relevant to the present – a present in which we are confronted by a new, but equally controlling and limiting technology for transmitting ideas. By mentally escaping the ‘book’ as a normal form and format, we can see it more clearly for what it was. And to this extent, the death of the book is a fantastic and liberating thing – the fascism of the format is beaten.At the same time, I think we are confronted by a profound intellectual challenge that addresses the very nature of the historical discipline. This transition from the ‘book’, to something new, fundamentally undercuts what we do more generally as ‘historians’. When you start to unpick the nature of the historical discipline, it is tied up with the technologies of the printed page and the book in ways that are powerful and determining. Our footnotes, our post-Rankean cross referencing and practises of textual analysis are embedded within the technology of the book, and its library.Equally, our technology of authority – all the visual and textual clues that separate a CUP monograph from the irresponsible musings of a know-nothing prose merchant – are slipping away. While our professional identity – the titles, positions and honorifics – built again on the supposedly secure foundations of book publishing – is ever less compelling. So the question then becomes, is history – particularly in its post-Rankean, professional and academic form - dead? Are we losing that beautiful disciplinary character that allows us to think beyond the surface, and makes possible complex analyses that transcend mere cleverness?And on the face of it, the answer is yes – the renewed role of the popular block buster, and an every growing and insecure emphasis on readership over scholarship, would suggest that it is. In Britain we shy away from the metrics that would demonstrate ‘impact’ primarily because we fear that we may not have any.Collectively we have put our heads in the sands, and our arses in the air, and seemingly invited the world to take a shot. A single and self-evident instance that evidences a deeper malaise is our current failure to bother citing what we read. We read online journal articles, but cite the hard copy edition; we do keywords searches, while pretending to undertake immersive reading. We search 'Google Books', and pretend we are not.But even more importantly, we ignore the critical impact of digitisation on our intellectual praxis. Only 48% of the significant words in the Burney collection ofeighteenth-century newspapers are correctly transcribed as a result of poor OCR. This makes the other 52% completely un-findable. And of course, from the perspective of the relationship between scholarship and sources, it is always the same 52%. My colleague Bill Turkel, describes this as the Las Vegas effect – all bright lights, and an invitation to instant scholarly riches, but with no indication of the odds, and no exit signs. We use the Burney collection regardless – not even bothering to apply the kind of critical approach that historians have built their professional authority upon. This is roulette dressed up as scholarship.In other words, we have abandoned the rigour of traditional scholarship. Provenance, edition, transcription, editorial practise, readership, authorship, reception – the things we query issues in relation to books, are left unexplored in relation to the online text we actually read.And as importantly, the way we promulgate our ‘history’ has not kept up either. I want television programmes with footnotes, and graphs with underlying spreadsheets and sliders. Yes, I want narrative and analysis, structure, point and purpose. I want to continue to be able to engage in the grand conversation that is history; but it cannot continue to be produced as a ragged and impotent ghost of a fifteenth century technology; and if we don’t do something about it, we might as well all go off and figure out how to write titillating tales of eighteenth-century sex scandals, because at least they sell.The book had a wonderful 1200 odd year history, which is certainly worth exploring. Its form self-evidently controlled and informed significant aspects of cultural and intellectual change in the West (and through the impositions of Empire, the rest of the world as well); but if, as historians, we are to avoid going the way of the book, we need to separate out what we think history is designed to achieve, and to create a scholarly technology that delivers it.In a rather intemperate attack on the work of Jane Jacobs, published in 1962, Louis Mumford observed that:‘… minds unduly fascinated by computers carefully confine themselves to asking only the kind of question that computers can answer and are completely negligent of the human contents or the human results.’I am afraid that in the last couple of decades, historians who are unduly fascinated by books, have restricted themselves to asking only the kind of questions books can answer. Fifty years is a long time in computer science. It is about time we found out if a critical and self-consciously scholarly engagement with computers might not now allow us to more effectively address the ‘human contents’ of the past.Luke Dearnley and I were last minute additions to the Web Directions South lineup last week. Coaxed by Maxine Sherrin to do a ‘fireside chat’ we sat comfortably by a digital fire and talked broadly around some of the exciting projects that are happening in the digital heritage space right now.We tried to cover a lot of ground and tease out some of the issues in the sector as libraries and museums around the world finally begin to build significant momentum around digital content. Taking these discussions to the web developer community is important because all this is happening at a time when the government is calling for discussion of the National Cultural Policy where there is talk about ‘emerging technologies’ and the NBN in the ‘arts’. (See the Ideascale on the digital culture response to the NCP.)Here’s a brief rundown of what we covered in our free-wheeling talk done without notes (and, sadly, much sleep).I started out looking at where we were at the Powerhouse in 2001. Back then we were talking about the ‘virtual museum’ and exploring 3D tours and building monolithic encyclopaedic resources using our ‘authority’. Whilst there was some amazing stuff built back then, that won awards (and we still get enquiries about), the web has changed.And now where we are in our thinking in 2011.Now it is all about being a data provider, getting the our knowledge and collections out into the community where they can be debated and gather feedback and attract interest. The social web and now the mobile web has made this possible at the kind of scale that wasn’t possible in 2001. At the same we now have ‘contextual authority’ rather than what we previously imagined was ‘overall authority’. Remember that in 2001 Wikipedia was only just starting and had only 6,000 articles.At the same time the user is firmly in control not only of how they navigate ever growing competing information sources, they also are using interfaces that fundamentally change how they perceive their computing devices. Touch and now voice interfaces, radically personalise, even anthropomorphise our devices. They are carried closer to us than ever before, creating a sense of intimacy and helping us form (unhealthy?) relationships with our mobile technologies. (“Excuse me while I just check my iPhone one more time – I haven’t touched it in the last five minutes.”)In the background of this slide you can see an early heat map that is produced by tracking the dwell time of visitors carrying wifi devices in one of our exhibitions (they don’t even need to be connected to our wifi to be picked up). I’ll be blogging about that shortly in a new post but for now it should serve as a reminder that this sense of personal connectivity comes at a high price of personal trackability. It isn’t simply bundled up under ‘privacy’ and there’s a long way to go in the public discussions and debate about the trade off between utility and privacy.The other big change is that of scale.A collection like that of the Powerhouse used to feel ‘large’ but in actual fact it is tiny. It’s value in the digital space now is no longer as an island but only in what it can contribute to national and international collections – a collection of collections. That’s a tough challenge for a State-funded museum whose majority of ‘visitors’ walking in the door live in Sydney.But at scale new possibilities emerge.At this point we started to look at some of the initiatives that are exciting us around the world at the moment. Initiatives where the ‘value’ wasn’t necessarily obvious at the beginning but emerged only after time.We showed and talked about ->- Tim Sheratt’s work with the digitised newspaper collections in Trove and the emergent stories he is starting to knit together by analysing the changes in language in newspaper articles over time, or by facial recognition in archival collections. These stories are only possible at scale – and even now they are terribly incomplete with uneven digitisation of each State’s newspapers in Trove – but they are getting better over time. Everyone (even you, dear reader) needs to go an read the transcript of Tim’s recent keynote at ANZSI. We are at the very very beginning of this but Tim’s work hints at some of the possibilities.- New York Public Library’s historical menus project and how marking these menus up in the way they have lets us observe the changes in diet and ingredients, as well as food prices over time. And how, of course, dining at the Possum Club in 1900 would have been quite an experience.- The other thing about the NYPL menus project is the way that, prior to releasing an API, they’ve done what we did at Powerhouse. They’ve released the whole data set as a ZIP. As we found with our own collection, a downloadable full dataset allows people to do mass scale analysis more quickly and easily (and with less drain on your server) than using an API.- Looking at scale we briefly showed the free ImagePlot toolkit from the Software Studies Institute at UC San Diego, and how it by allowing you to do image analysis of enormous corpora of image files new patterns and relationships can be discovered.- Luke talked about linked data and how connecting everything up is slowly becoming possible as more things and thesauri go online. We showed a couple of nice front-end examples of some of the possibilities when collections get connected up. Our very own infant site – the Australian Dress Register – which is slowly growing and bringing on new contributors; and the newly re-designed and re-configured Design and Art Australia Online (formerly Dictionary of Australian Artists Online). Here’s a biographical entry for one of the designers with lots of objects in the Powerhouse collection. Here it becomes possible to traverse her ‘associates’ as well as all the exhibitions etc she has been involved in all over the world.- We looked at some other exciting community transcription projects that are overcoming difficult issues of both relevance and specialised content. We showed the fantastic Old Weather project with the Citizen Science Alliance using old ship logs from the National Maritime Museum to gather geolocated climate data form the past. It is one of our personal favourites and Fiona Romeo at the NMM published a great paper on it at Museums and the Web earlier in 2011 which you should read. What we find really lovely about this project is that it finds deep value in the kind of collection that museums find very difficult to ‘exhibit’. Actual ships – easy and attractive to put in an exhibition but the ship logs – much harder.- We also showed the interface for another Citizen Science Alliance project called Ancient Lives. This project is getting citizens to help transcribe papyrus scrolls from the Oxyrhynchus collection whose story of acquisition and discovery is enough to encourage you to give it a go.In wrapping up we started to ask a number of questions that remain unanswered/unanswerable:- what the barriers to a Europeana-like project are in Australia, let alone a Digital NZ? Are they more cultural reasons than anything else? What is of ‘national significance’ that we can all agree upon? Is such agreement even possible in a fragmented nation?- does the ‘open’ in linked open data matter more than just linked data in the short term?- are libraries able to knuckle down and focus on digitisation better than museums because they aren’t expected to ‘also do exhibitions’? This looped back to an early slide where we talked about the ‘post-web accord’ that emerged in the mid 00s. Is this accord coming under pressure as a result of changing economic circumstances? Or is this just one of the many museum challenges that are under discussion in the sector.Tags:Last week I presented at the Great Lakes College Association’s New Directions workshop on digital humanities (DH), where I tried to answer the question “Why the digital humanities?” But I discovered that an equally important question is “How do you do the digital humanities”? Although participants seemed to be excited about the potential of digital humanities, some weren’t sure how to get started and where to go for support and training.Building on the slides I presented at the workshop, I’d like to offer some ideas for how a newcomer might get acquainted with the community and dive into DH work. I should emphasize that many in the DH community are to some extent self-taught and/or gained their knowledge through work on projects rather than through formal training. In my view, what’s most important is being open-minded, experimental, and playful, as well as grounding your learning in a specific project and finding insightful people with whom can you discuss your work.Determine what goals or questions motivate you. As with any project, a research question, intellectual passion, or pedagogical goal should drive your work. Digital humanities is not technology for the sake of technology. It can encompass a wide range of work, such as building digital collections, constructing geo-temporal visualizations, analyzing large collections of data, creating 3D models, re-imagining scholarly communication, facilitating participatory scholarship, developing theoretical approaches to the artifacts of digital culture, practicing innovative digital pedagogy, and more.Get acquainted with the digital humanitiesThe CUNY Digital Humanities Resource Guide, which was produced collaboratively, offers an excellent introduction to digital humanities, covering sample projects, syllabi, “hot topics,” journals, and more.Ask or answer a question on DH Questions & Answers, a “a community-based Q&A board” where people weigh on everything from designing a digital history curriculum to computational analysis of perspective in art.Look through Blackwell’s Companion to the Digital Humanities, which collects essays from some leading thinkers in DH (and based on my preliminary research seems to be the most commonly assigned text in DH courses). Another frequently assigned text is Dan Cohen and Roy Rosenzweig’s Digital History: A Guide to Gathering, Preserving, and Presenting the Past on the Web.Skim journals in the Digital Humanities, including Digital Humanities Quarterly, LLC, and Digital Studies / Le champ numérique.Participate in the DH community. Frankly, I think that the energy, creativity and collegiality of the DH community is one reason to become a digital humanist.Attend a THATCamp. At a THATCamp, participants spend the first session setting up the agenda, drawing from blog posts they contributed prior to the event. They devote the rest of the time to hands-on workshops and discussions about topics such as pedagogy, text visualization, and collaboration. These inexpensive, interactive, non-hierarchical unconferences typically are organized regionally (Toronto, New England, Bay Area, New Mexico) or by theme (pedagogy, publishing, liberal arts colleges, games, museums). You can learn a lot just by browsing the session proposals and summaries from past THATCamps, so I’m excited that PressForward, the innovative publishing venture from the Center for History and New Media, will soon publish Proceedings of THATCamp. (I enjoyed my THATCamp experience so much that I worked with Andrew Torget and Anita Riley Dryden to organize THATCamp Texas.)Go to a DH conference. The annual Digital Humanities conference sponsored by the Alliance of Digital Humanities Organizations (ADHO), features the latest research and a lively community. Disciplinary conferences such as the Modern Language Association and American Historical Association include a number of DH-related sessions.Participate in–or start up–a regional group, such as Decoding Digital Humanities (with chapters in London, Melbourne, Bloomington & Lisbon), Toronto Digital Scholarship (DISC), Digital Humanities in Boston & Beyond or Digital Humanities Southern California.Support a professional organization in digital humanities, such as the Association for Computers and the Humanities (I’m privileged to serve on the Executive Council) or the Association for Literary and Linguistic Computing.Participate in an online community, such as the Digital Americanists (I’m one!), Digital Classicists, Digital Medievalists, HASTAC, EighteenthCentury.org and Romantic Circles.Take part in crowdsourcing projects that engage the public in contributing to scholarly work, such as Transcribe Bentham.Review work in the digital humanities, such as through the open peer review process for Writing History in the Digital Age.Follow & interact with DH folks on Twitter. Not only is Twitter a great way to keep tabs on what’s going on in the community, but it can help connect you with people, so when you meet them for the first time at a conference you already feel that you sort of know them.Read and respond to blogs.Stay informedI always learn something from ProfHacker, a fantastic group blog focused on teaching, tools, and productivity. (By the way, ProfHacker was hatched at a THATCamp.)Subscribe to the Humanist Discussion Group, which is expertly facilitated by Willard McCarty and has supported conversation and information sharing since 1987.Check out Digital Humanities Now, which features the days’ most-discussed items among DH folks on Twitter and will be re-launching soon (so I learned through Twitter).Follow what people are bookmarking on Diigo or Delicious. (I’m a compulsive bookmarker, but not so good about annotating what I come across.)Explore what’s going on at digital humanities centers. Check out CenterNet, an “international network of digital humanities centers.”Explore examples for inspiration and models. To find projects, see, for example,Pursue training.Workshops and InstitutesThe Digital Humanities Summer Institute, hosted at the University of Victoria, has an excellentreputation and offers week-long workshops on topics such as text encoding, multimedia, Geographical Information Systems, project management, and digital pedagogy, taught by leaders in the field. Scholarships are available, and the ACH provides travel bursaries for graduate students.the Nebraska Digital Workshop, sponsored by the Center for Digital Research in the Humanities (CDRH) at the University of Nebraska–Lincoln (UNL), enables a select group of early career scholars to present their work to and get feedback from senior scholars.NEH Institutes explore key topics in the digital humanities and often cover travel costs. Upcoming institutes focus on text encoding, spatial humanities, linked data, “computational and corpus linguistics methodologies,” and “digital cultural mapping.”the NINES Summer Workshop offers in-depth training to scholars in 19th C British and American literatureOxford offers a week-long summer workshop in DHThe University of Virginia’s Rare Book School often includes sessions of interest to digital humanists, such as ” Born Digital Materials: Theory & Practice,” “XML in Action: Creating Text Encoding Initiative (TEI) Texts” and “Digitizing the Historical Record.”The Women Writers Project (WWP), which has significant expertise in the Text Encoding Initiative (TEI), provides seminars and workshops.Digital Humanities Commons is offering a workshop on “Getting Started in Digital Humanities with DH Commons” at the 2012 MLA conference. While this workshop is full (we were very impressed by the level of interest in it), we hope to hold future workshops, perhaps as part of conferences in addition to MLA. (I’m a member of the DH Commons team.)Online tutorialsOverviews: Stanford’s Tooling Up for DH provides helpful introductions to digitization, pedagogy, data visualization and moreDoing online research: William J. Turkel’s Going DigitalGeographical Information Systems: the Institute for Enabling Geospatial Scholarship’s tutorials on Spatial Humanities, UCLA’s GIS tutorialsText Analysis: TAPoR Portal Recipes (I’m a fan of TAPoR, which provides tools for analyzing and visualizing texts, and a member of the TAPoR advisory group)Text Encoding Initiative/ XML: TEI by Example; the WWP’s Resources for Teaching and Learning about Text Encoding; Laura Mandell, Brian Pytlik-Zillig, Syd Bauman, et al, XSLT-for-Humanists; John Bradley, Elena Pierazzo and Paul Spence, An XSLT TutorialProgramming: William J. Turkel and Alan MacEachern, The Programming Historian; Jason Heppler, The Rubyist HistorianLearn standards and best practices. If you want your project to have credibility and to endure, it’s best to adhere to standards and best practices. By talking to experts, you can develop a quick sense of the standards relevant to your project. You may also wish to consult:Find collaborators. Most DH projects depend–and thrive– on collaboration, since they typically require a diversity of skills, benefit from a variety of perspectives, and involve a lot of work.Making its debut at the aforementioned MLA workshop, Digital Humanities Commons will serve as an online hub (or matchmaking service) linking people, projects and tools. For instance, if you want to learn by doing, you will be able to use DH Commons find out about opportunities to work on existing projects. Beta accounts are now available.Talk with library and IT staff at your own institution. Although many library and IT professionals are necessarily focused on the day-to-day, there is also an increasing recognition that what will distinguish libraries and IT groups is their ability to collaborate with scholars and teachers in support of the academic mission. Be a true collaborator–don’t just expect technical (or content) experts to do your bidding, but engage in conversation, shape a common vision, and learn from each other. (Steve Ramsay offers great advice to collaborators in “Care of the Soul,” and the Off the Tracks Workshop devised a useful “Collaborators’ Bill of Rights.”) If you can bring seed funding or administrative backing to a project, that might make it easier to attract collaborators or garner technical support.Reach out to others in your community. By attending a THATCamp or corresponding with someone who shares your interests, you may discover people who can contribute to your project or help shape a common vision. You could also find a colleague in computer science, statistics or another field who has common research interests and would be eager to collaborate. You might able to hire (or barter with) consultants to help out with technical tasks or provide project advice; I understand that Texas A&M’s Initiative for Digital Humanities, Media, and Culture is exploring offering consulting services in the future to help advance the DH community.Engage students. While there can be risks (after all, students graduate), students can bring energy and skills to your project. Moreover, working on DH projects can give them vital technical, project management, and collaborative skills.Consider a DIY approach. As Mark Tebeau of Cleveland Historical wisely observed at the New Directions workshop, if your institution doesn’t provide the support you need for your DH project, why not strike out on your own? As Trevor Owens suggests in “The digital humanities as the DIY humanities,” it takes a certain scrappiness to get things done in DH, whether that’s learning how to code or figuring out how to set up a server. If you don’t think you have the time or skills to, say, run your own web server, consider a hosted solution such as Omeka. In the long term, it’s a good idea to affiliate with an institution that can help to develop and sustain your project, but you may be able to get moving more quickly and demonstrate the value of your idea by starting out on your own.Plan a pilot project. Rather than getting overwhelmed by trying to do everything at once, take a modular approach. At the New Directions workshop Katie Holt explained how she is building her Bahian History Project in parts, beginning with a database of the 1835 census for Santiago do Iguape parish in Brazil and moving into visualizations, maps and more. This approach is consistent with the “permanent beta” status of many Internet projects. Showing how a project moves from research question to landscape review to prototype to integration into pedagogy, Janet Simons and Angel Nieves of Hamilton’s Digital Humanities Initiative demonstrated a handy workflow and support model for digital projects at the workshp.Where possible, adopt/adapt existing tools, particularly open source software. Too often projects re-invent the wheel rather than adopting or adapting existing tools.Find tools via Digital Research Tools (DiRT) wiki (which I edit and which will soon be overhauled, thanks to the hard work of the fabulous Quinn Dombrowski and Bamboo).SHANTI’s UVa Knowledge Base offers useful information about technologies, teaching, and research approaches. (Aimed at the University of Virginia, but more widely applicable.)You can also poke around GitHub, which hosts code, to identify tools under development by members of the DH community such as CHNM and MITH.NITLE Can HelpIf you’re a veteran digital humanist, how did you get started, and what do you wish you knew from the beginning? If you’re a newcomer, what do you want to know? What worries you, and what excites you? What did I leave out of this overview? I welcome comments.[Updated soon after hitting publish to provide more info about TAPoR. I'm a reviser...]Like this:3 bloggers like this.Innovations in Digital Research: Challenges and Opportunities University of Nebraska Nebraska Digital Workshop October 14, 2011(The following talk was given for the 6th Annual Nebraska Digital Workshop. I’m grateful to Kay Walter and Ken Price for the invitation to serve as a presenter at the workshop and to Susan Brown for participating on the panel, and to Kirsten Uszkalo, Jentery Sayers, and Colin Wilder for their participation in the workshop.)I’m going to talk this afternoon about a central paradox of doing digital humanities–what Jerome Mcgann, one of the leading scholars of electronic texts, calls the problem of imagining what you don’t know.In Digital Humanities, what we think we will build and what we build are often quite different, and unexpectedly so. It’s this radical disjuncture that offers us both opportunities and challenges.As you just heard, my book from Yale came out in September. From the start I thought of the book and the digital project on Railroads and the Making of Modern America as complementary and inter-dependent. The web site was a publicly available platform for assembling research, integrating data, collaborating with other scholars, and experimenting with forms of argument and interpretation.And about a week before the book came out, I received a phone call from someone named Harvey Rochman. Mr. Rochman is a film producer, saw the book listed on Yale’s web site and saw the link to the digital project, and he said he wanted to make a film of . . . The Iron Way.I assured him that such an enterprise could not be undertaken for business purposes. Perhaps, this was some sort of elaborate tax shelter, or a front for something awful, like the illegal trade in exotic species, or maybe it was part of a charity program to benefit poor historians. He was calling from Florida–Key West–which was meant to impress me I think.He said that oh yes he was serious, I could find him in IMDB, he produced a studio film in 2008 [(Misconceptions--about a religiously conservative Southern woman who agrees to be a surrogate mother for two gay men in Boston, one is African American, much confusion and comedy ensue, apparently).] He was working with someone whose name I did not immediately recognize, that means he had not been on Dancing with the Stars recently, no one famous like the guys from Duran Duran, Ally Sheedy, or Kirsty Alley, or someone like that. Anyway, he said in total seriousness that all I needed was 3 acts, 1 to set up the conflict, one to have the conflict, and last a resolution. Three acts. Simple. Let’s see. The Civil War, there’s conflict, but there’s that resolution part. Humm. Railroads, there conflict, conflict, conflict.In any case, what I needed was a “treatment.” In this encounter with the producer, it was clear that everything could be reduced to “a treatment”. It was also clear as I stumbled to explain the digital project and the book–that we were worlds apart. I’d been to Key West but not his Key West. I could not translate the way the book and digital project worked together, the role the digital project played in shaping my argument.More seriously, this interest, however unexpected, does indicate something we also often forget–we work on subjects of great appeal and audiences are enthusiastic and compelled by the stories we tell in history, art, literature, poetry.It turns out that the digital project was my sub sub-library to borrow a phrase from Herman Melville and Moby-Dick. And the work of the sub sub-librarian was one of classification and interconnection–it required getting out in the world too, talking with other collectors and librarians. In a way it is a different scholarly identity.This is a role that in the digital humanities we embrace as public not private–and reverses what had been the practice for generations of guarding your sources and research plans. Here’s a test: who checks that box at the Special Collections sign-in “Yes, I am willing to be contacted about my work”? This public role of collecting, sharing, and opening the sub sub library is one that I have found I cherished.Even so, as Melville warned, the archive “however authentic” offers only “a glancing bird’s eye view of what has been promiscuously said, thought, fancied, and sung of Leviathan, by many nations and generations, including our own.”There was a further problem with the idea of a film script. Much of what I was doing–spatial approaches to history and, even, network approaches to history–was about understanding process rather than explaining causation, it was about exploring the making of modern America. I wanted in the book to hold up a correlation I saw in the archive: between what had been separately told stories–the Civil War and the expansion of railroads. The goal in the book was to explore this correlation and to tell the story of these processes unfolding simultaneously.In this view history is interactive not static, the digital project offers an immersive experience rather than a linear one, a way for readers to participate in the reading of history, in exploring the correlation I was seeing.When we produce a work of scholarship in whatever form, Jerome Mcgann reminds us that “to make anything is also to make a speculative foray into a concealed but wished for unknown.” I assure you that what I wished for in setting out on my digital project was not to produce a movie with Mr. Rochman or any other equally famous director, though I have not ruled out such a venture. This is a guiding principle of DH: keep your options open for as long as possible.So, the digital work that we create, Mcgann tells us, “is not the achievement of one’s desire: it is the shadow of that desire. . .”I am particularly aware of Mcgann’s disjunction right now, (and of Melville’s caution), I suppose, because my project on Railroads and the Making of Modern America is at the end of 5 years. With the Center here, we have created a large digital archive, databases, experimented with visualization models, and produced some scholarly research publications. We have a cohort of graduate students in digital history trained and experienced and new students interested and involved. We have an audience of readers, students, and citizens, the public, general interest reader.But Mcgann’s comment keeps raising its head. What we think we will build and what we build are not the same thing in digital humanities. Our archive might be only a “glancing bird’s eye view.”This is as true of a book or a film as it is of a digital work. But right now, at this moment in the development of the digital medium, I think we can see how far we are from understanding the genre–of how far we are from being able to say send me “a treatment”. The distance between our wish and our object is often so great because the forms and practices and procedures of creation in the digital medium remain profoundly unstable and speculative.Mcgann’s premise might be restated (he might not agree with this, I don’t know): if you have produced what you thought you would, perhaps you’ve not created anything really; if a digital project becomes what was specified it might not be a digital humanities work.We have been asked to speak about challenges and opportunities today. I’ll suggest a few examples from our experience in the last few years with the Railroads project. What we really are asking today is how does scholarly practice change with digital humanities? Or how do we do humanities in the digital age?This is very new. Everything is changing — our audiences, our procedures, our institutions.So, one question we face is in Digital Humanities is:1. IS AN ARCHIVE AN ARGUMENT? and a related question WHERE IS OUR SCHOLARSHIP?Most projects in digital humanities begin as a digital archive, creating a collection of documents that are digitized. I want to encourage this–in the disciplines we need more attention to this work as scholarship. But digital scholars also seek to both assemble and analyze, both examine and interpret.Five million books might be digitized, but the millions and millions of cubic feet of archival railroad records, well that was something else. What is a representative sample of railroad records?We built a digital archive topically arranged for easy access and usability by the widest audience possible. Railroad texts were structurally so dissimilar that we confronted a major classification problem, one that we could not effectively address.The point is that the architecture and encoding of a digital archive–what Johanna Drucker calls “creating the intellectual model”–must be undertaken speculatively. It must be adjusted, changed, explored. Interpretive archives cannot be built to spec.At least in digital history, on one level, it is the diversity of document types that has yet to be fully confronted. We can build models from long runs of legal case files or printed texts or runaway slave newspaper advertisements, but when we turn to a domain such as railroads, or slavery, or genocide, or the family, the intellectual model behind an archive, so often expressed in encoded texts, becomes unwieldy.This challenge is our opportunity–to reconsider the “digital archive” as intentional and interpretive–in our case to offer a new way to encounter the railroad–rather than focus attention on the board room, or the directors, the archive would open up a diverse array of railroad users and interfaces. Its argument would be to expose the ways railroads were used and thought of. We want to create a new history of the railroad.But as we create interpretive archives we need to be able to answer the question: where is our scholarship. This is where we need allies–libraries in particular–as partners in modelling, preserving, and making available this scholarship.The second question we face in digital humanities right now is how do we work differently.2. TEAMS OF SCHOLARS IN THE HUMANITIES:Digital humanities projects are often characterized as collaborative. In many respects this is the most obvious change in scholarly practice–we work with librarians, programmers, and colleagues in other disciplines.The opportunity here seems self-evident. But the model of historical and humanities scholarship has been sole-author, sole-researcher for a long time, and for most universities the evaluation for hiring, promotion, and tenure proceeds to assess candidates on this basis.In the Railroads project I wanted a team of graduate students to have the opportunity to gain experience in digital work, to advance their own scholarship, and where possible to participate in research publications. The challenge for digital humanities now is to make this work count where appropriate. We have begun keeping track of all research publications associated with the project–and we will be co-authoring new articles for the project with teams of researchers. In the early phase of digital humanities we built teams, and teams built projects. But now we are seeing teams contributing to publication streams.The social structures for these contributions are not as yet settled. At the beginning of the project, I had only a vague idea how student colleagues would participate beyond building the digital project. Now, we are beginning to see projects build in publication objectives and contributions at the start.A third question we face in digital humanities right now is:3. WHAT DOES SCHOLARLY ARGUMENT LOOK LIKE IN DIGITAL FORM?My colleagues at the Center for Digital Research in the Humanities and my graduate students in the Department of History patiently bore with me on this one. From the first I hoped to experiment with a new form for our historical interpretative work, and this is what we began to call an “assemblage” or a “view.” The view is a framed set of materials on a given subject that integrates sets of evidence and data around a specific historiographical problem or question, without directly narrating the subject. We wanted the views to inspire investigation and focus attention, to serve as interrelated starting points. We could have hundreds of views that build out of the collection.The tools to assemble a view proved challenging to create–we were after all asking for an authoring tool for the digital medium. The rise of the blog in this same period reduced the incentive for experimentation with scholarly argument and hypertext.The humble footnote is still the mark of scholarship and now we need to consider how we will migrate footnotes–the links and scholarly apparatus of a work–to digital form. This challenge and opportunity is surprising because the web is so good at linking. But we’ve not experimented as much as we could with discursive notes, linking, and narrative argument in digital form.The changes in publication models should be an opportunity. We are on the cusp of a new genre of hybrid digital and print publishing. Books are and will be supported with digital sources and verifiable links to the elements that went into the study. Journals will move into the publication of born-digital work also, integrating print and digital formats.In the humanities scholarly practice might shift toward a more fluid and open exchange of ideas and arguments characterized by a different sequence of activities:from openly available original researchto pre-print presentationto peer review publicationto a period of open verificationto a period of adjustment and re-examination.We know that opportunities and challenges here remain. We are in the early stages of this medium. We should look for ways to enchant readers, to hold attention, and to create long-form argument. Here we might be working against the medium (jumping through links) but the iPad and tablets appear to be opening up new opportunities for our scholarship.Finally, we are in a transition phase. We call what we are doing “digital humanities” or “digital history” but really we are doing humanities in the digital age, we are doing history in the digital age. This work might be characterized increasingly by three qualities:1. increasing the scale of research and data involved: 5 million books, 100,000 newspaper articles–this is the least important characteristic actually because it is limited to scholars, but the challenge will be not only to support this research with infrastructure but to come up with intellectual models for such large scale interpretation. Imagine how these “distant readings” fit in a U.S. history or literature survey.2. addressing the global distribution of discourse and materials: sources all over the world need to be brought together and the challenge will be to create new linkages in the cultural records of the world, from Cairo to Seville to London to Chicago. Language differences, copyright, and sheer distance will need to be overcome.3. using new models of production: we have students as colleagues and citizens as colleagues, and the challenge here will be to validate and credit their contributions, integrate their work, and do so in a way that enables further scholarship.We are doing nothing less than redefining our practices and at the same time the relationship of our society to the past, our literature, history, and culture.Our digital age presents a different medium in which to convey multiple sources of information and to render interpretive arguments. It is instantiating different ways of knowing, different ways of seeing, reading, and learning. What we think we will build and what we build are not the same but we can and should celebrate and inquire into the difference. And one day, when the call comes, you might be able to say, “Sure, I’ll send you my treatment.”Much has been made in our circles about Charles Joseph Minard’s map of the Napoleonic March, but Minard drew his first such graphs for railroads in France and developed his technique in works combining traffic and distances. In 1845 he published what he called his first “figurative map”Minard’s work, however, took more than 15 years to reach the sophistication we so admire. These 15 years years witnessed the vast expansion of railroad culture in Europe and the U.S. Minard experimented with the forms for conveying multiple sources of information, but the disjunction between what he wished to build and what he built took time to resolve.We are, Robert Darton argues, perhaps in a similar position–15 years into what he calls the 4th great Information Age in human history. We are learning how right now how to adjust!An Addendum:In the area of opportunities, I’ve lay out these, rather quickly and without any checking–areas of engagement in Digital Humanities research that are either being funded, or conducted, or appear to take the field in important new directions (these are not in order of any rank–the titles are mine and made up)–and as a word of caution I know little about many of these areas.A.) Big Data and Cyberinfrastructure: of the sort being done in NEH Digging into Data and various Mellon projects. These projects are following the directions largely pointed to in the 2005 ACLS Cyberinfrastructure report (Unsworth et al.). It seems unlikely that this trend will slow down and indeed all signs (n-grams) point to quite the reverse. The project most in the public is “culturomics”– Erez Lieberman Aiden and Jean-Baptiste Michel on “What we learned from 5 million books.” Although their example of “slavery” and its spike in the 1860s and 1960s seems strikingly obvious, the n-gram and big data approach potentially challenge what has been a defining DH practice: building an intellectual model around interpretive digital archives.B.) Brain Science and Humanities: This research would be an area of work suggested by Cathy Davidson’s new book, Now You See It, and the PBS digital nation special last year. Other recent research in this area includes for example the Center for Applied Linguistics’ Brain Research: Implications for Second Language Learning.C.) Performance/Materiality of Scholarship in DH/”Embodied” Research: the latest issue of Digital Humanities Quarterly, we see a piece directly on this subject from Helen J. Burgess and Jeanne Hamming and we have already seen some clear indications of this work in the Nebraska Digital Workshop, its range, significance, and theoretical and methodological tendencies. In fact, across the field of History, we are seeing more research focused on “sensory” history. Margaret Jacobs Bancroft Prize-winning work here at Nebraska on the removal of indigenous children and boarding schools adopts this analytical method. DHers are beginning to push toward broader “embodied” scholarship and explore how the multimedia technology would represent or immerse this scholarship.–Virtual Realities: This could be its own grouping probably but it seems to me to have shifted somewhat as a major area of research and could be subsumed under the above heading. Harvard’s “Mixed Reality City” and UCLA’s Hypercities are next generation virtual reality environments, as are dozens of other GPS enabled encoding projects (Philadelphia, Richmond, Chicago, . . .). These projects have a different set of research objectives from their virtual reality predecessors.D.) Beyond Markup–Hermeneutics and Encoding Theory and Practice: Here I’m way out of my area, but the questions are big and important, and a great deal of Digital Humanities work remains to be done in this area. The problems surround XML encoding, TEI, and the questions of linearity, multidimensionality, fluidity, subjectivity, and multiple perspectives. A good example of this work is presented in Digital Humanities Quarterly by Fiormonte, Martiradonna, and Schmidt “Digital Encoding as a Hermeneutic and Semiotic Act” but Johanna Drucker has been writing about this and so has Jerome McGann, Stephen Ramsay, and Espen Aarseth. The point here is that the structures of XML encoding were designed for information storage and retrieval but we find ourselves in Digital Humanities perhaps as McLuhan stated in 1967 “in the name of progress our official culture is striving to force the new media to do the work of the old.”E.) New Scholarly Formats:here I would refer not to the technologies or social practices to enable open “digital commons” or to defenses of open access, but to the cluster of research agendas around discovering new forms of scholarly communication through multi-use, mixed new media, copyright clearance, mash ups, e books, and experimentation of any kind with the delivery, dissemination, or arrangement of scholarly work. The focus here is on authorship, creativity, design, “new model scholarship”, born digital work, and readership. Some of these efforts have taken place in History–the original AHR articles (mine included), and more recently the Writing History in the Digital Age (underway), and the Hacking the Academy digital cultures project (just released). Stephen Ramsay here has pioneered some of this with the blog to book project. And Douglas Seefeldt has with the Sustaining Digital History project and thinking about short-form digital history scholarship. We are, however, a long way from knowing how this will turn out.F: Interfaces for Humanities: Johanna Drucker has written about this in SpecLab: Digital Aesthetics and Projects in Speculative Computing, and Stan Ruecker has written about this widely. William G. Thomas teaches U.S. history and specialize in Digital Humanities, Digital History, The Civil War, the U.S. South, and Slavery. He currently serves as the Chair of the Department of History at the University of Nebraska-Lincoln and as the John and Catherine Angle Chair in the Humanities. He graduated from Episcopal High School in Alexandria, Virginia, and Trinity College in Connecticut, and earned his M.A. and Ph.D. in History at the University of Virginia.Among the first scholars to work in the emerging field of digital humanities/digital history, Thomas worked with Edward L. Ayers on the Valley of the Shadow Project. He served as a project manager at the Institute for Advanced Technology in the Humanities, as a Teaching with Technology Fellow, and then an assistant professor and Director of the Virginia Center for Digital History at the University of Virginia. He served as Director of the Virginia Center for Digital History from 1998 to 2005. Edward L. Ayers, Anne S. Rubin, and Thomas were awarded the Lincoln Prize in 2001 from the Civil War Institute at Gettysburg College for the Valley of the Shadow project, and the James Harvey Robinson Prize from the American Historical Association in recognition of the project as an outstanding contribution to the teaching of history. The Valley Project continues to be recognized as a pioneering digital humanities project.Thomas was named a Mead Honored Faculty member in the College of Arts and Sciences at the University of Virginia in 2004-05. The Mead faculty propose a “dream project” and are recognized for outstanding undergraduate teaching. Thomas’s dream project was to produce a documentary film with undergraduate students, based on digitized films in his Civil Rights Television News digital history project. The result was “Rising Up”–a student-created documentary film that aired in 2008 on over 20 N.E.T.A. (PBS) affiliated stations–and recognized as a model “authentic learning” project by the EDUCAUSE Learning Initiative.At the University of Nebraska, he has received several fellowships and grants, including a Digital Innovation Fellowship in 2008 from the American Council of Learned Societies, and a highly competitive research grant from the National Endowment for the Humanities Office of Digital Humanities for “Railroads and the Making of Modern America.” He served as the Visiting Professor of North American Studies at the Eccles Centre for American Studies at the British Library, in London, England, in 2008. In 2012 Thomas received the Hazel R. McClymont Distinguished Teaching Fellow Award from the College of Arts and Sciences at the University of Nebraska for outstanding teaching.His recent book The Iron Way: Railroads, The Civil War, and the Making of Modern America (Yale University Press) was a 2012 Lincoln Prize Finalist from the Gilder Lehrman Institute of American History.He is the director or co-director of numerous digital history projects, including  “Railroads and the Making of Modern America” and the Digital History Project. He is leading a new digital project, The History Harvest, aimed at digitizing the nation’s family and community history.Documentary films are intensely collaborative efforts, and I feel fortunate to have had the opportunity to work on several teams. These have focused on the South, Virginia, Civil Rights, and the Civil War. Each one has been different, but I have written, produced, or co-produced:RISING UP“Rising Up: Virginia’s Civil Rights Movement” co-producer with Bill Reifenberger, University of Virginia, The Community Ideas Station, 2007, NETA, 2008. The film broadly covers the South, but concentrates on Virginia and follows major events with close, personal stories, including: Samuel W. Tucker’s 1939 library sit-in, Irene Morgan’s 1946 busing case before the Supreme Court, the school desegregation crisis in 1958-59, the 1960 sit-ins, the violence of Danville and Birmingham in 1963, and the resurgence of black voting and politics in 1965. Asking what made everyday people decide to take a stand in a time of transition and cultural conflict? This is the question at the heart and soul of Rising Up. In asking it and answering it, the film brings a fresh perspective to the civil rights struggle. Aired February 2008 on over 20 NETA affiliated stations. For a short from Rising Up, go to Rising Up–on Southern SpacesMASSIVE RESISTANCE “Massive Resistance” co-produced with George Gilliam, with the Community Ideas Stations, 1999. Massive Resistance earned an Emmy Nomination and has aired numerous times in the years since. The film chronicled the history of school desegregation in Virginia, and the massive resistance campaign to prevent school desegregation in the 1950s. Bringing alive the stories of this searing experience through interviews with black and white Virginians, Massive Resistance explored the student strike of 1951 in Prince Edward County, and the lawsuit that became one of the key cases in the Brown v. Board of Education case. Includes interview with Presidential Medal of Freedom recipient Oliver Hill. Aired September 15, 2000.THE GROUND BENEATH OUR FEET Documentary film series The Ground Beneath Our Feet was produced by Central Virginia Educational Television, with George H. Gilliam, including episodes on “Reconfiguring Virginia,” aired October 1, 1999, “New Deal Virginia,” aired February 26, 1999, “Massive Resistance,” aired September 15, 2000, and “Virginia Fights: World War II,” aired September 1, 2001.The Iron Way: Railroads, The Civil War, and the Making of Modern America with Yale University Press was released in September 2011. The book has been named:A 2012 Lincoln Prize Finalist from the Gilder Lehrman Institute of American HistoryThe 2012 Winner of the New York Book Festival in the History categoryMy next research project, tentatively The Petition: Mima Queen, Black Freedom, and Post-Revolutionary America, looks at newly discovered case files of African Americans in the District of Columbia courts for the period 1800 to 1860, and chronicles the story of Mima Queen’s historic petition for freedom from slavery which came before the U.S. Supreme Court in 1813.Recent books, chapters, and other publications include:“‘Swerve Me?’: The South, Railroads, and the Rush to Modernity” in The Old South’s Modern Worlds: Slavery, Region, and Nation in the Age of Progress ed. by Diane Barnes, Brian Schoen, and Frank Towers (forthcoming from Oxford University Press, April 2011)“Interchange: The Promise of Digital History,” in The Journal of American History, September 2008.“Black and On the Border,” co-author with Edward L. Ayers and Anne S. Rubin, in Slavery, Resistance, Freedom ed. Gabor Boritt and Scott Hancock, Oxford University Press, 2007.“Nothing Ought to Astonish Us: Confederate Civilians in the 1864 Shenandoah Valley Campaign” in The Shenandoah Valley Campaign of 1864, ed. Gary Gallagher, University of North Carolina Press, 2006.“Computing and the Historical Imagination,”  in A Companion to Digital Humanities, ed. Susan Schreibman, Ray Siemens, and John Unsworth, Oxford: Blackwell, 2004.“The Differences Slavery Made: A Close Analysis of Two American Communities,”co-author with Edward L. Ayers, in The American Historical Review. Lawyering for the Railroad: Business, Law and Power in the New South, Louisiana State University Press, 1999.The Civil War on the Web: A Guide to the Very Best Sites, co-author with Alice Carter and Richard Jensen, Rowman & Littlefield, 2000.Essays, Op-Ed articles, Digital works, and Other Writings:Page 99 Test for The Iron Way, The Page 99 Test, October 27, 2011.A Turning Point for Richmond: The Virginia Historical Society’s Civil War Exhibition, June 26, 2011, Southern Spaces“How Do We Remember Our Confederate History?” Roanoke Times, April 17, 2010.“William Jennings Bryan, The Railroads, and the Politics of ‘Workingmen,’” Nebraska Law Review, Vol. 86, 2007.“What is Digital History? A Look at Some Exemplar Projects,” with Douglas Seefeldt, AHA Perspectives, May 2009.“Writing a Digital History Journal Article from Scratch: An Account,”Digital History, 2008.“Shaping Nebraska: An Analysis of Railroad Land Sales, 1870-1880″ with Kurt Kinbacher, Great Plains Quarterly, Vol. 28, 2008.“The Countryside Transformed: The Eastern Shore of Virginia, the Pennsylvania Railroads, the Creation of a Modern Landscape,” with Brooks M. Barnes and Tom Szuba, Southern Spaces, 2007.“Television News of the Civil Rights Struggle: The Views in Virginia and Mississippi,”Southern Spaces, 2004.My great passions in life are my family, my writing and work, and sailing. Collected here are a few photographs of some of the highlights of each of these for me, going way back! I hope you enjoy them.FamilyWith my siblings, in the boxwood at Clarens, Alexandria, Virginia, 1976.With Heather, January, 2009.Work, Writing, and FriendsEd Ayers, me, and Anne Rubin, at the Museum of Frontier Culture, Staunton, Virginia, 1997.With Anne Rubin and Ed Ayers at the Lincoln Prize awards, Union League Club, New York City, 2001.With Ernest “Boots” Mead, at the Mead Honored Faculty Awards, 2005, University of Virginia Rotunda.Valley of the Shadow project reunion, January 7, 2012A lousy photograph but a great time with my Valley of the Shadow friends, from left Scott Nesbit, Andrew Torget, Amy Murrell Taylor, Edward L. Ayers, Anne Rubin, me. At one of my favorite places, Third Coast Cafe, Chicago.Sailing! Learning to sail a Sunfish, Rehoboth Bay, 1973. Freedom ’83 Campaign, on board Spirit of America, off Newport, RI, 1982. I had the great opportunity to be on the team in the summer of 1982, sailing Magic, Spirit of America, and the 1980 Cup winner Freedom.Sailing with my daughter, Lincoln Sailing Club, NE, 2009 (left) and U. S. National Championships, Snipe class, Pensacola Florida, 2009 (right). Reviews of The Iron Way: Railroads, the Civil War, and the Making of Modern America (Yale University Press, 2011).Wes Vernon, in The Washington Times, May 8, 2012.Charles Stephen, in The Lincoln Journal Star, March 11, 2012.Elizabeth Varon, in Civil War Monitor, October 26, 2011Bernard Kempinski, in U.S. Military Railroad Blog, February 9, 2012.Kevin Levin, Blog post, “The Future of Slavery,” Civil War Memory, November 25, 2011. Reviews of The Civil War on the Web: A Guide to the Very Best Sites (Scholarly Resources, 2000)Reviews of Lawyering for the Railroad: Business, Law and Power in the New South (Louisiana State University Press, 1999).Frank G. Queen,  Southern Cultures, 6.4 (2000).Steven Collins, Law and History Review, Vol. 20.2 (Summer 2002).Harold Hyman, Journal of Southern History, November 2001. Florida Historical Quarterly, October 2001. American Historical Review, June 2001. Journal of American History, March 2001.Christopher Tomlins, “How to Succeed in Business? ‘The first thing we do, let’s hire all the lawyers,’” Reviews in American History, September 2000. Journal of Illinois History, Autumn 2000.Virginia Magazine of History and Biography, Fall 2000. Richmond Times-Dispatch, July 23, 2000. Virginia Quarterly Review, Vol. 76, No. 2. Business History Review, Summer 2000. Register of the Kentucky Historical Society, Summer 2000. Georgia Historical Quarterly, Summer 2000.William Childs, H-Net Reviews, June 2000. July 1, 2012“The Pacific Railway Act of 1862: A War Measure,” National Park Service Homestead National Monument, Beatrice, NE, Laws that Built America Celebration.June 9, 2012The Iron Way book talk and the Civil War in Alexandria, Va., Gilder Lehrman Institute for Teachers, Virginia Theological Seminary, Alexandria, Va., and Manassas National Battlefield.June 5, 2012The History Harvest Project and the Civil War 150th,” Daughters of the American Revolution, Omaha, Nebraska.May 18, 2012“The Iron Way” book talk, The Library of Virginia, Richmond, Va., 12 noon.March 29, 2012“Railroads, Art, and the Making of Modern America,” Sheldon Museum of Art.February 18, 2012Railroads and the Making of Modern America,” ProRAIL Nebraska,Milo Ball Student Union, University of Nebraska-Omaha, 9:00 a.m.November 2, 2011Book Talk and Signing, University of Nebraska Bookstore, Lincoln, Nebraska.October 19, 2011OAH Distinguished Lecture, Railroads, the Civil War, and Modern America, Miami University, Ohio.October 14, 2011Innovations in Digital Research: Challenges and Opportunities, Nebraska Digital Workshop, Lincoln, Nebraska.September 23, 2011C-SPAN “The Contenders” series live, on William Jennings Bryan and the 1896 Campaign.June 26-27, 2011Teaching American History, Gilder-Lehrman Institute of American History, Cripple Creek, COJune 8-9, 2011National Endowment for the Humanities, Digging into Data Competition, Presentations, Washington, D.C.April 13, 2011“Railroads, the Making of Modern America, and the Shaping of the Great Plains,” Olson Seminar, Center for Great Plains Studies, Lincoln, NEMarch 17, 2011“African American Mobility after Emancipation,” Organization of American Historians conference panel, Houston, TXFebruary 26, 2011“Railroads and the Making of Modern America,” Railroad History Society, Great Plains Chapter, Grand Island, NEFebruary 16, 2011“The Civil War as a Modern War,” Des Moines Civil War Roundtable, Des Moines, IO, 6:00 p.m. at the Machine Shed RestaurantThese projects continue to grow and change online. All have been deeply collaborative efforts, and represent the teamwork of many scholars, students, and colleagues. I feel grateful to have been a part of these:“The Valley of the Shadow: Two Communities in the American Civil War,” co-author with Edward L. Ayers, Andrew Torget, and Anne S. Rubin, 1993-2008.“Railroads and the Making of Modern America,” University of Nebraska-Lincoln.“Digital History” co-produced with Douglas Seefeldt, University of Nebraska.“The Aurora Project: Spatio-Temporal Tools for History” This project is funded with a National Endowment for the Humanities Digging into Data grant. Co-directed with Richard Healey, University of Portsmouth (UK). University of Nebraska.“Television News of the Civil Rights Era,” Virginia Center for Digital History, University of Virginia.“The Countryside Transformed: The Railroad and the Eastern Shore of Virginia, 1870-1935,” co-produced with Brooks M. Barnes, Virginia Center for Digital History, University of Virginia.My family’s roots are in Virginia and Maryland. I’ve spend most of my life in Virginia, but recently moved to the Great Plains of Nebraska where I teach at the University of Nebraska-Lincoln.I was born in Alexandria, Virginia, in 1964. My father was born in Alexandria, Virginia in 1939, and my mother was born in Norfolk, Virginia. The Thomas side were Welsh immigrants arriving in Virginia, sometime in the early nineteenth century. My middle name is “Griffith,” about as Welsh as it gets. The Griffith Thomases still hold the annual Thomas family reunion at historic Mt. Zion Church in Aldie, Virginia, every June.I grew up on Seminary Ridge in a house at 318 N. Quaker Lane called “Clarens,” originally built in 1781. After the Civil War, the house was Va. U.S. Senator James Murray Mason’s home. Mason was an ardent secessionist, and of no relation to the Thomas family. After Alexandria was occupied by federal troops in 1861, Clarens was a hospital during the war for Union soldiers–it stood adjacent to “Cranford” — my grandfather’s house –and across the Quaker Lane from “Montrose” –my uncle’s house.All three Thomas homes were part of what was called “Fort Williams”–the Union fort next down the Seminary ridge from Fort Worth, a larger installation. Cranford next door was the “officer’s quarters,” and there is still today at that site a full Civil War-era powder magazine with brick walls underground and Union soldiers’ signatures on its walls, many of them barely visible now but clearly marked “1861″. And you can probably find mine and my cousins’ initials on those old walls as well, circa 1971. We played in the powder magazine and on the embankments of the fort that still stood around these houses.I love historic houses, gardens, landscapes, and places. The events of the American Revolution, the Civil War, and the colonial past did not seem so distant to me, growing up at Clarens. The gardens and walkways were lined with huge American boxwood–beautiful, gnarly bushes hundreds of years old. Up on Seminary ridge, from the window in my room, I could see the broad Potomac River in the distance to the east, and Old Town Alexandria with its brick warehouses and cobble stone streets. I could hear the railroads every night moving through Cameron Station to the west. My interest in history came from growing up in Alexandria with its diverse people and their complicated history, and living in and around old houses.When I’m not teaching, writing, or doing history, I’m sailing. I learned to sail at Rehoboth Bay Sailing Association in Delaware. Now, I’m teaching my children to sail at the Lincoln Sailing Club and racing sailboats whenever and wherever I can find open water, a fresh breeze, and good company.Humanities Computing dates back to the use of mainframe computers with museum catalogues in the 1950s. The first essays on Humanities Computing appeared in academic journals in the 1960s, the first conventions on the subject (and the Icon programming language) emerged in the 1970s, and ChArt was founded in the 1980s. But it isn't until the advent of Big Data in the 2000s and the rebranding of Humanities Computing as the "Digital Humanities" that it became the subject of moral panic in the broader humanities.The literature of this moral panic is an interesting cultural phenomenon that deserves closer study. The claims that critics from the broader humanities make against the Digital Humanities fall into two categories. The first is material and political: the Digital Humanities require and receive more resources than the broader humanities, and these resources are often provided by corporate interests that may have a corrupting influence. The second is effectual and categorical: it's all well and good making pretty pictures with computers or coming up with some numbers free of any social context, but the value of the broader humanities is in the narratives and theories that they produce.We can use the methods of the Digital Humanities to characterise and evaluate this literature. Doing so will create a test of the Digital Humanities that has bearing on the very claims against them by critics from the broader humanities that this literature contains. I propose a very specific approach to this evaluation. Rather than using the Digital Humanities to evaluate the broader humanities claims against it, we should use these claims to identify key features of the broader humanities self-image that they use to contrast themselves with the Digital Humanities and then evaluate the extent to which the literature of the broader humanities actually embody these features.This project has five stages:1. Determine the broader humanities' claims of properties that they posses in contrast to the Digital Humanities.2. Identify models or procedures that can be used to evaluate each of these claims.3. Identify a corpus or canon of broader humanities texts to evaluate.3. Evaluate the corpus or canon using the models or procedures.4. Use the results of these evaluations as direct constraints on a theory of the broader humanities.Notes on each stage:Stage 1I outlined some of the broader humanities' claims against the Digital Humanities above that I am familiar with. We can perform a Digital Humanities analysis of texts critical of the Digital Humanities in order to test the centrality of these claims to the case against the Digital Humanities and to identify further claims for evaluation.Stage 2There are well defined computational and non-computational models of narrative, for example. There are also models of theories, and of knowledge. To the extent that the broader humanities find these insufficient to describe what they do and regard their use in a Digital critique as inadequate they will have to explain why they feel this is so. This will help both to improve such models and to advance the terms of the debate within the humanities.One characteristic of broader humanities writing that is outside of the scope of the stated aims of this project but that I believe is worthwhile investigating are the extents to which humanities writing is simply social grooming and ideological normativity within an educational institutional bureaucracy, which can be evaluated using measures of similarity, referentiality and distinctiveness.Stage 3It is the broader humanities' current self-image (in contrast to its image of the Digital Humanities) that concerns us, so we should identify a defensible set of texts for analysis.There are well established methods for establishing a corpus or canon. We can take the most read, most cited, most awarded or most recommended articles established by a particular service or institution from a given date range (for example 2000-2009 inclusive or the academic year for 2010). We can take a reading list from a leading course on the subject. Or we can try to locate every article published online within a given period. Whichever criterion we choose we will need to explicitly identify and defend it.Stage 4Evaluating the corpus or canon will require an iterative process of preparing data and running software then correcting for flaws in the software, data, and models or processes. This process should be recorded publicly online in order to engender trust and gain input. To support this and to allow recreation of results the software used to evaluate the corpus or canon, and the resulting data, must be published in a free and open source manner and maintained in a publicly readable version control repository.Stage 5Stage five is a deceptive moment of jouissance for the broader humanities. It percolates number and model into narrative and theory, but in doing so it provides a test of the broader humanities' self-image.For the broader humanities to criticise the results of the project will require its critics to understand more of the Digital Humanities and of their own position than they currently do. Therefore even if the project fails to demonstrate or persuade it will succeed in advancing the terms of the debate.I just came back from sunny Würzburg where, facilitated by the impeccable organisation of Malte Rehbein and the splendid hospitality of Fotis Jannidis, I have spent a few days with colleagues and friends at the TEI Members' Meeting discussing over the future of the TEI and Philology in the Digital Age. If this Members Meeting will be remembered for something, it will certainly be the chocolate-flavoured keynote delivered by Edward Vanhoutte (the slides are also available). I have given a quite controversial paper there and I have been asked to share slides and content of such paper. As I'm a bit lazy (a.k.a. busy), it will take a while until I'm able to write my considerations down (they will come, I promise!!), but at least I can share the slides: You can read the abstract here (it will take a bit of scrolling, there is no direct linking to the single abstract, sorry, but you can take advantage of the many other interesting abstracts present there!), or down here:In the past years two complementary but somewhat diverging tendencies have dominated the field of digital philology: the creation of models for analysis and encoding, such as the TEI, and the creation of tools or software to support the creation of digital editions for editing, publishing or both (Robinson 2005, Bozzi 2006).These two tendencies are not necessarily mutually exclusive, as the creation of models can represent either the underlying structure or an exporting format for the development of tools. However, these two approaches have often been perceived in opposition, as a dichotomy. On the one hand we have the XML enthusiasts, the editors-as-encoders who apply XML markup to their texts and perhaps also develop publication strategies; on the other hand we have those who support out-of-the-box tools (the ‘magic’ or ‘black’ boxes), who proactively seek the development of fully comprehensive tools that present user-friendly interfaces with the explicit purpose of ‘covering the wires’, in particular hiding the much-abhorred angled brackets. But what are the implications of these positions with respect to the future development of digital (or computational) philology? How realistic is it to ask ‘traditional’ textual editors to turn into encoders? Conversely, how realistic and sustainable is the creation of ‘magic boxes’?In the past I have studied the difficulties and theoretical implications of using a TEI-based editorial model for an editorial team that was highly geographically dispersed (Pierazzo 2010, but presented as a paper in 2008). On that occasion I argued that the development of ‘magic boxes’ is a very ambitious item to have on the digital philology agenda because every edition, every scholar needs a very specialized, tailored set of tools. In the same article I expressed the opinion that, even if the scholars do not feel comfortable in using tags-on-view XML and the TEI, this was the only reasonable approach for digital scholarly editions. A couple of year later, my judgment has been mitigated somewhat. This was brought about largely by the interesting article by Tim McLoughlin (2010, to be read in combination with Rehbein 2010) which presents in an insightful way the difficulties and resistances in turning a consolidated editorial model into a digital TEI-based one, combined with the experience I gained on some collaborative research projects at King’s College London’s Department of Digital Humanities: these together have triggered questions about the role of technology when it comes to digital scholarly editing. As a matter of fact, the evolution of the editor into an editor-encoder has yet to be investigated in full; at the moment it seems that the attention has been mostly devoted to the steep learning curve necessary to master the techniques of encoding in XML but without reflecting on the deep and sometimes unwelcome changes in the editorial work and workload once a new editorial model is undertaken, particularly when that model is based on TEI. This model sometimes sees the editor-as-encoder evolving also in the editor-as-programmer, the editor-as-web-designer and editor-as-(self-)publisher (Sutherland and Pierazzo 2011). These changes in the editorial work and role of the editors necessarily result in somewhat parallel changes in the final editorial products.On the other hand the claim for the magic box seem to have receded somewhat, and we have witnessed the appearance of the interesting experience of creating configurable and standard-based tools that have the less ambitious goal of trying to help particular stages of the editorial work (collation, creation of stemmas and critical apparatus, transcription, annotation); this evolution is represented at best, in my opinion, by the tools developed within the Interedition (in particular with CollateX) and TextGrid projects.This paper will briefly present the background outlined above, and then turn to fundamental issues that arise from it about the nature of editors and editing for digital editions. In particular, it will address the following questions:Which are the competencies necessary for digital editors?Which are the roles that digital editors are expected to cover?What do editors expect the technology to do for them?Which parts of the editors’ work should be assisted by the computer and which must still be performed in the traditional way?In which ways is digital editing different from traditional editing, if any?Failing to understand how technology can really contribute to the editorial work will have serious consequences in the development and ultimately existence of digital editions.The paper will address these theoretical and methodological questions making use of concrete examples, particularly from the Jane Austen Digital Edition and from the ongoing editorial experience of the Early English Laws project.Bibliography:Bozzi, A. (2006). ‘Electronic Publishing and Computational Philology’. In The Evolution of Texts: Confronting Stemmatological and Genetical Methods, C. Macé, P. Baret, A. Bozzi and L. Cignoni (eds.). Pisa-Roma Istituti Editorali e Poligrafici Internazionali.Pierazzo, E. (2010). ‘Editorial Teamwork in a Digital Environment: The Edition of the Correspondence of Giacomo Puccini’. In Rehbein, M. and Ryder, S. (eds.). Jahrbuch für Computerphilologie, vol. 10, pp. 91-110. Also available at: computerphilologie.tu-darmstadt.de/jg08/pierazzo.htmlMcLouglin, T. (2010). Bridging the Gap. In Rehbein, M. and Ryder, S. (eds.). Jahrbuch für Computerphilologie, vol. 10, pp. 37–54. Also available at:computerphilologie.tu-darmstadt.de/jg08/mclough.pdfRehbein, M. (2010). ‘The Transition from Classical to Digital Thinking. Reflections on Tim McLoughlin, James Barry and Collaborative Work’. In Rehbein, M. and Ryder, S., (eds). Jahrbuch für Computerphilologie, vol. 10, pp. 55–67. Also available at: computerphilologie.tu-darmstadt.de/jg08/rehbein.pdfRobinson, P. M. W. (2005). ‘Current Issues in Making Digital Editions of Medieval exts ¬– or, Do Electronic Scholarly Editions Have a Future?’. Digital Medievalist, 1(1). Available at: www.digitalmedievalist.org/journal/1.1/robinson/Sutherland, K., and Pierazzo, E. (2011). The Author’s Hand: from Page to Screen. In Deegan M., and McCarty W. (eds.), Collaborative Research in the Digital Humanities. Aldershot: Ashgate (forthcoming).CollateX: https://launchpad.net/collatexEarly English Laws: www.earlyenglishlaws.ac.ukInteredition: www.interedition.euJane Austen Digital Edition: www.janeausten.ac.uk/index.htmlTextGrid: www.textgrid.deThis visualization explores the ups and downs of the Bible narrative, using sentiment analysis to quantify when positive and negative events are happening:Full size download (.png, 4000×4000 pixels).Things start off well with creation, turn negative with Job and the patriarchs, improve again with Moses, dip with the period of the judges, recover with David, and have a mixed record (especially negative when Samaria is around) during the monarchy. The exilic period isn’t as negative as you might expect, nor the return period as positive. In the New Testament, things start off fine with Jesus, then quickly turn negative as opposition to his message grows. The story of the early church, especially in the epistles, is largely positive.MethodologySentiment analysis involves algorithmically determining if a piece of text is positive (“I like cheese”) or negative (“I hate cheese”). Think of it as Kurt Vonnegut’s story shapes backed by quantitative data.I ran the Viralheat Sentiment API over several Bible translations to produce a composite sentiment average for each verse. Strictly speaking, the Viralheat API only returns a probability that the given text is positive or negative, not the intensity of the sentiment. For this purpose, however, probability works as a decent proxy for intensity.The visualization takes a moving average of the data to provide a coherent story; the raw data is more jittery. Download the raw data (400 KB .zip).Update October 10, 2011As requested in the comments, here’s the data arranged by book with a moving average of five verses on either side. (By comparison, the above visualization uses a moving average of 150 verses on either side.)Full size download (.png, 2680×4000 pixels).Update December 28, 2011: Christianity Today includes this visualization in their December issue (“How the Bible Feels”). This entry was posted on Monday, October 10th, 2011 at 12:43 pm and is filed under Sentiment, Visualizations. You can follow any responses to this entry through the RSS 2.0 feed. Both comments and pings are currently closed.There seems to be a lot of topic modelling going on at the moment. Any why not? Projects like Mining the Dispatch are demonstrating the possibilities. Tools like Mallet are making it easy. And generous DHers like Ted Underwood and Scott Weingart are doing a great job explaining what it is and how it works.I’ve talked briefly about using topic modelling to explore digitised newspapers, something that the Mapping Texts project has also been investigating. But I’ve also been following with interest Chad Black’s use of algorithmic techniques, including topic modelling, to look for local variations amidst the legal system of the early modern Spanish empire.As part of the Invisible Australians project, Kate and I are exploring the bureaucracy of the White Australia Policy. In particular, we’re interested in the interaction between policy and practice, between the highly-centralised bureaucracy and the activities of individual port officials. Like Chad, we’re interested in mapping local variations — to try and understand the bureaucracy from the point of view of an individual forced to live within its restrictions.I recently gave a presentation about the project at Digital Humanities Australasia (post coming soon!), and in preparation I decided to try a few topic modelling experiments. They were very simple, but I was impressed by the possibilities for exploring archival systems.The problem I started with was this. The workings of the White Australia Policy are well documented by records held by the National Archives of Australia. Some series within the archives are specifically related to the operations of the policy — such as those containing many thousands of CEDTs. But there are also general correspondence series created by the customs offices in each state, as well as the Commonwealth Department of External Affairs which administered the Immigration Restriction Act (responsibility was later taken by the Department of Home and Territories and it’s successors). These general correspondence series are important, because they often include details of difficult or controversial cases — those that required a policy judgment, or prompted a change in existing practices. But how do you find relevant files within series that can contain large numbers of items?Series A1, for example, is a correspondence series created by the Department of External Affairs. It contains more than 60,000 items. Past research tells us that amongst these 60,000 files are records of important policy discussions relating to White Australia. But these files tend to be labelled with the names of the people involved, so unless you know the names in advance they can be difficult to find.Mitchell Whitelaw’s A1 Explorer, part of the Visible Archive project, lets you to explore the contents of Series A1 in a easy and engaging way. But while the A1 Explorer provides new opportunities for discovery, it doesn’t offer the fine-grained analysis we need to sift out the files we’re after. And so… topic modelling.The process was pretty simple. While I can dip into my bag of screen-scrapers to harvest series directly from the NAA’s RecordSearch database, there was already an XML dump of A1 available from data.gov.au. So I extracted the basic file metadata from the XML and wrote the identifiers and titles out to a text file, one item per line. Following the instructions on the website I then loaded this file into Mallet:/Applications/Mallet/bin/mallet import-file --input ./A1.txt --output A1.mallet --keep-sequence --remove-stopwordsThen it was just a matter of firing up the topic modeller:/Applications/Mallet/bin/mallet train-topics --input ./A1.mallet --output-state ./A1.gz --output-doc-topics ./A1-topics.txt --output-topic-keys ./A1-keys.txt --num-topics 40Again, I just followed the examples on the Mallet site.Once it was finished I opened up A1-keys.txt to browse the ‘topics’ Mallet had found. The results were intriguing. There are a large number of applications for naturalisation in A1, so it’s no surprise that ‘naturalisation’ figures prominently in a number of the topics. What was more interesting was the way Mallet had grouped the naturalisation files. For example:naturalization christian hans hansen jensen petersen andersen nielsen larsen christensen johannes jens niels pedersen andreas johansen martin jorgensenandnaturalisation certificate giuseppe salvatore frank la leo samios spina sorbello leonardo fisher natale patane torrisi barbagallo luka rossi rossBased on the co-occurrence of names within the file titles, Mallet had created groupings that roughly reflected the ethnic origins of applicants. It makes sense when you think about what Mallet is doing, but I still found it pretty amazing.Mallet also found clusters around the major activities of the department, such as the administration of the territories. But of most interest to us was:1 0.55539 passport ah student exemption students lee wong chinese young deserter education sing wing chong readmission son hing chin wifeThe Chinese names alongside words such as ‘readmission’ and ‘wife’ suggested that this topic revolved around the administration of the White Australia Policy. This was easy to test. In A1-topics.txt was a list of every file in the series and their weightings in relation to each of the topics. I wasn’t sure what was a reasonable cut-off value to use in assessing the weightings, but after a bit of trial and error I fixed on a value of 0.7. I then just extracted the identifiers of every file that had a weighting greater than 0.7 for this topic. I used the identifiers to build a simple web page that Kate and I could browse. I also included links back to RecordSearch so we could explore further.Browse the full listIt’s a pretty impressive result. Instead of fumbling with the uncertainties of keyword searches, we now have a list of more than 1,300 files that are clearly of relevance to Invisible Australians. There’s a few false positives and there are likely to be other files that we’ll have missed altogether, but now we have a much clearer picture of the types of files that are included and how they are described.And that was at my first attempt, simply using the default settings. I’m now starting to play around with some of Mallet’s configuration options to see what sort of difference they make. I’m also keen to try out GenSim, a topic modelling package for Python.I’m really excited about the possibilities of these sort of tools for analysing the contents of archival descriptive systems, something I mentioned in my Digital Humanities Australasia paper. Much more to come on this I suspect…Right now Latent Semantic Analysis is the analytical tool I’m finding most useful. By measuring the strength of association between words or groups of words, LSA allows a literary historian to map themes, discourses, and varieties of diction in a given period. This approach, more than any other I’ve tried, turns up leads that are useful for me as a literary scholar. But when I talk to other people in digital humanities, I rarely hear enthusiasm for it. Why doesn’t LSA get more love? I see three reasons:1. The word “semantic” is a false lead: it points away from the part of this technique that would actually interest us. It’s true that Latent Semantic Analysis is based on the observation that a word’s distribution across a collection of documents works remarkably well as a first approximation of its meaning. A program running LSA can identify English synonyms on the TOEFL  as well as the average student applying to college from a non-English-speaking country. [1]But for a literary historian, the value of this technique does not depend on its claim to identify synonyms and antonyms. We may actually be more interested in contingent associations (e.g., “sensibility” — “rousseau” in the list on the left) than we are in the core “meaning” of a word.I’ll return in a moment to this point. It has important implications, because it means that we want LSA to do something slightly different than linguists and information scientists have designed it to do. The “flaws” they have tried to iron out of the technique may not always be flaws for our purposes.2. People who do topic-modeling may feel that they should use more-recently-developed Bayesian methods, which are supposed to be superior on theoretical grounds. I’m acknowledging this point just to set it aside; I’ve mused out loud about it once already, and I don’t want to do more musing until I have rigorously compared the two methods. I will say that from the perspective of someone just getting started, LSA is easier to implement than Bayesian topic modeling: it runs faster and scales up more easily.3. The LSA algorithm provided by an off-the-shelf package is not necessarily the best algorithm for a literary historian. At bottom, that’s why I’m writing this post: humanists who want to use LSA are going to need guidance from people in their own discipline. Computer scientists do acknowledge that LSA requires “tuning, which is viewed as a kind of art.” [2] But they also offer advice about “best practices,” and some of those best practices are defined by disciplinary goals that humanists don’t share.For instance, the power of LSA is often said to come from “reducing the dimensionality of the matrix.” The matrix in question is a term-document matrix — documents are listed along one side of the matrix, and terms along the other, and each cell of the matrix (tfi,j) records the number of times term i appears in document j, modified by a weighting algorithm described at the end of this post.A (very small) term-document matrix. That term-document matrix in and of itself can tell you a lot about the associations between words; all you have to do is measure the similarity between the vectors (columns of numbers) associated with each term. But associations of this kind won’t always reveal synonyms. For instance, “gas” and “petrol” might seem unrelated, because they substitute for each other in different sociolects and are rarely found together. To address that problem, you can condense the matrix by factorizing it with a technique called singular value decomposition (SVD). I’m not going to get into the math here, but the key is that condensing the matrix partially fuses related rows and columns — and as a result, the compressed matrix is able to measure transitive kinds of association. The words “gas” and “petrol” may rarely appear together. But they both appear with the same kinds of other words. So when dimensionality reduction “merges” the rows representing similar documents, “gas” and “petrol” will end up being strongly represented in the same merged rows. A compressed matrix is better at identifying synonyms, and for that reason at information retrieval. So there is a lot of consensus among linguists and information scientists that reducing the number of dimensions in the matrix is a good idea.But literary historians approach this technique with a different set of goals. We care a lot about differences of sociolect and register, and may even be more interested in those sources of “noise” than we are in purely semantic relations. “Towering,” for instance, is semantically related to “high.” But I could look that up in a dictionary; I don’t need a computer program to tell me that! I might be more interested to discover that “towering” belongs to a particular subset of poetic diction in the eighteenth century. And that is precisely the kind of accident of distribution that dimensionality-reduction is designed to filter out. For that reason, I don’t think literary applications of LSA are always going to profit from the dimensionality-reduction step that other disciplines recommend.For about eight months now, I’ve been using a version of LSA without dimensionality reduction. It mines associations simply by comparing the cosine-similarity of term vectors in a term-document matrix (weighted in a special way to address differences of document size). But I wanted to get a bit more clarity about the stakes of that choice, so recently I’ve been comparing it to a version of LSA that does use SVD to compress the matrix.Comparing 18c associations for "delicacy" generated by two different algorithms. Here’s a quick look at the results. (I’m using 2,193 18c volumes, mostly produced by TCP-ECCO; volumes that run longer than 100,000 words get broken into chunks that can range from 50k-100k words.) In many cases, the differences between LSA with and without compression are not very great. In the case of “delicacy,” for instance, both algorithms indicate that “delicate” has the strongest association. “Politeness” and “tenderness” are also very high on both lists. But compare the second row. The algorithm with compression produces “sensibility” — a close synonym. On the left-hand side, we have “woman.” This is not a synonym for “delicacy,” and if a linguist or computer scientist were evaluating these algorithms, it would probably be rejected as a mistake. But from a literary-historical point of view, it’s no mistake: the association between “delicacy” and femininity is possibly the most interesting fact about the word.The 18c associations of "high" and "towering," in an uncompressed term-document matrix. In short, compressing the matrix with SVD highlights semantic relationships at the cost of slightly blurring other kinds of association. In the case of “delicacy,” the effect is fairly subtle, but in other cases the difference between the two approaches is substantial. For instance, if you measure the similarity of term vectors in a matrix without compression, “high” and “towering” look entirely different. The main thing you discover about “high” is that it’s used for physical descriptions of landscape (“lies,” “hills”), and the main thing you discover about “towering” is that it’s used in poetic contexts (“flowery,” “glittering”).The 18c. associations of "high" and "towering," as measured in a term-document matrix that has undergone SVD compression. In a matrix that has undergone dimensionality reduction with SVD, associations have a much more semantic character, although they are still colored by other dimensions of context. Which of these two algorithms is more useful for humanistic purposes? I think the answer is going to depend on the goals being pursued in a given research project — if you’re interested in “topics” that are strictly semantic, you might want to use an algorithm that reduces dimensionality with SVD. If you’re interested in discourses, sociolects, genres, or types of diction, you might use LSA without dimensionality reduction.My purpose here isn’t to choose between those approaches; it’s just to remind humanists that the algorithms we borrow from other disciplines are often going to need to be customized for our own disciplinary purposes. Information scientists have designed topic-modeling algorithms that produce semantically unified topics, because semantic categorization is important for them. But in literary history, we also care about other dimensions of language, and we don’t have to judge topic-modeling algorithms by strictly semantic criteria. How should we judge them? It will probably take decades for us to answer that question fully, but the short answer is just — by how well, in practice, they help us locate critically and historically interesting patterns.A couple of technical notes: A fine point of LSA that can matter a great deal is how you weight the individual cells in the term-document matrix. For the normal LSA algorithm that uses dimensionality reduction, the consensus is that “log-entropy weighting” works well. You take the log of each frequency, and multiply the whole term vector by the entropy of the vector. I have found that this also works well for humanistic purposes.For LSA without dimensionality reduction, I would recommend weighting cells by subtracting the expected frequency from the observed frequency. This formula “evens the playing field” between common and uncommon words — and it does so, vitally, in a way that gives a word’s absence from a long document more weight than its absence from a short one. (Much of LSA’s power actually comes from learning where a given word tends not to appear. [3]) I have tried various ways of applying log-entropy weighting without compressing the matrix, and I do not recommend it. Those two techniques belong together.For reasons that remain somewhat mysterious (although the phenomenon itself is widely discussed), dimensionality reduction seems to work best when the number of dimensions retained is in the range of 250-350. Intuitively, it would seem possible to strike a sort of compromise between LSA methods that do and don’t compress the matrix by reducing dimensionality less drastically (perhaps only, say, cutting it by half). But in practice I find that doesn’t work very well; I suspect compression has to reach a certain threshold before the noise inherent in the process starts to cancel itself out and give way to a new sort of order.[1] Thomas K. Landauer, Peter W. Foltz, and Darrell Latham, An Introduction to Latent Semantic Analysis, Discourse Processes 25 (1998): 259-84. Web reprint, p. 22. [2] Preslav Nakov, Elena Valchanova, and Galia Angelova, “Towards Deeper Understanding of Latent Sematic Analysis Performance,” Recent Advances in Natural Language Processing, ed. Nicolas Nicolov (Samokov, Bulgaria: John Benjamins, 2004), 299. [3] Landauer, Foltz, and Latham, p. 24.Like this:One blogger likes this.The point of visualization is usually to reveal as much of the structure of a dataset as possible. But what if the data is sensitive or proprietary, and the person doing the analysis is not supposed to be able to know everything about it? In a paper to be presented next week at InfoVis, my Ph.D. student Aritra Dasgupta and I describe the issues involved in privacy-preserving visualization, and propose a variation of parallel coordinates that controls the amount of information shown to the user.Naive ApproachesAs with everything else, there is an obvious solution to this problem that doesn’t work. We started out by looking at the data mining literature, where preserving privacy has been an issue for a while. After running a dataset through an algorithm called k-members clustering, and adapting parallel coordinates to be able to show clusters instead of lines, we ended up with this:While this is obviously useless for visualization, this is the way the data can be passed on to third parties without knowledge about what they are going to do with it, while guaranteeing a minimum level of privacy.A Visualization SolutionBut what if we know a bit more? In this case, what if we know that the user will be looking at the data as a parallel coordinates-like visualization, the axis order in the visualization, and the size of the display? In that case, we can do a bit better:The key to this is realizing what the user can learn about the data from the visualization, and using that to guide the clustering. Traditional clusters break down the data space, but we cluster in visual space: we get different clusters between each pair of axes. We also use a different distance metric to figure out which clusters are best, again based on visual criteria rather than data space. Finally, we use the distribution of values to help keep clusters small while keeping the number of values per cluster the same (which is the key criterion for guaranteeing privacy).The result is much fuzzier than regular parallel coordinates, but that is of course the point. At the same time, we keep the utility of the resulting visualization much higher than just using a standard clustering approach. This is a very tough problem in data mining, because even very weak privacy guarantees make the data all but useless. With our technique, we believe that we can keep the utility much higher while still guaranteeing the same level of privacy.The key to all this is approaching the problem from a visual point of view, rather than from the data. I believe that there is a lot more work to be done not only in privacy preservation, but also more generally in figuring out how to make many existing data mining and other techniques work in a more visualization-centric way.Aritra Dasgupta, Robert Kosara, Adaptive Privacy-Preserving Visualization Using Parallel Coordinates, Transactions on Visualization and Computer Graphics (Proceedings InfoVis), 2011.The paper will be presented in the very first InfoVis session on Wednesday, October 26, 8:30-10:00am.Guy Massie and I recently gave a talk at the Carleton University Art Gallery on what we learned this past summer in our attempt to crowdsource local cultural heritage knowledge & memories. With the third member of our happy team, Nadine Feuerherm, we wrote a case study and have submitted it to ‘Writing History in the Digital Age‘. This born-digital volume is currently in its open peer-review phase, so we invite your comments on our work there. Below are the slides from our talk. Enjoy!Like this:Be the first to like this.The purpose of this ebook is to provide a brief overview of the Ruby programming language and consider ways Ruby (or any other programming language) can be applied to the day-to-day operations of humanities scholars. Once you complete this book, you should have a good understanding of Ruby basics, be able to complete basic tasks with Ruby, and hopefully leave with a solid basis that will allow you to continue learning.The best way to learn Ruby is not by reading this book. The best way to learn any programming language is by hands-on interaction. As you read through the lessons and exercises, I encourage you to write the programs in your own text editor and run them; figure out how things fit together, try changing things in the program, learn what those changes break or improve and understand the reason behind it. Some exercises in this book may seem trivial, others quite complex. My goal is to provide a foundation to help those new to programming (or even those with basic or advanced experience) become comfortable with programming. And don't stop here. I'm barely touching the surface of what can be done with Ruby. I'll point out some additional resources to encourage the burgeoning Ruby enthusiast inside of you as we go along.Before going any further, I want to thank Prof. Stephen Ramsay at the University of Nebraska for being the inspiration for this series. The structure of these posts, the topics of discussion, and some of the examples are directly correlated with his course I took in the Fall of 2010, ENGL 4/878: Electronic Text. Thanks, Steve, for encouraging the hacker in all of us.Why Ruby?So why am I writing about Ruby? Why not some of the other languages I know, such as Python? Or web language like PHP? I'm not suggesting here that Ruby is "the best" language but rather I hope to briefly sketch out the reasons why I think Ruby works as a beginner programming language.All programming languages, like any foreign language, necessarily contain a learning curve. For example, we could compare PHP with Ruby: they have similar structures, syntaxes, and the like, but PHP sometimes throws in syntaxes that require careful distinctions (the difference between sprintf and printf). I believe that simplicity in the syntax of a language makes a huge difference in beginning programmers to grasp concepts. I also greatly appreciate Ruby's simplicity. I'm going to jump slightly ahead for the sake of making a comparison. Let us say we wanted to create an array of authors for a bibliographic program. In PHP, you might write:$authors = array("Hemingway" => 3, "Dickinson" => 1, "Whitman" => 2);$keys = array_keys($authors);sort($keys);$sorted = array_slice($keys, 0, 3);We can achieve the same thing in Ruby much more simply:authors={"Hemingway"=>3,"Dickinson"=>1,"Whitman"=>2}sorted=authors.keys().sort().slice(0,3)Don't worry so much here about what exactly is going on, we'll get to that later. But notice how much easier this is to read. This has something to do with Ruby being a pure OOP (object-oriented programming) language versus PHP's bolt-on functionality. The result is Ruby code that is much more readable. But we're getting ahead of ourselves. The point here is to illustrate the simplicity of the Ruby language.Ruby also handles blocks well. Once again, lets compare PHP and Ruby. Imagine we wanted to sort a list of authors. In PHP, we would write:function sort_authors_by_count($a, $b) { if($a -> counts == $b -> counts) { return 0; } return($a -> counts > $b -> counts) ? +1 : -1; } usort($authors, "sort_authors_by_count");Ruby blocks are chunks of code between do . . . end. The Ruby syntax would look like this:authors.sortdo|a,b|a.counts<=>b.countsendOnce again, Ruby is much simpler. Even if you're not exactly sure what is happening, it is much easier to look up the Ruby syntax of <=> rather than try and decipher ? +1 : -1.Finally, everything in Ruby is an object. Ruby was designed as an object-oriented language, which makes writing programs much easier to create. Having everything as an object also makes code easier to handle. There's no need to check and see if something is an object and execute methods upon it. You can simply execute a method. Just as everything is an object, the results of manipulations on an object are also objects. There will be more on this later.We could also ask a broader question, related to the first: why program? Why should historians take the time to learn to program? My answer is in line with Douglas Rushkoff's general warning: program or be programmed. Using tools developed by others puts you at their mercy. Much of our scholarly lives have already become digital: our sources are in digital form, we write in word processors, we communicate through e-mail and Twitter, we place lecture notes on Blackboard, we extend classrooms with blogs. We use these tools without really understanding how they do what they do. I'm offering a glimpse into this world and hopefully equipping you with a set of tools that will be readily useful in your scholarly work.Wayne Graham has an entire list of why Ruby makes a great beginner language that I would also recommend checking out.The SetupI'm writing this for people who have access to a UNIX environment. If you are on Linux or Mac, you have this accessible to you already: simply fire up the terminal and you're ready to go. Ruby comes preinstalled on most Linux distributions and on Mac OSX 10.5+. On Windows, you'll want to download Cygwin, a UNIX-like environment for Microsoft Windows. UPDATE: Reader Gordon Thiesfeld recommends Windows users check out RubyInstaller over Cygwin.You'll also need a good text editor that you know your way around in. I work almost entirely in vim (or mvim). You might check out emacs or nano, or do your programming outside the terminal using TextMate (Mac), gEdit (Linux), or Notepad++ (Windows), or any other number of text editors. I would encourage you to find an editor that handles syntax highlighting, if only for making the code easier to read. And get ready for some battles.You could also set up an IDE, or integrated development environment. I would follow the steps in William Turkel's The Programming Historian to install Komodo Edit (but ignore the extensions for Firefox), with a few changes for the appropriate programming language. I can also highly recommend NetBeans as a really useful IDE system if you prefer this route. I won't be going through that setup here -- if you really want the instructions, drop me an email.Our First ProgramLet's get started! It is traditional to start programming in a new language by writing something that says "hello world" and terminates. The language we are using is interpreted (as opposed to compiled), meaning that a special computer program known as an interpreter reads the instructions from Ruby and then runs the program. There are two ways to run Ruby. The first is by running Ruby interactively in the shell prompt. Simply type irb into the command line to open the Ruby shell. Simply type in Ruby code and it will return the value of expressions under evaluation. Exit irb by typing exit or using the end-of-file character on your OS (normally Ctrl+D or Ctrl+Z). Alternatively, you can write these programs as files to your local disk or to a server and run them through the terminal. This is the preferred method for writing Ruby programs. In my case, I'll be running these programs locally through the terminal. I'll demonstrate briefly how irb works and looks, but all subsequent examples and programs will be written as files.Continuing with our comparative approach, generating "hello world" is a fairly straightforward process in many languages. In PHP, it looks like this:Ruby operates similarly:If you're running this in the interactive Ruby shell, you should see something like this:irb(main):001:0>puts"Hello world"Helloworld=>nilIf you're running Ruby files off a server or local disk, save the file as hello.rb and in the terminal run:Note the lack of parens in my puts function. Parentheses are absolutely accepted Ruby syntax, but you must make a choice between a parens or a space. puts("Hello world") and puts "Hello world" are the same thing, but you cannot do puts ("Hello world"). I tend to leave out parentheses unless I'm passing variables through a method.It is common practice to also include the "shebang" notation (#!) in the first line of the program, followed by introductory comments that usually include the name of the file, a description of what the program does, who wrote it and for what, and when it was last modified. Commented text is marked by #. For example, a "hello world" program might look like this:#!/usr/bin/ruby -w# helloworld.rb## Basic "hello world" program## Written by Jason A. Heppler for# The Rubyist Historian ebook project## Last modified: Tue Dec 28 21:21:43 -0600 2010puts"Hello, world!"puts"I became a Ruby programmer on #{Time.now}"Running ruby helloworld.rb in the terminal will return:Hello, world!I became a Ruby programmer on Tue Dec 28 21:21:43 -0600 2010And there you have it, your first Ruby program! But let's make things a little more interesting. Instead of just pushing static data, let's have Ruby work with data we give it through what's known as standard streams. For this we're going to use the methods gets() and chomp():puts"Please enter your name: "name=gets().chomp()puts"I, #{name}, began learning Ruby code on #{Time.now}."This will print to the screen:I, Jason, began learning Ruby code on Tue Dec 28 21:21:43 -0600 2010.Note the new notation #{}. By asking for an input we are using what is called interpolation, or passing a variable into a string. Variables are enclosed in #{var}. Take note that strings can be marked off by single or double quotes, but there is a distinction between their use. In order to interpolate, you must use double quotes. Single quotes will not allow interpolation, which has to do with Ruby attempting to optimize the code and [redacted boring technical jargon].There you go! Your first Ruby program that works with user data. Up next, we're tackling methods and classes.Additional ResourcesVisit the Rubyist Historian Table of Contents for more sections, and check out the Github repository for an archive of all the code examples.See something that's wrong? Examples that don't work? Explanations that are unclear or confusing? Embarrassing typographic errors? Drop me an email at jason.heppler+feedback at gmail and I'll fix things right up!Topic structure, examples, and explanations for the Rubyist Historian are inspired by, credited to, and drawn from Stephen Ramsay and his course Electronic Text.Creating a Hive of Activity: Why we need to adopt APIs for Digitised ContentPresentation from the 3rd EBLIDA-LIBER Workshop on Digitisation, October 2011Share and Enjoy October 13, 2011 | Filed Under Uncategorized | Author Alastair DunningCommentsThe following is a guest post from Trevor Owens, Digital Archivist with the Office of Strategic Initiatives.I’m excited to share this third interview for Insights, an occasional feature of The Signal sharing interviews and conversations between National Digital Stewardship Alliance Innovation working group  members and individuals working on projects related to preservation, access and stewardship of digital information.For our third interview, I am thrilled to have a chance to chat with Brett Bobley, the CIO and Director of the Office for Digital Humanities at the National Endowment for the Humanities. I wanted to catch up with him on how some of the work NEH is supporting under the Digging into Data grants might connect with issues around the preservation and access of digital content.Trevor: One of the repeated themes at the recent National Digital Stewardship Alliance meeting was the idea that digital content must be used to be preserved. In one strand of this thinking, Helen Hockx-Yu of the British Library stressed a need for librarians, archivists and curators working with web archiving to move away from document centric approach to web archives and start approaching them as corpora. As an example of the implication of this line of thinking, the British Library launched an n-gram viewer that actually acts as a search interface for the content of their web archives. It seems to me that Helen’s point about web archives has much broader implications for the future of digital preservation and access. As libraries, archives, and museums are increasingly gathering large sets of information, all of those sets of content can be thought of as both individual objects and as corpora. Given this context, could you give us some examples of how some of the projects from the first round of digging into data (or other related work you have seen come through the Office of Digital Humanities) that have approached bodies of digital content that we might think about as individual objects of study in a collection but that offers interesting insights for institutions stewarding digital collections?Brett: Sure. I think this is an important point that really gets at the heart of the Digging into Data Challenge. At our recent conference in June, I kicked things off by suggesting to the audience that “our ability to digitize materials has outstripped our ability to analyze them.” We’ve gotten quite good at scanning stuff — we can build big collections. But we haven’t changed the way we do research accordingly. We still tend to take the document centric approach — except we now have far more documents and no great way to find the ones of interest.One Digging project that tackled this head-on is the Criminal Intent Project, which is a US-UK-Canadian team working with the Proceedings of the Old Bailey collection. The Old Bailey site collects 197,745 criminal trials held at London’s central criminal court between 1674-1913. It is a remarkable resource for historians. The Criminal Intent team wanted to come up with a new way for users to view this collection. Not only to help them drill down to cases of interest, but also to see trends across time and across cases. They built a new API for the website that allows the user to take advantage of sophisticated tools like Voyeur, Tapor, and Zotero and — in my opinion — makes for a much more powerful environment for using the collection, both as a corpus and as a group of documents.Trevor: Do you have any advice for stewards of digital collections who would like to see their collections more actively used by researchers? There has recently been a lot of discussion about linked open data, many institutions already support the open archives public harvesting methods, and in the same spirit meetings like the NEH funded digital humanities API workshop have suggested approaches to providing cultural heritage collections through other methods, like REST APIs. As all of the Digging into Data grants involve international collaborations between multiple institutions I would be interested to know if and how any of these approaches played a role in those collaborations? More specifically, what kinds of approaches did grant winners take to enable collaborative work with their collections and what kinds of implications do you see from their approaches for stewards of digital collections who want to have their collections used for this kind of research?Brett: I’m glad to see so much interest in this area — linked open data, APIs, etc. If the past ten years was the decade of digitization, the next ten will be the decade of making collections more usable. I think libraries and archives are so important right now — they’ve got to continue to be leaders in making large digital collections usable, interoperable, and sharable.If you look at the Digging into Data projects from 2009, you’ll see many institutions collaborating in different ways. In many of these cases, the projects were very much about using these sharing, analysis, and visualization techniques to make large collections easier to navigate and understand. Another good example is the Digging into the Enlightenment project, which is about visualizing the correspondence of key Enlightenment figures like Locke, Voltaire, and Bentham, who wrote to each other and exchanged ideas using the social networking platform of their day. There is a great New York Times piece that describes the project.One general piece of advice I’d have to collection holders is to try to be as open as you can with regard to intellectual property rights restrictions. Often, the real value of these collections will only be realized when you allow your data to be harvested and mixed with other collections. If you hold a large digital collection, it can be helpful to have well-defined methods for researchers to use to get at your data in different ways.Trevor: The sessions from the digging into data conference earlier this year sounded fascinating. I am glad to see that the proceedings from the conference are all online and open access. If you were to suggest three must read papers for the conference for individuals working on collecting, preserving and providing access to cultural heritage content what would they be and why do you think they are must reads for this audience?Brett: I’d check out the Digging into Image Data white paper; the Criminal Intent white paper; and the Digging into the Enlightenment white paper.Trevor: I would be interested to know if you think there are any implications of how some of the digging into data work might feed into new modes of access and discovery for digital collections. For example, returning to the example of the UK Web Archive, they implemented the n-gram viewer, originally created as a research tool, as a new search interface to their collection. Do you think there are any potential implications for similar cross-pollination of research tools into new modes of discovery and access? If so, do you you have any thoughts for how some of the initial projects might be models for new modes for discovery and access?Brett: Most definitely. I very much hope that some of the research that comes out of Digging into Data will ultimately work its way into production use on collections. In fact, the API that the Criminal Intent team developed is scheduled to go onto the production version of the Old Bailey site soon (you can use it now, at this web address).My thanks to you and your colleagues at the Library of Congress for your great work. Also, let me thank my funding partners for Digging into Data: NSF, IMLS, JISC, SSHRC, AHRC, ESRC, and NWO. Having eight funders work together on one grant program demonstrates, in my mind, how important this topic is.Here at the University of Virginia a group of scholars from various disciplines meets periodically to discuss digital scholarship. We began meeting a few years ago, and we call ourselves EELS (it stands for “electronically enabled literary scholarship.”) At one of our earliest meetings we read articles by Mark Bauerlein and Nicholas Carr, readings we termed “anti-EELS.” Their arguments are rather played-out by this time, but a few years ago were gaining steam: Carr’s article “Is Google Making Us Stupid?” had just appeared in the Atlantic, and Bauerlein asked in the Chronicle whether online literacy is of a lesser kind (both have since expanded their arguments books). Forget about challenging the place of digital scholarship in the academy: Carr and Bauerlein challenged very idea that the internet and the digital age left us any ability to think at all.Even in 2008, of course, Carr and Bauerlein did not speak for everyone. Bauerlein cites Leah Price, for example, who argues that we need to expand our notions of what “reading” means. The response that most intrigues me, though, is Steven Pinker’s. In a New York Times op-ed Pinker (a cognitive scientist at Harvard) takes issue with Carr’s argument that the internet affects our brains: “cognitive neuroscientists roll their eyes at such talk. Yes, every time we learn a fact or skill the wiring of the brain changes; it’s not as if the information is stored in the pancreas. But the existence of neural plasticity does not mean the brain is a blob of clay pounded into shape by experience.” Pinker goes on to compare Carr to “primitive peoples who believe that eating fierce animals will make them fierce.” By writing “as if the brain takes on the qualities of whatever it consumes” Carr and Bauerlein seem to “assume that watching quick cuts in rock videos turns your mental life into quick cuts or that reading bullet points and Twitter postings turns your thoughts into bullet points and Twitter postings.”Digital humanists have of course found academic uses for Twitter, and might take exception to this particular example. But Pinker’s larger point is an accusation that I think we must take seriously. His response to Carr is, essentially “you just don’t understand the brain”: he draws a boundary between the cognitive neuroscientists rolling their eyes and the poor humanist who can’t tell the brain from the pancreas. So while I applaud Pinker’s argument, the tone has me a bit worried.Many humanists are no stranger to Steven Pinker, who ranks high on a short list of scientists whose work forms the basis of another emerging area of humanistic inquiry: cognitive literary studies. The 1990s was declared “the decade of the brain,” and during the same period in which the digital humanities have become so prominent, a parallel movement has connected humanistic research to brain science. But as Jonathan Kramnick has argued, this scholarship has its risks. While hoping to add a “scientific” basis to humanistic questions, proponents of cognitive approaches sometimes wind up, like Carr, drawing on a field without really understanding it. Literary Darwinism, says Kramnick, might not “bring us any closer to science. At the very least, the substance of the claim fails to represent debates within the sciences themselves.”Like cognitive literary studies, digital humanities must draw on other disciplines, using methods and tools that many humanities scholars aren’t comfortable with. And digital humanities has witnessed similar debates about the extent to which we must immerse ourselves in these other disciplines. Do we, as Stephen Ramsay suggests, have to know how to code, and build things? Do we have to be trained statisticians so that the our text-mining results are “statistically significant? Are we more or less rigorous than the proponents of culturomics, whose work many humanities scholars seem skeptical about? These are questions about method, and interdisciplinarity, and collaboration. And they’re not particularly new questions. But I do think the comparison between digital humanities and cognitive literary studies is a useful one: how can tools and methods from other disciplines help us answer questions in our own?As a parting note, I’ll point to Cathy Davidson’s upcoming course, which looks to be a model for interdisciplinary teaching. Perhaps this approach will begin to connect the humanities with the brain and the internet.Browse:Home/Image of the Week – Girl dressed as PierrotBy ottelizabeth on October 18, 2011This photograph from the Frances Benjamin Johnston Collection at the Library of Congress seems appropriate as we approach Halloween.Posted in Uncategorized | I recently had the experience of being interviewed for a newspaper article about the Rare Book School, where I work part time. The reporter, a forthright sort of journalist with a minimally cynical affect, told me his angle right from the beginning: the book, it seems—that is, the physical book, that persistent little brick of printed pages seen often cluttering the tops of coffee-tables or sagging the center of poorly constructed bookshelves—is dead. In this digital age we live in, we no longer need those moldering tomes to weigh us down. All the heavy weight of books is soon to be lifted, replaced by the ephemeral imp of digital ether.We talked over dinner, a communal affair at RBS, and while the two faculty members to my right argued about current trends in curation and collection, the reporter told me a little about his comic book collection and about reading children’s books to his son on his ipad. He asked me what I thought of his dead-books idea and I told him, gently I hoped, that there are always other angles. Why, I asked, must we perpetuate this battle between book and everything else in the modern world that vies for our attention? Radios, televisions, movies, and now the computer—what if they offer us not an alternative to print, but a series of complements, many voices in a multi-layered harmony. He seemed dubious, but listened politely enough.When the article came out, the print-is-dead premise remained, and I read it in two formats: on paper and online; someone even posted a link to it on facebook. There were of course differences in the two versions—different pictures accompanied the online text, and it had been shortened in such a way that made the prose at times choppy. I wondered which version the reporter preferred: the one that left a few inkstains on the hands of its reader or the one that ended up in my email inbox.So much of digital humanities scholarship has focused on how we embrace the physical world even when our arms are made of circuits and software. This, to me, is what makes the world of the digital humanities so very vitally important—far from living in the cloud, digital humanities has its feet in on the ground, worrying about how we manage data, where we store it, and how we keep it sustainable. It is often theoretical work, but the sort that makes possible the commingling of artifact and simulacra. Digital humanities scholarship can, I believe, if we invest enough of ourselves into it, give us a Grand Unifying Theory that explains why a man whose profession has grown fat on the persistence of print and who lovingly hordes each issue and edition of The Green Hornet first thumbed in childhood raptures would be so gleeful at the demise of the book.Months later, I find myself in conversation with fellow grad students about finding a well-edited digital edition of an essay on Tennyson. We denigrate the poor quality of OCR and trade googling strategies and talk about the difficulties of the facsimile.In the classroom, my students have laptops but also print-outs. Sometimes they even buy the book, and decorate the pages with a veritable rainbow of exuberant post-it notes. They text one another furiously, especially when they think I am not looking.Maybe the book is dead after all. But if this is so I want to be a necromancer, and with the magic of code I want to resurrect the ghosts of books–the real ghost in the machine. By alexgil on October 24, 2011This picture almost brought tears to my eyes. I was born and raised not far from here. The scene depicted in the picture takes place by the Ozama river. The wall on the right side only survives in part. The houses in the front of the picture have long been demolished to make way for tourists. The slightly taller building in the back is the old Atarazana (shipyard). The man walking up the street with a book and the photographer testify to the intellectual vibrancy of the old city. Many contemporary artists and writers continue to hang out around these parts of town.Posted in american studies, photography| Tagged Caribbean, colonial, Santo Domingo | By Dana Wheeles on October 25, 2011The first two documents produced during our 2011 NINES / NEH Summer Institute on Evaluating Digital Scholarship have been released: a Statement on Authorship and a set of Recommendations for Chairs of Language and Literature Departments for creating an atmosphere that is conducive to work in new media. We have others in the pipeline and will be releasing them in the coming months. We welcome readers’ feedback!The Call for Proposals and Participants in our 2012 Summer Institute has also been announced, with a special emphasis on the participation of department chairs. We’ll be revisiting the 2011 documents and refining them, and hopefully creating more guidelines for evaluating digital scholarship.Posted in digital humanities, summer institute | By Dana Wheeles on October 31, 2011After it emerged in the 1860s, thanks to an accidental double-exposure of a portrait taken by photographer William H. Mumler, spirit photography was wildly popular in the nineteenth century. Many believed that those ghostly after-images were actually the camera capturing departed loved ones and other souls from the beyond. While this method has been thoroughly debunked, it is clear that, even in the twenty-first century, photography remains a central tool of ghost hunting.Happy Halloween!Posted in features, photography | By Dana Wheeles on November 8, 2011This print from the Library of Congress is a satirical take on the Glentworth scandal of 1840, in which a tobacco inspector plotted to sway a New York election for the Whigs by illegal means. Originally, the term, “Locofoco” referred to a radical faction of the Democratic Party who were strongly in favor of workers’ unions and anti-monopoly, but by the time of this drawing, the term had been generalized to refer to all Democrats.In an attempt to mock attempts to sensationalize the scandal, the artist has portrayed Glentworth himself as a large cat causing a ruckus in the kitchen. Scullery maid “Miss Whiting” (a reference to New York District Attorney James Whiting) shouts, “The Whigs, The Whigs” as four prominent members of the Democratic Party come thundering down the stairs to intervene.Posted in american studies, features | By heatherbowlby on November 14, 2011Gaston Tissandier (1843-1899) was a French chemist, meteorologist, aviation pioneer, and adventurer. Along with his brother, Albert, he edited the French scientific journal La Nature, which aimed to popularize science. Gaston was particularly interested in ballooning, and during the Franco-Prussian War in September 1870, he made a spectacular escape from besieged Paris by balloon.This illustration from La Nature depicts the aftermath of Gaston’s most daring ballooning feat, in which he was able to reach the unheard-of altitude of 28,000 feet in April 1875. Both of Gaston’s companions, journalist Joseph Crocé-Spinelli and naval officer Henri Sivel, later died from the effects of breathing the thin air. Gaston survived, but became deaf.Undeterred, the Tissandier brothers continued to conduct aviation experiments, and in 1883 they developed the first electric-powered flight by fitting a Siemens motor to an airship.Posted in images, press, Uncategorized | Beyond Accessibility: Textual Studies in the 21st CenturyCall for PapersThe Textual Studies team of INKE (Implementing New Knowledge Environments) wish to invite presentation proposals for Beyond Accessibility: Textual Studies in the 21st Century . June 8, 9, and 10, 2012, University of Victoria, Victoria BC, Canada. Keynote speakers: Adriaan van der Weel (Leiden University) and Sydney Shep, (Victoria University of Wellington)At the end of the 20th century, textual studies witnessed a revolution in accessibility to texts with the explosion of the internet. Now we simply take it for granted that digital processes infuse every step of our study, editing, production, and dissemination of texts. The Textual Studies team of INKE invites presentations that address the questions “What is the state of textual studies in the 21st century? What is the important work of textual studies in the 21st century? What are the outstanding issues, challenges, concerns, emerging trends, methods, attitudes, and exciting developments in textual scholarship? Papers may address such questions as What is the state of the scholarly edition after the transition from print to print and digital? What is the impact on the material book and on book history of the different kinds of access enabled by the digital medium? How have authorship attribution studies been transformed by access to so many more searchable texts? How has the new age of access to materials affected the state of textual studies in various regions of the globe? How well are scholars being served by traditional and emerging infrastructures for the study, creation, production, and dissemination of texts? What is the future of, for example, the study of readership and letter writing, genetic editing, and reception history?INKE is a multi-national, multi-disciplinary research initiative, funded by the Social Sciences and Humanities Research Council of Canada and partnering organizations, to study, develop, and implement digital environments for reading and research (www.inke.ca). The Textual Studies Team of INKE is researching ways in which the age of manuscript and print production can inform our development and implementation of electronic reading technologies.We invite proposals for papers, posters/demonstrations, and roundtable discussions that address these and other issues pertinent to research in textual studies. Proposals should contain a title, a detailed and focussed abstract (of approximately 300 words) plus list of works cited, and the names, affiliations, and Website URLs of presenters. Please send proposals before 15 December 2011 to richard.cunningham@acadiau.ca.Potential participants in the conference, particularly those coming from abroad, might be interested to take advantage of the Digital Humanities Summer Institute, which will just before our conference, from 4-8 June, also at the University of Victoria (http://www.dhsi.org/). A limited number of scholarships for workshop tuition will be available for graduate students participating in the Beyond Accessibility conference. Also of potential interest is the annual conference of the Society for Digital Humanities (SDH/SEMI) at the Congress of the Humanities and Social Sciences at Wilfrid Laurier University, Waterloo, Ontario, 28-30 May, 2012 (http://www.sdh-semi.org/).Following last week’s symposium here at UVA, I found myself recalling Roger Lundin’s essay in Pedagogy from a few years ago: “the teachers who mattered most to me did so because of what they loved,” writes Lundin. “As I taught, in other words, I learned I had come to love what my most effective teachers had loved, and they had taught me how” (137). Lundin, riffing on Wordsworth’s Prelude – “what we have loved, others will love, and we will teach them how” – offers a viewpoint that I think was implicit in many of the discussions. The symposium marks in the inauguration of Institute of the Humanities and Global Cultures, and Steve Ramsay began his talk by praising institutes like this one for providing an opportunity for scholars to live an intellectual life in community with others. Community – which means, people – is as important to academic fields as the theories and methodologies that were the symposium’s explicit focus.Lundin again: “For the past several decades in the humanities, our discourse has been theory-rich, perhaps theory-saturated, and we have developed explanations for everything from the nuances of différance to the needs of the subaltern. But when have we thought about love?” (134). Love of our work, Lundin means, and in a real, non-theoretical sense. A flurry of recent posts (like Natalie Cecire’s and Jean Bauer’s) has considered the place of theory in digital humanities. And perhaps the most important argument to arise from symposium (besides the institute itself, of course) will be Bethany Nowviskie’s call for reform of graduate training, to match the methods and questions that will form the future. But in the words of the Black Eyed Peas, where is the love?For digital humanities, the response to the Black Eyed Peas comes from the Troggs: love is all around. At THATCamps, at MLA sessions, on Twitter – digital humanists seem to have a fondness for their work, an emotional connection to their theoretical arguments. Panels play to packed houses, in a way that other fields seem not to. This isn’t to say that everyone always agrees with each other, or that theoretical conversations don’t happen. The teachers who matter to us, Lundin is careful to state, are not necessarily the ones with whom we always agree: “my most influential teachers had religious commitments, political views, or theoretical understanding that differed sharply from my own” (137). Disagreement of course fosters insights. Responding to Bethany, Ryan Cordell hopes to reform undergraduate teaching as well. Ted Underwood, though, is “not yet sure about the implications at the undergraduate level. Maybe ten years from now I’ll be teaching text mining to undergrads … but then again, maybe the things undergraduates need most from an English course will still be historical perspective, close reading, a willingness to revise, and a habit of considering objections to their own thesis.” In considering how our pedagogical goals might change, Ted gives what I think is the best and most concise list of what those goals are now (at least the best I’ve heard).Academics are teachers, and I’m excited to see that teaching has become a center of the conversation in digital humanities, with both graduate students and undergraduates involved in digital scholarship. We can say to the leaders in the field, what you love we will love: teach us how.Continue reading at http://www.nines.org/news/?p=1395 .Entering the web of data[view the presentation...][view the triples...]Keynote delivered at the annual conference of the Australia and New Zealand Society of Indexers, 14 September 2011.This is me.Today, Wednesday, 14 September 2011, I’m honoured to be able to join you here in the luxurious surrounds of the Brighton Savoy Hotel for the ‘Indexing See Change‘ conference. This is an event, a moment in history; we can pinpoint ourselves, this gathering, both in time and in space.If we do that, if we move outside the moment and position ourselves on a timeline or a map, interesting things start to happen. Connections emerge.Here we are at number 150, The Esplanade, in Brighton. A bit over a kilometre away is the stately villa, Kamesburgh. For many years Kamesburgh was also known as the Anzac Hostel — a refuge for permanently-incapacitated World War One veterans.The Anzac Hostel opened on 5 July 1919. Here it is draped in its patriotic finery, from the collections of the Australian War Memorial. According to the caption, the Anzac Hostel was ‘a home, not an institute’.Also amongst the War Memorial’s holdings is a wheeled bed that was used at the hostel. This particular bed was apparently occupied by one man, Albert Ward, for forty-three years.Death notice for Alexander Kelley. Argus, 29 January 1944.It was probably in a bed just like this that Alexander Dewar Kelley passed away on 27 January 1944. Alexander Kelley was cremated, and his remains interred amongst the roses at what is now called the Springvale Botanical Cemetery. Not far from my own grandparents.Alexander Kelley spent close to half his life in the Anzac Hostel. Like many young men, he bravely answered his nation’s call to arms, but returned from war much changed. We can follow Alex’s war through his service record, easily-accessible through the website ‘Mapping Our Anzacs‘.Alex was a coach painter who enlisted in the AIF in January 1916. Within a year he was in France. In May 1917 he suffered a gunshot wound to the head, but was able to rejoin his unit in August. Less than a month later though, he was wounded again, this time more severely. For Alex the war was over, and he was shipped back to Australia in May 1918.‘Mapping Our Anzacs’ includes a scrapbook feature through which visitors to the site can attach notes or photographs to a service record. Amongst the the many thousands of postings is a fragment from a diary, found tucked inside the bible of Alexander Kelley’s mother. The diary entry reads simply: ‘Alex arrived from Front. Wet day. Saw him at “Caulfield”.’Alex had survived and had returned to his family. This was a day to remember. But there was sadness too, for Alex was not the same young man who had left for the battlefields of Europe. In the diary fragment, ‘Caulfield’ is enclosed in inverted commas, indicating perhaps that the reunion took place, not in the suburb, but in the Caulfield rehabilitation hospital. Alexander Kelley was wounded in the face, hands and legs. He was left blind in both eyes and his right leg was amputated. He would live the remainder of his life a little over a kilometre away from here at the Anzac Hostel.This is just one story. There are over 375,000 World War One service records held by the National Archives of Australia. How can we hope to understand a number like that? How can we hope to imagine the war’s impact on families, on communities?‘Mapping Our Anzacs’ uses familiar Google maps to display the places of birth and enlistment recorded in many of those service records. But technical limitations make it impossible to display all the places at once. You can, however, take the same data and open it in Google Earth. If you then zoom in on Victoria, you see something like this.Mapping Our Anzacs data viewed in Google Earth.Each marker represents a place where a service person was born or enlisted. It’s impossible to read, of course, but that’s the point. There is so little blank space. As you zoom further, more markers appear, more place names resolve. It’s simple, but it’s powerful. They came from everywhere. From the smallest village to the biggest city; nowhere was untouched.The ‘Mapping Our Anzacs’ scrapbook offers another perspective. It’s possible to extract the images posted to the scrapbook and present them on a 3D wall. Amidst an assortment of memorabilia, there are faces. Not places, or records — this is a wall of people.Mapping Our Anzacs Scrapbook photos viewed through CoolIrisIt’s worth noting too that like the markers on the maps, these faces link back to the actual service records. So they’re not just a new way of seeing the collection, they’re a new way of exploring it.But the records don’t stand in isolation, they themselves have a context. A couple of years ago, Mitchell Whitelaw from the University of Canberra, undertook a project called ‘The Visible Archive‘ to investigate ways of visualising the holdings of the National Archives of Australia. Have you ever wondered what 360km worth of records looks like?The collections of the NAA visualised by Mitchell's Series Browser.This represents the holdings of the National Archives. Files within the archives are organised into series, and each square in this image represents a single series — there are about 60,000 of them. Naturally the size of the square gives an indication of the size of the series itself. It’s a fascinating and strangely beautiful picture.It’s easy enough to pick out the World War One service records — Series B2455. In the interactive version of Mitchell’s series browser you can click on a box and display links between series, as well as other series created by the same government agency. Again, it’s not just a way of seeing the collection, but a means of exploring and interpreting it. As Mitchell says:Visualisation enables us to literally show everything, to display large volumes of data in a way that reveals patterns and communicates context, but also provides access to the fine grain of individual elements.[]But we can also employ such techniques to ask new kinds of questions. Can you imagine how Alexander Kelley and the other inhabitants of the Anzac Hostel must have felt in 1939? They had lost so much in the Great War, the ‘war to end all wars’, and yet within their own lifetime it was all happening again. More young men were answering the call, more lives were going to be destroyed.There must have been a dreadful, disheartening moment when Australians realised that the Great War was not an end, but a beginning — the first in a series of devastating global conflicts. At some point the ‘Great War’ became the ‘First World War’, but when?When did the 'Great War' become the 'First World War'? This is one possible answer. This graph draws its data from the 50 million or so digitised newspaper articles in Trove, the National Library of Australia’s discovery service. It shows the proportion of newspaper articles that included the phrase ‘the great war’ compared to the proportion containing ‘the first world war’ (and variations thereof). The lines cross late in 1941. With German victories in Europe and Africa, the opening of the Eastern Front and the Japanese attack on Pearl Harbour, 1941 makes sense.What is perhaps more intriguing is the dramatic peak in the occurrence of ‘the great war’ in 1939. It’s no surprise that the looming threat of a new conflict would provoke comment and comparisons, but it does make you wonder about the context of those discussions and how they might have changed as the reality of war edged closer.To start exploring this I’ve harvested the content of the 6,600 articles from 1939 that included the phrase ‘the great war’. Using an online text analysis service called VoyeurTools I can quickly generate a picture of their contents.This simple visualisation shows us the relative frequencies of words within the articles. It doesn’t reveal any great mysteries, but it does suggest some possibilities for further prodding. The prevalence of ‘time’ and ‘new’, for example — might these help us understand the shift in perspective from one war to the next? We can follow this up by browsing the different contexts in which the words were used.But what actually is it that we’re actually searching? We know that Trove includes newspapers from 1803 to 1954, but if we’re really going to analyse shifting words and ideas it’s important to have a clear picture of the sources of those words.Something like this perhaps. This graph shows the holdings of the Trove newspaper database on 4 August 2011, organised by state. You can see, for example, that if you’re searching on a topic between the 1920s and 1940s you’re probably likely to get more results from Queensland than anywhere else.So starting from our location here, today, we can make connections across time and space. We can pull back and look at the big picture, or dive in and examine the fabric of a single life. Through the web we can build and explore a rich and complex contextual network.It’s an exciting time to be a cultural data hacker. We now have a growing range of tools and technologies available for extracting interesting data from a wide variety of sources, both structured and unstructured.The ‘Visible Archive’ project started with well-structured data, courtesy of Peter Scott, the developer of the Series System — the descriptive framework used by many Australian archives. But we’re rarely so lucky.Even when the data starts off in nicely-organised fields in a database there’s no guarantee that that’s how it’s going to be delivered to our web browser. In order to extract the data from my Trove graphs, for example, I had to write a little program called a ‘screen scraper‘ to identify and save the important metadata elements from the raw web page itself.Where there are no subject keywords we can infer them using techniques such as topic modelling. Where there are no access points we can identify people, organisations, places and events using special tools developed for named entity extraction. Where there are no common identifiers across datasets we can employ record linkage technologies to find possible connections.We can count words, we can identify parts of speech, we can formulate a measure of the similarity of any two pieces of text. Once we have some useful data we can manipulate and enrich it. Place names can be geolocated — you simply send your place name off to a web service and get back its latitude and longitude.Increasingly these sorts of tools are becoming accessible to anyone. For historians they offer a means of wrestling with rapidly-growing bulk of source material that is becoming available in digital form. How do you make use of 5 million digitised books, 50 million newspaper articles or the complete archive of every public message ever sent on Twitter?The digital historian Dan Cohen has noted:These computational methods which allow us to find patterns, determine relationships, categorize documents, and extract information from massive corpuses, will form the basis for new tools for research in the humanities and other disciplines in the coming decade.[]Dan is involved in a number of interesting projects investigating the possibilities of these techniques — often grouped together under the heading ‘text mining’. One of these projects, ‘With Criminal Intent‘, is looking to see what patterns can be drawn out of the digitised proceedings of criminal trials held at the Old Bailey from 1645 to 1913. That’s 197,745 trials, in case you were wondering.Here’s one of their visualisations showing how the length of trials varies over time. Much to the surprise of the research team, this graph suggests a dramatic shift in legal practice around 1825 — defendants started pleading guilty!A visualisation by the With Criminal Intent project showing changing trial lengths.Rather than falter under the growing weight of digital sources, these technologies can actually thrive. The more raw material available, the more chance there is to observe and track new patterns. As digitisation continues apace will we ever reach the point when history can simply be read from a graph?There are some researchers at Harvard who seem to think that’s where we’re heading. Borrowing liberally from the store of scientific metaphors they have staked out the new field of ‘culturomics‘. By mining massive digital resources, like Google’s scanned books, they hope to map the ‘cultural genome’ that would enable us to follow the evolution of language and culture.But there’s something quite barren in this ambition. I prefer the vision of digital humanist Stephen Ramsay, who commented in regard to the ‘With Criminal Intent’ project:The Old Bailey, like the Naked City, has eight million stories. Accessing those stories involves understanding trial length, numbers of instances of poisoning, and rates of bigamy. But being stories, they find their more salient expression in the weightier motifs of the human condition: justice, revenge, dishonor, loss, trial. This is what the humanities are about. This is the only reason for an historian to fire up Mathematica or for a student trained in French literature to get into Java.[]Ultimately it’s the stories that nourish, anger, inspire and depress us. The closely-packed map of places recorded in World War I service records is so powerful because we know that under each marker are men, women, families, communities — each with their own story. These new technologies offer new perspectives, they raise new questions, and they challenge us with new contexts to explore and understand. But there is still space for stories and perhaps we can use them to give our stories new life and depth.This is another World War One service record. It belongs to Charlie Allen. Charlie enlisted three times in the AIF and was discharged on medical grounds each time. It seems he had a problem with his ankle.Charlie’s service record notes a tattoo, proclaiming his love for ‘Maud Gordon’. He married Maud in Sydney in 1917 and had two daughters soon after.Charlie survived the war without further injury, but was not so lucky in peace. On 11 March 1938, Charlie was crushed to death between two railway cars. The accident happened at the Bunnerong Power Station, only a short distance from his home in Matraville. He was buried nearby in the Botany Cemetery.We also know quite a bit about Charlie’s early life. Why? Because Charlie’s father was Chinese and he was therefore categorised as a ‘half-caste’, as someone who was not white, and therefore fell under the restrictions imposed by the White Australia Policy.Charlie was born in Sydney in 1896. His mother was Frances Allen (sometime sweet shop owner and brothel keeper), his father Charlie Gum (a buyer for Wing On company). Charlie was raised by his mother, but in 1909, at the age of 13, he was taken to China by his father.NAA: ST84/1, 1909/22/41-50This certificate granted Charlie an exemption to the Dictation Test. Without it, he may not have been allowed back into the country.Every time one of many thousands of non-Europeans resident in Australia sought to travel overseas and return home again they needed one of these certificates.Charlie’s father returned to Sydney, leaving him in China. He lived with relatives in the town of Shekki (inland from Hong Kong). Charlie was naturally homesick, but had no means of getting back to Australia. He wrote to his mother in 1910:Do try and bring me home every minute I think of you and long for a piece of bread and butter this tucker is not doing me well.[]His mother wrote to the Prime Minister Billy Hughes in an attempt to enlist government help but to no avail. Charlie finally returned to Australia in 1915.Despite this experience, Charlie visited China again in 1922 for 7 months. Once again carrying papers to grant him re-entry to the country of his birth.These fragments of Charlie’s life have been assembled by my partner, Kate Bagnall, a historian of Chinese-Australia. They are remarkable, and yet not so, because there are many thousands of stories like Charlie’s contained within the voluminous records generated by the administration of the White Australia Policy.We’re all of course familiar with the general outlines of the White Australia Policy, and the way it underpinned conceptions of Australia as a nation in the first half of the 20th century.But what we sometimes forget is that it was also a massive bureaucratic exercise.Forms and certificates were printed, issued, used and filed. Regulations were modified, guidelines were distributed and administering officers were managed and advised. Individual cases were reviewed, policy was changed and new forms and certificates were printed, issued, used and filed…Much of this system is now preserved in the National Archives.You can get a idea of the range of material available from a case study Kate has prepared focusing on the efforts of Poon Gooey, a successful businessman in Horsham, to keep his wife and family in Australia.If we look again at Charlie’s certificate from 1909 we can see that it contains a lot of interesting structured data:nameplace of birthageheightdestinationdate of departurename of shipWe estimate that there are probably about 50,000 of these forms remaining in the Archives, and then there’s case files and a variety of other government documents.Wouldn’t it be great if we could extract this structured data. If we could piece together the slivers of identity that remain within the Archives and give people back their lives.This is the dream of Invisible Australians, a project Kate and I are trying to turn into a reality. Our aim is to build systems that will enable this data to be extracted, aggregated, shared and connected — whether to a family tree, a cemetery record, or another document in another archive.Imagine being able to navigate the network of lives, families and relationships. To follow their journeys, to share their tragedies, to celebrate their small victories against a repressive system.Imagine being able to watch them age.We tend to assume that new technologies require us to change, to adapt. But sometimes they can take advantage of our strengths. Mitchell Whitelaw is interested in finding out what happens when you take large cultural datasets and try to ‘show everything’. Such an approach, he suggests, takes advantage of the raw processing power of computers, while giving us space to do what we’re good at — finding patterns, making connections, crafting meanings.The History Wall tries to create a similar sort of space. The History Wall brings together material from a range of different sources — newspaper articles from Trove, biographies from the Australian Dictionary of Biography, records from a database of NSW convicts, population statistics, collection items from the National Museum of Australia — you can pretty much plug anything in as long as it has a date attached to it.Irish History WallFor a particular year, the Wall retrieves a random sample from the available sources, jumbles everything up and then throws it onto the screen. As a result, no two views of the Wall are ever quite the same. This is not a traditional exhibition. There is no curator controlling the content or designing the structure. It’s ephemeral, it’s serendipitous — instead of relying on an authorial voice to smooth over the gaps and transitions, it leaves open the cracks and allows new contexts to seep in and around each item.As the pioneering digital historian Edward Ayers noted:even isolated and inert pieces of evidence — a list, a letter, a map, a picture — can assume new and unimagined meanings when placed in juxtaposition with other fragments.[]This is not an absence of narrative, but an opportunity for narration. Edward Ayers suggests that we’re actually quite comfortable filling in blanks and untwisting timelines:Humans, presented with pieces of information about people, put things into the form of a story. They need not be simple stories, for we know how to deal with unexplained lapses of time, flashbacks, and overlapping narratives. We know how to imagine, infer, things happening at the same time in different places. Film and television train all of us at early ages to weave strands of narrative out of intentional (if carefully constructed) confusion and to take pleasure in that weaving.[]And so I can show you a death notice, or a certificate and you will take those fragments, those isolated data points and you will construct a story — you will see the person behind them, you will imagine their life. It’s what we do. We’re good at it.Computers on the other hand will just see data.In her ode in praise of humanities data, digital humanist Amanda French wonders whether we always need to crunch our data into abstract, pliable forms:What I wonder is whether instead we can begin with the data, or with a datum, and simply watch for what it may tell us, even if what it tells us is simply a story.[]Yes we can. And we should teach computers how to do it as well. Not because we want them to take over. Not because they can necessarily do it faster or better. But because they can help us share, preserve and connect those stories.Let’s think again about the array of documents that Kate has assembled to piece together the story of Charles Allen. How can you share this sort of material? Typically you’d ‘write it up’. You’d capture the story behind the data and commit it to words. The documents would then become evidence — points of connection between your text and the historical record.So in order to share the meanings of these documents we remove them from the context of the person’s life and marshal them as allies to proclaim the authenticity of our rendering. Wouldn’t it be better if we could tell the story, but maintain within our texts the direct connections between sources and subject?What we need is a data framework that sits beneath the text, identifying people, dates and places, and defining relationships between them and our documentary sources. A framework that computers could understand and interpret, so that if they saw something they knew was a placename they could head off and look for other people associated with that place. Instead of just presenting our research we’d be creating a whole series of points of connection, discovery and aggregation.Sounds a bit far-fetched? Well it’s not. We have it already — it’s called the Semantic Web.The Semantic Web exposes the structures that are implicit in our web pages and our texts in ways that computers can understand. The Linked Data movement takes the basic ideas of the Semantic Web and turns them into a collaborative activity. You share vocabularies, so that other people (and computers) know when you’re talking about the same sorts of things. You share identifiers, so that other people (and computers) know that you’re talking about a specific person, place, object or whatever.Linked Data is Storytelling 101 for computers. It doesn’t have the full richness, complexity and nuance that we invest in our narratives, but it does at least help computers to fit all the bits together in meaningful ways. And if we talk nice to them, then they can apply their newly-acquired interpretative skills to the things that they’re already good at — like searching, aggregating, or generating the sorts of big pictures that enable us to explore the contexts of our stories.This is why we’ve always imagined Invisible Australians to be something more than an online database. We want to provide points of connection that other people can build into their own stories. But to do that we have to pay attention to things like vocabulary management and authority control, we have to construct web addresses that are not going to break every time we upgrade our software. We have to think about the sorts of things we’re talking about — not just people, but government agencies, legislation, certificates, and correspondence. How do we describe these entities and what sorts of relationships do they have?And of course we need to expose all these structures so that we can say, these things are people, these are events, these are places and these are documents.Or perhaps, to introduce Alexander Kelley.Or remember Charles Allen.You might be wondering why we don’t just leave it all to the computers themselves. Didn’t I just talk about all the exciting new tools and techniques that enable us to analyse the structures of texts? Perhaps we should just wait for the Culturomics guys to solve all the problems.But who defines the problems?Our postmodern sensibilities encourage a suspicion of neutrality. Labels like ‘the new museology’ or Archives 2.0 reflect an awareness that the way we describe and arrange our collections is itself culturally-determined. It’s not just a matter of what our descriptive systems show, but what they hide.Tim Hitchcock, another member of the ‘With Criminal Intent’ team, has described how online technologies can change the way we access archives. Instead of being forced to navigate the hierarchical structures that archives impose on records, which in turn tend to reflect the workings of the institutions that created the records, we can directly find the people whose lives were regulated, influenced, shaped or controlled by the policies of those institutions.Instead of merely hearing ‘the institutional voice… in all its stentorian splendour’, he says, we can listen in to ‘the quieter tones uttered by the individual’.[]This reminds us that search boxes, along with other digital tools, themselves embody arguments. There are assumptions built into their code about what is relevant, what is significant, what is necessary.We can build our own tools of course, and we can critique other people’s algorithms. But what if we just want to collect and share stories?Linked Data gives us a way to present an alternative to Google’s version of the world. We can argue back against the search engines, defining our own criteria for relevance, and building our own discovery networks.Changing the way we access resources changes the sorts of stories we can tell. Tim Hitchcock asks:What happens when institutions and archives are ‘decentred’ in favour of the individual? What changes when we examine the world through the collected fragments of knowledge that we can recover about a single person, reorganised as a biographical narrative, rather than as part of an archival system?[]Perhaps the invisible become visible.As one of the first of a ‘new style’ of museum online collections, launching several internet generations ago in 2006, the Powerhouse Museum’s collection database has been undergoing a rethink in recent times. Five years is a very long time on the web and not only has the landscape of online museum collections radically changed, but so to has the way researchers, including curators, use these online collections as part of their own research practices.Digging through five years of data has revealed a number of key patterns in usage, which when combined with user research paints a very different picture of the value and usefulness of online collections. Susan Cairns, a doctoral candidate at the University of Newcastle, has been working with us to trawl through oodles of data, and interviewing users to help us think about how the next iteration of an online museum collection might need to look like.I asked Susan a number of questions about what she’s been discovering.F&N – You’ve been looking over the last few years of data for the Powerhouse’s collection database. Can you tell me about the different types of users you’ve identified?Based on the Google Analytics, there seem to be four main types of OPAC users. I’ve given each of them a nickname, in order to better identify them.The first group is the FAMILIARS, composed of people who access the OPAC intentionally. FAMILIARS know of the collection through either experience (having used the online collection previously, or from visiting the museum), or via reputation (ie GLAM professionals, researchers or amateur collectors). FAMILIARS come to OPAC with the highest level of expectations and have the most invested in the experience. Trust and authority are hugely important for the people in this segment.The second group, I’ve called the SEEKERS. Like FAMILIARS, SEEKERS are driven by a desire for information they can trust. However, unlike FAMILIARS, SEEKERS do not yet know about the museum and/or its collection. This group includes people who are new to collecting communities, or student researchers etc. If they find what they are looking for on the OPAC, SEEKERS have the potential to become FAMILIARS.The final group for whom authority and trust in information is important are the UTILISERS. These visitors, primarily education users (like school students), have specific and particular research needs, which are externally defined (ie they might be looking for answers to set questions). This group is task-oriented.The last group that comes to the OPAC is the WANDERERS. These are casual browsers who seek fast and convenient information, but don’t necessarily need depth in their answers. Seb once nicknamed them “pub trivia” users, and that seems pretty apt. F&N – What sort of proportions do each of these make up? By far the greatest number of OPAC visitors are WANDERERS. More than 80% of all OPAC users – whether in a two-year period, or a six-month timeframe – visited the collection online once. Obviously not all of these will be WANDERERS, but a significant proportion of OPAC users are clearly coming to meet short-term information needs. At the opposite end of the scale, around 5% of OPAC users visited the collection five times or more during the last six months. These visitors have the most invested in the current OPAC, having spent time learning to negotiate it.F&N – Have these users changed over time? (As other collections have come online etc) The actual make up over time doesn’t seem to have changed that much, although the numbers of visitors dropped a little after a peak in early 2010.Having said that, there are seasonal trends in the users. The search terms that UTILISERS often use to find the collections (such as “gold license”) are more popular during the school year than at other times. Similarly search terms go through peaks, depending on media interest, such as a high number of searchers who come to the OPAC looking for Australian media personality Claudia Chan Shaw, whose dress is in the collection.Some search terms are just weird. One of the most popular search terms ever was “blue fur felt” which skyrocketed to popularity in January – July 2010, but has not been used to bring visitors to the OPAC since.F&N – Are overseas users different from Australian ones? During the last six months, the OPAC actually had more international users than domestic ones, with the top ten international countries visitors coming from the USA, UK, Canada, New Zealand, India, Germany, France, Netherlands and Philippines. The search terms that lead international users to the OPAC are very different from those within Australia. After all, many of the most searched for items are that link up with the school curriculum, and that is very Australia-specific. These items also make up a significant proportion of the most-looked-at references.The search terms overseas users to access the collection are often far more specific – such as particular clock brands etc, which would indicate a higher proportion of amateur collectors (SEEKERS and FAMILIARS) than WANDERERS.Australian users spend longer on the site, and have a far lower bounce rate, so once on site they engage more.F&N- You’ve been speaking to our curators about how they use ours and others collection databases. What are some of the things you’ve learned from this?Talking to the curators has been absolutely fascinating. Every single curator that I have spoken to has his or her own ways of researching and gathering collection information. Some curators rely heavily on books, while others spend a significant amount of time conducting face-to-face interviews. Others use websites like Trove, or conduct community consultation online, using wikis and blogs. However, every researcher utilises Google and the Web in some way in their search for information.No matter how a curator conducts collection research however, all are looking for two main types of information. The first is the broad contextual information for an object that places it into an historical and social framing. This includes the broader history or biography of the creator or manufacturer, and information on the social period in which it is or was used.The second type of information is specific to the object itself, and includes information about maker’s marks, the object’s history (including provenance, such as how, when and why it came into the collection, why it was owned and used), and any stories that relate specifically to the object.In order to find this information however, very few of our curators use museum collection databases – even those curators who conduct a significant amount of their research online. The reasons for this varied, but emerging themes included a difficulty navigating online collections (once it could be located on the institution website in the first place), a sense of frustration at being unable to find relevant information/objects, and most important, a lack of trust in online collection databases.Not one curator that I spoke to trusted either our own OPAC or other online collections as a resource that could provide complete and authoritative information. Where a number of curators did find online collections useful however, was in providing immediate access to images of objects and to get a sense of whether another institution held objects that might be important to their own search. Knowledge about what was in a collection was useful, but not necessarily the collection knowledge that was included in the online record.A number of curators did use our own OPAC to see what information was being communicated to the public, and to answer public enquiries. However, it was very clear that there are ongoing issues with trust and authority.Two things that did increase trust for curators however were good quality images (through which they could get a visual sense of the object), and PDFs of original documents. Curators trust that which they can see themselves. For most curators, their expertise is such that they will have an intuitive sense when information they come across is likely to be correct.–Following Susan’s initial work we started looking at the SEEKERS in more detail. Why were they coming to the site? And, more importantly, were they satisfied with what they found?We’ve had a pop up survey running for the last two months – again using Kiss Insights – and the numbers have started coming in.In order to survey only the SEEKERS we have set the survey to only show to visitors who’ve arrived via organic search, have visited at least three pages, and, obviously, are in the museum’s online collection. The survey, thus, has quite a limited reach and has been triggered by only 3900 visitors in the time – and has been completed by 229 respondents.It is somewhat heartening to find that the largest subgroup of Seekers – those doing ‘amateur research, hobbyist and collectors’ – feel the content they find is ‘good’, and that the lowest positive ratings are for the ‘other’ group. This is especially interesting if we look by object and see which object records are being rated as ‘poor’. Here we find a mix of well documented (at least according to us) and very scantily documented (no image, metadata last copied from a paper stock book entry in the 1980s).Once we get to a critical mass of respondents – 1000 or more – in this group we should have some more actionable findings. Then we move on to looking at the the other groupings.Tags:A number of our Web Science students are doing work analysing people's use of Twitter, and the tools available for them to do so are rather limited since Twitter changed the terms of their service so that the functionality of TwapperKeeper and similar sites has been reduced. There are personal tools like NodeXL (a plugin for Microsoft Excel running under Windows) that do provide simple data capture from social networks, but a study will require long-term data collection over many months that is independent of reboots and power outages. They say that to a man with a hammer, the solution to every problem looks like a nail. And so perhaps it its unsurprising that I see a role for EPrints in helping students and researchers to gather, as well as curate and preserve, their research data. Especially when the data gathering requires a managed, long-term process that results in a large dataset.EPrints Twitter Dataset, Rendered in HTMLIn collecting large, ephemeral data sets (tweets, Facebook updates, Youtube uploads, Flickr photos, postings on email forums, comments on web pages) a repository has a choice between: (1) simply collecting the raw data, uninterpreted and requiring the user to analyse the material with their own programs in their own environments (2) partially interpreting the results and providing some added value for the user by offering intelligent searches, analyses and visualisations to help the researchers get a feel for the data. We experimented with both approaches. The first sounds simple and more appropriate (don't make the repository get in the way!), but in the end the job of handling, storing and providing a usable interface to the collection of temporal data means that some interpretation of the data is inevitable. So instead of just constantly appending a stream of structured data objects (tweets, emails, whatever) to an external storage object (a file, database or cloud bucket) we ingest each object into an internal eprints dataset with appropriate schema. There is a tweet dataset for individual tweets, and a timeline data set for collections of tweets - in theory multiple timeline datasets will refer to the same objects in the tweet dataset. These datasets can be manipulated by the normal EPrints API and managed by the normal EPrints repository tools: you can search, export and render tweets in the same way that you can for eprints, documents, projects and users. EPrints collects Twitter data by regular calls to the Twitter API, using the search parameters given by the user. The figure on the left shows the results of a data collection (on the hashtag "drwho") resulting in a single twitter timeline that is rendered as HTML for the Manage Records page. In this rendering, the timeline of tweets is shown as normal on the left of the window, with lists of top tweeters, top mentions, top hashtags and top links together with a histogram of tweet frequency on the right. These simple additions serve to give an overview of the data to the researcher - not to try to take the place of their bespoke data analysis software, but simply to help understand some of the major features of the data as it is being collected. The data can be exported in various formats (JSON, XML, HTML and CSV) for subsequent processing and analysis. The results of this analysis can themselves be ingested into EPrints for preservation and dissemination, along with the eventual research papers that describe the activity. All this functionality will soon be released as an EPrints Bazaar package; as of the time of writing we are about to release it for testing by our graduate students. The infrastructure that we have created will then be adapted for other Web temporal data capture sources as mentioned above (Flickr, YouTube, etc).No CommentsIt has been six months since Digital Humanities Now relaunched in version 2.0 through the support of the PressForward Project, funded by the Sloan Foundation. The first version, run between 2009 and 2010, was an automated survey of Twitter. Version 1.5 was a one-man operation by Dan Cohen to vet the material using traditional methods of editorial section.Now four editors spend a total of 15-20 hours a week to survey approximately 1,000 items per week produced or shared by digital humanists in all corners of the field.In the past six months, we have winnowed down more than 20,000 items to highlight 175 pieces as Editors’ Choice, along with 7 “round-ups” that grouped together related posts. We have shared 586 news items, including 218 new resources, 144 job announcements, 111 calls for papers or participation, 65 reports, and 30 funding opportunities.In addition, we selected 22 Editors’ Choice pieces from the last quarter of 2011 and solicited five new works for the inaugural Journal of Digital Humanities. These pieces were guided to publication by more than 30 additional comments during open peer review and by the careful work of our editors and guest editor Natalia Cecire.What did we learn?Digital humanists are very active. At the moment our aggregation and curation process would be difficult (though perhaps not impossible) to sustain solely through volunteer efforts. And there is enough work happening to necessitate a sister publication, Global Perspectives on Digital History, which highlights digital history work from around the world, complete with multilingual editors. We hope to expand that publication in the coming year.Digital humanists use blogs to talk about the field as a community and a practice more than to report their scholarship. There are fewer pieces of original research (i.e., new results of work done) shared than we had expected, and many of those research-oriented pieces focus on individual text- and data-mining projects, with fewer reports from other areas of DH or from collaborative projects.Digital humanities has a gender gap and most of the content is created by individuals. Like other fields, many more males have brought their research blogs to our attention. (Indeed, the gap in DH may be smaller than other fields, but still conspicuous.) The individuals who identify as part of the DH community in our registry are split 40% female and 60% male (see “DH Registry”). But among those who provided an RSS feed to monitor their work (see “Compendium”), the ratio goes down to 33% female, 67% male. Although organizations make up about 10% of our feeds, we do not see a lot of these groups reporting on the process or results of their work, except for project launches.What will we do next?Expand our Editorial Board: We already have placed a call for editors to help distribute the labor and expand the networks surveyed starting in June. So far more than thirty editors-at-large have signed up to help by survey the field and nominate items for recirculation.Highlight more scholarly posts from the broad field: We will continue to prioritize highlighting original research (very broadly construed) in the Editors’ Choice selections. We hope to see and find more examples of works in progress, as well as white papers and reports from finished projects. In addition, we will begin to highlight scholarly projects (e.g., new tools and websites) as Editors’ Choices rather than Resources.Strongly encourage everyone, and especially women and collaborative groups, to share more work. We can only highlight what we see and can link to. We will work to expand the networks that we survey, and find ways to supplement the currently limited group of mostly self-nominated individuals. Can you think of other ways we can improve? Please add your suggestions in the comments below or email us at dhnow@pressforward.org.CommentsAs I wrote here a couple of weeks ago, I’m playing around with a variety of clustering techniques to identify patterns in legal records from the early modern Spanish Empire. In this post, I will discuss the first of my training experiments using Normalized Compression Distance (NCD). I’ll look at what NCD is, some potential problems with the method, and then the results from using NCD to analyze the Criminales Series descriptions of the Archivo Nacional del Ecuador’s (ANE) Series Guide. For what it’s worth, this is a very easy and approachable method for measuring similarity between documents and requires almost no programming chops. So, it’s perfect for me!I was inspired to look at NCD for clustering by a pair of posts by Bill Turkel (here, here) from quite a few years ago. Bill and Stéfan Sinclair also used NCD to cluster cases for the Digging Into Data Old Bailey Project. Turkel’s posts provide a nice overview of the method, which was proposed in 2005 by Rudi Cilibrasi and Paul Vitányi.1 Essentially, Cilibrasi and Vitányi proposed measuring the distance between two strings of arbitrary length by comparing the sum of the lengths of the individually compressed files to a compressed concatenation of the two files. So, adding the compressed length of x to the compressed length of y will be longer than the compressed length of (x|y). How much longer is what is important. The formula is this, where c(x) is the length of x compressed:NCD(x,y) = [C(x|y) - min{C(x),C(y)}] / max{C(x),C(y)C(x|y) is the compression of the concatenated strings. Theoretically, if you concatenated and compressed two identical strings, you would get a distance of 0 because [(Cx|x) - C(x)]/C(x) would equal 0/1, or 0. As we’ll see in a bit, though, this isn’t the case and the overhead required by the various compression algorithms at our disposal make a 0 impossible, and more so for long strings depending on the method. Cilibrasi and Vitányi note that in practice, that if r is is the NCD, the NCD will be 0 ≤ r ≤ 1+ ∊, where ∊ is usually around 0.1, and accounts for the implementation details of the compression algorithm. Suffice to say, though, that the closer to 0 the result is, the more similar the strings (or files in our case) are. Nonetheless, the distance between two strings, or files, or objects as measured with this formula can then be used to cluster those strings, files, or objects. One obvious advantage to the method is that it works for comparing strings of arbitrary length with one another.Why does this work? Essentially, lossless compression suppresses redundancy in a string, while maintaining the ability to fully restore the file. Compression algorithms evolved to deal with constraints in the storage and transmission of data. It’s easy to forget in the age of the inexpensive terabyte hard drive what persistent storage once cost. In 1994, the year that the first edition of Witten, Moffat, and Bell’s Managing Gigabytes was published, hard disk storage still ran at close to $1/megabyte. That’s right, just 17 years ago that 500GB drive in your laptop would have cost $500,000. To put that into perspective, in 1980 IBM produced one of the first disk drives to break the GB barrier. The 2.52GB IBM 3380 was initially released in 5 different models, and ranged in price between $81,000 and and $142,000. For what it’s worth, the median housing price in Washington, DC in 1980 was the second highest in the country at $62,000. A hard disk that cost twice as much as the median house in DC. Obviously not a consumer product. At the per/GB rate that the 3380 sold for, your 500GB drive would have cost up to $28,174,603.17! In inflation-adjusted dollars for 2011 that would be $80.5M! An absurd comparison, to be sure. Given those constraints, efficiency in data compression made real dollars sense. Even still, despite the plunging costs of storage and growing bandwidth capacity, text and image compression remains an imperative in computer science.As Witten, et.al. define it,“Text compression… involves changing the representation of a file so that it takes less space to store or less time to transmit, yet the original file can be reconstructed exactly from the compressed representation.”2This is lossless compression (as opposed to lossy compression, which you may know from messing with jpegs or other image formats). There are a variety of compression methods, each of which takes a different approach to compressing text data and which are either individually or in some kind of combination behind the compression formats you’re used to– .zip, .bz2, .rar, .gz, etc. Frequently, they also have their roots in the early days of electronic data. Huffman coding was developed by an eponymous MIT graduate student in the early 1950s.In any case, the objective of a compression method is to locate, remove, store, and recover redundancies within a text. NCD works because within a particular algorithm, the compression method is consistently imposed on the data, thus making the output comparable. What isn’t comparable, though, is mixing algorithms.Without getting too technical (mostly because I get lost once it goes too far), it’s worth noting some limitations based on which method of compression you chose when applying NCD. Shortly after Cilibrasi and Vitányi published their paper on clustering via compression, Cebrián (2005) published a piece that compared the integrity of NCD between three compressors– bzip2, gzip, and PPMZ.3 The paper is interesting, in part, because the authors do an excellent job of explaining the mechanics of the various compressors in language that even I could understand.I came across this paper through some google-fu because I was confused by the initial results I was getting while playing around with my Criminales Series Guide. Python has built-in support for compression and decompression using bzip2 and gzip, so that’s what I was using. I have the Criminales Series divided into decades from 1601 to 1830. My script was walking through and comparing every file in the directory to every other one, including itself. I assumed that the concatenation of two files that were identical would produce a distance measurement of 0, and was surprised to see that it wasn’t happening, and in some cases not even close. (I also hadn’t read much of anything about compression at that point!) But that wasn’t the most surprising thing. What was more surprising was that in the latter decades of my corpus, the distance measures when comparing individual decades to themselves were actually coming out very high. Or, at least they were using the gzip algorithm. For example, the decade with the largest number of cases, and thus the longest text, is 1781-1790 at about 39,000 words. Gzip returned an NCD of 0.97458 when comparing this decade to itself. What? How is that possible?Cebrián (2005) explain how. Different compression methods have upper limits to the size of a block of text that they operate on before needing to break that block into a new blocks. This makes little difference from the perspective of compressors doing their job, but it does have implications for clustering. The article goes into more detail, but here’s a quick and dirty overview.bzip2The bzip2 compressor works in three stages to compress a string: 1. a Burrows-Wheeler Transform; 2. a move-to-front transform; and, 3. a statistical compressor like Huffman coding.4 The bzip2 algorithm can perform this method on blocks of text up 900KB without needing to break the block of text into two blocks. So, for NCD purposes, this means that if a pair of files are concatenated, and the size of this pair is less than 900KB, what the bzip compressor will see is essentially a mirrored text. But, if the concatenated file is larger than 900KB, then bzip will break the concatenation into more than one block, each of which will be sent through the three stages of compression. But, these blocks will no longer be mirrors. As a result, the NCD will cease to be robust. Cebrián (2005) claim that the NCD for C(x|x) should fall in a range between 0.2 and 0.3, and anything beyond that indicates it’s not a good choice for comparing the set of documents under evaluation.gzipThe gzip compressor uses a different method than bzip2′s block compression, one based on the Lempel-Ziv LZ77 algorithm, also known as sliding window compression. Gzip then takes the LZ77-processed string and subjects it to a statistical encoding like Huffman. It’s the first step that is important for us, though. Sliding window compression searches for redundancies by taking 32KB blocks of data, and looking ahead at the next 32KB of data. The method is much faster than bzip2′s block method. (In my experiments using python’s zlib module, code execution took about 1/2 the time as python’s bzip on default settings.) And, if the text is small, such that C(x|x) < 32KB, the NCD result is better. Cebrián (2005) find that gzip returns an NCD result in the range between 0 and 0.1. But, beyond 32KB they find that NCD rapidly grows beyond 0.9 — exactly what I saw with the large 1781-1790 file (which is 231KB).lzmaCebrián (2005) offer a third compressor, ppmz, as an alternative to bzip2 and gzip for files that outsize gzip and bzip2′s upper limits. Ppmz uses Prediction by Partial Match for compression, and has no upper limit on effective file size. PPM is a statistical model that uses arithmetic coding. This gets us to things I don’t really understand, and certainly can’t explain here. Suffice to say that the authors found using ppmz that C(x|x) always returned an NCD value between 0 and 0.1043. I looked around for quite a while and couldn’t find a python implementation of ppmz, but I did find another method ported to python with lzma, the compressor behind 7zip. Lzma uses a different implementation of Lempel-Ziv, utilizing a dictionary instead of a sliding window to track redundancies. What is more, the compression-dictionary can be as large as 4GB. You’d need a really, really large document to brush up against that. Though Cebrián, et.al. didn’t test lzma, my experiments show the NCD of C(x|x) to be between 0.002 and 0.02! That’s awfully close to 0, and the smallest return actually came from the longest document –> 1781-1790.In a way, that previous section is getting ahead of myself. I started with just zlib, and then added bzip2 and gzip, and eventually lzma for comparison sake. Let me clarify that just a bit. In python, there are two modules that use the gzip compressor: 1. gzip, which is for file compression/decompression; and, 2. zlib, which is for compressing/decompressing strings or objects. I was unsettled by my early zlib returns, and tried using gzip and file I/O, but got the same returns. Initially I was interested in speed, but reading Cebrián changed my mind on that. Nonetheless, I did time the functions to see which was fastest.I based the script on Bill Turkel’s back from 2007. (Bill put all of the scripts from the days of Digital History Hacks on Github. Thanks to him for doing that!)So, for each compressor we need a function to perform the NCD algorithm on a pair of files:# Function to calculate the NCD of two files using lzma def ncd_lzma(filex, filey): xbytes = open(filex, 'r').read() ybytes = open(filey, 'r').read() xybytes = xbytes + ybytes cx = lzma.compress(xbytes) cy = lzma.compress(ybytes) cxy = lzma.compress(xybytes) if len(cy) > len(cx): n = (len(cxy) - len(cx)) / float(len(cy)) else: n = (len(cxy) - len (cy)) / float(len(cx)) return nThere are small changes depending on the API of the compressor module, but this pretty much sums it up.We need to be able to list all the files in our target directory, but ignore any dot-files like .DS_Store that creep in on OS X or source control files if you’re managing your docs with git or svn or something:# list directory ignoring dot files def mylistdir(directory): filelist = os.listdir(directory) return [x for x in filelist if not (x.startswith('.'))]Just as an aside here, let me encourage you to put your files under source control, especially as you can accidentally damage them while developing your scripts.We need a function to walk that list of files, and perform NCD on every possible pairing, the results of which are written to a file. For this function, we pass as arguments the file list, the results file, and the compressor function of choice:def walkFileList(filelist, outfile, compType): for i in range(0, len(filelist)-1): print i for j in filelist: fx = pathstring+str(filelist[i]) fy = pathstring+str(j) outx = str(filelist[i]) outy = str(j) outfile.write(str(outx[:-4]+" "+outy[:-4]+" ")+str(compType(fx, fy))+"\n")That’s all you need. I mentioned also that I wanted to compare execution time for the different compressors. That’s easy to do with a module from the python standard library called profile, which can return a bunch of information gathered from the execution of your script at runtime. To call a function with profile you simply pass the function to profile.run as a string. So, to perform NCD via lzma as described above, you just need something like this:outfile = open('_lzma-ncd.txt', 'w') print "Starting lzma NCD." profile.run('walkFileList(filelist, outfile, ncd_lzma)') print 'lzma finished.' outfile.close()I put the print statements in just for shits and giggles. Because we ran this through profile, after doing the NCD analysis and writing it to a file named_lzma-ncd.txt, python reports on the total number of function calls, the time per call, per function, and cumulative for the script. It’s useful for identifying bottlenecks in your code if you get to the point of optimizing. At any rate, there is no question that lzma is much slower that the others, but if you have the cpu cycles available, it may be worth the rate from a quality of data perspective. Here’s what profile tells us for the various methods:zlib: 7222 function calls in 16.564 CPU seconds (compressing string objects)gzip: 69460 function calls in 18.377 CPU seconds (compressing file objects)bzip: 7222 function calls in 21.129 CPU secondslzma: 7222 function calls in 115.678 CPU secondsIf you expected zlib/gzip to be substantially faster than bzip, it was, until I set all of the algorithms to the highest available level of compression. I’m not sure that’s necessary or not, but it does affect the results as well as time. Note too that the gzip file method requires many more function calls, but with relatively little performance penalty.The Series GuideA little bit more about the documents I’m trying to cluster. Beginning around 2002, the Archivo Nacional del Ecuador began to produce pdfs of their ever-growing list of Series Finders guides. The Criminales Series Guide (big pdf) was a large endeavor. The staff went through every folder in every box in the series, reorganized them, and wrote descriptions for the Series Guide. Entries in the guide are divided by box and folder (caja/expediente). A typical folder description looks like this:Expediente: 6 Lugar: Quito Fecha: 30 de junio de 1636 No. de folios : 5 Contenido: Querella criminal iniciada por doña Joana Requejo, mujer legítima del escribano mayor Andrés de Sevilla contra Pedro Serrano, por haber entrado a su casa y por las amenazas que profirió contra ella con el pretexto de que escondía a una persona que él buscaba.We have the place (Quito), the date (06/30/1636), the number of pages (5), and a description. The simple description includes the name of the plaintiff, in this case Joana Requejo, and the defendant, Pedro Serrano, along with the central accusation– that Serrano had entered her house and threatened her under the pretext that she was hiding a person he was looking for. There is a wealth of information that can be extracted from that text. The Series Guides as a whole is big, constituting close to 875 pages of text and some 1.1M words. I currently have text files for the following Series Guides–> Criminales, Diezmos, Encomiendas, Esclavos, Estancos, Gobierno, Haciendas, Indígenas, Matrimoniales, Minas, Obrajes, and Oficios totallying 4.8M words. I’ll do some comparisons between the guides in the near future, and see if we can identify patterns across Series. For now, though, it’s just the Criminals striking my fancy.The 18th CenturySo, what does the script give us for the 18th century? Below are the NCD results for three different compressors comparing my decade of interest, 1781-1790, with the other decades of the 18th century:zlib:cr1781_1790 cr1701_1710 0.982798401771 cr1781_1790 cr1711_1720 0.987881971149 cr1781_1790 cr1721_1730 0.977414695455 cr1781_1790 cr1731_1740 0.97668311167 cr1781_1790 cr1741_1750 0.975895252209 cr1781_1790 cr1751_1760 0.975088634189 cr1781_1790 cr1761_1770 0.975632632389 cr1781_1790 cr1771_1780 0.973381605357 cr1781_1790 cr1781_1790 0.974582153107  cr1781_1790 cr1791_1800 0.972256091842 cr1781_1790 cr1801_1810 0.973325329682bzip:cr1781_1790 cr1701_1710 0.954733848029 cr1781_1790 cr1711_1720 0.96900988758 cr1781_1790 cr1721_1730 0.929649194095 cr1781_1790 cr1731_1740 0.923066504131 cr1781_1790 cr1741_1750 0.906271163484 cr1781_1790 cr1751_1760 0.903237166463 cr1781_1790 cr1761_1770 0.902912095354 cr1781_1790 cr1771_1780 0.849356630096 cr1781_1790 cr1781_1790 0.287823378031 cr1781_1790 cr1791_1800 0.850331843424 cr1781_1790 cr1801_1810 0.850358932683lzma:cr1781_1790 cr1701_1710 0.965529663402 cr1781_1790 cr1711_1720 0.976516942474 cr1781_1790 cr1721_1730 0.947607790161 cr1781_1790 cr1731_1740 0.94510863447 cr1781_1790 cr1741_1750 0.931757289204 cr1781_1790 cr1751_1760 0.931757289204 cr1781_1790 cr1761_1770 0.92759202972 cr1781_1790 cr1771_1780 0.885106382979 cr1781_1790 cr1781_1790 0.0021839468648 cr1781_1790 cr1791_1800 0.880670944501 cr1781_1790 cr1801_1810 0.887110210514First off, even just eyeballing it, you can see that the results from bzip and lzma are more reliable and follow exactly the patterns discussed by Cebrián et.al. The bzip run provides a C(x|x) of 0.288, which falls in the acceptable range. The lzma run returns a C(x|x) NCD of 0.0022, not much more needed to say there. And, as I noted above, with zlib/gzip we get 0.9745. Further, by eyeballing the results on the good runs, two relative clusters appear in the decades surrounding 1781-1790. It appears that from 1771 to 1810 that we have more similarity than in the earlier decades of the century. This accords with my expectations based on other research, and in both cases the further back from 1781 that you go, the more different the decades are on a trendline.If we change the comparison node to, say, 1741-1750 we get the following results:bzip:cr1741_1750 cr1701_1710 0.888048411498 cr1741_1750 cr1711_1720 0.919398218188 cr1741_1750 cr1721_1730 0.826189275508cr1741_1750 cr1731_1740 0.80795091612 cr1741_1750 cr1741_1750 0.277693730039 cr1741_1750 cr1751_1760 0.785168132862cr1741_1750 cr1761_1770 0.803655071796 cr1741_1750 cr1771_1780 0.879983993015 cr1741_1750 cr1781_1790 0.906271163484 cr1741_1750 cr1791_1800 0.883904391852 cr1741_1750 cr1801_1810 0.886378259718lzma:cr1741_1750 cr1701_1710 0.905551014342 cr1741_1750 cr1711_1720 0.932600133759 cr1741_1750 cr1721_1730 0.862079215278 cr1741_1750 cr1731_1740 0.848926209408 cr1741_1750 cr1741_1750 0.00587055064279 cr1741_1750 cr1751_1760 0.830746598014cr1741_1750 cr1761_1770 0.844162055066 cr1741_1750 cr1771_1780 0.90796460177 cr1741_1750 cr1781_1790 0.929573342339 cr1741_1750 cr1791_1800 0.908149721264 cr1741_1750 cr1801_1810 0.913968518045Again, the C(x|x) show reliable data. But, this time bzip’s similarities look a fair amount different that lzma when eyeballing it. I’m interested in the decade of the 1740s in part because I expect more similarity to the latter decades than for other decades in, really, either the 18th or the 17th century. I expect this for reasons that have to do with other types of hermeneutical screwing around, to use Stephen Ramsey’s [pdf] excellent phrase, that I’ve been doing the records lately. Chief among those (and an argument for close as well as distant readings) is that I’ve been transcribing weekly jail censuses from the 1740s the past week and some patterns of familiarity have been jumping out at me. I have weekly jail counts from 1732 to 1791 inclusive, and a bunch others too. I’ve transcribed so many of these things that I have pattern expectations. And, the 1740s has jumped out at me for three reasons this week. The first is that in 1741, after a decade of rarely noting it, the notaries started to record the reason for one’s detention. The second is that in 1742, and particularly under the aegis of one particular magistrate, more people started to get arrested that previous and subsequent decades. The third is that, like in the period between 1760 and 1790, those arrests were increasingly for moral offenses or for being picked up by during nightly rounds of the city (the ronda). The differences are this– in the latter period women and men were arrested in almost equal numbers. There are almost no women detainees in the 1740s. And, there doesn’t seem to be an equal growth in both detentions and prosecutions in the 1740s. This makes the decade more like the 1760s than the 1780s. The results above bear that out to some extent, as distance measures show to be more like the 1760s than the 1780s.I also had this suspicion because a few months ago I plotted occurrences of the terms concubinato (illicit co-habitation) and muerte (used in murder descriptions) from the Guide:If you click on that chart and look at the larger version you’ll see that right on decade of the 1740s there is a discernible, if much smaller than later, bump for concubinato. I was reminded of this when transcribing the records.OK, at this point, this post is probably long enough. What’s missing above is obviously visualizations of the clusters. Those visualizations are pretty interesting. For now, though, let me conclude by saying that I am impressed initially to see the clusters that emerged from this simple, if profound, technique for clustering. Given that the distinctions I’m trying to pick up are slight, I’m worried a bit about the level of precision I can expect. But, I am convinced that it’s worth sacrificing performance for either bzip or lzma implementations depending on the length of one’s documents. Unless your files are longer than 900KB, it’s probably worth just sticking with bzip.Cilibrasi and Vitányi, “Clustering by Compression,” IEEE Transactions on Information Theory 51.4 (2005): 1523-45, PDF.Witten, et.al. Managing Gigabytes: Compressing and Indexing Documents and Images 2nd Edition (Academic Press, 1999): 21.Manuel Cebrián, Manuel Alfonseca, and Alfonso Ortega, “Common Pitfalls Using the Normalized Compression Distance: What to Watch Out for in A Compressor,” Communications of Information and Systems, 5.4 (2005): 367-384.Cebrián, et.al., 372.Historians often hope that digitized texts will enable better, faster comparisons of groups of texts. Now that at least the 1grams on Bookworm are running pretty smoothly, I want to start to lay the groundwork for using corpus comparisons to look at words in a big digital library. For the algorithmically minded: this post should act as a somewhat idiosyncratic approach to Dunning's Log-likelihood statistic. For the hermeneutically minded: this post should explain why you might need _any_ log-likelihood statistic. What are some interesting, large corpuses to compare? A lot of what we'll be interested in historically are subtle differences between closely related sets, so a good start might be the two Library of Congress subject classifications called "History of the Americas," letters E and F. The Bookworm database has over 20,000 books from each group. What's the difference between the two? The full descriptions could tell us: but as a test case, it should be informative to use only the texts themselves to see the difference. That leads a tricky question. Just what does it mean to compare usage frequencies across two corpuses? This is important, so let me take this quite slowly. (Feel free to skip down to Dunning if you just want the best answer I've got.) I'm comparing E and F: suppose I say my goal to answer this question:What words appear the most times more in E than in F, and vice versa? There's already an ambiguity here: what does "times more" mean? In plain English, this can mean two completely different things. Say E and F are exactly the same overall length (eg, each have 10,000 books of 100,000 words). Suppose further "presbygational" (to take a nice, rare, American history word) appears 6 times in E and 12 times in F. Do we want to say that it appears two times more (ie, use multiplication), or six more times (use addition)? It turns out that neither of these simple operations works all that well. In the abstract, multiplication probably sounds more appealing; but it turns out to only catch extremely rare words. In our example set, here are the top words that distinguish E from F by multiplication, by occurences in E divided by occurrences in F. For example, "gradualism" appears 61x more often in E than in F. daimyo aftre exercitum intransitive castris 1994a 114.00000 103.00000 101.00000 82.33333 81.66667 77.00000 sherd infti gradualism imperforate equitum brynge 71.71429 66.00000 61.00000 59.33333 57.00000 56.00000 (BWT, I simply omit the hundreds of words that appear in E but never appear in F; and I don't use capitalized words because they tend to _very_ highly concentrated and in fictional works in particular can cause very strange results. Yes, that's not the best excuse.) So what about addition? Compensating for different corpus sizes, it's also pretty easy to find out the number of more occurrences than we'd expect based on the previous corpus. (For example, "not" appears about 1.4 million more times than we'd expect in E given the number of times it appears in F and the total number of words in E.) to that the not had it 3432895.3 2666614.4 2093465.8 1427220.8 1360559.0 1342948.2 be general we but our would 1208340.5 990988.4 974849.0 841842.6 819680.5 798426.0 Clearly, neither of these is working all that well. Basically, the first group are so rare they don't tell us much: and the second group, with the intriguing addition of "general", are so common as to be uninformative. (Except maybe for "our"; more on that later). Is there any way to find words that are interesting on _both_ counts? I find it helpful to do this visually. Suppose we make a graph. We'll put the addition score on the X axis, and the multiplication one on the Y axis, and make them both on a logarithmic scale. Every dot represents a word, as it scores on both of these things. Where do the words we're talking about fall? This nicely captures our dilemma. The two groups are in opposite corners, and words don't ever score highly on both. For example, "general" appears about 1,000,000 occurrences more in class E than we'd expect from class F, but only about 1.8x as often; sherd appears about 60x more often in class E, but that adds up to only 500 extra words compared to expectations, since it's a much rarer word overall. (BTW, log-scatter plots are fun. Those radiating spots and lines on the left side have to do with the discreteness of our set; a word can appear 1 time in a corpus or twice in a corpus, but it can't appear 1.5 times. So the lefternmost line is words that appear just once in F: the single point farthest left, at about (1.1,2.0) is words that appear twice in E and once in F; a little above it to the right is words that appear three times in E and once in F; directly to its right are words that appear four times in E and twice in F; etc.) One possible solution would be to simply draw a line between "daimyo" and "that", and assume that words are interesting to the degree that they stick out beyond that line. That gives us the following word list, placed on that same chart:...which is a lot better. The words are specific enough to be useful, but common enough to be mostly recognizable. Still, though, the less frequent words seem less helpful. Are "sherd" and "peyote" and "daimyo" up there because they really characterize the difference between E and F, or because a few authors just happened to use them a lot? And why assume that "that" and "daimyo" are equally interesting? Maybe "that" actually _is_ more distinctive than daimyo, or vice-versa. To put it more formally: words to the left tend to be rarer (for a word to have 100,000 more occurrences than we'd expect, it has to be quite common to begin with); and there are a lot more rare words than common words. So by random chance, we'd expect to have more outliers on the top of the graph than on the bottom. By using Bookworm to explore the actual texts, I can see that "daimyo" appears so often in large part because Open Library doesn't recognize these twobooks are the same work. Conversely, that "our" appears 20% more often in E than in F is quite significant; looking at the chart, it seems to actually hold true across a long period time. If this is a problem with 20,000 books in each set, it will be far worse when we're using smaller sets. That would suggest we want a method that takes into account the possibility of random fluctuations for rarer ones. One way to do this is a technique called, after its inventor, Dunning's log-likelihood statistic. I won't explain the details, except to say that like our charts it uses logarithms and that it is much more closely to our addition measure than to the multiplication one. On our E vs F comparison, it turns up the following word-positions (in green) as the 100 most significantly higher in E than F:Dunning's log-likelihood uses probabilistic statistics to approximate a chi-square test; as a result, the words it identifies tend to come from the most additively over-represented, but it also gives some credit for multiplication. All of the common words from our initial sets of 12 additive words, and none of the rare ones, are included. It includes about half of the words my naive straight-line method produced: even "skirmisher", which seemed to clump with the more common words, isn't frequent enough for Dunning to privilege it over a blander word like "movement". Is this satisfying? I should maybe dwell on this longer, because it really matters. Dunning's is the method that seems to be most frequently used by digital humanities types, but the innards aren't exactly what you might think. In MONK, for example, the words with the highest Dunning scores are represented as bigger, which may lead users to think Dunning gives a simple frequency count. It's not--it's fundamentally a probability measure. We can represent it like it has to do with frequency, but it's important to remember that it's not. (Whence the curve on our plot). Ultimately, what's useful is defined by results. And I think that the strong showing of common words can be quite interesting. This ties back to my point a few months ago that stopwords carry a lot of meaning in the aggregate. If I didn't actually really find the stopwords useful, I'd be more inclined to put some serious effort into building my own log-difference comparison like the straight line above; as it is, I'm curious if anyone knows of some good ones. As for results, here's what Dunning's test turns up in series E and in series F, limiting ourselves to uncapitalized words among the 200,000 most common in English:Significantly overrepresented in E, in order:[1] "that" "general" "army" "enemy"[5] "not" "slavery" "to" "you"[9] "corps" "brigade" "had" "troops"[13] "would" "our" "we" "men"[17] "war" "be" "command" "if"[21] "slave" "right" "it" "my"[25] "could" "constitution" "force" "what"[29] "wounded" "artillery" "division" "government"Significantly overrepresented in F, in order:[1] "county" "born" "married" "township"[5] "town" "years" "children" "wife"[9] "daughter" "son" "acres" "farm"[13] "business" "in" "school" "is"[17] "and" "building" "he" "died"[21] "year" "has" "family" "father"[25] "located" "parents" "land" "native"[29] "built" "mill" "city" "member" At a first pass, that looks like local history versus military history. At a second pass, we'll notice 'constitution' and 'government' in F and 'he' and 'parents' in E and realize that E might include biographies as well as local histories, and that F probably includes a lot of legal and other forms of national histories as well. The national words might not have turned up by my straight-line test, which seemed intent on finding all sorts of rarer military words ("skirmishers", for example). Looking at the official LC classification definition (pdf), that turns out to be mostly be the case. (Except for biography--that ought to mostly be in E. That it isn't is actually quite interesting.) So this is reasonably good at giving us a sense of the differences between corpuses as objectively defined. So far, so good. But these lists are a) not engaging, and b) don't use frequency data. How can we fix that? I never thought I'd say this, but: let's wordle! Wordle in general is a heavily overrated form of text analysis; Drew Conway has a nice post from a few months ago criticizing it because it doesn't use a meaningful baseline of comparison, and uses spatial arrangement arbitrarily. Still, it's super-engaging, and possibly useful. We can make use of the Dunning data here to solve the first problem though not the second. Unlike in a normal Wordle, where size is frequency, here size is Dunning score: and the word clouds are paired, so each one represents two ends of a comparison. Here's a graphic representing class E:And then Class F: (We could also put them together and color-code like MONK does, but I think it's easier to get the categories straight by splitting them apart like this). One nice thing about this is that the statistical overrepresentation of 'county' in class F really comes through. On some level, this is going to seem unremarkable--we're just confirming that the LC description does, in fact, apply. But a lot of interesting thoughts can come from the unlikely events in here. For example, 'our' and 'we' are both substantially overrepresented in the national histories as opposed to the local histories. (BTW, I should note somewhere that both E and F include a fair number of historical _documents_, speeches, etc., as well as histories themselves. Here, I'm lumping them all together.) There's no reason this should be so--local histories are often the most intensely insular. Is there a historical pattern in the second-person-plural? Bookworm says yes, emphatically--in a quite interesting way. "We's" and "Us's" are similar across E and F in the early republican period, and then undergo some serious wiggling starting around the Civil War; that leads to a new equilibrium around 1880 with E around its previous height, and F substantially lower. Now, there doesn't have to be an interesting historical explanation for this. Maybe it's just about memoirs switching from F to E, say. But there might be: we could use this sort of data as a jumping off point for some explorations of nation-building and sectionalism. For example, clicking on the E results around 1900 gives books that use the words 'we' and 'our' the most. One thing I find particularly interesting there are the presence of many books that, by the titles at least, I'd categorize as African-American racial uplift literature. (That's a historian's category, of course, not a librarian's one). If we were to generalize that, it might suggest the rise of several forms of authorial identification with national communities (class, race, international, industrial) in the late nineteenth century, and a corresponding tendency to not necessarily see local history as first-person history. Once we start to investigate the mechanics of that, we can get into some quite sophisticated historical questions about the relative priority of different movements in constructing group identities, connections between regionalism in the 1850s vs. (Northern?) nationalism in the 1860s, etc. We aren't restricted to questions where the genres are predefined by the Library of Congress. There is a _lot_ to do with cross corpus comparisons in a library as large as the Internet Archive collection. We can compare authors, for example: I'll post that bit tomorrow. This isn't stuff that Martin and I could integrate into Bookworm right away, unfortunately. It simply takes too long. The database driving Bookworm can add up the counts for any individual word in about half a second; it takes more like two minutes to add up all the words in a given set of books. For a researcher, that's no time at all; but for a website, it's an eternity. Both from the user end (no one will wait that long for data to load) and from the server end (we can't handle too many concurrent queries, and longer queries means more concurrent ones). But Wordle clouds and UI issues aside, the base idea has all sorts of applications I'll get into more later.Paradox Number One: Social media foments revolution, but a sudden removal of social media can increase mobilization and create even more unrest.We can all stand witness to the ways in which social and news media can spread a movement within and across nations. I know an Egyptian who claimed that her family and friends knew that the revolution was going to occur in the weeks and days before it actually happened. How? Just by the messages on social media and between individuals. In a similar fashion, social media proposed and flamed the fires of the occupy wall street movement in the weeks before it emerged, grew, and took hold as a real story in mainstream media outlets.The protest was set to start on the 17th. At first, there was a kind of silence. People questioned whether it was happening at all.Interestingly, Al Jazeera was one of the media outlets which first recognized the plan for a protest. Other small news organizations online followed the story from September 17th on. The New York Times City Room blog picked up the story on September 19th, while nothing was put into print until September 25th, when a version of a September 23rd online article titled “Protesters Are Gunning for Wall Street, With Faulty Aim“ and beginning with the sentence “By late morning on Wednesday, Occupy Wall Street, a noble but fractured and airy movement of rightly frustrated young people, had a default ambassador in a half-naked woman who called herself Zuni Tikka,” was published.Since then the General Assembly of the occupation has released a declaration and the movement has its own subreddit. However, the lack of specific demands, particularly from the outset, has been seen as a weakness and has led some people to propose their own.Clearly, social media has played a key role in this movement. Yet, ultimately, social media doesn’t stray very far from a standard news cycle. Here are Google searches and news stories for occupy wall street:(courtesy of Google Trends)And here are the tweets containing occupywallstreet:(taken from Trendistic)The tweets, Google searches, and news reference frequency all have peaks on the first day of the protest, on Sept. 25 when images of pepper spray being used by the NYPD spread and a high number of arrests occured, and on Oct. 1 when 700 people were arrested on the Brooklyn Bridge. Eventually, though, whether the movement has succeeded or not, it will fall out of the news cycle and off of people’s radar. Even though as I type this Egyptians are protesting military rule in Tahrir Square, not many Americans do searches related to Egypt these days:It’s unfortunate, but it appears that social media news runs alongside the news cycle. Facebook posts can catch our attention, but only for so long, and what seems to be fueling tweets about the protest are acts of violence rather than its actual rationale. Also, isn’t there a risk that we are beginning to confuse posting items on Facebook with really exercising our civic duty? Last week five or more of my friends posted about the execution of Troy Davis, but how many actually took action in contacting local representatives or representatives in Georgia?In fact, a Yale student recently claimed to have proven that, based on what occurred in Egypt, a “sudden interruption of mass communication accelerates revolutionary mobilization and proliferates decentralized contention.” A journalist quickly used the study to point out how mass media, even as it spreads consciousness, can create a passive public.Paradox Number Two: Social media brings networks of people with like interests together, but in doing so it can create information bubbles.In May of this year Eli Pariser presented a TED Talk in which he warned about how Google, Facebook, and other online companies use algorithms that customize what information is presented to people based on their individual tastes:Thus, just by virtue of being ourselves, our internet is filtered. We go further to filter our own experience when we read websites that cater to our cultural background or to our political interests. Despite a study which seems to indicate that this personal filtering is not an issue, Bill Davidow and Ethan Zuckerman have argued that online media can give too much attention to extreme groups and views, and that “positive feedback” loops might push us to take more extreme views ourselves. Eric E. Schmidt, the chief of Google, takes a middle ground view on the issue, acknowledging that for those who don’t know how to curate their own information, the internet can be a breeding ground of ignorance.In the classroom, discussing and giving assignments that reflect on how media is curated, either invisibly or explicitly, in different contexts (on Wikipedia, in academic journals, on Facebook, in Google Scholar) can give students a wake-up call regarding how they navigate the web (and increasingly, how the web navigates them).IDEAS TOWARDS INTERFACING DIGITAL HUMANITIES RESEARCH Hans Walter Gabler What 'humanities' encompasses: Literary Studies Cultural Studies History Philosophy Musicology etc. What engagement in and with 'humanities' comprises: Scholarship verifies, secures foundations of subject Criticism creates and secures understanding of subject Research engages exploratively, learningly with subject Teaching imparts subject imparts research techniques imparts critical skills imparts methods of scholarship All this has long-standing traditions and is, as the saying goes, 'what we have always done'. The challenge to our engagement in and with the humanities today is the digital medium. This engagement moves into fresh light and focus in consequence of the medium, since, through the new mediality, 'what we have always done' is no longer a matter of course, hence remaining unreflected in itself, but demands instead reflection and questioning. This goes hand in hand, moreover, with the circumstance that we encounter the matter of the diverse humanities' subjects, and the materiality of their objects, more and more at a radical remove from themselves. They have become, or are fast in the process of becoming, virtualised—which is the main condition for their becoming accessible digitally. Virtualised, however—and this is a paradox to be savoured—the material remains from the processes and traditions of history move into closer reach than ever. This is true not only because many a journey to libraries, archives, museums, cities living and dead, need no longer be made. It is also true since a material object virtualised in many respects permits closer scrutiny than does 'the thing itself'. That is, we engage, interact, 'interface' with the virtualised object in the digital medium in other and different ways than we do, through our senses unaided, with the palpable object. Some ways to scrutinize in order to understand, it is true, are not available in the digital medium: we can't smell or touch the objects or sense an aura from their specific setting, from all contingencies in their and our world of experience. By contrast, however, we can enlarge, refract, filter, segment a virtualised object with the aid of every digital technique available and discover in it and about it what was never discernible to the naked eye. Take, for example, the case of facsimiles of pictures or of manuscript or printed pages. Photographed and printed, they were a form of visual mediation well before the age of digitization. But it is interesting to note how little thought was given to the facsimile as mediator between the material manuscript, say, and our scrutinizing eye. In the Gutenberg era, the facsimile was simply a two-dimensional material object like the page in a book or a leaf of manuscript – even though it was less than these, actually, since it provided only the front view of either. While thus defective in terms of the original, however, it was yet material just like the original. To set off the facsimile from the original, it was enough to say generally that the facsimile simply never sufficed in every respect as a shortcut, let alone a stand-in for the original. Yet just how it lost out was seldom accounted for under categories of adequacy or loss. The researcher and scholar can and does not exclusively, or on a regular day-to-day basis, work with and from original documents.1 His or her resources are commonly copies—which I'll here specify as facsimiles. The situation is conditioned by distance and loss. [John Milton would have said, 'by distance and distaste', but I will not go that far.] Yet the degree of loss differs significantly between the aspects of raw materiality, inscription, and textual record. Under the aspect of raw materiality, of course, a facsimile, by definition, preserves nothing of the original: not its size, nor its paper, paper quality, its foldings, quirings, creases and tears, its inks, crayon markings or pencilings. The textual record in a facsimile, on the other hand – that is: the conventionalized graphics of letters, numerals and marks of punctuation, doubly controlled by the conventions of the alphabet, and of grammar, syntax, and semantics – ideally loses nothing in the reproduction. Wherever the text, that is, under the double control of the conventions of writing and of language, is unambiguous, the facsimile reproduction is wholly adequate. The reason for this is that text, qua text, is allographic, meaning that it is infinitely reproducible to say always the same thing. In terms of any given original document, however, the textual record must be recognized also as a sub-category of the inscription; and the inscription, janus-faced, must consequently be recognized, too, as a sub-category of the document materiality. It is the inscription that renders an original manuscript properly iconic, This paragraph takes a little further some notions previously developed in my "On Textual Criticism and Editing: The Case of Ulysses." Palimpsest: Editorial Theory in the Humanities, ed. George Bornstein and Ralph G. Williams. Ann Arbor: The University of Michigan Press, 1993, 195-224; pp. 213-214. 1 2 that is: autograph. The elements of a manuscript's iconicity, moreover, its positionings, spacings, shapes, and sizes of the marks on paper, importantly including, too, its doodles and all other manner of graphics, these are all random and unpredictable, non-conformant as they are to any secondary control conventions. (Nor should we forget that the actual tracings on paper of the textual record would be equally thus perceived but as random marks, as pre-Rosettastone hieroglyphics, so to speak—or, in other words, as unintelligible—were we not familiar with the control conventions of writing and of language.) It is evident that a facsimile in the Gutenberg era was simply a visual copy, and if reliable as such, then properly reliable only as copy from which to read the texts that were genuinely, that is: materially, inscribed in the original documents. They were and are adequate, in other words, for their allograph dimension alone. Their autograph dimension, however, which they also somehow, it is true, seemed to convey since they were visual copy, remained inert. For autograph material objects simply cannot (as we know) be copied into identities of themselves. Their copies have merely an illustrative, an 'almosting' quality. The facsimile of a material object, as fac-simile, can never be anything other than an illustration. This, I suggest, has been taken as a truth so self-evident that it seems seldom to have been reflected upon. With regard to the digital medium specifically, moreover (in these, the medium's early days that we still live in), so strong has as yet been the pull of habit and convention that facsimiles are mostly seen to be inserted here, too, merely as illustrations (an instance among many of how habits and modes of organisation have been and are still being deployed in the new digital medium that simply mime the world and environment of the material manuscript and book). What I wish to suggest, by contrast, is that a digital facsimile, a digitized visual copy of an original, may occupy a genuine interface position between the material object digitized and the humanities' scholar and researcher. (In terms of modeling theory, the digital visual copy might be said to perform a modeling function in relation to the material object whose place it holds in the epistemic system constructed in the digital medium.) Of course I am not saying that the fundamental difference between material facsimile and digital facsimile has never been thought of before. I merely wish to draw particular attention to some ways in which such thinking has in diverse quarters already been realized. If it has hitherto mainly been so realized intuitively and pragmatically, perhaps, without much drawing of systematic conclusions, it seems to me well worth considering as a concrete example of how the taking-over of conventions from the pre-digital era of ineluctable materiality may be profitably re-thought and so 'made new' in the transition. 3 Let us not fall into the trap, therefore, of regarding the digital facsimile as more or less the same thing as the photograph or printed facsimile. Let us instead focus on the difference. The digital image differs radically from images we are traditionally accustomed to encountering in that it is not a material counterfeit of the material original. It is a virtual image, a mirage, we might even say, projected onto that other interface surface, the computer screen. Though only in this manner virtually visualisable, it does still have a material substratum. Yet its materiality, as we know, differs from the materiality of (let us say) a replica on film of the original material object. It consists of a magnetically chargeable carrier medium for digital information from which in turn the virtual image on the computer screen gets generated. Nor do the given sets of bits and bytes have natural-language labels or headings. They have digital addresses—essential, as we shall see, for making use of the digital image, and making use of it as an interface entry to humanities' research, scholarship and teaching. A feature about a digital facsimile, seemingly innocuous, though at the same time, at least at first sight, slightly upsetting, is that you can cut it up. Imagine cutting up a material copy, let alone a manuscript original, or a painting. It were a destruction if not perhaps beyond repair, yet nonetheless beyond full re-integration. It is otherwise with a digital facsimile. Cutting it up may prove a fruitful way to activating it and putting it to use. In the pioneer project of a digital edition of Gottfried Keller's, the Swiss author's, works, for example – an edition of some twenty years' standing – the editor, Walter Morgenthaler, hit on a simple trick in aid of deciphering seemingly illegible manuscript readings for transcription. He cut out the images of single words whose graphics yielded determinable transliterations and matched them by digital image imposition with less unambiguously decipherable graphic tokens. For such adroit trickery, admittedly, the image is not absolutely required as digital image. The ploy as such, however, is a step in the direction of recognising the digital image as differing in kind from the image on a material carrier medium. What you manipulate is not visual matter (this only seems to be what you do – such is the illusion of virtuality!). It is delimited digital information that can be correlated to other delimited digital information. The correlation and the match or non-match in the relation of the sets of digital information become in their turn analyzable digitally – become determinable by machine interfacing. The digital interfacing is the counterpart to the intellectual assessment and judgement required critically to solve the task in hand within the given project of digital humanities research and scholarship. The task in my example was simply – simply! – to help deciphering a manuscript inscription correctly. 4 From here, we can make the circles widen. To this end, I'll be hovering still in the area of manuscript analysis and text study, however partial a field of humanities' scholarship at large this is. Take this image: seemingly a visualisation, an illustration merely of a James Joyce manuscript draft. But, since it is a digital image, I can begin to do things to it with impunity that would be anathema with an original. Photoshop helps to underlay pre-analysed and pre-defined areas of the image in different colours: 5 This is a preliminary, and in truth a fake step towards what is required, and possible, at the level of bits-and-bytes: namely, defining the areas digitally through coordinates and thus digitally to isolate them for scrutiny and (potential or actual) interactive engagement for research and criticism. To go beyond the photoshop stage, genuine algorithmic treatment is required. At the level of the computer interface, take this option: http://www.hypernietzsche.org/demo/bksailehwgabler-31 [ click 'synoptic'; click 'image' in the left column] In the left column is now highlighted the right-hand page from the previous visualisation (where the page was sub-segmented in shades of blue). Here, there is just one area, underlaid in pink; the upper and lower segments are brownish (and should ideally be phased out). Only the underlaid sub-segment is relevant, relating as it does to the transcription in the right-hand column of the two-column screen presentation. Thanks to the software I have had the opportunity to use – it is the original HyperNietzsche software, developed for genetic analysis of manuscripts some eight years or so ago – I am able to shift in the transcription column from one level of inscription to any other ('No'[='no overlay'], '1', '2', '3') and so study the composition process of the text. This is digital humanities' explorative research in action via the visual interface of a chosen manuscript segment that is not identical with a material page but instead digitally cut out from the source digital image of that page. If this example has demonstrated how a digital image as interface was capable of supporting an analysis of the coming into being of a text, here is another example that moves the 6 focus much closer to analysing a writing process as such. I am now looking at the upper area of the left-hand page in the double-page manuscript opening from Joyce's working draft: http://www.hypernietzsche.org/demo/bksailehwgabler-33 [ click 'wide', click 'Fit page in window', click 'Transcription' and move the magnifying glass over the digital image] By means of the software option of a digital magnifying glass, I correlate digital image and transcription differently. The transcription is here not textually oriented. It serves as an aid to visually analysing the inscription—though, mind you, the digital magnifying glass doesn't resolve what wasn't decipherable in the material original, with or without optical aids, or (as the case might be) aids from chemistry or physics (chemical analyses of inks, for instance, have been known to be used in manuscript autopsies; or so-called hyperspectral imaging in palaeography to recover sub-layered text in palimpsests2). What the arrangement of the transcription under the digital magnifying glass allows is to direct the eye again and again at the digital image of the manuscript segment so as to enable the researcher to confirm or dissent from the transcription given and, building upon such scrutiny, to arrive at interpretive conclusions about thought processes and meanings behind the writing. In terms of the intellectual input such engagement demands, the point to be made is then of course not that the interpretive conclusions couldn't have been reached in any other way: they could, for the intellectual input is not computer-dependent. However, and nonetheless: interfacing with material documents through digital images of them as interface visualisations on the computer's technical interface area, the screen, constitutes a qualitative leap in humanities research and scholarship. It contributes to the foundation of what we properly, in our day, term 'digital humanities'. At this point, critique of many a technical aspect of the procedures I have demonstrated is of course perfectly in order. As said, the software employed became operable some eight to ten years ago. As operative in my examples, it incorporates as yet very little of what are hot areas of development today, such as, for example, dynamic imaging. See for example Malte Rehbein with Patrick Shiel and John Keating: “The Ghost in the Manuscript. Hyperspectral Text Recovery and Segmentation.” Kodikologie und Paläographie im digitalen Zeitalter, Norderstedt 2009: 159–174. 2 7 http://kundigebok.stadtarchiv.goettingen.de Just take in for a moment the entrance page of a digital edition of a late medieval book of town ordinances for Göttingen in Germany3 that kept being modified and expanded over the years it was kept: you can at a glance intuit how it represents a continuous text, and then begin to study the digital edition for the historic information it accesses in highly differentiated analytical spreads. You can click yourself into the transcriptionally edited text passages under your chosen fields of interest – beer-brewing, perhaps? or taxation? or cattle-farming? or clothing regulations? – or you can view the digital facsimiles of the book's pages, again even under a digital magnifying glass. This edition doesn't either, as you can see, take on the problem of a bidirectional imagetext correlation – another hot area of development today, along with, for instance, interactive query and response procedures; or (since what I have shown are examples of manuscript and text editions) collation modules – to name just these items from what one might expand into a substantial list of desirables. But such critique is not the direction I wish to give to this talk. For even from the way that software has been realised for these specimens such as they are, the contours are discernible of the potential for interfacing digital humanities research and scholarship in widening relational networks. Let's continue to look at what may be developed from digital images, segmented at will under the precepts of a research plan. Let's say we are interested in the development of an idea and in its progressive contextualisation. From what has been possible to explore about the genetic progression of a text in a single draft manuscript, we may assume that the progression of an idea – say, a philosophic idea with Friedrich Nietzsche – may be explored through the progression of the texts articulating, developing and variously correlating the idea through a sequence of documents. With all these documents digitized, we digitally cut out from them all relevant passages and concatenate them. We survey on the computer screen what this concatenation yields for us in the form of paths linking the segmented images, like this: Malte Rehbein, 1. Kundige Bok. Digitale Edition. Göttingen: Stadtarchiv Göttingen, 2010 (http://kundigebok.stadtarchiv.goettingen.de) 3 8 or like this: 9 or like this: These are all so-called rhizomes of relationships of text (genetic) and ideas (thematic) – of thought inscribed in, and as, text – interfaced via digital images of material document pages, or more precisely: digital images of segments from the digital images of the real pages incorporating the member elements of these relationships. We could open one or the other of the colour-underlaid, and therefore sensitized segments. http://www.hypernietzsche.org/WS,8 [click 'Rhizome', and then click in any of the thumb-nails the highlighted segment] Working our way back, click by click, through any of these segments reveals each to be variously visualisable and analysable in the same way as my Joyce examples were. The HyperNietzsche system of which I thus profited was the original invention of Paolo D'Iorio,4 the Italian philosopher and digital humanities pioneer attached to the ITEM in Paris and working for a few years in Munich under a major German research grant – that's how we came into contact. At the conceptual core of his invention was the idea of just how to store the individual digital images, be they page or segment images, and how to concatenate them to provide a computer-interface display from which the images could function as research interfaces, as windows and doors to exploration. The idea was as simple as it is original. Each image was The foundational publication was Paolo D'Iorio, HyperNietzsche. Modèle d’un hypertexte savant sur Internet pour la recherche en sciences humaines. Questions philosophiques, problèmes juridiques, outils informatiques. Paris: PUF, 2000. 4 10 provided with a stable URL—ah, but that sounds as if the image was the real thing, and was just provided with a label. Yet that is not the way to put the matter. To be logically correct, one should say: the real world of the materially extant objects (books, manuscripts, manuscript pages—or anything of real material existence digitizable into the virtuality of bits-andbytes storage—all such was modeled in the digital medium as a set of URLs. In the materialreality world of, say, a library, shelf-marks or signatures function analogously. It is they that essentially constitute the material as well as the intellectual order of the library. Any given book is, by comparison, an incidental appendage to the library shelf-mark—as becomes evident when a book is stolen and simply replaced by another copy. As for URLs in the system of digital storage: not for nothing are they understood as stable digital addresses. They make up an essential set of place-holders constituting the empty form and container, as it were, for a digital research site with its defined (definable) provision of discrete and individual, yet at the same time correlatable and concatenable content. To the URLs can successively be hooked up the digital images of the material objects, plus digitized transcriptions of the inscriptions they bear (or of whatever other distinguishing and identifying marks they carry), plus any discursive prose (annotation, commentary, research essays or the like) relatable to any or all units defined as and by the given set of stable URLs within the research site – or, for that matter, throughout the world wide web wherever it holds relatable (for example, semantically relatable) content.5 It now I hope becomes clear why I have insisted on the digital image being distinct in nature from the image on any material carrier medium. Within the digital medium, defined as it is through bits and bytes only, the digital image is what can be appended to a URL, and what needs to be so appended in order to provide the visualisation by which to interface with what material-reality item the URL virtually stands in for. Moreover, by not labelling the digital image with a URL, but vice versa to label the URL with an image by which to interface with what object of analysis it points to (and so, by way of the image label, making the URL humanly perceivable) one creates the digitally-technical opening for the digital modelling of segments of material reality in diversities and complexities of ever increasing density or expansion. Such as, in terms of higher specificity and granularity, by sub-dividing URLs governing images of whole material pages into sub-sets of URLs for page segments only; or else, for instance, by generating those rhizomes I have exemplified, roping in clusters of referents in the real world of material objects. 'Generating' these rhizomes, however, be it clearly From an original involvement in devising the HyperNietzsche infrastructure has grown the Semantic Digital Library Framework MURUCA, to be explored for its potential at www.muruca.org 5 11 understood, is only seemingly an automatic performance of the software. In truth, it happens on the condition only that the links have been pre-defined. The pre-definition takes the shape – simply, you will agree, once the idea has been formed – of a selection list of URLs. The stable addresses are stepping stones for our interfacing with the simulacrum in the digital medium of real-world objects and a real-world, community knowledge we endeavour to analyse and research in and through the digital medium. With such perspectives on the nature and logic of digital humanities research and scholarship gained, let's now look, if briefly, at what is happening on your own frontiers. When opening the Cardiff Database of Mid-Victorian wood-engraved Illustration, http://www.dmvi.cf.ac.uk/ you might think (especially if you chose the 'browse' mode) you were entering a gallery of pictures. But such an attitude, which of course you do not hold, were a mite pre-Kopernikanian (that is: concluding from appearances that the sun circled round the earth – illusioning from seeing the digital image that you were looking at the real, material thing: the wood engraving in any given printed book). What you manifestly do, is of course: you enter the digital site (the databank) under a research agenda – choosing 'Keyword search' or 'Advanced search' – and while you find visualisations galore of Mid-Victorian wood-engraved illustrations, they are obviously there as digital images precisely serving as interfaces to generate, that is to call up, diverse ranges of information. Playing around most superficially as I briefly did, I entered 'George Eliot' under 'Advanced Search' as author, and 'Romola' as title in the category 'Fiction'. This gave me a set of digital images as interfaces for further enquiry. Even by surveying the list itself before going further, though, I was able to conclude that this novel of George Eliot's apparently was only in the serialised publication in the Cornhill Magazine supplied with wood engravings, but not in the subsequent book publication. This distinction could for me be an incentive to pursue the question how, in terms of illustrations, George Eliot's novels in book compared with the books of other Victorian authors. I didn't for the moment follow up that question. I switched instead to the digital images themselves, just choosing one at random, and selected the 'Iconography' rider for it. This offered me a rich array of categories defined for the image chosen, each of which I felt invited to click for further enquiry. The illustration coded ROM37, for instance: http://www.dmvi.cf.ac.uk/imageIconography.asp?illus=ROM037 represents, as its iconography categories claim, an Italian street exterior in Renaissance style with architectural features of archways, pillars and steps, and four people in it subsumable under the categories of 'men', 'women', 'children' and 'babies' (one, consequently, of each). 12 What the image interfaces in 'iconography' mode, moreover, are search options for 'activities' and 'objects', under which latter category are actually classified 'animals', and under 'animals', 'cows'. Never mind that it took me the longest time even to make out that there actually was a cow's head in the picture: clicking the category 'cows' takes me completely out of the interface image I started with and throws me instead into indeed a virtual gallery of illustrations featuring cows. I'll here refrain from herding them in. What, instead, I wish to point out: the Cardiff website of Mid-Victorian wood-engraved Illustration, in calling itself a 'Database', underplays, I feel, its own nature. 'Database' is a past-generation term denoting a kind of bulk data storage space on which the Cardiff website is a significant advance. Taking just the sequenced category list given under the 'Iconography' rider, we notice, in terms of form, that the sequencing is hierarchical—which, once we have hooked on to the image on the computer screen, is significant for efficient and successful access to the digitally stored data. Looking at the categories themselves, we recognise that the list is generated out of a systematics applicable in art history. This, needless to say, is a discipline distinct from literary studies as we know them, while we nonetheless of course include both among the 'humanities'. We might say, therefore, that even while we take our departure when looking at the wood engravings from terms provided by art history, we quickly discover that the art-historical criteria the website offers soon get intermixed with terms usable for literary and textual or book-historical exploration. The site is thus a site properly for digital humanities' scholarship, which is interdisciplinary by definition—and indeed for digital teaching in the humanities, an aspect to which I shall return. By way of an intermittent summary, then: the website's contents are opened up and rendered explorable via the interface of images sensitised at the machine interface, the computer screen. The ways in which the website's subject matter is organised and made accessible via a categorized terminology support boundary crossings between humanities' disciplines as traditionally demarcated. The site, in my understanding, is a good example, and qualifies by its specific design to set a pattern, for what may be termed a knowledge site in the fields of digital humanities' scholarship. The term 'knowledge site' was brought into circulation quite recently through Peter Shillingsburg's book From Gutenberg to Google of 20066 and has meanwhile gained widespread, though rather undifferentiated currency. It designates an advance in the digital medium beyond the kind of accumulation of positive information that tends to dominate in books 6 Peter Shillingsburg, From Gutenberg to Google: Electronic Representations of Literary Texts. Cambridge: CUP, 2006. 13 (by necessity, in that medium). The 'knowledge site', therefore, is a transformation in the digital medium of the 'information site' that books provide. The transformation is effected by means of the capability of the digital medium to store information in relational forms of organisation, in relational web structures. Moreover: to deposit information in such manner in the digital medium means to input it always already in view of retrieval. This activity itself must rely on powers and strategies of pre-organisation, which obviously cannot but already spring from humanly pre-structured knowledge. Or, in other words: the knowledge in knowledge sites, and the building of knowledge into them, grows out of creatively participatory intelligence. In consequence, of course, it also initiates intelligently generated knowledge. However, the knowledge generated from deployments of currently realised knowledge sites takes shape only at the far end, beyond the moment of output from the digital site. Present-day scholarly websites are consequently definable largely as uni-directional in design. They render usable and disposable knowledge assembled in them. Yet while they assist their users in expanding and enriching their individual knowledge, they do not at the same time also make provision for the users to respond to or expand or enrich the site. Even 'uni-directional' as they are, though, the 'knowledge site' scholarly websites increasingly put at our disposal are unquestionably of great, often stunning encyclopaedic use. And anyhow, it is (by and large) in this mode that 'knowledge-site' scholarly websites are actually operative and work (as evidenced by the Cardiff website of Mid-Victorian wood-engraved Illustration). Beyond is trial and experiment. Where the trend 'beyond' appears to be leading, is towards the bi-directional research site, dynamically progressive, capable of interactive modification, and above all sensitive and selective in terms of the semantics of its core contents and their potential aggregation and enrichment. This is where, true enough, we meanwhile have visions of grand design, which we also shall continue to require. While (I fancy) not entirely beyond my imagination, they are by and large beyond both my expertise and my practical experience. Hence I will no more than mention that to seize on the semantics of digital contents, you need procedures to establish, and/or machine-generate, what in the IT world are termed 'ontologies': semantic, that is: meaning-related, (pre-)defined filters that help to raise the efficiency of searches through bulk digital content. (The hierarchized lists of terms leading from the interface digital images to the digital contents of the Cardiff website of Mid-Victorian wood-engraved Illustration are, I would say, prototypes of such 'ontologies'. More modestly, and indeed less pretending to implications of philosophy, your term for these lists are 'vocabularies'.) Putting such ontologies, or vocabularies, to use on digital content is obviously another mode of interfacing, using not 14 digital images as windows and doors to the materials to be researched, but verbal terms. You may wonder in passing what is the difference between using a search term and a vocabulary term for data exploration. Searches in pre-semantic-web days were always string searches: your computer algorithms grazed the binary data record linearly, albeit with the speed of lightening, to pick out the binary string that represented the term you searched for. Data exploration via semantic-web potential, by contrast, uses the relational nature of the web, any web. It concatenates data to correspond not as string, but as meaning to your vocabularydefined, and thus semantically selective, query. Your query, as one might say, interfaces into the digital medium to weave, out of the bits-and-bytes of the data stored, the on-the-fly-configured web carrying the semantics, the meanings, you have (pre-)defined – and should you experience that more is handed back to you from out of the digital medium than you anticipated receiving, you will tend to wonder a little: as if there were magic in the web, like in that of Desdemona's handkerchief. The sense of wonder arises from much seemingly autonomous hardware and software operation and 'machine-intelligence', into which in turn much ingenuity has been and is constantly being invested. The initial intelligence, however, required to establish ontology semantics cannot of course but be human—or there would be no chance in the first place of interacting, interfacing, with the digital medium as it today promises to hold our research sites of the future. Such research sites will be bi-directional. Conceptually, the relational combination of content in the knowledge site should provide nodes of knowledge for the user to engage with. But then, as we observed, the engagement cannot but generate enhanced knowledge. The knowledge site must consequently open up to enlargements of content and a deepening of hermeneutic understanding. In terms of design, this would mean that the nodes of engagement be made to function as points of interface from which interactively and dynamically to feed back the knowledge modifications and enhancements into the site. That is, the knowledge site of present conception should mutate further into a genuine research site. However: the visions and conceptions tend still to be heavily biased towards the construction of research sites in terms of their technical design and content organisation. Research-site uses, by contrast, still tend to be less focussed on. Hence, where the deployment of the digital medium in the humanities will in future demand increased attention is in the areas both of technical and of subject-dependent operations and interactive interfacing procedures. A strong incentive to further developments in such directions may, I believe, come from thoroughly reconsidering and re-conceiving the role of teaching. Teaching, as we should become aware, is not a secondary and derivative task. It is instead a seminal activity of the 15 cultural endeavour of science and scholarship, whether circumscribed as 'humanities' or otherwise. A simple axiom therefore from which to start would be: research is essentially a learning process. Hence, to receive teaching as well as to teach is to lay the foundations for acquiring both research skills and knowledge by means of such skills, and so to qualify for, and to do, autonomous work of scholarship. Consequently, it would make a great deal of sense to structure the medial support of digital humanities "bottom-up", so to speak, that is, from foundations laid through teaching and in learning. Just imagine the teaching-and-learning situation as the exercise ground for engaging with the multiple ranges of the humanities digitally: to learn simultaneously to handle the medium and to comprehend the subject fields; to build the units of subject matter and the research tools in reciprocal tandem 'on the fly'; thus to interface equally with the digitally held and widely ranging subject contents and with the team of learners-and-teachers of which one is a member; and altogether to communicate throughout with the fellow learners and teachers on foundations of a shared pursuit of explorative and dynamically progressive, digitally-based research aimed to further the common endeavour of humanities' scholarship. This wishful thought concludes for today my ideas towards interfacing digital humanities research. Yet wishful though it may be, it need not be taken as mere fancy. It should be understood as a expressing an urgent demand, and need. [Delivered on 29 September 2011 in Special Collections of the University of Cardiff Library as the "Inaugural Annual Cardiff Rare Books and Music Lecture".] 16Nathan Yau recently wrote a posting about the different words used for visualization and infographics. His definitions are interesting because they reveal quite a bit about his background and main focus, and his blind spots give some insights into the community he’s working in.DefinitionsHere are Nathan’s definitions with my comments. Since I have something to say about almost all of them, I decided to quote his entire list. I don’t claim that my view is better or more correct, I simply want to provide a second opinion.data visualization — Graph-like image or interactive, usually tied with data exploration and analysis.First definition, first major difference: a lot of people in the visualization field would consider data visualization to denote scientific visualization (i.e., volume or flow visualization of data with spatial dimensions), for whatever reason. There may be simple historical reasons for this, or the assumption that scientific visualization deals with more data.In any case, data visualization has a particular slant, and is not the same as visualization in general, and certainly does not refer to charts or ‘interactives.’visualization — Similar to data visualization and often is, but can also be the later described information visualization.It’s the generic term for all of visualization, though historically it has denoted scientific visualization. Traces of that are still there, like the Vis track at VisWeek, that would be much better served by the term SciVis (as the counterpart to InfoVis). That would also eventually make it possible to use “Vis” again as a shortcut for the entire conference, not just one track.viz — A shorter version of visualization in both length of word and thoughtfulness in design and data.Nathan nailed that one. I have no idea where the z comes from, it makes absolutely no sense. I wish people stopped using that spelling.vizzes — Plural of viz and evokes an image of urinals.A horrible term, and I also agree with Nathan on that one.information visualization — Usually encapsulates what data visualization is about, but usually makes an effort to provide “actionable insights.”Apart from the data visualization part (see above), I would also argue with the insights. Visual analytics has played those up a lot more, while infovis is a lot more about the basic representation and interaction issues. It’s not an easy line to draw, and there are many counterexamples, but I don’t think that “actionable” is the first thing that comes to mind when most people think of information visualization.InfoVis — An annual conference that most visualization researchers go to.Agreed, except it’s information visualization researchers.infovis — Research of information visualization that people talk about at InfoVisAgreed.infoviz — Often a crappier version of infovis and closer to what will follow shortly. However, it could just be an indicator of the person using the word, and the work might be good.Vis with a z is simply wrong. I don’t think there’s much you can deduct about either the work or the person using it, but they should stop doing it nonetheless.information graphic — Serious work from journalist-type folks who provide a narrative with data.Agreed. Though just like with visualization, different people have different ideas about the word, and they all think their use is correct.infographic — A toss-up between information graphic and [INFOGRAPHIC], but usually the latter and often unnecessarily big.Short for information graphic, just like infovis is short of information visualization. I don’t think it’s helpful to draw lines between the full and contracted versions of words, that just leads to confusion.[INFOGRAPHIC] — A gigantic graphic with lots of graphs, numbers, icons, and fancy-ish typefaces. Often used in blog post title. Always useless.Speaking of confusion, it took me some time to figure this one out: it’s the tag that’s often used in a title to tell people to click on it to see the wonderful infographic “after the jump.” They’re often a sign to stay away, true, but sometimes they can be good.infograph — No idea who started using this term, but it’s dumb. Stop it.I had never seen that one before, but let’s stop it before it even starts. It’s bad.chart — Typically looks very statistical and close to a table.This mostly sounds like it’s static, and it does have a statistical ring to it. I imagine crisp, black lines, printed on paper, with only a handful or data points. I don’t think it’s really that clearly defined though, and I’ve seen a lot of interactive and colorful charts. And they don’t even all have to be made with lots of care, the term chart junk isn’t just an accident.It’s also a generic term for something that depicts data, like a bar chart or a pie chart. These can be part of some fancy, animated, 3D extravaganza, and still have chart in their names.data chart — Even closer to a table of numbers. And kind of redundant.Quite redundant, yes.graph — It’s like a chart, but it sounds more visual, because it’s the root of “graphic.”In the visualization community, graph is typically used in the mathematical sense, where it refers to a network. There is a whole subfield of graph visualization and a mostly unrelated field of graph drawing, both of which exclusively deal with network graphs. In fact, seeing the term graph refer to some kind of chart or visualization is rather rare (and confusing) in the visualization community.data graphic — It’s an ambiguous term I like to use that doesn’t upset people who like to argue what visualization is and what it’s for, but clear enough that most people know what it is. Also implies that data comes first and is the driving force behind the graphic.It’s not a bad term, especially because it conveys the importance of the data. To me, it sounds more infographic-y than a pure visualization, i.e., I’d expect it to have some kind of story and be designed by a designer rather than straight out of a program.ObservationsIt’s interesting to see Nathan completely ignore scientific visualization, though it’s also not surprising: he is not a product of the academic visualization community. His focus is on statistical graphics and more information graphics-style things than visualization in general.He also doesn’t seem to be aware of visual analytics and its focus on insight, sense-making, and decision support. This distinction is of course still up for debate, but I think it would help to clarify at least that one particular line, since all others are so fuzzy and ill-defined.Nathan’s list is a good reminder that people outside the academic community have very different ideas about visualization, even – or perhaps especially – if they’re working in closely related fields.Teaser image via FlickrÓIf there are two things that academia doesn’t need, they are another book about Darwin and another blog post about defining the digital humanities. But it’s always right around this time of year that I find myself preparing for my digital history course and being pulled down the contemplative rabbit hole about how describe the nature of the digital humanities to a new and varied audience. But rather than create my own definition, I wanted one cobbled together from everyone else.There have been some very good digital humanities definition pieces recently (those by Rafael Alvarado and Matt Kirschenbaum spring to mind). But many of the longer ones, as smart and provocative as they are, often muddy the introductory waters more than clarify them. Sometimes that’s precisely their point, but I’m always on the lookout for a reductionist, well-precipitated overview mixture for my class (and for myself, when I get confused) that can be progressively dissolved into a more homogenous solution. I wanted a list of acceptable DH definitions that was as simple as possible—but no simpler—from the community itself. Even better, i wanted a shortlist of types of definitions that sketch out the contours of the field.For this kind of exercise, there’s no better resource than the TAPoR wiki on “How do you Define Humanities Computing / Digital Humanities?“, which presents pithy definitions from ~170 people, who for one reason or another were compelled (thankfully) to offer their own take on the nature of digital humanities or humanities computing. The format surely encouraged sound bites rather than nuanced formulations, but the quick take still reveals the sentiment of the community—perhaps better than longer essays would have. What follows is my categorization of the responses from 2011. I raced through these at the end of #clioF11(tues)’s first meeting, so I post them here mostly as a reference for my whiplashed students. But I’d love to know if anyone else finds it useful.At first glance, it appeared that the fascinating but disparate variety of responses more closely resembled an unruly pile of pick-up sticks than any useful guide to the DH community. After a bit more perusing, however, i found that the definitions could be cleanly sorted into a relatively small number of categories. Needless to say, my scheme is neither the only possible grouping, nor necessarily the best. But i found mutually exclusive sorting to be simple and easy, perhaps an indicator that i wasn’t being too arbitrary or forceful. Surprisingly few definitions landed on the boundaries between categories. In those cases i filed them under what i took to be the predominant sentiment. A handful responses that did not seem to really say anything (besides explicit refusals to offer a definition) were left uncategorized (my working label was “what?”).Due to a few tricky category decisions, categorical gray areas (though i tried to minimize possible overlap), and arithmetic failure, the numbers themselves are meaningless as precise counts. But i think that they are rather suggestive and illustrative as to how the Wiki contributors see the field. What’s most interesting is how the relatively few categories themselves grew organically from the responses—with what i consider very little invention on my part.The Categories (and some observations)55 – variation on “the application of technology to humanities work” 22 – working with digital media or a digital environment 15 – minimize the difference between DH and humanities 12 – umbrella or blanket nature of DH label; issues that humanists now face 12 – using digital AND studying digital 12 – refusals to define the term 10 – methods AND community 9 – digitization / archives 9 – studying the digitalThe most popular response—the application of technology to humanities work—was unsurprising to say the least. It is perhaps worth noting that phrases like “application of technology to humanities work” were about 3 times as common as those like “intersection of technology and the humanities”. Either way, i still can’t shake a vague unease about defining DH in such broad technology terms (the umbrella responses described below focused less on technology per se), and i think it raises important questions about the nature of how much technology (given its pervasive nature) needs to explicitly figure into the digital humanities. As other respondents hinted: isn’t everyone using technology in the humanities these days? And increasingly so? Perhaps it’s innovative use that’s important (as Doug Reside suggests), but isn’t innovation always required in scholarly pursuits? Or maybe we just need to be technologically innovative w/r/t the analog humanities? But does a technology emphasis detract from the humanistic value of our work, and shift the focus to research methodologies rather than results?Reluctance to foreground research (dare i say computing?) over communication and workflows perhaps led some respondents to emphasize the use of digital media, or publishing and collaborating in new media environments. Though responses took the nature of “the digital” as crucial to DH, ranging widely across the spheres of publishing and networking within humanistic scholarship. Obviously there is a fine (if extant) line between using new media and using technology. But i think the responses—in both spirit and language—warrant separate categories between using technology and using digital media, even if that difference can be considered one of emphasis rather than of kind.The next cluster of four categories, most explicitly the (usually glib) refusals, foreground the difficulty in crafting any kind of definition. Others responded more thoroughly, and more helpfully, but refused to differentiate digital humanities from the humanities at large. This is a valid point, but to hold this position is to suggest that DHers don’t have any different kind of concerns (methodological, theoretical, practical, professional) than anyone else in the humanities. This doesn’t seem to be true right now. Truer to the status quo, in my opinion, were the several responses in that same category that emphasized the (ideally) fleeting nature of any difference between the digital humanities and the humanities—that is, digital humanities as the future of the humanities; different in some ways now, but not fundamentally so. Or, perhaps digital humanities is simply akin to new media in that its core characteristics ride the wave of technological change.The responses I have labeled as umbrella- or blanket-like, I think are some of the most helpful because they pursued a discriminatory inclusivity—a middle ground between open arms and gate-keeping—that embraced the variety of issues that digital humanists like to talk about (in addition to new research methods and digital media of the other categories) like copyright, access to information, curation and use of digital resources, publishing, and so on, without making it inordinately difficult to think of something that wasn’t included under the DH tent.A solid group of responses must be located at the intersection of using the digital and studying the digital. In a way, this group overlaps with the “applying technology” group, but are singled out here for the insistence that studying the effects of the digital is just as important as using it. These two efforts might well be two shows under the same DH tent with different performers and audiences. Whether this is mutually beneficial or distracting remains an open question, i think.Not surprisingly, community and methodology tended to get mentioned together; i wouldn’t characterize any definition as pointing to one without the other. But it isn’t just methods that make the community, either: respondents rightly and broadly construed “methods” as ranging over various aspects of research, teaching, and broader communication, all the while embracing the variety of methodological approaches that their colleagues take. To me, this suggests that the community has some autonomous existence outside of methodological similarities.Responses that foregrounded digitization or studying the digital (without consideration for the affect on humanities scholarship itself) thankfully received the fewest nods. At least as formulated on the wiki, these definitions were the most restrictive, and in many ways fundamentally contrary to the general sentiment of the community about what kinds of efforts really characterize the digital humanities. This is not to say that digitization efforts and the corresponding challenges are not to be included as part of the digital humanities (they are!), but that a useful definition must be more inclusive.Some representative (usually short) examples of the categories:some variation of “the application of technology to the humanities”The intersection of humanities and computer technologies -Lorna Richardson, UCL, UKDigital humanities is the intersection of work in the humanities (research, teaching, writing) with technology (tools, networks, interactions), when the practitioner is consciously exploring a humanistic subject and a technological method, at the same time. -Elli Mylonas, Brown University, United StatesDigital Humanities are the application and the use of computing tecnologies for the research, teaching and investigation in the disciplines of the humanities. -Ali Albarran, Universidad Nacional Autónoma de México, Mexico5 responses included in the above category emphasized computingUsing computational tools to do the work of the humanities. -John Unsworth, University of Illinois, USAThe theorizing, developing and application of/on computational techniques to humanities subjects. -Edward Vanhoutte, Centre for Scholarly Editing and Document Studies / Royal Academy of Dutch Language and Literature, BelgiumUse of digital media/medium/environmentAnything a Humanities scholar does that is mediated digitally, especially when such mediation opens discussion beyond a small circle of academic specialists. -David Wacks, University of Oregon, USAThe performance of humanities related activities in, through and with digital media. -Christopher Long, Penn State University, USAFor me, but this is very specific, Digital Humanities is to interconnect humanities researchers, software developers and infrastructure providers in order to contribute to the research and the research possibilties in this discipline. -Douwe Zeldenrust, Meertens Institute (Royal Netherlands Academy of Arts and Sciences), The NetherlandsEmphasis on its umbrella or blanket natureI think of digital humanities as an umbrella term that covers a wide variety of digital work in the humanities: development of multimedia pedagogies and scholarship, designing & building tools, human computer interaction, designing & building archives, etc. DH is interdisciplinary; by necessity it breaks down boundaries between disciplines at the local (e.g., English and history) and global (e.g., humanities and computer sciences) levels. -Kathie Gossett, Old Dominion Univ, USAWe use “digital humanities” as an umbrella term for a number of different activities that surround technology and humanities scholarship. Under the digital humanities rubric, I would include topics like open access to materials, intellectual property rights, tool development, digital libraries, data mining, born-digital preservation, multimedia publication, visualization, GIS, digital reconstruction, study of the impact of technology on numerous fields, technology for teaching and learning, sustainability models, and many others. -Brett Bobley, NEH, United StatesRefusalsWith extreme reluctance. -Lou Burnard, UKI hate this question, and I don’t have an answer for it. Neither, it seems, does a large portion of the people who might be called Digital Humanists. I’ll leave it at that. -Justin Tonra, University of Virginia, USAStudying the digitalAn area of study that focuses on the digital in our daily lives–how we study, think, and interact. -Pollyanna Macchiano, , USDigital Humanities is the acknowledgement that human creativity is, for the moment, deeply entangled with our technological tools and networks. The media extensions cannot be separated from our reality. -Anastasia Salter, University of Baltimore, USAexplicitly using digital AND studying digitalI am currently using a short definition, which is that Digital Humanities is a combination of using computer technologies to study human cultures and studying the effect of computer technologies on human cultures. -Scott Kleinman, California State University, Northridge, USAI see ‘Digital Humanities’ as an umbrella term for two different but related developments:1) Humanities Computing (the specialist use of computing technology to undertake Humanities research) and 2) the implications for the Humanities of the social revolution created by ubiquitous computing and online access. Since the late Noughties the latter seems to have become the driving force in DH with responsibility for much of the ‘boom’ in public interest and funding. -Leif Isaksen, University of Southampton, UKmethod AND communityIt is both a methodology and a community. -Jason Farman, University of Maryland, College Park, USASomewhere between a toolset and a mindset, to do DH is to confront the assumptions and implications of the long analog history of the word. -Matthew Fisher, UCLA, USAThe digital humanities is a name claimed by a community of those interested in digital methodologies and/or content in the humanities. -Rebecca Davis, National Institute for Technology in Liberal Education, United StatesMinimal difference between DH and humanities“We don’t distinguish digital sociology or digital astronomy, so why digital humanities? Just because computers are involved doesn’t mean the basic nature of the subject area is any different than it has been been traditionally.”Digital Humanities is, increasingly, just Humanities – as far as I’m concerned. New tools lead to new methodologies, new perspectives, and new questions that all humanists should be aware of and concerned with. -Benjamin Albritton, Stanford University, USAHumanities gone digital and vice versa -Anna Caprarelli, università degli studi della Tuscia (Viterbo), Italy5 included in the above category emphasized the fleeting nature of any present differenceA name that marks a moment of transition; the current name for humanities inquiry driven by or dependent on computers or digitally born objects of study; a temporary epithet for what will eventually be called merely Humanities. -Mark/Marino, University of Southern California, USADigital Humanities is what humanities will be in the future. It is public, dialogical, collaborative and made of collectives. It allows for remixing and re-imagining how we think and analyze traditional forms of knowledge creation, knowledge sharing and knowledge storage. -Jade E. Davis, University of North Carolina at Chapel Hill, USAFocus on Digitization and ArchivesDigital libraries are a great example of an outcome of Digital Humanities. The interaction and combination of the new digital era with history, librarianship, literature, etc. gives a wider frame for researchers of all different branches to work in. Now the full texts of important writers are just a click away! -Ines Jerele, National and University Library, Slovenia, SloveniaDeveloping tools and workflows to create comprehensive, interoperable, and innovative digital resources. -Jennifer Stertzer, Papers of George Washington, University of Virginia, USASo what?The categories here are hardly surprising. More interesting, to me at least, was that one needs only a handful of categories to cleanly parse so many formulations; and these categories deliberately highlighted rather subtle differences in definitions.Even if attempts at constructing (and reading) definitions grow tired, periodically taking the pulse of the DH community seems worthwhile in that it reflects both recent and future developments in how the field is being shaped by those who consider itself its practitioners. Scholarly legitimacy, for which DH work seems continually reaching, requires some disciplinary boundaries, at least for now. Without it, people who will make important judgments and decisions about our scholarship, funding, and jobs cannot properly evaluate our proposed or completed work (hence Matt Kirschenbaum’s apt definition of DH as a “term of tactical convenience”). Feeling out boundaries is not always fun. I admit that I vacillate between wanting to draw some disciplinary lines in the sand, to pee on some research hydrants, and to simply throw up my hands in the face of an utterly pointless and futile debate.The relatively few categories suggest some important questions: Are all of the definitions and their crucial qualities (community, communication, methodology, digitization, etc) worth equal emphasis? Are some more representative of “the field” as it is or as it should be? Perhaps the DH label has gotten enough traction within the broader community that the more pressing question is: what should NOT be included within the big tent of the digital humanities? That’s the subject for a different essay, of course.Although I haven’t attempted a comparison to 2010, my sense from the definitions (especially with the many references to digital media and studying the digital) was that the field seems to be sending out tendrils in all directions, and in particular moving away from its original Humanities Computing roots. I couldn’t make a cruder measure, but i found it interesting that “computing” appears about 30 times among 170 total responses in 2011, and about 40 times among 70 total responses in 2010.In the endI must tip my cap to Eric Forcier, whose reply adroitly eschews disciplinary rigor in favor of admirably capturing the spirit of the DH community—especially in painting DH as an ephemeral, seemingly idiosyncratic curiosity that either attracts or repels people, and often changes them fundamentally:When I first applied to this grad program, my understanding of what DH was all about was crystalline in its purity. Not so today. My idea of DH is that it’s sort of like a highway oil slick on a sunny day. When you look at the slick, depending on the angle, you might get a psychedelic kaleidoscope of reflected colours; if you’re lucky you might spot your reflection in it; then again, all you might see is darkness. And if you feel compelled to step in it, don’t be surprised if you slip. Those stains will not come out. -Eric Forcier, University of Alberta, CanadaAll of a sudden, I’m starting to pick up signs of a digital humanities backlash. That’s a shame because there’s a big difference between digital humanities and online education since faculty can seemingly control the first thing, but not necessarily the second. The digital humanities help us do what we already do better. Online education…well, since I don’t feel like linking to my entire archive for the last three months, let’s just say I’m not convinced it helps us do anything.Nonetheless, it appears that both these technologically-driven phenomena have employment implications, as Natalia Cicere describes here:So it seems quite natural that there should be wariness and resistance to the growing presence of digital humanities. Perhaps there is some bitterness that you might get your new Americanist only on condition that her work involves a Google Maps mashup, because it was easy to persuade people that your department needed a new “digital humanist,” whatever the hell that is, and it was not easy to persuade people that you needed somebody to teach Faulkner.The situation is not improved by the confrontational attitudes of certain factions of the digital humanities establishment (such as it is), which are occasionally prone to snotty comments about how innovative DH is and how tired and intellectually bankrupt everybody else’s work is. (Not so often, I find—but even a little is enough to be a problem.) Under those circumstances, DH seems clubby and not liberating; not a way of advocating the humanities but an attack on it, and specifically on the worth of that Faulkner seminar that you teach, and that non-digital research that you do. Why, an established scholar might reasonably ask, should I even deal with this “digital humanities” nonsense? Shouldn’t I just keep teaching my Faulkner seminar, because somebody ought to do it, for Christ’s sake?She’s not suggesting that anyone ignore the digital humanities, but it appears as if the impact of this technology on our profession is a lot more important than those people whose eyes glaze over at its very mention seem to think. Take, for instance, this:I don’t think Dan Cohen is intending to be self-important there (since DH is what he does). He’s just describing how he sees the future. Whether that’s a good thing or a bad thing is totally irrelevant. What’s important is that that would be a huge change from the way things are done today, and we all need to be prepared for the changes it brings.The Postal Service is apparently going to go bankrupt soon. Its effective demise is apparently inevitable, but do we want it to disappear tomorrow (or by 2025 for that matter)? Suppose you want to continue to teach your Faulkner seminar the same way that you always teach it. Is there anything necessarily wrong with that? Not necessarily, assuming that you’re a good teacher. However, if almost everyone else’s seminars go digital somehow, there will be serious pressure on those people who don’t do this sort of thing to start doing it.Just because the digital humanities offer a different way to teach history does not mean that the old ways are necessarily bad. The key is to keep the process in the hands of professors rather than the people who administer their departments since I hope we’ll all agree that technology should be a tool rather than a club.Neither can my father, although both are proficient readers. My sister and her family have multiple televisions, cable, a gaming system and most recently, they have acquired cell phones (the un “smart” sort), but they do not own a computer. This is not their choice. They are regular hard working people, laboring in the service sector in long-held stable, but low paying jobs. They worry about paying for a serviceable car, not the web. Typical of many working class people, they are much less connected to the world through the internet than are their wealthier and more educated peers.In fact, they are not connected at all. I am their connection to a digital landscape they keep hearing about but have never seen themselves. They have never seen Facebook, and Google is not one of their verbs. They call me weekly, sometimes daily, for info they don’t have access to, from answers to health questions, to vender and business info, to info about my nephew’s public school, to local news in my sister’s small town, to trivia.We are not alone in this arrangement. Like many first-generation college graduates, I oscillate between these disparate social spaces — fast enough sometimes to affect a sense of self and identity — moving forward and backward between digital and analog, the material and theoretical, filial and heretical, oral and textual traditions and cultures, though “native” to none. Being a doctoral student associated with a university continues to digitally connect, arrange and organize me. Social stratification in access to the expansive communicative vehicles and habitats that are encompassed by the terms “social media,” “information technology,” or “digital tools” exacerbate and further entrench the other multitude of ways that I am distinguished/marked from my family of origin. Such digital resources (informational as well as tool-based) guide and color the way we think and live, shape the very way we come to understand ourselves. What are the consequences of this class-stratified access to information, knowledge and tools?Luke has previously posted this Michael Wesch video in his discussion of the new media revolution and its promise of greater social connectivity. I think it’s worthwhile to repost Wesch’s “Web 2.0 … The Machine is Us/ing Us” here as a dramatic accounting of the potential deprivation that digital exclusion represents.Panic arose while viewing this video as I newly considered the degree of impoverishment faced by those of us (i.e., my family) who are absent from this “revolution,” who don’t “teach the machine,” who are not tagging, or naming, the (digital) world, who shape it by their absence from it, who become more invisible, less active social participants even as others become more productive and more participatory in our society. Indeed, “we will need to rethink a few things,” particularly equality, democracy, citizenship.Working-class studies scholar, Denise Narcisse details the issues and consequences surrounding the protracted and growing digital inequality faced by the poor and working class in the U.S. Both Sherry Linkon and Jane Van Galen take up this discussion in subsequent blog posts, outlining the challenges of integrating new media and digital tools at their campuses, and in their classrooms, when their student bodies disproportionately come from working class families and communities. All of the hardships they detail for the student populations the work with are even less surmountable for those outside of a school system, with more limited access to digital technology. As a student and employee within the university setting, I have been honing my meta medium capacities for some time and on multiple levels: content, context, medium, form, speeds — most of my family are a full 5 steps back in the “meta media fluency” endorsed by Gardner Campbell.A case in point: This summer, my father conferred with me about which laptop he should buy, now that he could afford one. I was at a complete loss of how to advise him, especially given that we live on opposite sides of the country and navigate a technological knowledge gap between us that feels even greater. He was silent at my suggestion to head to a decent retail store and trying some out. When I then suggested that he explain to the sales staff that he is a new user, he promptly cut me off and suggested to me that they would have to pick themselves up off the floor laughing. I then realized that he actually has a lot of shame around his lack of knowledge and previous inexperience, and had come to me for real help. I then dutifully identified a highly rated, moderately priced notebook, printed out the reviews and specs, and snail mailed it to him with a print out of where he could purchase it in person. Two weeks later, he was completely technologically up to date in terms of hardware — he even has a mobile hotspot.It is fall now, and he has yet to get online, access email, see the world through the web. Despite that year of Geek Squad service he purchased, his contract with a major ISP, and at least one member of his immediate family with passable digital competencies, he is still living in a very different social world from the majority of Americans overall – though very much as many working class Americans do. Perhaps that’s not a bad thing, as suggested by this recent Toyota commercial, which privileges material world pursuits over cyber ones in their efforts to sell a SUV.Turns out, if given the choice between a new car or a new computer system, my sister would take the car, reasoning that a computer was a luxury. I say, it depends on who you ask.I’m delighted that the edited version of Hacking the Academy is now available on the University of Michigan’s DigitalCultureBooks site. Here are some of my quick thoughts on the process of putting the book together. (For more, please read the prefaceTom Scheinfeldt and I wrote.)1) Be careful what you wish for. Although we heavily promoted the submission process for HTA, Tom and I had no idea we would receive over 300 contributions from nearly 200 authors. This put an enormous, unexpected burden on us; it obviously takes a long time to read through that many submissions. Tom and I had to set up a collaborative spreadsheet for assessing the contributions, and it took several months to slog through the mass. We also had to make tough decisions about what kind of work to include, since we were not overly prescriptive about what we were looking for. A large number of well-written, compelling pieces (including many from friends of ours) had to be left out of the volume, unfortunately, because they didn’t quite match our evolving criteria, or didn’t fit with other pieces in the same chapter.2) Set aside dedicated time and people. Other projects that have crowdsourced volumes, such as Longshot Magazine, have well-defined crunch times for putting everything together, using an expanded staff and a lot of coffee. I think it’s fair to say (and I hope not haughty to say) that Tom and I are incredibly busy people and we had to do the assembly and editing in bits and pieces. I wish we could have gotten it done much sooner to sustain the energy of the initial week. We probably could have included others in the editing process, although I think we have good editorial consistency and smooth transitions because of the more limited control.3) Get the permissions set from the beginning. One of the delays on the edited volume was making sure we had the rights to all of the materials. HTA has made us appreciate even more the importance of pushing for Creative Commons licenses (especially the simple CC-BY) in academia; many of our contributors are dedicated to open access and already had licensed their materials under a permissive reproduction license, but we had to annoy everyone else (and by “we,” I mean the extraordinary helpful and capable Shana Kimball at MPublishing). This made the HTA process a little more like a standard publication, where the press has to hound contributors for sign-offs, adding friction along the way.4) Let the writing dictate the form, not vice versa. I think one of the real breakthroughs that Tom and I had in this process is realizing that we didn’t need to adhere to a standard edited-volume format of same-size chapters. After reading through odd-sized submissions and thinking about form, we came up with an array of “short, medium, long” genres that could fit together on a particular theme. Yes, some of the good longer pieces could stand as more-or-less standard essays, but others could be paired together or set into dialogues. It was liberating to borrow some conventions from, e.g., magazines and the way they handle shorter pieces. In some cases we also got rather aggressive about editing down articles so that they would fit into useful spaces.5) This is a model that can be repeated. Sure, it’s not ideal for some academic cases, and speed is not necessarily of the essence. But for “state of the field” volumes, vibrant debates about new ideas, and books that would benefit from blended genres, it seems like an improvement upon the staid “you have two years to get me 8,000 words for a chapter” model of the edited book.Yesterday I presented the preliminary findings of my analysis of 134 syllabi. If you are interested in adding your syllabus to the collection, you can email it to me, or you can join the Digital Humanities Education Zotero group and place it into the Syllabi>ContributedSyllabi folder. Thanks!Also, if you’d like to explore the corpus yourself, you can now do so using Voyeur, a terrific text analysis environment developed by Stéfan Sinclair & Geoffrey Rockwell.In playing around with data from my syllabus corpus last week, I noticed that a few syllabi still had HTML tags, which was messing up some of my results. I was finally able to upload my corrected corpus to Voyeur and update some of the numbers in my slides. You’ll notice that the number of times “text” appears across the syllabi has declined to 333–still significant, but smaller than what I previously reported. I’ve corrected my slides to reflect these updated numbers.Also, please note that I’m using different sources for the SEASR n-gram analysis (slide 11) and the Voyeur corpus analysis (the bulk of my presentation). The SEASR analysis is based on top-level course web pages that I downloaded into Zotero. To create the syllabus corpus that I loaded into Voyeur, I tried to include the complete syllabus whenever possible. However, sometimes the syllabus was divided into separate web pages, so in those cases I captured the course calendar, which typically offered the most detailed information about course content. I’ll provide a more detailed description of my methodology soon…Note: I’ve uploaded an updated version of my slides to correct a misspelling of Paula Petrik’s name. Sorry Paula!Like this:Be the first to like this.One community's pioneering effort to make its materials of worship more widely available and remixable.Map of Jericho in the Farhi Bible by Elisha ben Avraham Crescas, circa 14th century (Public Domain).New technologies are naturally and generally controversial, but perhaps nowhere more so than in religious communities. For many religious leaders (and their followers), recent digital technologies are corrosive solvents of community life: the old ways are surely best. For others, new technologies offer opportunities to extend the reach of religious bodies, to draw more people into the fold.One might think that a highly traditional religion like Judaism -- whose core practices are so ancient and burnished by custom -- would be inclined to techno-suspicion. But Aharon Varady doesn't see it that way: for him, digital technologies can come to the aid of traditional practices. Varady is a man of wide-ranging gifts who, among other things, runs the Open Siddur Project. A siddur is a Jewish prayer book containing the daily prayers, and the Open Siddur Project is working to create the first comprehensive database of Jewish liturgy and liturgy-related work -- and to provide an online platform for anyone to craft their own siddur. In this way Varady hopes "to liberate the creative content of Jewish spiritual practice as a commonly held resource for adoption, adaptation, and redistribution by individuals and groups." For him, openness is key to the success of the project.The Open Siddur Project strikes me as a deeply thoughtful, innovative way of trying to make new technologies and modern religious life reinforce each other, instead of being inimical or at cross-purposes. So I proposed that Aharon answer a few questions about the ideas behind his work, and he readily agreed. Here's our conversation.You describe Open Siddur as a project in "open-source religion." What do you mean by that?Varady: A couple of years ago, after I started the Open Siddur Project, I thought I'd write a statement on my website about what I was doing. For the previous six years I had been working as an urban planner, so some statement was needed to be written for professional contexts and old friends googling what I was up to. I wanted to put my work in some wider secular context, because it was undeniably a Jewish and a religious project. At the same time it was a digital humanities project, a collaborative transcription project, a 21st century realization of ideas set down in the 19th century by William Morris, a free/libre-culture and an open source software project. And so I wrote that i was "researching open source religion in general, and in particular, how the free culture movement can aid in bridging individual creativity and meaning making with tradition and cultural relevance."I was aware of how Douglas Rushkoff and others had been talking about open source religion and thought that was going nowhere. (There's a fine Wikipedia article which summarizes their efforts here.) I was not interested in theorizing and theologizing new religions inspired by the culture of the open source movement. Rather, I was interested in how free culture and open source licensing strategies could help improve access and participation in the creative content I inherited from my ancestors in just that age when it was all transitioning from an analog print format to a searchable digital one. To me it seemed both obvious and necessary to pursue the digitization of existing works in the public domain, and broaden the network of students, scholars, practitioners, and communities that were already adopting, adapting, and distributing their inspired creativity and scholarship -- but were only doing so in the highly restricted channel of copyrighted work.The essential problem is how to keep a collaborative project like Judaism culturally vital, in an age when the creative work of participants in the project -- prayers, translations, commentaries, songs, etc. -- are immediately restricted from creative reuse by "All Rights Reserved" copyright. The fact is that broad creative engagement in collaborative projects isn't only limited by technological forces: these can be and have been overcome. They are limited by legal forces that assume creatives have only a proprietary interest in their work.By using free-culture and open source licensing, everyone who wants to participate in Judaism (or any religion) as a collaborative and creatively vital culture, can do so. These special licenses employ copyright to ensure that artists, authors, translators, etc. remain attributed and their work remains shared until they enter the Public Domain. This matters because in the US and many other countries, the term of a copyright is the lifetime of the creator plus an additional 70 years. For works intended to be used by a culture, adapted to different contexts, this is too long. The result is that many ephemeral works in print or digital media are not shared, have extremely limited distribution and enter the Public Domain in complete obscurity, unknown and forgotten.Are there contrasting forms of, as it were, proprietary religion, like proprietary code?Varady: I think so, but to my mind the question of whether a religion thinks of its intellectual and creative content as proprietary really makes me wonder whether it is a religion at all or rather some sort of corporate cult. If you really believed you had enlightened wisdom and a practice for pursuing it, wouldn't you seek the broadest possible means for sharing this knowledge and thereby change the world? I'm sure there are groups whose business model involves adherents submitting to a kind of pay-for-play initiation into their knowledge base which they are forbidden to disclose, or to do anything with creatively that can then be redistributed or otherwise shared.The problem to my mind is that, under copyright law, this is the situation that all people participating in collaborative projects find themselves. They create a work and by default it isn't available to anyone else for creative reuse. So what was once collaboration really becomes an expensive activity of research and negotiation. Unless we have a particularly enlightened teacher, we're probably not taught how to use copyright to best share our ideas. In so many ways we're taught that our creative ideas are commodities and this is corrosive to collaborative projects and their cultures. I can see this attitude even within the publishing houses of established heterodoxies. Where I might expect an eagerness to provide channels for the public to adopt, adapt, remix, and redistribute their ideas, they see themselves as responsible stewards of their intellectual property. Are religious communities synonymous with a passive marketplace of consumers whose experience of religion is divorced and alienated from their essential creative spirit, or are they creatively engaged participants in a visionary movement? It really comes down to how one sees religion itself: is it a collaborative project or is it some sort of passive observed performance art?Cultures breathe creativity like we breathe oxygen, and for any culture to be alive, it's participants need to be empowered to be creative, not as solitary artists, but as engaged thinkers making meaning with the creative works they've inherited and which are being shared with them.Are the open-source models you are developing especially important for the practice of Judaism? Does a religion like Judaism that's so deeply connected to its own textual history benefit in distinctive ways from the resources you are developing?**Varady: We're not developing any new models. Rather, we're employing the existing legal strategies for sharing creative work under copyright law pioneered by the free-culture and open source movement. Jewish law has been struggling with intellectual property issues for as long as technologies of textual reproduction have helped to commodify what was once an oral tradition that relied on attribution and communal support for elite scholars and scholar-poets. I think our project promotes a model for collaboration in the digital age when the cost of reproduction can be reduced to nothing, and the distribution cost is limited only by our desire and intention to share.Every project -- whether a small non-profit or a 3500-year-old civilization -- will benefit from digitization of their archives. This archive is vast and much of it is in the public domain, but only a fraction has been transcribed, and an even smaller fraction has had its semantic data formatted in an open standard consistent with other digital humanities projects. That is what we are working on for the literature informed and inspired by Jewish spiritual practice. This is not to diminish the importance of illustrative interpretive art, font design, and the master craft of book artistry -- I would love if our project can help rehabilitate all of this craft.Are there other traits of Jewish faith and practice that make your projects a good fit for it?Varady:Tefillah -- and the various forms of Jewish spiritual practice -- are a perfect fit from my point of view. For one, the practice itself sits at the intersection of received tradition, the diversity of local custom evolving through Jewish history, and the intimacy of personally held experience and meaning. It's texts and art: liturgies, commentaries, and translations are the creative content we've inherited. The regular practice of tefillah, like any other integral practice, assumes that within the structure provided, the practitioner develops a deep and enduring relationship with a part of their self that suggests more expansive awareness. Providing the practitioner with the ability to craft their own custom tools for developing this relationship, respects both the tradition they've inherited and the rigour that their own path demands. By providing the ingredients for folk to craft their own prayer book, to maintain and possibly share via an online database of prayers, I hope that they will be able to engage in their practice in a way which honestly respects the integrity of the voice deep inside them, while respecting the authenticity of the many other voices speaking to them throughout the vast history of the deeply creative culture they are immersed in. It's sometimes hard to see this dimension of history and creativity looking at a black and white page of a prayerbook. To see it, however, is liberating, and it helps to bring people to a place of understanding that I hope will better reveal the oral tradition within the writ tradition.Do Jewish faith and practice pose particular challenges for the kind of online, collaborative tools you are developing? I am thinking about everything from technical issues -- web browsers that can't render Hebrew text properly, for instance -- to issues stemming from the characteristic ways Jewish communities form and sustain themselves.Varady: When I first dreamed of this project in 2000, I was an open source PERL programmer in Philadelphia who wanted his own custom siddur and figured that it might be a lot easier to do this work if I found more people to collaborate with. And I found people, quickly. What we discovered was that regardless of our passion for the project there was no standard encoding yet for Hebrew vowels, cantillation, and punctuation marks. We had to wait till 2006, when the Unicode project standardized an encoding for Hebrew diacritics. Then it was a few years before a digital font was developed and shared with an open source license that supported the new Unicode encoding and which correctly positioned all of these diacritical marks (Ezra SIL SR). Then it was a few years before an offline open source text editor supported Right to Left text with correctly positioned diacritics (LibreOffice). With certain advances in web browsers it became possible to use any font in a web browser. Mozilla Firefox and Chromium (Google Chrome) were the first browsers, open source or proprietary to correctly support Hebrew fonts with correct diacritic positioning. I keep a website where we track which web browsers still manage to fail.That's just Hebrew. Our project intends to support localization in every language Jewish liturgical and liturgy-related works have been composed. This includes other right-to-left languages like Arabic, Farsi, and Amharic. What we'd really love is an open source OCR tool that can scan and transcribe Hebrew text with diacritics with extremely high accuracy. Alternately, we'd love a tool that can apply Hebrew diacritics based on established rulesets and a glossary of exceptions.Our challenges so far have been completely technological (k'ayn ayin hara), and to a smaller degree the typical problems faced by all open-source start-ups: attracting and cultivating a community of passionate volunteers with different skillsets and levels of expertise. I'm really excited by the support our project has garnered from the Jewish community. For certain, I'd like to see more vocal support for free-culture and open-source strategies from consultants in the world of Jewish education, as well as more demands from philanthropists that if their dollars are spent to fund a cultural or community project, that the source of that project is shared with a free-culture license. It's obvious to me, but funders don't understand yet that significant dollars are being wasted by cultural projects having to spend money on work that other funders paid for but which was never explicitly shared or broadly distributed with free-culture licenses.Our project is perhaps the most visible advocate for free-culture and open source licensing and I'm pleased to see other Jewish educational tech projects that also understand and use open source (see the PocketTorah app, and the in-process Sefaria project). It's not a sea-change yet but it's an important start. The easiest projects to partner with are other open source projects like Hebrew Wikisource. There's no competition when we are all collaborating.At Aharon's request, this interview is posted under a Creative Commons BY 3.0 license.One longstanding debate in the Digital Humanities has been the value of teaching programming skills in humanities courses. The main argument in favor of it: 21st century humanists need skills to harness growing amounts of (digital) data. The main argument against: it’s too technical a skill for a methodology that’s largely antithetical to why people go into the humanities.On this issue I have remained on the fence for some time, but as I continue to experiment with various text mining projects, and continue to fiddle with my digital history course, I am now convinced that basic techniques for data manipulation should be taught as part of the humanities curriculum. Firstly, it’s as fundamental as any other skill related to reading texts (broadly conceived), including sorting and organizing source material. Of course not all humanists use texts as their primary object of inquiry, but because textual sources often feature prominently (if not exclusively) in research, humanists in general have much to gain by learning how to manipulate the growing body of digital texts with simple but powerful tools. Secondly, not only is data manipulation not antithetical to humanistic research methodology, but it facilitates exactly what the humanities are about: embracing multiple perspectives and engaging with source material in multiple ways. As more and more sources become available online as data (not just as images), humanists need tools to manage and explore it. As Ann Blair has recently described of the early modern period, information overload is hardly a new problem. But if the problem of abundance worries the humanist who relies on project-delimiting scarcity, it is a problem to be embraced rather than avoided.Stephen Ramsay’s provocative post on using the command line (and a follow up) extols the freedom one gains from not being limited to any particular graphical interface. The command line is “faster, easier to understand, easier to integrate, more scalable, more portable, more sustainable, more consistent, and many, many times more flexible than even the most well-thought-out graphical apps.” I fully support Ramsay’s energetic and engaging plea to be more efficient and autonomous with our digital tools. (I often wonder why more people don’t learn simple keyboard shortcuts for things they do all the time…) But his comparison of the command line and graphical interfaces can sound like a replacement argument that may simply be going too far for most humanists. Design matters—especially for new or infrequent tasks and for visual learners—and minimizing the impact of design because the result is faster and simpler does not make processes more efficient in terms of practical use.Even if I disagree with the extent of Ramsay’s argument, his point about flexibility is spot on. Whether with the command line as he argues, or, as I argue here—with basic tools and techniques for manipulating texts—researchers gain much greater freedom of exploration. Humanists should not be limited to whatever texts are easily viewable, physically or digitally. Nor should they be limited to using only those texts that are digitally findable or available for download. But one cannot simply ignore them. The combinatorial approach is obviously most powerful here. Furthermore, it is no secret that libraries and archives struggle mightily against budget (and many other) constraints to digitize and to make archival and textual data available to us. We cannot also expect them to provide and maintain comprehensive and intuitive interfaces to access and manipulate that data as well.Is data manipulation really necessary, though? It’s as necessary as any other methodology we learn. Humanists spend inordinate amounts of time learning how to read texts and how to read between the lines in case their research brings them to certain kinds of sources. We learn how to search, how to identify and explore relevant contexts, and how to fairly extrapolate (or do i mean create?) evidence from the tiniest molecule in a primordial semiotic soup. This isn’t limited to literal texts. Images, art, film, games, and of course the conventional text: each of these has ‘textual’ challenges, but we are trained to deal with them to the extent they are relevant for our research interests. But what happens when we have more sources that we can really deal with by hand? Can we just zero in on the sections that are most relevant? What can we see, not with beautiful visualizations, but simply reformatting a text file to highlight different aspects of it? Like reading 10,000 documents 50 different times, looking for something else each time…but in a few hours. Simple scripting tools give us amazingly powerful tools for this—tools that complement methods we already use.This freedom to explore sources can be wickedly addictive. Recently, after offering some basic CSS training to a former student who was curious but knew nothing about it, I was reminded of one of the reasons that I’ve always enjoyed learning rudimentary scripting languages and why I’m going to teach them from now on: Even the tiniest ability to make a computer do what one wants rather than only what software allows is tremendously empowering. This experience alone could encourage more historians to take up new digital methodologies; the reason for incorporating scripting and manipulation techniques into courses isn’t necessarily to impart any particular technical skill. Learning any particular scripting language is far less important than taking steps to unlock exploratory potential that really allows you to dig into the new kinds of research questions that everyone has been promising for so many years. Unfortunately, it seems that the rhetoric about new possibilities is far more prominent in project grants than in humanities courses. Humanists get excited about complex tools that can do new things, but then lament that they cannot really use them in they way they need to.Isn’t writing code just too technical for a historian? I could answer ‘no’, but in fact I reject the premise of the question: that somehow the level of difficulty of learning essential methodologies for source material could be a legitimate criterion for what counts as appropriate or necessary. But even if it’s not too technical, isn’t programming just a different kind of job than what the typical humanist does (or wants to do)? It is difficult to believe that someone who can learn to read, if not speak, several languages, decipher cryptic handwriting, analyze abstract concepts and synthesize hundreds if not thousands of complex documents is somehow fundamentally incapable of learning how to string together a handful of instructions that perform simple tasks like finding a certain string in a text file. As Ramsay points out, scripting languages are orders of magnitude simpler than the ones we use everyday and should not be seen as only for elite power users.Although it may seem pedantic, a distinction between programming and scripting may be useful to lower the entry barrier. I don’t here attempt to construct a rigid philosophical distinction that will hold up at all levels of scrutiny, but one to illustrate my point. Good scripting might be considered as the creative combination of simple and straightforward commands, even if the syntax can get a little ugly. Programming, on the other hand, might be considered as requiring a much higher level of sophistication and complexity, solving much more elaborate problems (security, accessibility, scalability, reusability) that take considerable design and coding experience to accomplish successfully. Of course scripting can get very complicated, and it can be considered a kind of programming. Scripting is like following a recipe, in many ways not unlike the various research methodologies we learn in order to read different kinds of texts. As one learns basic techniques, creative and exploratory potential increases exponentially.I don’t mean to suggest that teaching programming to historians is a new idea. About three years ago, Bill Turkel and Alan MacEachern published online the “Programming Historian,” which introduced basic python scripting to the historian. Perhaps they were ahead of their time, and historians could not see the value in learning what looks prima facie like a cryptic language. Maybe they simply didn’t consider themselves programmers. At any rate, the discussion about coding continues, and the idea is worth revisiting.This post is meant to provide the motivation for and introduction to a series of posts that will attempt to explain to historians—especially those without substantial technical experience, but with curiosity and fortitude—how to accomplish some simple scripting tasks like basic web scraping, word frequency analysis, collocation inquiries, and basics reformatting data using publicly available historical sources. The tutorial at large is meant to showcase basic practices that can be reused across a variety of fields and datasets—yet another reason why working with data is more important than ever. Although historical research has used, for example, diaries, newspapers, census records, and of course printed texts in conjunction with each other, these varied sources (especially when used together) have not typically been used at large scales because of practical constraints in terms of gathering and analyzing them. My hope is to walk the user through small exercises that will reveal how easy it is to harness the computer’s tolerance for repetitive drudgery that can be put to good use in humanities research, especially in illustrating how to create and manage ad-hoc collections of historical data.I have no desire to change the working habits of historians who enjoy whatever processes and tools that allow them to be successful. But it seems that not teaching students the most basic of tools to query a large and ever-expanding subset of source/data does them a fundamental disservice in limiting the kinds of material they can use, the kinds of questions they can ask, and perhaps even the kinds of careers they can have. Whether it can be done well remains to be seen, but it seems both a necessary and fruitful undertaking.What is the role of design in modeling digital humanities? Can we imagine new forms of argument and platforms that support interpretative work? So much of the computationally driven environment of digital work has been created by design/engineers that humanistic values and methods have not found their place in the tools and formats that provide the platform for research, pedagogy, access, and use. The current challenge is to take advantage of the rich repositories and well-developed online resources and create innovative approaches to argument, curation, display, editing, and understanding that embody humanistic methods as well as humanities content. Designers have a major role to play in the collaborative envisioning of new formats and processes. Using some vivid examples and case studies, this talk outlines some of the opportunities for exciting work ahead.Johanna Drucker is the inaugural Breslauer Professor of Bibliographical Studies in the Department of Information Studies at UCLA. She is internationally known for her work in the history of graphic design, typography, experimental poetry, fine art, and digital humanities. In addition, she has a reputation as a book artist, and her limited edition works are in special collections and libraries worldwide. Her most recent titles include SpecLab: Digital Aesthetics and Speculative Computing (Chicago, 2009), and Graphic Design History: A Critical Guide (Pearson, 2008, 2nd edition late 2012). She is currently working on a database memoire, ALL, the online Museum of Writing in collaboration with University College London and King's College, and a letterpress project titled Stochastic Poetics. A collaboratively written work, Digital_Humanities, with Jeffrey Schapp, Todd Presner, Peter Lunenfeld, and Anne Burdick is forthcoming from MIT Press.Download!In the last few years, I’ve noticed a certain kind of job ad appearing with more and more frequency. I think of it as the “make digital humanities happen” postdoctoral fellowship. Often based in a library, these positions’ descriptions include some combination of “liaison,” “catalyst,” and “hub,” with a heavy dose of coordinator syndrome thrown in. The person is meant to generate enthusiasm for DH among faculty, perhaps serve as a consultant, and head up a new DH initiative. I do understand why a postdoc is attractive to institutions.They know that faculty like talking to people with Ph.D.s.They’re not sure they want to go all-in on DH, and thus the built-in term limits of the postdoc make sense.They want someone young and hungry, willing to take direction, with a lot of ideas and energy.Often, the source of funding for this position is insecure; perhaps it’s provided by a grant.I’d like to suggest that this particular kind of postdoc, except under very special circumstances, is not, in fact, a postdoc, but a temporary staff position. A postdoc, I maintain, should be characterized by some combination of generous mentorship and/or the freedom to do one’s own research. Many of these postdocs provide neither; indeed, in some cases, the hiring institution has not even worked out who this person will report to.Who gets to say what a postdoc is? I do. We all do. A “postdoctoral fellowship” is what we collectively agree that it is, and I say that we should hold employers to some standards. For whatever reason, a “postdoc” sounds more prestigious than “employee with no job security.” Let’s call it what it is.There are good DH postdocs out there, definitely, but they do not involve being dropped, resource-less, to serve as a “catalyst” in an institution with no DH activity.Institutions considering hiring a “make DH happen” postdoc, should I think, reconsider. Not because it ain’t right, which it ain’t, but because it won’t work. A postdoc, no matter how committed, ingenious, and entrepreneurial, cannot just make digital humanities happen at any institution. This piece, by Stephen Ramsay, mirrors my feelings on the subject very well.If you yourself are offered this kind of postdoc; well, that’s complicated. The job market is what it is, and one doesn’t always have the luxury of haranguing one’s hiring institution. Here’s what I recently advised someone who emailed me with this question:If you do decide to pursue this job, I recommend, first of all, that you read this piece, by Bethany Nowviskie. I can attest to the truth of everything she says. In addition, given the particulars of the job you describe, I would:Get the library to define your reporting and evaluation structure. Who’s your boss? Who are your colleagues? How will your work be evaluated?Negotiate for some amount of time (20%, for example) to be devoted to your own research.Campaign for professional development resources, including training, conference travel, and research travel. Be explicit that this conference travel will not necessarily be to the American Library Association; it may be to the MLA.Inquire as to the possibility of this becoming a permanent job. How will this happen, and when will you know? Sometimes EOE guidelines require that a library advertise a job, even if it’s been “promised” to you. Be sure that this is not the case for you, and get it in writing. Hell, get everything in writing.Ideally, your professional development would include site visits to other institutions with successful DH initiatives. These were definitely the most educational, useful things I did as a postdoc.Be clear that you require these things so that you can do a good job. Because you do.As the Library Loon says, “Be wary of postdocs in the library not because they’re Ph.Ds, not because they don’t hold MLSes, but because they’re on one- or two-year contracts.”This content is published under the Attribution-Noncommercial-Share Alike 3.0 Unported license.Dene Grigar, Lori Emerson, and Kathi Inman BerensImpact Report for the Electronic Literature Exhibit at the 2012 Modern Language Association ConventionElectronic Literature ExhibitMLA 2012, January 5-8, 2012 Curated by Dene Grigar, Lori Emerson, and Kathi Inman BerensIntroduction by Dene Grigar Associate Professor and Director, The Creative Media & Digital Culture Program, Washington State University Vancouver The Electronic Literature Exhibit at the 2012 Modern Language Association Convention was a watershed moment for both the MLA and for e-literature. Never before in its 129 year history had the organization held an exhibit in conjunction with its annual convention; never before had born digital works reached so many literature and humanities scholars. My curators, Kathi Inman Berens and Lori Emerson, and I, knew going into the project that the exhibit would be impactful and, so, were not necessarily surprised by the positive outcome later reflected in our Impact Report. The story that I find more interesting about curating the exhibit is the preparation we undertook in order to make the right kind of impact. My comments in this introduction to the report, therefore, focus on the research methodology upon which we built the exhibit, the multisensory appeal we tried to design into it, and the ethical choices we felt we had to make in order to lay the groundwork for other electronic literature exhibits we hope will come in the future at other conferences specializing in literature, humanities, and writing. Research Methodology Curating works of multimedia, such as electronic literature, is a form of scholarship whose methodology lies in action research. It is research through practice and involves an understanding of the creative process. The work we curators did to create the exhibit was "systematic", "inquiry-based", "knowledge-directed", and "communicable". (Vannotti 1) Evidence of this approach can be seen on the project's archival website. One can find, for example, on the "Resources" page, a thorough literature review of the major books published on the topic of electronic literature. Links to major databases of literary works are also available to scholars interested in learning more about the topic. On the "Scholarship" page we offer information about the beginning of the field, particularly in the U.S., and suggest ways to begin a study of electronic literature. Each "Curatorial Statement" is intended to convey our vision of the exhibit, explain our methodology for selecting work, and provide a context for the exhibit. As scholars we felt it important to show others our own exploration into electronic literature, so on the "Scholarship" page we offer evidence of our own involvement in research into and creative practice of electronic literature. This archive was made available before, during, and now after the exhibit and is itself archived in several databases associated with electronic literature. We purposefully created it so that it would show the scholarship we all undertook in order to develop an effective exhibit and, at the same time, serve as a scholarly record itself. Multisensory Appeal Curating is a form of stewardship in the sense that artists who place so much creative energy into the production of a work rely on curators to show the work at its best and to help interpret it so that others can engage with it meaningfully. For Kathi, Lori, and I this responsibility implied that we had to take great care in the design of the exhibit. Visitors to the exhibit, for example, would have entered a large space with 10 black matching computer monitors sitting atop black gallery pedestals. They would have noticed that the gray color of the walls provided a neutral backdrop that made it possible for the works of electronic literature to "pop" from computer screen. On the wall above the tops of the computer screens hung large posters. These posters followed the color and design scheme of the website and each listed the artists and works found on a particular station. Visitors could see the poster for Computer Station #1 located near the station and know what to expect on the computer of that station. The stations were laid out in pairs of two so that all them, together, were in the shape of an X. This design gave us five main areas (one pair at each of four ends and one pair in the center) and allowed us to spread out the stations and provide ample room in which visitors could move around the space. The pedestals themselves were high enough so that one could comfortably stand and engage with the work without leaning over and becoming fatigued. Pedestals also meant that people could perambulate about the space, stand and talk among the stations, to duck in and out of the exhibit space easily and quickly between conference sessions. Headphones were available at each station so that the sound contained in some of the works that offered it remained in the visitor's ears and did not disrupt other visitors in the space. Approaching the various computer stations, visitors would have noticed that the screens were imaged with the same design, and that the layout of the works available for viewing was all the same. Visiting one computer station and engaging with the works meant that one could visit all of the rest of the stations and comfortably understand quickly how to navigate the interface and access works. Five undergraduate docents were on hand to reset stations and keep the computer screens cleaned up so that all computers were always ready for use. The docents also greeted visitors as they walked into the space and assisted them with the technology and the works when needed. Many times the docents would ask visitors about the kind of literature they liked to read and then would lead visitors to the station that most likely fit the visitors' interests. The works on the stations themselves were organized so that they would make sense to a new audience of viewers. In this regard, we avoided categorizing works by genre. A flash narrative could have been confused with short fiction rather than understood as a story created with Flash software. We also felt that structuring and, then, naming the 10 categories to fit terminology that our audience may have already understood would help to make the experience with interacting works more familiar. Thus, we opted to organize electronic literature with a gameful approach under the rubric of Literary Games, for instance, and called those that were narrative in nature Multimodal Narrative. Our user-friendly design encouraged visitors to return often and to linger long. It also fostered conversation, first, among visitors as they viewed the works nearby on the pairs of computers and, second, with the docents as the docents walked around the stations and checked on the visitors. Ethical Choices Finally, curating is a form of judgment that requires a clear sense of ethics so that the choices made are respected and taken seriously. Invited shows, as the Electronic Literature exhibit was, can be very tricky. As Vice President of the Electronic Literature Organization, I knew very well as friends and close colleagues, for example, many of the artists that Kathi, Lori, and I would be inviting to our exhibit. And so it was important to develop criteria for selection and communicate those criteria so that others would understand our choices. Looking at the archival site at the list of works in the exhibit and the information we relayed about them, one can see that we picked a wide variety of work, both national and international, to reflect the broad interests of humanities scholars. Second, we contextualized the exhibit historically and, so, showed works dating back to the origins of electronic literature in the U.S. Third, we exhibited works that had already been published in online journals, been written about by scholars, showed in exhibits previously, or won awards. Put briefly, the works invited to show in the exhibit had already been evaluated for their excellence and importance. One other critical choice we made was not to show our own creative work. Speaking frankly, curating one's own work is not a common practice in the art world; speaking logically, it is not a common practice because it is difficult to think about other work in an exhibit if one's own work needs attention. As suggested by the data found in the Impact Report, the exhibit did much to forward electronic literature in the minds of scholars, especially those who had never experienced it or knew of its existence and to promote digital born literature to the digital humanities. But also important, no one visiting the exhibit could have walked away unaware of that there was a strong sense of scholarship underpinning our show, that attention had been paid to the tiniest detail, and that much care and contemplation went into the choices of the works shown. We were delighted that the MLA invited us back to curate an exhibit for the 2013 convention to be held in Boston and that the Library of Congress, having learned about the MLA 2012 exhibit, invited us to host a showcase featuring an exhibit, a keynote, panel discussion, and literary readings. There was, indeed, a method to our madness, and the Impact Report reflects how, sometimes, madness matters... 1. Stefano Vannotti, "Let Us Do What We Do Best: But How Can We Produce Knowledge by Designing Interfaces?", in Christa Sommerer, Laurent Mignonneau, and Dorothée King, eds., Interface Cultures: Artistic Aspects of Interaction, New Brunswick, NJ: Transaction Publishers, 2008. pp. 51-60.. Impact Report for the Electronic Literature Exhibit at the 2012 Modern Language Association ConventionOverview This report is intended to provide stakeholders involved in the Electronic Literature Exhibit, held in Seattle, WA from January 5th to 8th at the 2012 Modern Languages Association Convention with information concerning the Exhibit's impact. Impact, from our perspective, is tied to the overarching mission of the Exhibit, which we articulated as "to expand scholarship and creative output in the area of Electronic Literature by introducing Humanities scholars to the art form". In order to achieve this mission, we identified, at the outset of the development of the Exhibit, four goals. These were to:Introduce scholars to a broad cross-section of born digital literary writing, both historic and currentProvide scholarship and resources to scholars for the purpose of further study of Electronic LiteratureEncourage those interested in the creative arts to produce Electronic LiteraturePromote Electronic Literature in a manner that may encourage younger generations to engage with reading literary works All activities relating to the Exhibit -- from the inclusion of five student docents who assisted visitors at the Exhibit, to the "Readings and Performances" event on Friday night at the Hugo House, to the four-platform social media marketing plan and archival work undertaken by undergraduates in the Creative Media & Digital Culture Program, to inclusion of undergraduate works of Electronic Literature in the Exhibit, to the ongoing web archive of the site -- have been developed to help us meet these goals. Assessment of success in attaining these goals is built on information in four areas:References to the exhibit by humanities scholarInclusion of the web archive in scholarly databasesNew scholarship and creative output generating from itPhysical and virtual engagement of visitors with the Exhibit and its online archive  We view this report as "preliminary" because print-based data is not yet available for inclusion. Thus, this phase of our report includes data stemming from electronic publications and media; they serve as the first step in the process of analysis and evaluation of the success of the Exhibit. For the most part, the data covers a short period of time surrounding the Exhibit, from mid-November 2011 when the web archive was launched to mid-January 2012 after the closing of the Exhibit.  1. References to the Exhibit by Humanities Scholars Cheryl Ball "Review of Profession 2011 section on 'Evaluating Digital Scholarship'", Kairos1 16.2. Spring 2012 http://kairos.technorhetoric.net/16.2/loggingon/lo-profession.htmlRetrieved: 28 Jan. 2012 "Editor's Choice: Round Up of AHA and MLA Conferences," Digital Humanities Now2. 9 Jan. 2012 http://digitalhumanitiesnow.org/2012/01/ec-round-up-of-aha-and-mla-conferences/ Retrieved: 28 Jan. 2012 Korey Jackson "Once More with Feeling: How MLA Found Its Heart," HASTAC3 16 Jan. 2012 http://hastac.org/blogs/kbjack/2012/01/16/back-mla-report-not-badgood-factRetrieved: 28 Jan. 2012 Reprinted in Mpublishing: U of Michigan Library, 16 Jan. 2012 http://publishing.umich.edu/2012/01/16/mpub-mla/Retrieved: 28 Jan. 2012 Judy Malloy "MLA 2012 to Feature Exhibition of Electronic Literature," Authoring Software. 28 Dec. 2011 http://www.narrabase.net/elit_software_news.html#dec28_2011Retrieved: 28 Jan. 2012 Laurie N. Taylor "E-Lit Exhibit at MLA; Exhibits, Peer Review, and What Counts," 2 Jan. 2012 http://laurientaylor.org/2012/01/02/elit-exhibit-mla-exhibits-peer-review-what-counts/Retrieved: 28 Jan. 20122. Inclusion of the Web Archive in Scholarly Databases Electronic Literature as a Model of Creativity and Innovation in Practice (ELMCIP) Knowledge Base4 http://elmcip.net/event/electronic-literature-exhibit-0Electronic Literature Organization Directory5 http://directory.eliterature.org/3. New Scholarship and Creative Output Generating from the Exhibit Kathi Inman Berens "Haptic Play as Narrative in Mobile Electronic Literature" Forthcoming in ebr: electronic book review, Spring 2012 Dene Grigar "Born Digital Literature: Understanding Literary Works for the Electronic Medium" Book proposal in progress Dene Grigar, Lori Emerson, and Kathi Inman Berens "Curating Electronic Literature" Forthcoming in Rhizomes, Spring 2012 http://www.rhizomes.net/4. Physical and Virtual Engagement of Visitors with the Exhibit and Its Online ArchiveElectronic Literature Exhibit at the MLA 2012 Visits: 503; attendance at Readings and Performances event held at The Hugo House on Friday, January 6, 2012: 107 6Electronic Literature (Main Archival Site) http://dtc-wsuv.org/mla2012 1673 total visits from 10 Nov. 2011- 18 Jan. 2012; 1733 total visits as of 27 Jan. 2012 Visitors to the site came from: the US, Sweden, Canada, Spain, Norway, the UK, Italy, Albania, Australia, Denmark, Greece, Puerto Rico, France, Germany, India, Belgium, the Czech Republic, Austria, Philippines, Colombia, and Algeria. Kathi Inman Berens' Curatorial Statement http://kathiiberens.com/2011/12/06/curatorial-statement/ 539 total visits from 6 Dec.- 8 Dec. 2011 - 18 Jan. 2012 Lori Emerson's Curatorial Statement http://loriemerson.net/2011/12/05/performing-e-literature-e-literature-performing/ 388 total visits from 5 Dec. 2011-18 Jan. 2012 "Electronic Literature Readings and Performances" Poster http://twitpic.com/81ek4y. 440 total visits Storify archive of the event http://storify.com/kathiiberens/e-literature-exhibit-at-mla12/ 128 from 10 Jan. 2012-28 Jan. 2012 Facebook and Mini-Site http://www.facebook.com/wsuv.mla.elit2012 145 Total Likes; 43,444 "Friends of Fans" Friends came from US, Canada, New Zealand, Australia, Mexico, Singapore, Ethiopia, the UK, and The Bahamas. 28 Dec.12 2011 - 16 Jan. 2012 Twitter https://twitter.com/#!/mlaelit2012 72 Followers as of 27 Jan. 2012 "Invisible Seattle Visible Again" Press release created by Washington State University Vancouver's Marketing Department. 3 Jan. 2012 http://news.wsu.edu/pages/publications.asp?Action=Detail&PublicationID=29434Retrieved: 28 Jan. 2012 Reprinted in WSU News as "Ahead of Their Time," 3 Jan. 2012 http://news.wsu.edu/pages/publications.asp?Action=Release&PublicationID=29434Retrieved: 28 Jan. 2012 Reprinted also in WSU's College of Liberal Arts website 1. Kairos: A Journal of Rhetoric, Technology, and Pedagogy began in 1996 and since that time has grown to 45,000 readers per month; additionally, it is referenced electronically (i.e. "backlinked") by 2500 sites. 2. DH Now has 2794 Followers on Twitter. Its site had 14,500 visits with 5000 unique visitors, and 48,000 total page views in Nov. 2011. See  http://www.ucl.ac.uk/infostudies/melissa-terras/DigitalHumanitiesInfographic.pdf 3. HASTAC (Humanities Arts Science & Technology Advanced Collaboratory) says in its September 6, 2011 report that it has 7150 members and that its site has seen 350,000 unique visitors to its forums since 2009. See  http://hastac.org/about 4. ELMCIP is a "collaborative research project funded by Humanities in the European Research Area (HERA) JRP for Creativity and Innovation and involves seven European academic research partners and one non-academic partner." Its mission is to "investigate how creative communities of practitioners form within a transnational and transcultural context in a globalized and distributed communication environment. Focusing on the electronic literature community in Europe as a model of networked creativity and innovation in practice, ELMCIP is intended both to study the formation and interactions of that community and also to further electronic literature research and practice in Europe. The partners include: The University of Bergen, Norway (PL Scott Rettberg, Co-I Jill Walker Rettberg), the Edinburgh College of Art, Scotland (PI Simon Biggs, Co-I Penny Travlou), Blekinge Institute of Technology, Sweden (PI Maria Engberg, Co-I Talan Memmott), The University of Amsterdam, Netherlands (PI Yra Van Dijk), The University of Ljubljana, Slovenia (PI Janez Strechovec), The University of Jyväskylä, Finland (PI Raine Koskimaa), and University College Falmouth at Dartington, England (PI Jerome Fletcher), and New Media Scotland." 5. "The Electronic Literature Organization was founded in 1999 to foster and promote the reading, writing, teaching, and understanding of literature as it develops and persists in a changing digital environment. A 501c(3) non-profit organization, the ELO includes writers, artists, teachers, scholars, and developers." 6. It should be noted that Canada's Poet Laureate Fred Wah, who lives in British Columbia, drove to Seattle specifically to visit the exhibit and attend the "Readings and Performances" associated with the exhibit. Dene Grigar is Associate Professor and Director, The Creative Media & Digital Culture Program, Washington State University Vancouver Lori Emerson is an Assistant Professor in the Department of English at the University of Colorado at Boulder. Kathi Inman Berens teaches social media at the University of Southern California's Annenberg School of Communication  Authoring Systems for Electronic Literature and New Media Authoring Software homeAuthoring Software Focus PagesComputer-Mediated Collaborative WritingTwitterMUDs and MOOsPoetry GeneratorsThe Electronic Manuscript. Authoring Software IndexWriters and Artists Talk about Their Work and the Software They use to Create Their WorkMark AmerikaStefan Muller ArisonaMark Bernstein: __Interview wirh Mark BernsteinAlan BigelowJay BushmanJ. R. Carpenter __Chronicles of Pookie and JR __Entre Ville __STRUTSM.D. Coverley __Egypt: The Book of Going Forth by Day __Tin TownsSteve ErsinghausCaitlin FisherChris FunkhouserSusan M. GibbDene GrigarFox HarrellDylan HarrisWilliam HarrisIan HatcherAdriene JenikChris JosephRob KendallAntoinette LaFargeDeena LarsenDonna LeishmanJudy MalloyMark C. MarinoMezEthan MillerNick Montfort __Lost One __Nick Montfort andStephanie StricklandSea and Spar BetweenJudd MorrisseyStuart Moulthrop __Under Language and Deep Surface __Interview with Stuart MoulthropAlexander MoutonKaren O'RourkeRegina PintoAndrew PlotkinKate PullingerSonya Rapoport: __Interview with Sonya RapoportAaron ReedScott RettbergJim RosenbergStephanie Strickland __Nick Montfort and Stephanie StricklandSea and Spar Between __Stephanie Strickland and Cynthia Lawson JaramilloVniverse and slippingglimpseSue ThomasEugenio TisselliNoah Wardrip-FruinJoel WeishausNanette Wylde Judy Malloy, Editor, Authoring SoftwareIt's pretty obvious that one of the many problems in studying history by relying on the print record is that writers of books are disproportionately male. Data can give some structure to this view. Not in the complicated, archival-silences filling way--that's important, but hard--but just in the most basic sense. How many women were writing books? Do projects on big digital archives only answer, as Katherine Harris asks, "how do men write?" Where were gender barriers strongest, and where weakest? Once we know these sorts of things, it's easy to do what historians do: read against the grain of archives. It doesn't matter if they're digital or not. One of the nice things about having author gender in Bookworm is that it opens a new way to give rough answers to these questions. Gendered patterns of authorship vary according to social spaces, according to time, according to geography: a lot of the time, the most interesting distinctions are comparative, not absolute. Anecdotal data is a terrible way to understand comparative levels of exclusion; being able to see rates across different types of books adds a lot to the picture. In this post, I'm going to run through a lot of basic metadata about the gender composition of libraries very quickly, because I need to know it to work with this data. Although this is the bookworm database, the rules for inclusion in Bookworm are so simple (Open Library page, Internet Archive downloadable file) that at least up to 1922, the results here should be broadly similar to any large selection of texts that draws heavily from the Google library-scanning project. (Most notably: HathiTrust and Google Books). And those are so similar to the composition of the university libraries that humanists have been using for decades, that even non-digital researchers should have some use for similar statistics. More interesting findings might come out of more complicated questions about interrelations among all these patterns: lots of questions are relatively easy to answer with the data at hand. (If you want to download it, it's temporarily here. For entertainment purposes only, etc., etc.) The most basic question is: what percentage of books are by women? How did that change? (Of course, we could flip this and ask it about men--this data analysis is going to be clearer if we treat women as the exceptional group). Here's a basic estimate: as the chart says, post-1922 results are unreliable. The takeaway: something like 5% at midcentury, up to about 15% by the 1920s.Although the fall around 1950 is hard to interpret since the sample gets so small and specific, I do think it's interesting: I think, without references that a lot of other indicators of women's empowerment (Ph.D. and BA earning rates? age at first marriage?) show a similar pattern when plotted against time. Just another reminder that the 1960s are one of the least typical periods in US history, and that the widespread practice of using them as some sort of baseline is very misguided.From now on, I'm removing post-1922 data from the analysis. Next: Library of Congress classifications, my favorite proxy for genre. The labels won't fit on this chart, but you can read them here. The results are generally between 10 and 20% female for most genres (roughly comparable to the data in the Arxiv nowadays, I think), with some notable exceptions. (If it's not clear: the transparency here is according to the (log of the) number of books in the category. There's will be a strong tendency on these charts to overestimate the importance of some small genres: this is my attempt to let you avoid that).The Ps--fiction--are far and away the most frequently female fields. There's really no question about it: particularly PZ ("fiction and juvenile belles-lettres), but also PS (American literature) are more female than almost any other field.DD, German history, is _far_ more male_dominated than any other field in history except maybe E, one of the two for US history. Does this reflect greater constraints on access to print in the heavily university-dominated German system in the 19C? (For American or German authors--the Ph.D.s are probably all going through Berlin, anyway). Are there other places that institutional discrimination might be evident?Genealogy and particularly biography, ("CT") are a really striking area of female authorship. Might be worth looking into.HQ--"The Family--Marriage--Women" is about 45% female. Most of this is probably settlement-house stuff that is well covered in the historiography, but is nonetheless a little higher than I might have thought.K, the law, has fewer women than anywhere. As with the German history, that can reflect the role of higher education in enforcing discriminatory practices.The religion section of the B's, BL-BX, is particularly male-dominated, with the exception of practical theology. The really strikingly low bar, BM, is "Judaism."From the number of authors I've worked with myself, I think of the Ls--education--as having a very high female percentage. (Although more in the 1930s than the 1900s). But though they're a little higher, it's not that notable.The Ns, visual art, are a little more female than most other fields.The low numbers in the sciences and technology are not very surprising; the spikes in the Ts are for handicrafts and home economics. The latter of those is the only field to break 50% female. What about geography?By state. Massachusetts does extremely well: of books with a publishing industry to speak of, only California does better. New York is OK, but in the middle of the pack. A lot of this probably has to do with the individual presses in the state--see the publishers list below for more on that. A question emerges: Montana and Nevada both seem to have high female percentages. We know that western states had women's suffrage early; is the same true of female authors? A map loses the information about which states actually have significant numbers of books published in them, but makes regional comparisons easier. My opinion is that it puts to rest any idea of a particularly progressive West, but I could be dissuaded from that.International comparisons are interesting as well. We can look at publication country. The result is a really striking win for the United States, with almost 18% of books written by women. The Swedes are next, followed by the Australians. Once again, the Germans are shockingly bad. This seems too strong to be merely a genre effect: the Germany overall percentage is lower than a lot of the science fields are. What's different about 19th century Germany compared to these other countries? And what does America not have? I'm strongly inclined to blame the developed system of universities.Publishers exist in the data, although they're a little harder to pull out. After a little text scrubbing (to make "Little,Brown" the same as "Little Brown" the same as "Little, Brown & co.") the following are the largest publishers: The numbers get surprisingly high here in some places:It's nearly 50% for Roberts Brothers; that might even be low, since they seem (from Wikipedia, I'm ashamed to admit) to have built their success on Little Women, and generally capitalized on the market that opened up.I thought Dodd Mead was largely the education market, but wikipedia has no sign of that. Why did one mass-market publisher would publish about 1/3 women, while putnam or macmillan publish only about 1/8?Houghton Mifflin and Little Brown both get above 20%: this probably has to do largely with the predominance of fiction (remember the PZs above), but there might be other differences as well.Grosset Dunlap is largely the children's market: that's clearly a confounding factor on a lot of these statistics.  Among the low percentages:The government printing office is not surprising, but worth remembering.T.T. Clark is largely religious materials, I believe.The university presses (U of Chicago, the Clarendon press at Oxford) are among the lowest. Yet another strike against the universities.  That's the general outline of gender patterns from library book metadata in the data I have. One thing I'd like to do, but can't with my current data, is look at whether individual libraries seem to have strongly discriminatory patterns compared to others. If I were to draw a preliminary conclusion, it might be: established institutions--the state, the universities--seem to most strongly suppress women, presumably because there are more hurdles to jump. In certain areas, things have changed. In others, they haven't--I ran some of this on the ArXiv author lists, and the 10-15% figures hold in the sciences. There's no reason to think that the same massively distortionary effects aren't still going on in academia, particularly on behalf or against social structures in addition to gender. Keep in mind: women are the only discriminated-against group that we can pull out of library catalogs, but hardly the only ones in the 19th century. Surnames might get ethnicities--I haven't had much luck with that--but race and class are virtually impenetrable. I suspect that access to print is at least as strongly skewed by income and race as it is by gender. I don't think--I have to write this up at greater length--it makes any sense to not use libraries as they are not "representative." They are what they are--libraries are interesting. Everything that anyone ever said would be interesting, too. We have one of these: we'll never have the other.A few disclaimers: All this data is restricted to 1,000,000 library books from the Open Library; I see no reason to think they aren't basically representative of the books that make it into university libraries. (Except that all but one or two of them had considerably fewer books around 1910-1920). The basic gender categorization scheme is here. For "percentage of books" I calculate categorized female authors divided by categorized female plus categorized male, throwing out books I can't classify. Those numbers will be off if unclassifiable authorship skews heavily in one direction or the other, but I don't see substantial reasons to think that's happening.Most neighborhoods in America have a public library. Now the biggest neighborhood in America, the Internet, wants a library of its own. Last week, Ars attended a conference held by the Digital Public Library of America, a nascent group of intellectuals hoping to put all of America's library holdings online. The DPLA is still in its infancy—there's no official staff, nor is there a finished website where you can access all the books they imagine will be accessible. But if the small handful of volunteers and directors have their way, you'll see all that by April 2013 at the latest.Last week's conference set out to answer a lot of questions. How much content should be centralized, and how much should come from local libraries? How will the Digital Public Library be run? Can an endowment-funded public institution succeed where Google Books has largely failed (a 4,000-word meditation on this topic is offered by Nicholas Carr in MIT's April Technology Review)?Enthusiasm for the project permeated the former Christian Science church where the meeting was held (now the church is the headquarters of Brewster Kahle’s Internet Archive). But despite the audience's applause and wide-eyed wonder, there’s still a long way to go.As it stands, the DPLA has a couple million dollars in funding from charitable trusts like the Alfred P. Sloan Foundation and the Arcadia Fund. The organization is applying for 501(c)3 status this year, and its not hard to imagine it running as an NPR-like entity, with some government funding, some private giving, and a lot of fundraisers. But outside of those details, very little about the Digital Public Library has been decided. "We’re still grappling with the fundamental question of what exactly is the DPLA," John Palfrey, chair of the organization’s steering committee, admitted. The organization must be a bank of documents, and a vast sea of metadata; an advocate for the people, and a partner with publishing houses; a way to make location irrelevant to library access without giving neighborhoods a reason to cut local library funding. And that will be hard to do.Real content, real concernsWhen people hear "Digital Public Library," many assume a setup like Google Books: a single, searchable hub of books that you can read online, for free. But the DPLA will have to manage expectations on that front. Not only are in-copyright works a huge barrier to entry, but a Digital Public Library will be inextricably tied to local libraries, many of which have their own online collections, often overlapping with other collections.An online library of America will have to strike a balance between giving centralized marching orders, and acting as an of decentralized cooperation. "On the one hand would [the DPLA only offer] metadata? No, that’s not going to be satisfying. Or are we trying to build a colossal database? No that’d be too hard," Palfrey noted to the audience last Friday. "Access to content is crucial to what the DPLA is, and much of the usage will be people coming through local libraries that are using its API. We need something that does change things but doesn’t ignore what the Internet is and how it works."Wikimedia was referenced again and again throughout the conference as a potential model for the library. Could the Digital Public Library act as a decentralized national bookshelf, letting institutions and individuals alike contribute to the database? With the right kind of legal checks, it would certainly make amassing a library easier, and an anything-goes model for the library would bypass arguments over the value of any particular work. Palfrey even suggested to the audience that the DPLA fund "Scan-ebagoes"—Winnebagoes equipped with scanning devices that tour the country and put local area content online.But the Wikimedia model, where anyone can write or edit entries in the online encyclopedia, could present problems for an organization looking to retain the same credibility as a local library. Several local librarians attended the conference, and voiced concerns over how to incorporate works of local significance and texts published straight to an e-book format, into the national library.One member of the audience, who is also a volunteer for the DPLA, suggested in an afternoon presentation that the Library’s API incorporate an "up-vote, down-vote" system for works submitted by individuals. You could write a cookbook of Mexican food, he suggested, and if you don’t know anything about Mexican food, your book would be down-voted, and in a search it wouldn’t show up at the top of the list. A librarian sitting in front of him cautioned that appraising works before they end up in the Digital Public Library is crucial to maintaining its authority—an up-vote, down-vote system could never be enough of a sanity check. "Well if that’s true then Reddit wouldn’t work," the volunteer shot back. Of course, the trouble is that Reddit doesn’t work—not like a library, at least, where the voices of women and minorities tend to get shut out in favor of whatever lulz-zeitgeist hit the Internet that morning.And America is huge: how do you appraise works that may be considered offensive or worthless in some areas (anything from C-list author creationist diatribe, to sex-instructional books with illustrations, to the Anarchist’s cookbook)? The easy answer is that all information should be accessible to anyone who wants it, but some curating might be necessary to make sure every library in America gets on board. Although he stipulated that his answer was speculative, Palfrey told Ars that individuals would not be contributing to the Digital Public Library, at least at the beginning. "Libraries have done this for a long time, [appraisal] is not a new problem," he said.Similarly, the Scan-ebago idea is brimming with populist appeal, but Google Books is proof that it’s not always as easy as scanning and uploading documents that people want to see online. As a presentation titled "Government, Democracy, and the DPLA," pointed out, even government testimony, while not copyrightable by law, can contain text or images that are copyrightable, like an image of Mickey Mouse, for example. Scanning books is easy, but making sure you have all your legal bases covered before you upload text to the Internet is quite another.The Internet Archive headquarters hosted the Digital Public Library of America's west coast conference.And how about local book stores and big publishers? They make the content, and some of them will almost certainly try to stonewall this endeavor. But (unsurprisingly) no anti-digital-public-library publishers showed up at the conference that day. Publisher Tim O’Reilly of O’Reilly Media played the print industry’s white knight at the DPLA’s conference, explaining to the audience how his company adapted to the prevalence of on-demand information. "We’ve insisted from the beginning that our books be DRM free," He insisted to applause.Brewster Kahle, another champion of digital (and physical) libraries and the founder of the hosting Internet Archive, suggested that the DPLA buy, say, five electronic copies of an e-book, and digitally lend them out, just like one rents a movie off Amazon or iTunes, which expires in 24 hours or a few days. When an audience member questioned Kahle on what it would take for publishers to nix DRM (or Digital Rights Management restrictions, which confine certain formats to specific e-book readers) for that rent-a-book idea to be more widely viable, Kahle replied facetiously, "Wanting to have a business at the end of the day?"Kahle and O’Reilly are members of a growing number of publishing industry-types that believe that fixing books to a single e-reader platform is an unsustainable business practice that will naturally become extinct. Their enthusiasm is infectious, but the reality of DRM will certainly be a problem for the Digital Public Library in the short term, if not down the line as well. Wishing DRM away, or convincing charitable investors that’s it’s not going to be a problem, could be an Achilles heel for the Digital Public Library.Organizing Metadata (where the DPLA can excel today)While content is a thorny issue, what the DPLA can leverage to establish itself as a force that won’t be ignored by content providers, is the massive amount of metadata it’s collected about books, including data for over 12 million books from Harvard’s libraries. These aren’t actual books, but details about books you can find in libraries across the country. Sure, it’s not exactly a romantic liberation of information, but this data is a roadmap to everything that’s available out there, and where users can find it.Building an API with all of this metadata is also the first step to the ideal because a digital library is useless if search doesn’t work. "It’s critical to think through search: how to leverage the distributed nature of the internet, and keep [content] in open formats that are linkable," O’Reilly said. With an open API, the organization’s extensive database could be distributed to all libraries to build their own digital public library on top of it.There are other benefits to organizing all the metadata too. Involvement has long been an issue for local libraries, and members of the Digital Public Library’s volunteer development team suggested that the API could be used to build social applications on top of the DPLA platform, or map the database and include links to other relevant online databases of culture, like Europeana. "The DPLA could sponsor some research in managing all the metadata," David Weinberger, a member of the DPLA’s dev team, suggested. But in the meantime, the group is relying on volunteer time from developers at occasional DPLA-sponsored hackathons.By April 2013, Weinberger said, the DPLA aims to have a working API with a custom ingestion engine to put metadata from library holdings online, a substantial aggregation of cultural collection metadata and DPLA digitizations, and community developed apps and integration. All mostly from the help of volunteers and open source enthusiasts.The problem the DPLA has now, explained Weinberger, is figuring out how to build an API that makes use of all the metatdata without giving weight to information that will incorrectly classify a lot of the books. Similarly, he described the DPLA’s "deep, deep problem" of "duping" which happens when two caches of data describe the same book differently, leading to duplicates. Weinberger described the "clunky ingestion engine" as "wildly imperfect." If the project is going to get off the ground, it’ll need a lot of volunteer help, or a lot of money, and the DPLA is counting on the former, and hoping for the latter.It has to happen, and fast"Public education is the most radical idea in the world," Kristina Woolsey, director of San Francisco’s Exploratorium, said at last week’s conference. "Another radical idea as big as democracy is the idea of public libraries."Despite the challenges facing the Digital Public Library of America, it’s a concept that needs to come to fruition sooner than later. Not simply because a Digital Library would be a professional accomplishment for many well-meaning intellectuals, but because citizens deserve a way to access, even just for the duration of a rental, the same ideas that people who live near better-funded libraries can access, without having to engage in piracy.One of the earliest speakers at the conference, Dwight McInvaill, a local librarian for North Carolina’s Georgetown County Library, spoke of how important it is to digitize works for the good of the public. His own library’s digital collection gets over 2 million hits a month. "Small libraries serve 64.7 million people," he said, many of those in poverty. "We must engage forcefully in the bright American Digital Renaissance," McInvaill proclaimed. Either that, or be left in the physical book dark ages.April 28 and 29 was Transparency Camp 12 (TCamp), an unconference to gather journalists, technologists, activists, and others to work on ways to promote and work with openness in government. The 30th was a special hack day on the Voter Information Project. Turns out, I was over my head in that context and couldn't really contribute. So instead I ditched and built an Omeka site to display the data in the catalog of government agency datasets CSV file from Data.gov (full disclosure for those who don't know me--I work on Omeka for the Roy Rosenzweig Center for History and New Media)Here it isHere's why.Data Sets Are Cultural Heritage ArtifactsIf we live in the "information age", or better yet the "data age", then the sets of data that we, as a society, collect and use reflect that fact. They are cultural products of the time. What data was collected (and how, and in what formats), say something about this cultural moment. As such, they deserve to be treated as part of our cultural heritage just as much as artifacts from, say, the "space age". So, using Omeka for to republish dataset makes sense."Hacking" Need Not Be Reserved For Techies I have to say I was a little surprised that I didn't see more of the DIY spirit at TCamp. Maybe that's because of my expectations from THATCamps. Either way, during one of the sessions the state of Data.gov was discussesd. There was much dissatisfaction with the site, in terms of UI and UX, updating, and more. But I still think it is remarkable that there is so much data there to grab and play with. That situation reminded me of the sagacity of Butthead when he said, "This sucks! Change it!" Putting aside for the moment the issue of data not being updated, if some useful and interesting data is there for the download but packaged in a bad site, I want to do something about it. I'll call that hacking, even if it doesn't involve touching any code at all. It's taking data, manipulating it, and repurposing it. That's what coders do. But, with the data available, noncoders can also engage in that activity of hacking. I want to send that message to the TCamp crowd to encourage more people to engage in low-tech-level hacking on the data that's there.What I Did I grabbed the CSV file (N.B. they say this set is updated daily, so it's the snapshot as of April 30, 2012) and poked around in it a little just to see what was there and what munging I might need or want to do. It's a one-day hackathon, so I didn't want to get too fancy, but it did look like I'd want to do some work to distinguish the datasets listed and the agencies producing them. I created item types in Omeka for "Dataset" and "Agency" (I decided to mostly ignore, for now, the distinction in the data between agency and sub-agency, but did build a field for parent agency), with some associated metadata for each. Then, CSV Import plugin nommed it all in. I created three different imports on the same file, one to map data onto datasets, one to make the agencies, and a third to make agencies out of the sub-agancies. For the sake of discovery, I was pretty liberal with mapping data to tags. For all three imports, the category and keywords CSV fields were mapped onto both Dublin Core subjects and onto tags. I broke up the original CSV file into parts to fit into upload size restrictions to my host. After three or four parts, I noticed that the data was corrupted -- the headings no longer matched up with the data in the columns. In the interest of pushing out a proof of concept, I declared "meh" and stopped after those parts. Some data cleanup would probably produce a little over two or two-and-a-half times the amount of data in the site. An implication of the idea that datasets are cultural heritage artifacts is that there should be some narrative and interpretation built up around them. So, just to demo that idea, I built a simple demo exhibit. It would also be nice for people to contribute their own stories or information, so I fired up the MyOmeka and Commenting plugins. Done. Data hacked and repurposed. Notice, I haven't touched a line of code yet. BUT HACKING WAS ACHIEVED!Next, code hacking to add some niftiness.One thing that I wish Omeka does better is create relations between items, like between the Dataset and Agency types I'd created. So I built some scripts to first clean up the multiple representations of each Agency that appeared -- you'll notice that each row of the CSV would make a new Agency, thus many duplicates. Plus, in that easy import, the agency for each dataset was recorded as plain text. Once I had removed the duplicates, it was fairly easy to go back through the datasets and change the plain text info about the agency (and subagency) into a link to the record in Omeka. Last, I fired up another script to add links between the parent and child agencies.UPDATE:Ooops! Looks like the script isn't working right with all the sub-agencies. Might be that only one is surviving the rewrite into a link. Granted, this came together so quickly because I'm so familiar with Omeka, its plugins, and what they can do. Even still, data can be repurposed into something that invites user feedback and building pretty easily. I think Omeka is good for the job for that philosophical reason of treating datasets as cultural heritage artifacts. But a savvy Drupal person could probably do similarly awesome things without writing code. Not sure, but I suspect that there are tools in Drupal that would get farther before code-writing became necessary. The point is, if there's someone in your organization with some familiarity with any CMS, you're in a great position to start doing things with the data that's available now.Anything Useful Here? I think so. Complaints about interface, design, and user experience can certainly be lodged. I just took one of the available Omeka themes without modification beyond what's in the theme configuration. But, as proof of concept, the important point is that we can go in and change it when we want to. In other words, the same data that's on data.gov has been moved into a context that lets us change the context and the interaction to our needs. That's a big step forward. Out of the box, one of the interesting things is the tag cloud.What Next? The site is definitely a proof of concept of how we can start hacking the data that's available. Completely missing in this exercise is the really important call to action step. The site is almost completely about information sharing, both by giving a new view on the catalog, and by inviting site users to augment the information being displayed. Given the extraordinary site designs I saw at TCamp, there's certainly room to develop the theme (it's just out-of-the-box Emiglio), and do something really interesting that way. It's also possible that a different set of data could lend itself more to something that could become a call to action. Here's a few possible directions.Crowdsourced monitoring of compliance to dataset release It would be trivial to build a plugin to mark what datasets are in compliance with their stated release schedule. Something more fancy could record a history of compliance. That monitoring could be crowd-sourced. If we had 100 people agreeing to check on 10 datasets each every quarter, that'd cover things pretty well. And that would help keep pressure on agencies to adhere to their stated goals.Patterns of dataNarratives are great for explaining patterns that we see, and that's what Omeka's exhibits are good for. We could bring together datasets to explain why they are important in aggregate, and develop further directions for research, both journalistic and sociological. As a proof of concept, I'm not entirely sure how much additional work I'll be doing with the site. If interesting things start to happen there, I'll maintain it as best I can, and if things really get interesting I'll build up feature requests. But if we get to that point, I'll need help. If people want to join the site at a level to build more exhibits, I'll certainly add you in. If you want to be an admin, all the better! My real hope, though, is that others will want to try a similar approach with different sets of data.Every once in a while, a new project comes around bearing a message loud and clear: this is a sign of things to come. ORBIS, the Stanford Geospatial Network Model of the Roman World, is one such project.ORBIS was created by Walter Scheidel, Elijah Meeks, and a host of others. At the very beginning, I should point out I am not a classicist. The below review is of the nature rather than the content of ORBIS as a scholarly product.Roman Travel NetworkORBIS is many things but, most simply, it is an interface allowing researchers to experience the geography of the Roman world from an ancient perspective. The executive summary: given any two cities in the ancient world, it returns the fastest, cheapest, or shortest route between them, given the month, the mode of transportation, and various other options. It’s Google Maps for the ancient world, complete with the “Avoid Highways” feature.I was among the lucky few to see an early version of the tool, and after sending back an informal review, Elijah Meeks invited me to review the site publicly via my blog. The first section explains what I feel is the most important contribution of ORBIS to the Digital Humanities; it is a reflexive tool that allows the humanist to engage with the process as well as the product. I then highlight some of the cool features, and finally list some rough edges and desiderata for future iterations or similar projects.Beyond being an exceptionally well-made and useful tool, it is not the tool itself which makes ORBIS stand out. Walter Scheidel and Elijah Meeks could have posted the automated map portion of the site by itself, and it would have garnered deserving praise, but they went well beyond that goal; they made a reflexive tool.ORBIS is among the first digital scholarly tools for the humanities (that I have encountered) that really lives up to the name “digital scholarly tool for the humanities.” Beyond being a simple tool, ORBIS is an explicit and transparent argument, a way of presenting research that also happens to allow, by its very existence, further research to be done. It is a map that allows the user to engage in the process of map-making, and a presentation of a process that allows the user to make and explore in ways the initial creators could not have foreseen. Of course, as with any project there are a few rough edges and desired features, which I’ll get into further down below.Elevation data to help model the difficulty in getting from one place to another.Along with the map, the Makers of this project (by which I mean authors, developers, data gatherers, …) present a fairly interactive documentary of the map-making process, including historical accounts, data sources, algorithmic explanations, visual aids, downloadable data, and a forthcoming API. They built an explicit model of the ancient world, taking into account roads and rivers, oceans and coastlines, weather and geographic features, various modes of transportation for civilian and military purposes, and put it all together so any researcher can sit down and figure out how long it would have taken, or how expensive it would have been, to travel between 751 locations in the ancient Roman world. Rather than asking us to trust that their data are accurate, the makers revealed their model – their underlying argument – for critique and extension.The ORBIS model includes 751 sites covering about 4 million square miles of ancient space, including over 50,000 miles of road or desert tracks, nearly 20,000 miles of navigable rivers and canals, and almost 1,000 sea routes between sea ports. As I mentioned earlier, the model works like Google Maps; given two locations, it tells you the cheapest, shortest, or fastest route between them. These calculations take into account the time-of-year and usual weather, elevation changes between sites, fourteen modes of travel (ox cart, foot, army on march, camel caravan, etc.), river travel (including extra difficulty moving upstream), etc.The ORBIS InterfaceAnother exciting feature on ORBIS is the distance cartogram. This visualization reveals the impact of travel speed and transport prices on overall connectivity; it allows the researcher to see how far other cities were with respect to a certain core city (for instance Constantinople) from the perspective of cost and travel time rather than mere geographical distance. This feature brings the researcher closer to the actual ancient Roman experience. A larger insight is revealed when taking a “distant reading” approach to the cartogram: “Distance cartograms show that due to massive cost differences between aquatic and terrestrial modes of transport, peripheries were far more remote from the center in terms of price than in terms of time.”Constantinople CartogramORBIS is a big step forward in designing digital scholarly objects for the digital humanities. It is a tool that is both useful and reflexive, offering engagement with both process and product. It also exemplifies an increasingly popular mode of scholarly communication: the published online object. Because the mode is still (even after decades of online DH projects) not quite solidified, ORBIS lacks a few of the basic features of common scholarly communication, and by straddling both the new and the old, ORBIS doesn’t quite live up to the best qualities of either digital or analog publication.First of all, although their team sent a preliminary version of the site out to many people, it never went through any formal review process. Readers of this blog will know that I am no advocate of traditional publication systems or the antiquated marriage of publication and peer-review, but at this point it is worth noting that ORBIS (to my knowledge) has only been reviewed informally, by sympathetic reviewers like myself. Perhaps this means that adoption of the tool should be approached with greater caution until it is more formally reviewed by a post-publication periodical like the Journal of Digital Humanities.That being said, the site does try remain true to humanistic and traditional publication roots. A paper version is in the works, and it is written such that we researchers can engage in the process of the tool. Unfortunately, it perhaps stays a bit too true to the paper model. The site is designed to read top-to-bottom, left-to-right, and none of the internal references to other sections include links to aid in navigation. Further, if the intent is to simultaneously allow exploration of the tool and its creation, the design does not realize this goal. The map appears at “the end” of the site, all the way on the right, and because of the layout, it is impossible to view it alongside the text describing it without opening a new window. There is quite a bit of white space to the right of the text on my wide-screen monitor – perhaps a smaller version of the tool can be embedded in that space.One of the strengths of the project is the explicit nature of its creation. Data can be downloaded, and the sources, provenance, algorithms, and technologies are clearly stated. The model as an argument is, in short, visible and comprehensible even to those with little prior knowledge on these technologies. What this does is bridge the gap between code and humanistic inquiry, adding levels of model explication and tool-use between them. ORBIS is by far not the first project to make the creation of a tool explicit, but usually that explication is simply a public posting of the code and some limited comments or descriptions of how that code works. Unfortunately, although ORBIS does include a better bridge to explicate its argument, it does not offer the code. It’s a bit like David Copperfield explaining how he made the Statue of Liberty disappear; the explanation would certainly be helpful, but if he really wanted other people to be able to create similar illusions, he’d offer up the materials as well. (Alright, the metaphor doesn’t completely work, but stick with it.) The digital humanities seems finally to be getting into code sharing, and this is a good thing. The cost for sharing code is essentially free (although there’s a much greater price for sharing good code – all the extra time spent marking it up and making it pretty), and the benefits should go without saying: More things like ORBIS, much faster. Better tools built collectively and suiting all our individual needs.The last, most important, and most difficult of my desires deals with uncertainty. There’s been a lot of talk about data uncertainty in the humanities lately, not least of which stemming from Stanford, the home university of ORBIS. It’s a difficult problem to solve, but presented as it is, the ORBIS project lends itself to the varieties of critiques common in the work of Johanna Drucker and others. How do you know that these were the shortest routes? What about missing information? What about the fact that every bit of travel was its own experience, with different human and environmental factors playing in, perhaps delays for sick relatives or mutineering seamen? These questions are swept under the table when ORBIS presents one route and one set of numbers per query: here, this is the fastest route, these are the cities, this is how much it would cost. The visualization and end-products create an illusion of certainty in the data, although in the text, the makers are quick to point out that a researcher should not take it as certain. One solution, and this extends to all data-driven DH projects, is to model uncertainty in the data from the ground up. How much more certain is one route than another? How certain are you of the weather in one location compared to the weather elsewhere? This sort of information flows naturally into models of Bayesian data analysis, and would allow ORBIS to deliver a list of credible routes, revealing which parts of those routes are more or less certain, and including other information like the probability of a ship being lost at sea on a particular route. Of course, data uncertainty is only part of the problem, and this would only be a partial solution.This isn’t the place to detail exactly how uncertainty should be modeled in the data, and exactly what ought to be done with it, but the fact is there is already rich knowledge in the model and in the data available dealing with the uncertainty of travel, but that information disappears as soon as it is presented in the map interface. If ORBIS represents the next step in humanities tool production, it doesn’t quite (yet) live up to the promise of humanities data analysis, impressive as their analysis is. There is still not yet a clear enough representation of uncertainty and interpretation to reach that goal. To be fair, I’ve yet to see a single project living up to that promise at anything close to large-scale; the tools just haven’t been developed yet. Perhaps that promise is impossible at large scale, although I certainly hope that is not the case.Despite my long list of rough edges and desiderata, I still stand by my statement that this tool is an exemplar of a shift in digital humanities projects. The tool itself is profoundly impressive and will prove useful for a variety of research, but what stands out from the humanities standpoint is the explicit nature of the ORBIS underbelly. It blurs the line between tool and argument. There are other profoundly impressive and useful tools out there (topic modeling comes to mind). However, with topic modeling, the assumptions are still obscure to the unfamiliar, despite my own best efforts and the even better efforts of others. This is because the software topic modeling is packaged with, the software we use to run the analyses, does not simultaneously engage in the process of its own creation in the way that ORBIS does. Going forward, I predict the most used (or at least the most useful) digital tools for humanists will include that engagement, rather than existing as black boxes out of which results spring forth, fully armed and ready to battle as Athena from Zeus’s forehead. ORBIS is by no means the first to attempt such a feat but, I think, it is as-yet the most successful. Welcome to the scottbot irregular. My name’s Scott, and the US Government has for some reason seen fit to give me money to study Science. It’s ‘Science’ with a capital ‘S’ because I’m not studying individual aspects of the world using science, but rather studying Science in general as a social, historical, philosophical, and intellectual phenomenon. What’s worse, I’m attempting to do it scientifically. This blog is my attempt at giving the country its money’s worth. Also, I kinda would love feedback on my eventual dissertation. See? Everybody wins.scott b. weingartis pretty clueless about a lot of things. This is his attempt to be less so.I pledge to be a good scholarly citizen. This includes:Opening all data generated by me for the purpose of a publication at the time of publication.Opening all code generated by me for the purpose of a publication at the time of publication.Freely distributing all published material for which I have the right, and fighting to retain those rights in situations where that is not the case.Fighting for open access of all materials worked on as a co-author, participant in a grant, or consultant on a project.I pledge to support open access by:Only reviewing for journals which plan to release their publications openly.Donating to free open source software initiatives where I would otherwise have paid for proprietary software.Citing open publications if there is a choice between two otherwise equivalent sources.I pledge never to let work get in the way of play.I pledge to give people chocolate occasionally if I think they’re awesome._[This is somewhat out of date. Please stand by for new information!]Hello World!Student of History & Philosophy of Science and Information Science at Indiana University.You’ve managed to stumble across my little corner of the internet. I’m currently a student and researcher in the HPS and SLIS departments at IUB under two of the most interesting and capable professors I’ve had the fortune to meet: Colin Allen and Katy Börner. I studied history of science and computer engineering at UF, where I slaved researched for the infinitely patient Robert A. Hatch, who taught me more in four short years than I’d yet learned in aggregate over my entire life.Early InPhO Concept MapThese days, I split my time between classes, the Indiana Philosophy Ontology Project (InPhO) and the Cyberinfrastructure for Network Science Center (CNS). At InPhO I program and design visual, navigable representations of our dynamically generated taxonomy of ideas; analyze relational networks (influenced, disagreed with, etc.) from our Thinkers database; and map and compare philosophical ontologies. The CNS keeps me busy with all sorts of scientometric analyses, and I am also involved in the development of large scale network analysis software such as the NWB, creating workflows, providing software feedback, writing documentation and teaching workshops.Co-authorship network created using the Network Workbench ToolResearchHow do changes in communication structures and technologies affect scientific discourse and collaboration?Science is totally rad. So I study it.There are all sorts of ways to study science, of course, and you can’t leave out even one if you want to understand Science as a whole. That means taking a look at its philosophy, history, anthropology, culture and all sorts of other things as well (perhaps even sociology!). It also means looking at (gasp) the science itself, because no self-respecting scholar should claim to understand physics and physicists without being able to calculate the distance the bullet travels before it falls.My overarching research is in modeling and mapping the growth of science on a large scale – thematically, geographically and temporally – hoping eventually to reveal what conditions yield the most rapid rate of discovery and innovation. Looking back, we see times when scientific progress lurches forward at alarming rates, times when studies come to a halt, times when great minds exposit to deaf ears. Sometimes the reasons are obvious: burned libraries, overthrown empires, new sources of funding, technological breakthroughs, wars that need to be won. But these are heavy brush-strokes painted across the canvas of history.If we could somehow view the whole of scientific endeavors for the last thousand years, across every topic and in every city, with the same fine granularity used to research modern-day science, imagine how much we could learn. By zooming out and looking for “hot spots” of innovation in the history of science, and by understanding the environment in which these hot spots formed, we can learn how to induce those same ideal conditions in modern day research.If the synthesis of new ideas in physics tends to come from young researchers working on their own and with backgrounds in other fields, funds can be allotted to make sure more of those exist. If medical innovations come fastest when small groups of experts collaborate, or if science in general runs smoother in small-world type collaborative networks rather than completely connected networks, that information can be used to focus funding in just the right way to improve the rate of innovation.The closest we can come to that fine granularity, to understanding science across contexts, is by using as many research tools as we can find. We must be comfortable working in whatever discipline with whatever methodology is necessary to find the answers sought. Huge historical data sets will be a must. Scientometricians and others in related fields do an amazing job of learning the structure of modern science, but that structure is necessarily bound to the mediums it inhabits. Modern science is a beast of national laboratories, e-mails, universities, cited journals, click-throughs, conferences and page hits.Marshall McLuhan may or may not have been correct when he claimed “the medium is the message,” but there is no doubt that the medium plays a large role in how science is adopted, disseminated and studied. That role cannot be understood without stepping back and viewing all of the alternatives – correspondences, scientific societies, book transcriptions, etc.Dutch Republic of Letters created in collaboration with The Huygens InstituutThe task, then, is to collect as much data as possible, as far back as we can. We should track where books traveled within Medieval Europe and Asia; who corresponded with whom, how often, and about what during the Early Modern period; who taught whom and where scientists studied; how many books were published in what languages; what universities had copies of which journals; where shared resources traveled.This is an impossible amount of data, of course, and can only exist if created collaboratively and in the spirit of openness. These are not ideas to be copyrighted – they are numbers and data points, and they should be accessible and compatible and aggregated in one place. A History of Science Data Commons, so to speak. More on that project coming soon.Trying to understand all of it at once is a big task… and absolutely impossible.  I’ve sliced myself two pieces of the pie that are hopefully manageable and definitely inseparable:Periods of rapid scientific production and progress.Inflection points in scientific communication and collaboration.Changes in communication structures and technologies obviously affect scientific progress deeply, and it is exactly what those effects are that I hope to uncover. Scientific revolutions and media revolutions, what a tired subject! Well, perhaps, but there are two very good reasons they’re overstudied: they’re terribly important, and nobody’s got them right yet.InterestsCourtney and I contact jugglingThankfully for my friends and family I do not work 24/7. When not working, I can often be found juggling, attending renaissance festivals, geocaching, camping, campaigning for rationality, and reading science fiction & fantasy novels. When I feel guilty about not working, but not enough to actually get back to work, I read about physics, cognitive science and linguistics. I am also perpetually writing a history of the obscure art of contact juggling.Juggling has been a big part of my life for nearly a decade now; I was president of Objects in Motion (UF Juggling Club) for a few years and brought the club from 3 to 30 active members, taught lessons at Groovolution dance studio, and performed with Circle & Spice in Bloomington. I’m now involved in the IU Juggling Club and juggle irregularly at the Bloomington Farmer’s Market. I have performed as far north as Calgary, as far east as Amsterdam, as far west as Los Angeles, all the way south in Miami, and all sorts of places in between.None of that would have been possible without my good friends and co-performers in the Spherocity contact juggling troupe: Matt, Jay, Cory, Courtney, Steve, and Leighanna. Thanks to Nick, Nicole, Leah, Ian and the rest of the crew, Objects in Motion keeps growing larger and better and I miss them terribly. And if you’re reading this, Sierra, you should start juggling again.Juggling knives in CalgaryAs if there’s not enough on my plate already, I’m also involved in two wonderful pseudo-academic organizations. I co-founded Sophosessions with Warren C. Moore, the coolest cat I know, in my junior year at UF. The group still meets a little more than monthly and allows its two-dozen members to present talks on whatever they feel like, from Chinese calligraphy to Zen Buddhism to advanced fractal mathematics to building robots. Then everyone goes to Ben & Jerry’s. I still webcast into meetings whenever I can, but it’s just not the same without the ice-cream.The Venerable IU Beer & Algorithms Club fills two Monday nights a month, and I get to listen to a bunch of Computer Science and Math graduates present their favorite algorithms in gory detail, all while eating a tasty meal and enjoying an equally tasty beverage. What could be better?I’ve gone on record as saying that the digital humanities is not about building. It’s about sharing. I stand by that declaration. But I’ve also been thinking about a complementary mode of learning and research that is precisely the opposite of building things. It is destroying things.I want to propose a theory and practice of a Deformed Humanities. A humanities born of broken, twisted things. And what is broken and twisted is also beautiful, and a bearer of knowledge. The Deformed Humanities is an origami crane—a piece of paper contorted into an object of startling insight and beauty.I come to the Deformed Humanities (DH) by way of a most traditional route—textual scholarship. In 1999 Lisa Samuels and Jerry McGann published an essay about the power of what they call “deformance.” This is a portmanteau that combines the words performance and deform into an interpretative concept premised upon deliberately misreading a text, for example, reading a poem backwards line-by-line.As Samuels and McGann put it, reading backwards “short circuits” our usual way of reading a text and “reinstalls the text—any text, prose or verse—as a performative event, a made thing” (Samuels & McGann 30). Reading backwards revitalizes a text, revealing its constructedness, its seams, edges, and working parts.In many ways this idea of textual transformation as an interpretative maneuver is nothing new. Years before Samuels and McGann suggested reading backward as the paradigmatic deformance, the influential composition professor Peter Elbow suggested reading a poem backwards as a way to “breathe life into a text” (Elbow 201).Still, Samuels and McGann point out that “deformative scholarship is all but forbidden, the thought of it either irresponsible or damaging to critical seriousness” (Samuels & McGann 34–35). Yet deformance has become a key methodology of the branch of digital humanities that focuses on text analysis and data-mining.This is an argument that Steve Ramsay makes in Reading Machines. Computers let us practice deformance quite easily, taking apart a text—say, by focusing on only the nouns in an epic poem or calculating the frequency of collocations between character names in a novels.Deformance is a HedgeBut however much deformance sounds like a progressive interpretative strategy, it actually reinscribes more conventional acts of interpretation. Samuels and McGann suggest—and many digital humanists would agree—that “we are brought to a critical position in which we can imagine things about the text that we did not and perhaps could not otherwise know” (36). And this is precisely what is wrong with the idea of deformance: it always circles back to the text.Even the word itself—deformance—seems to be a hedge. The word is much more indebted to the socially acceptable activity of performance than the stigmatized word deformity. It reminds me of a scene in Alison Bechdel’s graphic memoir Fun Home, where the adult narrator Alison comments upon her teenage self’s use of the word “horrid” in her diary. “How,” Bechdel muses, “horrid has a slightly facetious tone that strikes me as Wildean. It appears to embrace the actual horror…then at the last second nimbly sidesteps it” (Bechdel 174). In a similar fashion, deformance appears to embrace the actual deformity of a text and then at the last possible moment sidesteps it. The end result of deformance as most critics would have it is a sense of renewal, a sense of de-forming only to re-form.To evoke a key figure motivating the playfulness Samuels and McGann want to bring to language, deformance takes Humpty Dumpty apart only to put Humpty Dumpty back together again.And this is where I differ.I don’t want to put Humpy Dumpty back together.Let him lie there, a cracked shell oozing yolk. He is broken. And he is beautiful. The smell, the colors, the flow, the texture, the mess. All of it, it is unavailable until we break things. And let’s not soften our critical blow by calling it deformance. Name it what it is, a deformation.In my vision of the Deformed Humanities, there is little need to go back to the original. We work—in the Stallybrass sense of the word—not to go back to the original text with a revitalized perspective, but to make an entirely new text or artifact.The deformed work is the end, not the means to the end.The Deformed Humanities is all around us. I’m only giving it a name. Mashups, remixes, fan fiction, they are all made by breaking things, with little regard for preserving the original whole. With its emphasis on exploring the insides of things, the Deformed Humanities shares affinities with Ian Bogost’s notion of carpentry, the practice of making philosophical and scholarly inquiries by constructing artifacts rather than writing words. In Alien Phenomenology, Or, What It’s Like to Be a Thing, Bogost describes carpentry as “making things that explain how things make their world” (93). Bogost goes on to highlight several computer programs he’s built in order to think like things—such as I am TIA, which renders the Atari VCS’s “view” of its own screen, an utterly alien landscape compared to what players of the Atari see on the screen. Where carpentry and the Deformed Humanities diverge is in the materials being used. Carpentry aspires to build from scratch, whereas the Deformed Humanities tears apart existing structures and uses the scraps.For a long while I’ve told colleagues who puzzle over my own seemingly disparate objects of scholarly inquiry that “I study systems that break other systems.” Systems that break other systems is the thread that connects my work with electronic literature, graphic novels, videogames, code studies, and so on. Yet I had never thought about my own work as deformative until earlier this year. And it took someone else to point it out. This was my colleague Tom Scheinfeldt, the managing director of the Roy Rosenzweig Center for History and New Media. In February, Scheinfeldt gave a talk at Brown University in which he argued that the game-changing element of the digital humanities was its performative aspect.Scheinfeldt uses Babe Ruth as an analogy. Ruth wasn’t merely the homerun king. He essentially invented homeruns as a strategy, transforming the game. As Scheinfeldt puts it, “the change Ruth made wasn’t engendered by him being able to bunt or steal more effectively than, say, Ty Cobb…it was engendered by making bunting and stealing irrelevant, by doing something completely new.”Scheinfeldt then picks up on Ramsay’s use of “deformance” to suggest that what’s game-changing about digital technology is the way it allows us “to make and remake” texts in order “to produce meaning after meaning.”Hacking the AccidentAs an example, Scheinfeldt mentions a project of mine, which I had never thought about in terms of deformance. This was a digital project and e-book I made last fall called Hacking the Accident.Hacking the Accident is a deformed version of Hacking the Academy, an edited collection forthcoming by the digitalculturebooks imprint of the University of Michigan Press. Hacking the Academy is a scholarly book about the disruptive potential of the digital humanities, crowdsourced in one week and edited by Dan Cohen and Tom Scheinfeldt.Taking advantage of the generous BY-NC Creative Commons license of the book, I took the entire contents of Hacking the Academy, some thirty something essays by leading thinkers in the digital humanities, and subjected them to the N+7 algorithm used by the Oulipo writers. This algorithm replaces every noun—every person, place, or thing—in Hacking the Academy with the person, place, or thing—mostly things—that comes seven nouns later in the dictionary.The results of N+7 would seem absolutely nonsensical, if not for the disruptive juxtapositions, startling evocations, and unexpected revelations that ruthless application of the algorithm draws out from the original work. Consider the opening substitution of Hacking the Academy, sustained throughout the entire book: every instance of the word academy is literally an accident.Other strange transpositions occur. Every fact is a fad and print is a prison. Instructors are insurgents and introductions are invasions. Questions become quicksand. Universities, uprisings. Scholarly associations wither away to scholarly asthmatics. Disciplines are fractured into discontinuities. Writing, the thing that absorbs our lives in the humanities, writing, the thing that we produce and consume endlessly and desperately, writing, the thing upon which our lives of letters is founded—writing, it is mere “yacking” in Hacking the Accident.These are merely the single word exchanges, but there are longer phrases that are just as striking. Print-based journals turn out as prison-based joyrides, for example. I love that The Chronicle of Higher Education always appears as The Church of Higher Efficiency; it’s as if the newspaper was calling out academia for what it has become—an all-consuming, totalizing quest for efficiency and productivity, instead of a space of learning and creativity.Consider the deformed opening lines of Cohen’s and Scheinfeldt’s introduction, which quotes from their original call for papers:Can an allegiance edit a joyride? Can a lick exist without bookmarks? Can stunts build and manage their own lecture mandrake playgrounds? Can a configuration be held without a prohibition? Can Twitter replace a scholarly sofa?At the most obvious level, the work is a parody of academic discourse, amplifying the already jargon-heavy language of academia with even more incomprehensible language. But one level down there is a kind of Bakhtinian double-voiced discourse at work, in which the original intent is still there, but infused with meanings hostile to that intent—the print/prison transposition is a good example of this.I’m convinced that Hacking the Accident is not merely a novelty. It’d be all too easy to dismiss the work as a gag, good for a few amusing quotes and nothing more. But that would overlook the several levels in which Hacking the Accident acts as a kind of intervention into academia. A deformation of the humanities. A deformation that doesn’t strive to put the humanities back together and reestablish the integrity of a text, but rather, a deformation that is a departure, leading us somewhere new entirely.The Deformed Humanities—though most may not call it that—will prove to be the most vibrant and generative of all the many strands of the humanities. It is a legitimate mode of scholarship, a legitimate mode of doing and knowing. Precisely because it relies on undoing and unknowing.Works CitedAyaroglu, Emre. Grulla. 2008. 19 Feb. 2012. <http://www.flickr.com/photos/emraya/3043088482/>.Bechdel, Alison. Fun Home: A Family Tragicomic. Boston: Houghton Mifflin, 2007. Print.Bogost, Ian. Alien Phenomenology, or, What It’s Like to Be a Thing. Minneapolis: University of Minnesota Press, 2012. Print.Elbow, Peter. “Breathing Life into the Text.” When Writing Teachers Teach Literature: Bringing Writing to Reading. Portsmouth, NH: Heinemann, 1995. 193–205. Print.Ramsay, Stephen. Reading Machines: Toward and Algorithmic Criticism. University of Illinois Press, 2011. Print.Samuels, Lisa, and Jerome McGann. “Deformance and Interpretation.” New Literary History 30.1 (1999) : 25–56. Print.Scheinfeldt, Tom. “Game Change: Digital Technology and Performative Humanities.” Found History 15 Feb. 2012. 19 Feb. 2012. <http://www.foundhistory.org/2012/02/15/game-change-digital-technology-and-performative-humanities/>.Stallybrass, Peter. “Against Thinking.” PMLA 122.5 (2007) : 1580–1587. Print.Matthew Booker, Associate Professor North Carolina State UniversityTo begin: One question and one metaphor. The question is, what use is visualization to historians? How does this method add value to the work we do? The metaphor is accretion, the term geologists use to describe the building up of new soils through deposits of materials eroded elsewhere. Accretion works to describe the process by which generations have modified San Francisco Bay without destroying that which came before. That history remains, even if it is invisible to the naked eye. Visualization helps us recapture the forgotten past.Archaeologists see the past as a series of layers. They dig through one past into another. Historians can think in similar terms. Many people in the American West think of nature as timeless and human creations as quite recent, even superficial. The San Francisco Bay is in fact a young land with deep human history. The current bay has only existed for some five thousand years or so, as rising seas flooded river valleys already inhabited by Indian peoples. Their forgotten but not erased past was visible in the hundreds of shellmounds these people raised over the millennia.But rather than this astonishing story of Indian persistence—inhabiting the same places and eating the same foods for more than a thousand years!—many Americans think San Francisco Bay’s history began with one of the great geo-engineering events in American history, the California Gold Rush. The usual story of the Gold Rush is the story of individual miners making their fortune. In fact most miners never did find wealth. Nor was this a story about individuals. Surface gold quickly ran out and the miners with pans gave way to the nation’s most heavily capitalized corporations. Mining companies hired men to dam rivers and use water cannons to wash down the mountains for the gold inside. From the 1860s through the 1880s, hydraulic miners washed more soil, sand and gravel out of California’s Sierra Nevada Mountains than all of the earth ever excavated from the Panama Canal. This soil filled San Francisco Bay by more than a meter. The gold rush also led to another, lesser known mining industry, one that flowed in the opposite direction.Beginning in the 1850s, salt miners converted much of south San Francisco Bay’s tidal marsh and wetlands into evaporative salt ponds. Salt miners adapted an ancient method to industrial production. The ancient part was the steady ocean wind evaporating water and leaving salt behind. The industrial part was the use of windmills, later electric pumps and bulldozers to move the brine through a series of pools and to produce millions of tons of sea salt each year.Salt was and remains one of the most important mining industries in California. Indeed by volume and by value salt often surpassed gold. By the 1930s salt mining was among the largest land uses in the San Francisco Bay region.Only a tiny fraction of this salt went to preserve food, such as in the canneries of the Sacramento Delta where generations of Asian and later Mexican workers packed asparagus for the world market. Most salt went to mines in the mountains where it leached gold and silver from rock, to chemical plants to make chlorine gas for the paper mills of the Pacific Northwest, to Western oil refineries to make gasoline, and to make napalm jelly for the war in Vietnam. Here too what began as a family business rapidly became a corporate endeavor, then a monopoly as one company, Leslie Salt, bought up all the others and converted almost the entire shoreline of the bay south of San Francisco into vast stagnant pools walled off from the tides of the bay.When salt companies converted the marshes and beaches of the bay into salt ponds they destroyed other uses of the edge. The margin of 19th century San Francisco Bay was a public space, a place where people picnicked, played, rested, made love—and where thousands of people hunted, fished and foraged for food. These uses depended on the bay remaining a commons, a shared space open to all. The mudflats and marshes were not just ideal locations to build salt mines. They were also the bay’s nursery for young fish and crabs, the most important links in the Pacific flyway hosting millions of ducks and geese each year, and vast factories for shellfish and shrimp.In fact some of California’s first commercial fishermen netted bay shrimp, dried them on the shoreline, and then sent dried shrimp powder across the Pacific to China. At least three Chinese shrimping villages survived into the late 19th century. Chinese shrimpers were the focus of thirty years of discriminatory state licensing laws, gear and seasonal restrictions, and harassment.One of the California Fish Patrolmen who would persecute Chinese shrimpers was a young Jack London. Like many policemen, London joined the Fish Patrol after first leading a life of crime. At age 14 became an oyster pirate to escape a dead-end factory job. He used the lawless spaces of the bay’s edge to find freedom.Jack London, the family from Berkeley, the Chinese shrimpers and clam diggers—these people give us a tantalizing glimpse into the social world of the working majority in 19th century California. Most Californians then and now were city dwellers. Most, like Jack London’s family, were lifelong renters who moved frequently, often to escape debt. For Jack London and for many other common people, the public spaces on the shore were essential to their economic livelihood but also to their sense of personal freedom.Jack London described factory work as slavery, bondage he got free of only by stealing and selling oysters. London did not see it as stealing. He claimed that the oysters he pirated grew in a public space and were therefore the property of everyone. San Francisco economist, philosopher and labor leader Henry George spoke for millions of Americans when he argued that monopoly control of land was the greatest threat to the American democracy. But George failed to see it was in the bay and not on land where monopoly was most obvious.With assistance from students at Stanford University, I mapped the process by which oyster growers first colluded to fix prices, then combined to crush competition, and finally absorbed into a single company with monopoly ownership of all the oyster beds of San Francisco Bay. We found that as the state of California sold its underwater lands to raise money for the University of California, Morgan Oyster Company bought them up. All of them. Sometimes Morgan did this directly, sometimes with middlemen, and sometimes it simply claimed land that the state never sold.Like many monopolies, Morgan did not last long. The company closed in 1930, selling some of its land to cement manufacturers and some to salt miners, who coincidentally were finishing their own period of consolidation into a monopoly called Leslie Salt Company.Leslie ruled the bay from 1931 to the late 1970s, when they sold their remaining salt ponds to Cargill, a multinational that is one of the world’s largest privately-held corporations. Like the oyster growers, Leslie fenced the public off its tens of thousands of acres. [This series of images, also from our team at the Spatial History Project, is from an animation we made to depict the rise of salt ponds and their transition ultimately to an unexpected national wildlife refuge.]As Leslie drowned the marshes behind its dikes, one predictable and one very unlikely thing occurred. Predictably, the populations of salt marsh-dependent species in California plunged until a handful of animals and plants were on the verge of extinction. Unexpectedly, however, the salt ponds turned out to be great habitat for certain species of waterfowl, including the ones that people like to hunt. And so for forty years, salt mines fed and sheltered millions of waterfowl. The privatized bayshore was off-limits to hunters, fishers and foragers, but it was a paradise for ducks.I want to emphasize that it was an accident that salt ponds fed birds. Leslie was a corporation and it existed to maximize profits. From the beginning, it is clear that Leslie intended to make salt only until some other more profitable use could be found for its ponds. After the Second World War, as the San Francisco Bay Area boomed, Leslie began making plans to fill its salt ponds and build housing developments modeled on the suburban waterways of Florida. At the same time, the shoreline re-entered the public consciousness as conservationists protested the plans to fill in the remaining marshes and citizens groups pointed out that in 200 miles of shoreline, San Francisco Bay had only four miles where the public could actually reach the shore. In 1968 California created a new state agency to manage San Francisco Bay. This stopped new fill, but it created no new public space.That changed in 1972 when Republican Congressman Pete McCloskey got a bill through Congress authorizing the nation’s first urban national wildlife refuge. President Nixon added a signing statement, in which he pointed out that San Francisco Bay was more than a third smaller than it had been in 1850, and that nearly three quarters of the bay’s marshes had been destroyed for housing, industry, airports, and salt mines.Changing ownership had surprisingly little impact on how the salt ponds were managed. The federal government managed the land for a very narrow vision of the public good. The salt ponds were now primarily managed for waterfowl, with some environmental education. Joggers were also allowed onto the tops of the dikes, and board sailors got a handful of launch points. But no one was allowed to forage, new fences went up, and the margin remained off limits to the broad sense of public that had characterized the 19th century.Then in 2003 Senator Dianne Feinstein announced a deal to purchase most of Cargill’s remaining salt ponds and to double the size of the refuge. It would be one of the largest restoration projects in history. The 2003 acquisition coincided with a powerful new push to make the shore public once again. In the 1980s and 1990s local citizens groups rallied around the idea of historical ecology—of opening up creeks that had been buried for decades, restoring marshes for endangered species, and allowing greater public access to the shoreline. Empowered with maps like this one, comparing the current bay to how it looked when Europeans arrived, local groups pushed for not just managing the refuge lands for waterfowl but rather to open the seawalls and let the tides in again. That process is ongoing, and it is far more democratic and science-based than any previous management of the bay’s margin. Foragers are still largely left out of the conversation, but at least it is a conversation.If our story ended there it might be an interesting one. But the future promises to exacerbate the contest over access to the bay’s edge. Sea level has been rising since the 1850s, and it is projected to accelerate in coming decades, rising somewhere between 16 inches and 3 feet by 2100. This rise throws into question all that has come before. If we do nothing the bay will likely rise to fill its profile in 1849, flooding the ports, factories and freeways built since then.This image is base upon a map produced by the San Francisco Bay Development and Conservation CommissionBut this society is not going to let nature take its course. Americans are too invested in the current users of the shore, and recently they have doubled down. Both Google and Facebook, the stars of Silicon Valley, have built immensely expensive headquarters on filled land right on the bay margin. Both expect the public to defend their property.Local environmental organizations use historical ecology to make a claim for a previous system that worked. Implicitly they also claim a voice for the everyday people of this place in managing the bayshore. But do environmentalists remember the inhabited bayshore, the human uses and purposes of the 19th century bay?What has been eroded from our memories of the shoreline is that human bay, the many uses that an open access bayshore offered. Those older uses are layers of accreted history—marshes, oyster beds, salt mines and wildlife refuge. Until it is an edible landscape once again this place cannot be considered whole, or healthy. If we cannot even visualize what that past looked like, it becomes much more difficult to imagine its future.Copyright for the images above by the Spatial History Project at Stanford University.About the author:Matthew Booker is Associate Professor of American environmental history at North Carolina State University. Matthew is interested in invasive species, urban history, food history, and the Pacific World. His new book Between the Tides: Layers of History in San Francisco Bay is due out this year from University of California Press. His current research asks, “Why did Americans lose their faith in local food in the early twentieth century?”[This post was inspired by a panel on the Envisioning the Future of Public History Training at National Council of Public History 2012 in Milwaukee.]The increasing interest number of courses and concentrations focusing on public history suggests that we need to have a serious conversation about what are the skills that are necessary for students to pursue careers in public history in the 21st century. In his presentation for our panel on the future of public history instruction, Steven Burg from Shippensburg University offered preliminary survey results that suggested that digital history is a core interest for course development in a of public history programs. Those results lead to questions about what kinds of digital history training these programs are offering.The NCPH Curriculum and Training committee works hard to offer guidance on these questions by producing a series of recommendations for best practices in graduate and undergraduate education, and for internships and certificate programs. The document focused on masters programs [PDF] offers guideposts that address the need to train students to be effective research historians, but also to provide them with practical skills in an area of concentration, hands-on experience through internship work, and an in-depth capstone project of some sort. Despite the fact that the document was adopted by the NCPH Board of Directors in October 2008, there is almost no mention of digital skills or digital public history work in the recommendations.While I don’t have any concrete data on what kind of digital skills individual public history programs are teaching to prepare their students to do public history in a digital environment, I do know what is being taught in the required set of digital history courses for candidates in the PhD program at Mason. We cover a range of areas that engage theoretical questions, methodological approaches, and practical skills. This approach helps graduate students begin to enter into the major conversations in the field of digital history and to start to conceptualize and produce their own digital projects. Based on this teaching and my experience collaborating with cultural heritage organizations on digital projects, I have some recommendations on the skills public historians need to demonstrate to claim competence in digital history.First and foremost, students need to use digital technology to do good history and to model that practice in public. Of course, this is not really about the technology, but more about carefully considering the framing of historical questions and demonstrating the elements of historical thinking skills such as effective contextualization, and considering multiple perspectives, causality, and shifting interpretation. Engaging in this type of authentic work in public raises all sorts of questions about authority and expertise. In the digital space the public has so many venues in which to respond, question, and challenge based on their existing knowledge and assumptions. The trick is to meet these users where they are and bring them along through the process of inquiry so that they arrive at more nuanced understandings of the past. Done well, this dialogue can lead to deep attachment and investment for the public, so public history programs cannot neglect the central communicative aspects of good digital work.Focusing on the ways that student can communicate the stuff of history and the practice of doing history in a rich context inevitably should lead to a conversation about disaggreggating the far-too-vague concept the “audience” or the “user.” Being able to identify and target specific audience segments will be essential for doing good digital public history work. There is no single or general user profile. Students need to know how to identify and assess the needs of their core constituencies. Retirees with an interest in local history will have vastly different concerns than middle school history teachers, who in turn will have different needs than scholarly researchers.Next, students need an introduction project development and planning that embraces agile development. Most history majors never consider issues of collaboration and management in their course work, but public history is about project work. Students need to learn what it takes to plan a collaborative projects and see it to completion. In the digital realm, this will be both a question of clarifying intellectual goals and creating technical work plans. I have written elsewhere about teaching project management, but it’s worth emphasizing here why agile principles are important. Agile development began as a movement to resist the heavily managed and bureaucratic methods of software development. Based on the notion that frequent builds, assessment, and correction allow for more nimble and responsive development, agile methods offer a model for digital public history too. Projects can start small with existing tools and resources. From that base the project team can gather information about audience response and content framing that can then be rolled into the project work flows. The emphasis here is on moving quickly and learning constantly. These practices are particularly well-suited to small organizations with slim technical infrastructure and staffing.Turning to the technical front, students also need a sense of sound design and development practices, including the basic forces that make the web work: HTML, CSS, content management systems, etc. It is too much to expect that students would graduate with deep knowledge of these building blocks, but they need to be familiar enough with the structural elements that they can interact with developers and designers who do possess the technical knowledge to build customized sites and applications.Despite the need for this general technical knowledge, students must have a sense of how effectively to use existing software and services to stand up quickly a digital project. In some cases this will be as simple as meeting audiences where they are already gathering on the web by sharing collections and expertise on HistoryPin or Wikipedia. In other cases, it will mean using existing outreach and communication venues such as Twitter, YouTube, Tumblr or WordPress to share the exciting work of public historians and their organizations. Furthermore, students need to have a sense of how to use software-as-service tools such as Omeka.net, Drupal Gardens, Dipity, Viewshare, and GeoCommons to produce narrative exhibits and data and geospatial visualizations that can communicate historical complexity and can engage audiences with historical questions and materials.These are just the basics, but I don’t see how we can ask anything less from our public history students if we are to claim to send them out into the world with training in digital history.[Note: This post was a DHNow Editor's Choice on May 1, 2012.]The research I am doing presently uses visualizations to show latent patterns that may be detected in a set of poems using computational tools, such as topic modeling. In particular, I’m looking at poetry that takes visual art as its subject, a genre called ekphrasis, in an attempt to distinguish the types of language poets tend to invoke when creating a verbal art that responds to a visual one. Studying words’ relationships to images and then creating more images to represent those patterns calls to mind a longstanding contest between modes of representation—which one represents information “better”? Since my research is dedicated to revealing the potential for collaborative and kindred relationships between modes of representation historically seen in competition with one another, using images to further demonstrate patterns of language might be seen as counter-productive. Why use images to make literary arguments? Do images tell us something “new” that words cannot?Without answering that question, I’d like instead to present an instance of when using images (visualizations of data) to “see” language led to an improved understanding of the kinds of questions we might ask and the types of answers we might want to look for that wouldn’t have been possible had we not seen them differently—through graphical array.Currently, I’m using a tool called MALLET to create a model of the possible “topics” found in a set of 276 ekphrastic poems. There are already several excellent explanations of what topic modeling is and how it works (many thanks to Matt Jockers, Ted Underwood, and Scott Weingart who posted these explanations with humanists in mind), so I’m not going to spend time explaining what the tool does here; however, I will say that working with a set of 276 poems is atypical. Topic modeling was designed to work on millions of words, and 276 poems doesn’t even come close; however, part of the project has been to determine a threshold at which we can get meaningful results from a small dataset. So, this particular experiment is playing with the lower thresholds of the tool’s usefulness.When you run a topic model (train-topics) in MALLET, you tell the program how many topics to create, and when the model runs, it can output a variety of results. As part of the tinkering process, I’ve been working with the number of topics to have MALLET use in order to generate the model, and was just about to despair that the real tests I wanted to run wouldn’t be possible at 276 poems. Perhaps it was just too few poems to find recognizable patterns. For each topic assignment, MALLET assigns an ID number to the topic and “topic keys” as keywords for that topic. Usually, when the topic model is working, the results are “readable” because they represent similar language. MALLET would not call a topic “Sea,” for example, but might instead provide the following keywords:blue, water, waves, sea, surface, turn, green, ship, sail, sailor, drownThe researcher would look at those terms and think, “Oh, clearly that’s a nautical/sea/sailing” topic, and dub it as such. My results, however, on 15 topics over 276 poems were not readable in the same way. For example, topic 3 included the following topic keys:3 0.04026 with self portrait him god how made shape give thing centuries image more world dread he lands down back protest shaped dream upon will rulers lords slave gazes hoe futureI don’t blame you if you don’t see the pattern there. I didn’t. Except, well, knowing some of the poems in the set pretty well, I know that it put together “Landscape with the Fall of Icarus” by W.C. Williams with “The Poem of Jacobus Sadoletus on the Statue of Laocoon” with “The New Colossus” with “The Man with the Hoe Written after Seeing the Painting by Millet.” I could see that we had lots of kinds of gods represented, farming, and statues, but that’s only because I knew the poems. Without topic modeling, I might put this category together as a “masters” grouping, but it’s not likely. Rather than look for connections, I was focused on the fact that the topic keys didn’t make a strong case for their being placed together, and other categories seemed similarly opaque. However, just to be sure that I could, in fact, visualize results of future tests, I went ahead and imported the topic associations by file. In other words, MALLET can also produce a file that lists each topic (0-14 in this case) with each file name in the dataset and a percentage. The percentage represents the degree to which the topic is represented inside each file. I imported the MALLET output of topics and files associated with them into Google Fusion Tables and created a dynamic bar graph that collects file-ids along the vertical axis and along the horizontal axis can be found the degree that the given topic (in this case topic 3) is present in the file. As I clicked through each topic’s graph, I figured I was seeing results that demonstrated MALLET’s confusion, since the dataset was so small. But then I saw this: [Below should be a Google Visualization. You may need to "refresh" your browser page to see it. If you still cannot see it, a static version of the file is visible here.]If the graph’s visualization is working, when you pass your mouse over the lines in the bar graph, the ones that are higher than 0.4, then the file-id number (a random number assigned during the course of preparing the data) appears. Each of these files begin with the same prefix: GS. In my dataset, that means that the files with the highest representation of topic 3 in them can all be found in John Hollander’s collection The Gazer’s Spirit. This anthology is considered to be one of the most authoritative and diverse—beginning with classical ekphrasis all the way up to and including poems from the 1980s and 1990s. I had expected, given the disparity in time periods, that the poems from this collection would be the most difficult to group together because the diction of the poems changes dramatically from the beginning of the volume to the end. In other words, I would have expected the poems to blend with the other ekphrastic poems throughout the dataset more in terms of their similar diction than by anything else. MALLET has no way of knowing that these files are included in the same anthology. All of the bibliographical information about the poems has been stripped from the text being tested. There has to be something else. What something else might be requires another layer of interpretation. I will need to return to the topic model to see if a similar pattern is present when I use other numbers of topics—or if I include some non-ekphrastic poems to the set being tested—but seeing the affinity in language between the poems included in The Gazer’s Spirit in contrast to other ekphrastic poems proved useful. Now, I’m not inclined to throw the whole test away, but instead to perform more tests to see if this pattern emerges again in other circumstances. I’m not at square one. I’m at a square 2 that I didn’t expect.The visualization in the end didn’t produce “new knowledge.” It isn’t hard to imagine that an editor would choose poems that construct a particular argument about what “best” represents a particular genre of poetry; however, if these poems did truly represent the diversity of ekphrastic verse, wouldn’t we see other poems also highly associated with a “Gazer’s Spirit topic”? What makes these poems stand out so clearly from others of their kind? Might their similarity mark a reason for why critics of the 90s and 2000s define the tropes, canons, and traditions of ekphrasis in a particular vein? I’m now returning to the test and to the texts to see what answers might exist there that I and others have missed as close readers. Could we, for instance, run an analysis that determines how closely other kinds of ekphrasis are associated with Gazer’s Spirit’s definition of ekphrasis? Is it possible that poetry by male poets is more frequently associated with that strain of ekphrastic discourse than poetry by female poets?This particular visualization doesn’t make an “argument” in the way humanists are accustomed to making them. It doesn’t necessarily produce anything wholly “new” that couldn’t have been discovered some other way; however, it did help this researcher get past a particular kind of blindness and helped me to see alternatives—to consider what has been missed along the way—and there is, and will be, something new in that.Below is the text from a talk I gave at the Geographies of Desire conference, held at the University of Maryland on April 27-28. Almost everything that I said there is something that I’ve said here before, so faithful readers won’t find much that’s new. But I promised I’d stick it up here, so here it is! If you’re simply looking for the set of links to the resources I mentioned, you can find those on Pinboard. I haven’t included all of my slides here, but you can find those here. I haven’t included all my ad-libbing either, but you would have had to have been there for that.“Where material book culture meets digital humanities”Discussions about early modern books and digital tools have tended to focus on one of two responses. One of the first things that people focus on is the amazing access that digital tools have given us to early modern works. Instead of schlepping from library to library across the globe—a series of journeys that many scholars could not easily afford—we can access nearly all extant early modern printed English books, and many continental ones, from our desktops. Thanks to EEBO (Early English Books Online), ECCO (Eighteenth Century Collections Online), and Gallica (the digital collection of the Bibliothèque nationale), among others, digital facsimiles are available for us to consult and download entire works from the early modern printed world.There are limitations, of course. One is the quality of the images. EEBO consists of digital facsimiles not of early books, but of microfilms of early books. As a result, it doesn’t always capture what we might want it to. Here we see an image from EEBO of the second quarto of Hamlet.opening from a Folger Q2 Hamlet, as in EEBOYou can see one column of text on each page, along with a whole bunch of other junk. [slide] Here’s the same page opening from the Folger’s reproduction of that book:same opening, same copy, in a high resolution image from the FolgerThere’s still ink bleeding through from the other sides of these leaves, but it’s a bit easier to sort out what’s what.Then there’s this, another image of not-quite visible ink mixed in on the page:opening from a 1557 Primer, as in EEBOBut this is an instance of red ink not reproducing clearly.same opening, in a high-resolution image from the FolgerAnd because the red isn’t visible, you miss in the EEBO copy what’s really a great mistake on this page, the moment where the phrase “of the five corporall joyes of our Ladie” is really a correction for the mistaken “joyes of our lorde.”never mix up your lord and your ladyMy favorite EEBO moment, however, is this one: the title page of a 1612 elegy mourning the death of Prince Henry.the title page of STC 23576 as in EEBOThis is how the image appears in EEBO; but this is how the image appears in their reproduction of the second state of this edition.title page of STC 23577 as in EEBODo you see what happened? It’s a mourning book, and it was printed on pages bordered in black and sometimes entirely in black, with a xylographic title page, that is, a title page in which white lettering appears on a black background. But when the microfilm was being processed, someone clearly didn’t believe what they were seeing and they assumed it was a mistake, that it should be black on white, and so they reversed the negative, producing a facsimile of a book that doesn’t exist.There are resources that provide higher quality digital facsimiles of early modern books and that, unlike EEBO and ECCO, are free to use. The Folger has digitized many works in their entirety, including all copies of the pre-1642 Shakespeare quartos and a couple of first folios. The British Library has digitized some of their collection, cover-to-cover, as have many other libraries, including that of the University of Pennsylvania, Princeton, University of Oklahoma, and the Bavarian State Library. The English Broadside Ballad Archive now includes some high-resolution color facsimiles, and the Universal Short-Title Catalogue (covering all books printed in Europe in the 15th and 16th centuries) includes links to digital copies from many European libraries.Digital tools have, without a doubt, increased our access to facsimiles of early modern books. If I can sit in my study in Rockville and study Erasmus’s 1516 translation of the New Testament by looking at a copy currently held in Basel, that’s a win.If one dominant way of thinking about digital tools and early modern books is in terms of access, another has been in terms of text. Access is about text of course—what we’re gaining access to is the ability to read texts. But there are also digital tools that don’t simply read texts, they distant read them. EEBO-TCP can make research a bit easier if you’re interested, say, in sassafras and want to find instances of it being discussed. In the right hands, you can do much more interesting types of computational analysis that can reveal things that would be difficult to see otherwise. Recent work by Michael Witmore and Jonathan Hope, for instance, reveals that genre is marked not only in terms of plot, but also linguistically at the sentence level—histories and comedies and tragedies are genres that are grammatically inflected. That seems like a win to me, too.These tools that I’ve just described rely on the ways we have always read books, albeit with increases in distance or speed (you can read a book held at the Folger Shakespeare Library from your study in Gdansk; you can analyze the texts of the entire Shakespeare corpus in a matter of minutes rather than years). I want to take this moment to wonder what new possibilities we might imagine. How might we use digital tools to look at texts differently? How might we use digital tools to represent texts differently? Can we move away from reading text to studying the physical characteristics of text, characteristics that can reveal important information about the content of the text and the cultural and historical creation of the artifact?The multi-spectral imaging done by the Lazarus team of the Archimedes Palimpsest gives a hint of how digital tools might let us see things that would otherwise go unseen. The Archimedes Palimpsest is a 13th-century Byzantine prayerbook written over a 10th-century manuscript containing writings of the Greek mathematician Archimedes, as well as multiple other works from various periods. Using multi-spectral imaging, along with other tools, the team was able to recover visual access to much of the earliest writings in the book. Google took the project’s dataset and made a “Google book” of the earliest state of the codexresulting in a digital reproduction of a book that exists, but is not visible to us just by looking at it.One recent paper about the use of densitometers to study levels of dirt on the pages of medieval manuscripts suggests that we can learn about book usage through analyzing how and where dirt is distributed across a book. It might seem obvious that pages that are used more often will be dirtier, and that is in part what the author found, but the use of the densitometer revealed that it’s more complicated than we can always assess with the naked eye. The paper’s author, Kathryn Rudy, points out, for example, that she had assumed that two different patterns of dirt on an opening came from two different users, but the densitometer’s analysis suggested that the patterns were similar enough that they were likely to have been made by the same person—perhaps they held the book in different ways suitable for different prayers. The analysis also pointed out that even books that retain visible marks might have been cleaned by modern owners to such a degree that the dirt is no longer viable as an analytical tool, something that might help us think about the changes books undergo during modern ownership.Studying the distribution of dirt is just the beginning of how we might begin to use technology to help us understand books in new ways. A colleague in Antwerp reports that German books held in Belgium smell different than German books held in Germany. The cause lies in how the paper was treated: paper needs to be treated with sizing agents so that it handles ink properly (instead of absorbing ink, ink sits on the surface of the paper and dries there, producing crisp and legible marks). His speculation is that books in Germany were sized in a multi-stage sequence, with the last step taking place after the book had been printed, perhaps as part of the binding process. Books that remained in Germany after they were printed went through this final process; books that were shipped outside of Germany seem to have missed that final stage, resulting in a noticeably different smell because of their different chemical properties. If this is the case, the smell of early German books can help scholars understand not only the physical acts of making paper and books, but can help us trace the circulation of early printed works. Using computers to analyze the smells of books and software to map those smells could help researchers learn how books were made and sold and used.We could also use new technologies to explore other the other senses we use when handling books. The feel of paper (or parchment) is another element of books that has more to offer than nostalgic fetishizing: the thickness, color, and pliability of paper can tell us about the costs of production, in part, but also give insight into the experience of using the book and its intended audience. How might the characteristics of feel be represented in digital media? Could a 3D printer replicate samples of different paper qualities? Could we project back from a paper’s physical characteristics today to how it might have appeared and felt when it was made?The three-dimensional aspects of paper extend beyond what can be felt by human touch. The process of making books in the letterpress period—and the process of writing on leaves of paper and parchment in all periods—is a process of putting pressure on the paper, leaving behind an indentation on one side of the leaf and an extrusion on the other side of the leaf. In most cases, the indentations are visible because the instrument causing them (type, woodblock, stylus) left behind ink markings. In other cases, there are indentations without ink, sometimes caused when two sheets of paper are accidentally run through the press, sometimes left behind when the bearing type used to even out the blank spaces in a page leaves behind blind impressions.Folger STC 7043.2, leaf F1v under raking light (click to see this image compared to one under normal light)There are also the indentations left behind during the papermaking process from the wires and frames used in the forms. Once we start thinking in these terms, we can find more topographical variations on leaves of paper: wormholes, dog-eared corners, holes left from stitches sewing gatherings and the binding together, plate marks from engravings. What might we learn from visualizing books not as texts to be read but as topographical maps?Another option would be to use digital tools to visualize the context of books, to encounter them not in isolated codices, but in libraries. This 360° panoramic view inside the Strahov Monastery’s Library in Prague lets you see not only the entire room, but to zoom in to see the titles of the books on the shelves. This is primarily a pretty picture, but imagine if this technology was married to something that let you look at catalog records of the books that you’re seeing, or to switch from catalog records to a view of a book on a shelf.screenshot of a zoomed out view of Strahov Librarytitles on books at the far end of the libraryIf we could use digital tools to estrange ourselves from our books, to defamiliarize what we think we know, we might learn something new about how they were made and how they are used. People keep pointing out to me that we are in the incunabula age of digital texts. We are. And that’s what makes it so exciting.By Nick Poole, CEO, Collections TrustAs Douglas Adams once memorably said, 'lovers of print are simply confusing the plate for the food'. The message is the thing, not the medium through which it is conveyed. But if this is true of print, will it not turn out to be equally true of 'Digital'? There appears to be some confusion about which 'Age' it is that we are living through. Some call it the 'Digital Age', and characterise the revolution as being one driven by technology. Others would have it as the 'Information Age', regarding the technology merely as a tool through which the rich information layer of daily life is exposed, manipulated and enhanced.There is something gloriously oxymoronic about writing a blog for a website section called 'Sustaining Digital' when digital is not a thing in itself which can either be sustained or preserved. It is a portmanteau term covering a range of activities, technologies, business models and skills which focus on transcoding information into binary and transmitting it through wires and circuits.It is a peculiarity, perhaps, of the pace of postwar technological development that for some 'Digital' is still new, and filled with potential, while for others it is yesterday's idea. Wired magazine runs a regular feature called 'Wired, Tired and Expired'. Digital, in the culture sector at least, appears to be all 3 at the same time. There is a fun game you can play (for those of us without social lives) of trying to figure out where someone is on this journey when you meet them for the first time.But if we can agree at least that these developments emerge, disrupt and are then assimilated into the mainstream, then it seems likely that in a few short years from now, we won't be using the term 'Digital' much at all, in the same way that we seldom use 'Atomic' and 'Electronic' has come to acquire an ironic cyberpunk charm. And if this is the case, I thought it might be interesting to speculate about what we will be using instead, and what that might mean for museums, archives and libraries.There are, of course, the current contenders - 'mobile', 'social' and 'cloud'. Of the 3, two are about atoms and electrons, and one (social, in case you were wondering) seems to be about people. In Mia Ridge's wonderful phrasing 'technology changes, people don't', and certainly, the dizzying growth of social networks and the integration of the 'social graph' into e-commerce and e-Government seem to point towards a fundamental development in technology which serves a basic human purpose.There are two things which prevent me from suggesting we're heading into a 'Social Age', though. The first, most obvious, is that we have alway been in a social age. Social networking has been the foundation of most of human experience, and it is telling that Dunbar's number seems to hold as true for Facebook as it does for tribes. In many ways, then, the social graph represents the normalisation of technology into existing patterns of behaviour. It is not so much a radical departure as a reappropriation of technology for a very basic human purpose.The second is that the 'social' experience online is a peculiarly stylised one - as though a software engineer who had never attended one had been asked to code a dinner party. Facebook is less a 'social' experience, and more like a virtual equivalent of Rear Window, allowing users to peer into a strange public/private hybrid rendition of our lives. LinkedIn is us at a job interview, and Google+ gives rise to the highbrow joke 'I thought it was only Dante that liked to put his friends in Circles'. While social certainly extends the museum, library or archive's arsenal of engagement and partnership with its audiences, it remains a different kind of relationship from a truly 'social' one.So if 'social' is an expansion of 'Digital', and if both are in the process of assimilation into mainstream culture, then where might we be going next? The answer, I suspect might come not from technology but from the far greater context of global economic and social change.The next two generations will have grown up against a backdrop of geopolitical, social and personal change. They may be experiencing and interacting with this brave new world through screens, but it is the world, and not the screen that matters.Contrary to the Daily Mail fantasy of feral, socially-irresponsible children roaming the streets and ram-raiding JB Sports on their stolen mopeds, and at the risk of social stereotyping, many children and young people are reaching political and social maturity early (often of necessity) and taking their social responsibilities very seriously indeed. I suspect that the low levels of political engagement reported among the 18-25 demographic are far more to do with a disaffection with the way we do politics than with politics itself.The challenges facing the next two generations are significant. Restore faith in the integrity of the state, adjust expectations of personal wealth and progressive growth, sustain the momentum of tolerance and integration, adjust to a career based on flux and uncertainty, find innovative, practical solutions to environmental change and the scarcity of resources. That's on top of the usual concerns of health, education, security and welfare. And somewhere in this mix they will need to begin to find answers to profoundly important questions of transparency, equality and social justice.In this world of 10 years' time, bandwidth will be ubiquitous in industrialised nations, and convergence will have delivered consumer-oriented devices that allow the physical and digital worlds to become one and the same - information will become physical, the Internet of things will be all around us. The 'Digital' Agenda will be the agenda, and 'Digital Culture' will be 'Culture'. Sensors will make things responsive and intelligent, enabling people to encode the millions of actions and interactions of their daily lives and interpret them as data.So it will not be the 'digitality' of this age that counts. It will be the connectedness of things and people, and they ways in which technology allows us to create and manipulate those connections that counts. I have sought high and low for an expression to describe the age we're heading into, and I can find no better formulation than that it will be a Connected Age - in which people are connected socially, digitally, personally and politically in a kind of augmented communitarianism.And in a Connected Age, our serious 18 year olds will become serious adults, taking responsibility for the construction of a transparent, accountable and just society - which even if they fall short of the Utopian ideal, will help correct some of the iniquities they will have inherited.So what would a Connected society mean for museums, archives and libraries? Everything. Connection is what we do - showing people the global implication of their personal context, demonstrating that cultures across the world share more in common than in conflict, empowering literacy in the fullest sense - linguistic, informational and cultural - to equip this future generation with the tools both intelligently to navigate the abundance of information and to use it to achieve social justice.The idea of museums provides a Connected society with depth, validity and context - it makes their advance incremental rather than cyclical. The library is a place in which people become connected and which, critically, can help overcome the increasing risk of disenfranchisement and illiteracy. The archive provides a fund of prior knowledge upon which to build future ideas. All 3 domains play a vital role in a society that seeks to use connection to make itself both more stable and more prosperous - which is why the more enlightened emerging economies are busily investing in cultural infrastructure as a token of economic and social development.When we think of the challenges which confront museums, archives and libraries today, they are not simply challenges of marketing or presentation, funding or political profile. Nor are they challenges of how to 'go digital'. They are challenges of relevance - our fluency with social media will define the confidence with which we step into the Connected Age. Our comfort with shared authority and interpretation will define the extent to which we empower or disenfranchise our users from creating and exploring their own connections. Our commitment to integrity and transparency will define the extent to which the coming generations will see us as part of the problem or part of the solution. Our deftness with open business models will define whether our future customers understand, and are willing to pay for, the value we can add.Progress is rarely smooth, and predictions are usually wrong, but as we wrestle with the questions at the heart of Digital, it might well be worth looking up every now and again and thinking about how what we do today will decide whether we have a place in tomorrow.As we reach a point where many of the classic books of literature and science published before the magical date of 1923 have been digitized, it is time to consider the quality of those copies and the issue of redundancy. A serious concern in the times before printing was that copying -- and it was hand-copying in those times -- introduced errors into the text. When you received a copy of a Greek or Latin work you might be reading a text with key words missing or mis-represented. In our digitizing efforts we have reproduced this problem, and are in a similar situation as that of the Aldine Press when it set out to reproduce the classics for the first time in printed form: we need to carry the older texts into the new technology as accurately as possible. While the digitized images of pages may be relatively accurate, the underlying (and uncorrected, for the most part) OCR introduces errors into the text. The amount of error is often determined by quality of the original or the vagaries of older fonts.If your OCR is 99.9% accurate, you still have one error for every 1,000 characters. A modern book has about 1500 characters on a page, so that means one error for every page. Also, there are particular problems in book scanning, especially where text doesn't flow easily on the page. Tables of contents seem to be full of errors:IX. Tragedy in the Gra\'eyard 80 X. Dire Prophecy of the Howling Dog .... 89 XL Conscience Racks Tom 98  In addition, older books have a tendency to use hyphenated line breaks a great deal: and declined. At last the enemy's mother ap- peared, and called Tom a bad, vicious, vulgar child, These remain on separate lines in the OCR'd text, which is accurate to the original but which causes problems for searching and any word analysis. The other issue is that for many classic works we have multiple digital copies. Some of these are different editions, some are digitizations (and OCR-ing) of the same edition. Each has different errors. For the purposes of study, and for the use of these texts for study, it would be useful to have a certified "Urtext" version, a quality digitization with corrected OCR that scholars agree represents the text as closely and accurately as possible. This might be a digital copy of the first edition, or it might be a digital copy of an agreed "definitive" edition. We have a notion of "best edition" (or "editions") for many ancient texts. Determining one or a small number of best editions for modern texts should not be nearly as difficult. Having a certified version of such texts must be superior to having students and scholars reading from and studying a wide variety of flawed versions. Professors could assign the Urtext version to their classes, knowing that every one of the students was encountering the same high quality text.(I realize that Project Gutenberg may be an example for a quality control effort -- unfortunately those texts are not coordinated with the digital images, and often do not have page numbers or information about the edition represented. But they are to be praised for thinking about quality.)In digital space, everything we do is networked. Real thinking doesn’t (and can’t) happen in a vacuum. Our teaching practices and scholarship don’t just burst forth miraculously from our skulls. The digital academic community is driven by citation, generosity, connection, and collaboration. The work we do as hybrid and critical pedagogues, digital humanists, and alternative academic publishers depends on our sharing ideas as part of a much larger project or conversation.Two weeks ago, in his post “Catching the Good”, Dan Cohen explained how the community of digital scholars should be looking to “catch” others doing great work, rather than focusing on older models that utilize negative reinforcement like the submission/rejection process. Cohen recognizes key qualities that orbit digital publishing practices, namely experimentation and interdisciplinarity. At the end of the post, he writes:An attention to the psychology of positive-reinforcement in publishing has been a long time coming and is motivated by, among other things, the decrease in resources (money and time) required to publish. Clay Shirky argues, in , that the drop in expense digital practices precipitate opens fields of scholarly and social activity previously limited to insular professional communities. Considering Shirky’s attention to the economic impact of the web on social behavior and Cohen’s gesture toward the psychological impact of positive-reinforcement on digital scholars, we propose to define four virtues of digital media citation:In most digital publications, no one person shoulders the brunt of the entire project. Instead, we bring together teachers, philosophers, designers, technicians, coders, researchers, authors, editors, and commenters. On an even deeper level, our work on Hybrid Pedagogy is built from close engagement with our sources -- engagement with the coterminous work of ProfHacker, JITP, or the Journal of Digital Humanities, but also with seminal works like Paolo Freire’s Pedagogy of the Oppressed or Ralph Waldo Emerson’s “The American Scholar.”While citation has long been a defining feature of academic work, it happens more frequently and with different intentions in the best digital scholarship. We all build mental maps of posts, articles, projects, and arguments that guide our work. Good digital citation practices, though, aim to make these maps explicit, reconstructing them online by weaving together direct links to the intersecting texts that help shape our own ideas.Consider the sometimes frenetic consumption of web content. We read an article on a computer screen, or we skim it on a smartphone as we walk to class. As we read, we might be reminded of something we can’t quite put our finger on. We annotate and leave even ourselves reminders. We build lists on Evernote or Diigo; we “favorite” tweets of articles we’ve enjoyed or need to return to. We consume so many texts, though, across so many fields, in so many different ways (consider the difference between sitting down to read The New Yorker and scanning across leads in a digest of new posts on ProfHacker) that we risk losing locations on the map of our own consumptive practices.If this is true for digital media consumers (a condition championed by Tyler Cowan and lamented by Nicholas Carr), as digital media authors, we must work even harder to build thorough maps for the audiences of our scholarship. We attribute not just for rhetorical effect, but for intertextual familiarity. Sources no longer deliver merely arguments or data; they create an interactive critical network.Espousing deference as an academic virtue may sound odd, especially within a discussion of citation. From a more traditional scholarly publishing perspective, competition often pits potential collaborators against each other. In the rush to publish the results of an experiment or the impact of a particular theory, scholars might actively avoid acknowledging their indebtedness to the work of their peers. A singularly influential journal article can make someone a celebrity in the traditional publishing model, a kind of ideological authority that invites rhetorical challenge.In digital discourse, however, an article has less claim to such authority because it is in such immediate contiguity with parallel scholarship -- the Burkian conversation metaphor brought to fruition. Rather than everyone talking over one another, the best digital texts talk in turn, express appreciation and connection, and are honest about their indebtedness to related works.Digital scholarship has a different surface than traditional scholarship, beyond just being communicated across a different medium. That surface is more evenly perforated. It can and should allow more academic voices to permeate its boundary as references. When that happens, our attitude toward those sources becomes more deferential. The work of others completes a kernel of an idea we’ve only just begun to consider. We propose a question and call others to answer. Through hyperlinks and post-publishing tweets that make the scholarly invitation explicit, we demonstrate our humility. Liberal citation communicates a deference to the scholarly community that guides our work and may be best suited to extend it.This has all been said even better, and more succinctly, by Cormac McCarthy. In a rare interview about his work, he says, “The ugly fact is books are made out of books. The novel depends for its life on the novels that have been written.”There is nothing particularly new or innovative about curation, but the practice is currently being championed and re-imagined by web authors. For example, the Chronicle’s Arts and Letters Daily is a beautifully simple and fresh list of “ideas, criticism, and debate” across a range of academic disciplines. The democratic format of its short entries and links eschews authorial-name dropping, instead enticing readers with thought-provoking blurbs. Arts and Letters Daily’s curated links strip ethos away, allowing readers more direct access to an article’s pith. With more attention to design, Brain Pickings is an explosion of connected, structured, interdisciplinary material. Brain Pickings is “the brain child ofMaria Popova, a cultural curator and curious mind at large” and offers scores of inventive lists, reviews, and aesthetic curiosities. Finally, consider the curated media space In Media Res, where scholars propose, present, and contextualize contemporary media artifacts.Our goal with Hybrid Pedagogy is to be neither here nor there, and the best way for us to sit both inside and outside a thing (like teaching) is to think carefully about its boundaries, and to examine closely how that thing is connected to other things. We’ve been a part of (and have learned a great deal from) conversations among K-12 teachers, college teachers, #altac professionals, classroom teachers, online teachers, and educational technologists. The impetus of this project is to bring all these folks into rooms together (both virtual and physical), and thus to encourage cross-pollination of ideas about what it is to learn and about the changing nature of the tools and technologies we use for learning. Our citation practice (and the philosophy it engenders) is just one way through which we form and shape these rooms.Digital curation is not just about assembling a random list of links and web objects; rather, the best curatorial practice is more overtly intertextual, bringing those links and web objects (and the people behind them) into meaningful conversation, making explicit and implicit connections between them.Hyperlinks are just one of the ways that we put works into conversation on the web. The hyperlink (both as a literal device in digital texts and as a metaphor) draws a direct line between things at a conceptual distance, pushing them (no matter how disparate) into direct (metonymic) contact. The hyperlink is a call-to-action in at least two ways. It asks the reader to venture off the page and into a sourced work. It also invites the author of the sourced work into the conversation, through the trackback that tells them when and where they’ve been cited.Kathleen Fitzpatrick writes in Planned Obsolescence: Publishing, Technology, and the Future of the Academy,To the degree that scholarship is about participating in an exchange of ideas with one’s peers, new networked publishing structures can facilitate that interaction, but will best do so if the discussion is ongoing, always in process. This foregrounding of conversation, however, will likely also require authors . . . to relinquish a certain degree of control over their texts, letting go of the illusion that their work springs wholly from their individual intelligence and acknowledging the ways that scholarship, even in fields in which sole authorship is the norm, has always been collaborative. (12)Taking a cue from Fitzpatrick, Hybrid Pedagogy aims to rethink how digital media citation happens by moving away from citation as context-shaping or name-dropping and toward citation as critical-positioning and community-building. In this way, each citation and each hyperlink preempts the peer review process by inviting other scholars and pedagogues into the conversation. We don’t cite because someone has written the “best thing”; rather, we cite to offer feedback and to invite dialogue. Thus, digital scholarship becomes less about the delivery of knowledge and more about fostering critical engagement. It becomes less about knowing and more about doing -- more about a collection of practices, a methodology. These practices and methodology are born out of experimentation and out of discussions we have at conferences (especially ones like THATCamp), via Twitter, in our classrooms, and on the pages of a journal.Rather than end with neat and tidy conclusions, in the spirit of engagement, we offer these questions:1. What digital artifacts or scholarship have changed or overwritten the concept of traditional citation for you?2. Does the hyperlink amend or replace the static bibliographic record?3. How has social media reinvented citation practices?4. How should we teach research and citation in light of digital media practices?[Photo by mmechtley]TweetTags: Diigo, Pete Rorabaugh, Jesse Stommel, Evernote, Citation, Peer Review, CurationTice, James, Erik Steiner and Allan Ceen. “Imago Urbis: Giuseppe Vasi’s Grand Tour of Rome.” University of Oregon. http://vasi.uoregon.edu/Imago Urbis: Giuseppe Vasi’s Grand Tour of Rome was created in 2008 by Jim Tice and Erik Steiner and remains, in my mind, one of the finest examples of the integration of spatial and image data into a single digital scholarly work. Only a year after the introduction of Google Street View, Tice and Steiner gave us a street view of 18th century Rome, linked to an impressive and beautiful map of enormous proportions from the same period.As I continue to struggle with describing the difference between digital collections and digital scholarly works, I felt that the examination of existing digital scholarly works would allow me to better understand this distinction. Obviously, I am neither an art historian nor a geographer, and so my review of Imago Urbis is oriented toward the object as a digital publication, to see if it provides a cohesive and compelling representation of scholarly claims and research, as well as to see if that representation takes advantage of the data-driven or computational aspects of the digital object and is not limited to the narrative text.To be clear, just placing information on a map does not make it a scholarly argument. The use of the map to index information is no different than using categories, LOC headings or the Dewey Decimal System to index information. If there is no greater goal than to present data in an effective, indexed, searchable way, then the digital scholarly publication in question is a collection or archive, albeit a spatially-indexed one. The creators describe their own sense of the use of this work without reference to any of the arbitrary categories I’ve introduced above:Vasi’s Grand Tour places the work of these two masters in their cultural context: 18th century Rome and the Age of the Grand Tour examining the cartographic and artistic legacy that they inherited. It gives an account of Nolli’s work in light of this context but especially focuses on the vedutismo tradition and its impact on the work of Vasi and his contribution to the vedute genre. Over two hundred and forty of Vasi’s topographic prints are presented in detail and in relationship to Nolli’s map. Vasi’s work and methods are subsequently interpreted through an analysis of their topographic, artistic, and historic content.The site has as much narrative text as an in-depth scholarly article, and the Select Bibliography contains nearly thirty references. But unlike a traditional article, it also contains a massive and sophisticated, Flash-based interactive object. This enormous map, while not the only interactive element on the site, is the only data-driven element–the various beautiful high-resolution images that are provided via Zoomify are firmly rooted in the category of illustrations. That we can now provide illustrations with the level of detail witnessed here is extraordinary, and in my mind still not taken advantage of by most digital scholarly works produced nearly half a decade later.Vasi's "Panorama of Rome" in very high-resolution, provided by the Getty Research Institute and viewable using the Zoomify tiled viewer.Detail of the above.It is telling that the actual design of the site reinforces through UX the idea that this interactive map is meant to be engaged with after absorbing the narrative text that it is embedded in. The map itself lives on the rightmost side of the menu, after seven tabs (the last three of which contain a total of 19 sub-tabs of information). In this way, the interactive map is implied, through the format of the site, to be the Conclusion–and as such the reader should expect it to draw together the various points leading up to it and see within it some kind of final remarks. That is, of course, if this immersive and masterfully produced site is meant as the expression of a particular scholarly claim. It could also be an exhibit highlighting the work of Vasi and Nolli or an exploration of two “fundamental” ways (pictorial and cartographic) of describing Rome. On the Imago Urbis page, we see one of the core claims that can be explored spatially:The connection between the work of Nolli and Vasi is direct. Nolli contributed technical information for Vasi at the end of Volume I of the Magnificenze, recording the distances between city gates. Vasi’s credits Nolli by noting measurements “of the distances between city gates supplied to the author of this work by the architect and surveyor Gio. Battista Nolli from Comeo”. Most of Vasi’s views date from the twenty-year period directly following the publication of the Grande Pianta, so Vasi evidently used this useful tool for his work.And, later:It is instructive to see how Vasi uses these themes in his depiction of the city in both its urban center and ex-urban periphery. It is possible to interpret his views as illustrated stage sets which depict the special character of places in the city and he does so with a stage director’s eye for both drama and dramatis personae. The street life of the city adds an important dimension to his work and typically enhances our knowledge about the parts of the city he represents and how they functioned. For example he depicts boisterous markets such as the Campo dei Fiori and the Portico d’Ottavio as swarming with shoppers and shopkeepers. The river ports are teeming with boats and workers while the major basilicas have pilgrims, clergy and the ubiquitous beggar in the forefront of the scene.I would have preferred a greater emphasis in the design and implementation of both the site as a whole and the interactive map (dealt with in more detail below) in emphasizing and proving these claims about urban and ex-urban periphery, as well as a more exhaustive description of the categories of representation of urban life and the spatial patterns of those categories. Without them, this seems more like the authoritative text that accompanies an exhibit, which seeks to pass on well-understood historical knowledge to an educated but lay audience. But if this is an exhibit, then it would stand to reason from a publishing perspective to place the map and the various collections of images in a more prominent position.Likely my own inability to describe exactly where this digital scholarly object resides in a strict taxonomic sense is because it straddles the fence between exhibit and research. The map, being a useful tool for the traditional humanistic expert analysis (though not computational spatial analysis) of patterns, is presented along with the narrative results of such analysis, not so much to illustrate arguments about the spatial patterns of Vasi’s work, but rather as a resource for later scholars to further explore patterns such as the relationship between accuracy and urban density:In general, the visual liberties taken by Vasi’s decrease directly with the increase of real space between viewer and subject. Outside the densely built center Vasi shows his subjects with a remarkable degree of accuracy rivaling those of the camera lens. The views that occur outside or along the Aurelian walls, in the disabitato, and along the banks of the Tiber demonstrate this fact (Books I, V and X). These are the most consistent photographically accurate portrayals of the city and its buildings and spaces in the Magnificenze. The fact that these views had the benefit of unobstructed viewing space suggests that Vasi’s perspectival manipulations are the result of a carefully calculated plan to adjust the method of representation to circumstances and context and were assuredly not the result of whim or even less, inept draftsmanship. When Vasi moves into the dense urban center his methods change accordingly. In this tightly bound context streets and piazze are widened, viewing points of the same scene change from point to point for better viewing and buildings and other features are moved or deleted as necessary.Ultimately, the language used to describe the sections of the text, as well as the lack of footnotes, tends to reinforce the view of this being an exhibit. This is further accentuated in the Interpreting section, which seems more like public history. Despite the rich multimedia nature of the site, though, the text is typically divorced from the illustrations, as seen in the Related Views page, where a long narrative introduction is followed by a barely annotated list of images. This is fine for an collection, less so for an exhibit and jarring for a scholarly work–if one accepts formal definition of these different digital objects. The decision to divorce text from image does not follow, however, with the View Types page, which contrasts the various methods for representing sites within Rome based on perspective and composition.Like much collaborative work done in digital humanities scholarship, this is likely the result of an art historian and geographer collaborating to develop not only research but tools and objects. In the GIS Methods section, the spatial humanities effort seems to be directed not at proving spatial patterns but on proving a methodology and providing a resource for later scholars:The GIS paradigm relies on the principle that it is now feasible to precisely locate—and therefore relate–all features in geographic space whether historic or contemporary. In addition to being an efficient and intuitive method for cataloging historical documents, the ability to geo-reference diverse resources onto a single, accurate base will dramatically enhance the possibilities for the direct inspection and comparison of the architectural, natural, social and artistic dimensions of cities such as Rome. This method provides a paradigm for similar research in other urban centers and for other disciplines that treat a broad range of geographic contexts and issues. The underlying premise of this presentation is that by geo-referencing Nolli and Vasi and by exploring their distinct methods for describing the city one will be better equipped to understand the profound geo-spatial structure of the Eternal city.While I agree wholeheartedly that this is a path-breaking paradigm, and wish to have seen in the last five years its adoption and extension by historical GIS scholars (which may become more common with the growth of sites like History Pin) it is notably lacking in claims based on this method. In the short “Benefits” section, the map is described as providing a useful tool to facilitate interpretation and scholarship, language traditionally associated with digital archives or collections. The inclusion of elements such as a browseable timeline–hidden away in a link at the bottom of a sub-tab, reinforce this theme. Though, to be fair, the creation of rich, interactive chronological data visualization is still a problem half a decade later.Nolli and Vasi's life and selected works shown chronologically using Zoomify to provide readers with access to a static, large-image timeline.It is not until the Notes tab that we receive a distinct statement of problem–a choice of terminology that more evokes art and museum exhibits than a (hypo)thesis. The statement itself also presents the work (and here it is not clear if the work in question is the entire site or the synthetic, interactive map, though it would seem to imply the latter) as a tool for study and not the presentation of a particular claim:Given that Nolli and Vasi were contemporaries and collaborators focusing on the same subject, it seems obvious that their work is intrinsically related; up to now no vehicle existed to effectively synthesize their individual achievements into a single resource that effectively evokes Settecento Rome. We believe that it will be extremely informative to place these 18th century documents into their 21st century context so that spatial relationships can be drawn and new conclusions reached about their continuing significance to the understanding of the city. Our overarching objective is to document and integrate two distinct graphic modes for representing the Eternal City: the pictorial view and the ichnographic plan. In concert, we believe they can present a compelling image of the city and in the process inform and inspire its study.This interactive map, which is one of the finest interactive examples of the linking of image and spatial data that I’ve ever seen in the field of spatial history, bears out the interpretation of its purpose as being a tool for later research. In this sense, it is an index using spatial attributes to organize documents. Don’t let that antiseptic description fool you–an index of historical panoramas of Rome with a highly detailed historical map of Rome provides an uncanny effect. One could imagine how an entire genre of digital scholarly object such as this could provide scholars in various fields of history with a more sophisticated understanding of historical events.I cannot emphasize enough my admiration for the work Tice and Steiner have done by presenting so much rich media, especially in the interactive map. The text presents an engrossing and full account of the objects and individuals referenced throughout. However, I do think this tension between its role as an exhibit or a research tool or scholarly argument is problematic. While the map presents categories of theme and image style that are referred to throughout the text, there is no unifying course of argument that adequately defines how the two are related. It’s become popular to gesture toward the Spatial Humanities and claim that quantitative historical GIS is not suitable for the study of much of the qualitative and necessarily fuzzy subjects that make up the humanities. But with the quantitative measures already present in the Vasi images, both their spatial location as well as the image attributes–such as perspective, content and architecture–then some kind of demonstration of the correlation between the two using spatial analysis should have been integrated into both the map and the narrative for this to full embrace the role of digital scholarly argument. Such computational claims need not be divorced from narrative claims, though integrating the two is not an easy task.That said, this does not need to be a scholarly argument. As an exhibit, it is a success par excellence. I have to iterate my shock that I’ve seen nothing to compare with it made in the last five years. But if it is an exhibit, even a great one, then there is a very serious criticism to be made of the site. The placement of the interactive map, as a link on the far right after a series of links to the text about the map, firmly implies that the map, rather than being the centerpiece, is the conclusion of a long, scholarly narrative. This may seem like a silly design issue, but these design issues need to be taken seriously in the production of digital scholarly works. If the centerpiece of a work is an interactive map which is used as a spatial index for two hundred and fifty images, and the text of the work is annotation, analysis and commentary on that map and those images, then the link to the map cannot be placed in a subordinate position to the text.Half a decade is typically treated as “forever” in terms of production of digital media, while it is “barely yesterday” in terms of academic research. That such an object could still prove digitally impressive hints at the stability of digital scholarly works and highlights the importance of developing methods to publish and review such a mature genre. While publishing and maintaining such works is a pernicious issue, review of them is no less so. Peer reviewers of journal articles need not worry about pagination and formatting, but design elements are integral to rich digital objects and even as the genre stabilizes, will continue to be so into the foreseeable future. The traditional sticking point to reviewing highly collaborative work is that it is difficult to find reviewers with sufficient knowledge in the areas of scholarship being addressed, and so adding to this the need to be aware of design issues in the presentation seems to make practical peer review of digital scholarly works even less achievable. I think an equally valid, and more optimistic, interpretation is that as we come to terms with being able to review these works, these requirements for necessary categories and commensurate skills grow a bit, but also grow more stable. I hope I’ve also highlighted the need to firmly distinguish between types of genres within the spectrum of digital scholarly object, so that we can grapple with what makes a good argument, tool, exhibit or other type that seems to have a formally recognizable set of goals, audience, source material and design.Update: Oh, neat! This post was a DH Now Editor’s Choice on April 24th, 2012.This post is cross-posted from the Technoromanticism course blog.and the Maryland Institute for Technology in the Humanities blog.Team MARKUP evolved as a group project in Neil Fraistat’s Technoromanticism graduate seminar (English 738T) during the Spring 2012 term at the University of Maryland; our team was augmented by several students in the sister course taught by Andrew Stauffer at the University of Virginia. The project involved using git and GitHub to manage a collaborative encoding project, practicing TEI and the use of the Oxygen XML editor for markup and validation, and encoding and quality-control checking nearly 100 pages of Mary Shelley’s Frankenstein manuscript for the Shelley-Godwin Archive (each UMD student encoded ten pages, while the UVa students divided a ten-page chunk among themselves).Team MARKUP wrote a collaborative post covering the different phases of the project in detail, so I’ll use this post to concentrate on some specifics of my personal experience with the project.Affective editing is effective editing? One of my favorite quotations–so beloved that it shapes my professional work and has been reused shamelessly on my Ph.D. exams list, a Society for Textual Scholarship panel abstract, and at least one paper–is Gary Taylor’s reasoning on the meaningfulness of editing: “How can you love a work, if you don’t know it? How can you know it, if you can’t get near it? How can you get near it, without editors?”*. Encoding my editorial decisions with TEI pushed me a step closer to the text than my previous non-encoded editorial experience, something I didn’t know was possible.The Creature speaks! TEI for the first page of the Creature's monologue in Shelley's Frankenstein.My ten pages happened to be the first pages of the Creature’s monologue; hearing the voice of the Creature by seeing its true creator’s (Mary Shelley’s) handwriting gave me shivers–meaningful shivers accompanied by a greater understanding of important aspects of Shelley’s writing, such as the large editorial impact made by her husband Percy and the differing ways she crossed out or emphasized changes to her draft. Moving between the manuscripts images and the TEI encoding–so similar to my other work as a web designer and developer–also emphasized the differences in the writing process of my generation and the work that went into inscribing, organizing, and editing a book without the aid of a mechanical or digital device.Project management. Because we didn’t know what to expect from the project until we were in the thick of encoding–would everyone be able to correctly encode ten full pages? how would we control quality across our work? what would our finished pages look like in terms of encoding depth?–we spent most of the project functioning as a large team, which was both sometimes as unwieldy as our large GoogleDoc (trying to find a time when eight busy graduate students can meet outside of class time is difficult!) and sometimes made sense (I was one of the few people on our team comfortable with GitHub and encoding at the start of the project, so I helped with a lot of one-on-one Skype, in-person, and email sessions early on). If I did the project over, I would have held a single Bootcamp day where we all installed and pushed within GitHub and encoded one page of manuscript up on the projector screen, then delegated my role as team organizer by dividing us into three subgroups. I also might have insisted on people agreeing ahead of time on being available for specific in-person meeting times, rather than trying to schedule these one or two weeks beforehand. I do think things worked out pretty well as they did, largely because we had such a great team. Having the GoogleDoc (discussed more below) as a central point for tech how-tos, advice, and questions was also a good choice, though in a larger project I’d probably explore a multi-page option such as a wiki so that information was a) easier to navigate and b) easily made public at the end of our project.Changing schemas and encoding as interpretive. Encoders who started their work early realized that their efforts had good and bad results: because the schema saw frequent updates during our work, those who finished fast needed to repeatedly update their encoding (e.g. a major change was removing the use of <mod type>s). Of course it was frustrating to need to update work we thought was finished–but this was also a great lesson about work with a real digital edition. Not only did the schema changes get across that the schema was a dynamic response to the evolving methodology of the archive, it prepared us for work as encoders outside of a classroom assignment. Finally, seeing the schema as a dynamic entity up for discussion emphasized that even among more seasoned encoders, there are many ways to encode the same issue: encoding, as with all editing, is ultimately interpretative.Encode all the things! Or not.Depth of encoding was a difficult issue to understand early on; once we’d encoded a few pages, I began to have a better sense of what required encoding and what aspects of the manuscript images I could ignore. Initially, I was driven to encode everything, to model what I saw as thoroughly as possible: sums in the margins, different types of overstrikes, and analytical bibliography aspects such as smudges and burns and creases.Encode all the things... or not. Remixed from image by Allie Brosh of Hyperbole (hyperboleandahalf.blogspot.com).What helped me begin to judge what to encode was understanding what was useful for Team MARKUP to encode (the basics that would apply to future encoding work: page structure and additions and deletions), what was useful for more advanced encoders to tackle (sitting in on the SGA staff meetings, I knew that some of our work would be subject to find-and-replace by people more experienced with Percy and Mary’s handwriting styles), and what our final audience would do with our XML (e.g. smudges and burns weren’t important, but Percy’s doodles could indicate an editorial state of mind useful to the literary scholar).Editorial pedagogy. Working on Team MARKUP not only improved my markup skills, it also gave me more experience with teaching various skills related to editions. As I mentioned above, acting as organizer and de facto tech person for the team gave me a chance to write up some documentation on using GitHub and Oxygen for encoding work. I’m developing this content for this set of GitHub Pages to help other new encoders work with the Shelley-Godwin Archive and other encoding projects. Happily, I was already scheduled to talk about editorial pedagogy at two conferences right after this seminar ends; the Team MARKUP experience will definitely become part of my talks during a panel I organized on embedding editorial pedagogy in editions (Society for Textual Scholarship conference,) and a talk on my Choose-Your-Own-Edition editorial pedagogy + games prototype at the Digital Humanities Summer Institute colloquium in Victoria.Ideas for future encoding work. I’ve started to think about ways to encode Frankenstein more deeply; this thinking has taken the form of considering tags that would let me ask questions about the thematics of the manuscript using Python or TextVoyeur (aka Voyant); I’m also interested in markup that deals with the analytical bibliography aspects of the text, but need to spend more time with the rest of the manuscript images before I think about those. So far, I’ve come up with five new thematic tagging areas I might explore:Attitudes toward monstrosity: A tag that would identify the constellation of related words (monster, monstrous, monstrosity), any mentions of mythical supernatural creatures, metaphorical references to monstrosity (e.g. “his vampiric behavior sucks the energy out of you”), and reactions/attitudes toward the monstrous (with attributes differentiating responses to confronting monstrosity with positive, negative, and neutral attitudes). I could then track these variables as they appear across the novel and look for patterns (e.g. do we see less metaphorical references to monstrosity once a “real” monster is more prevalent in the plot?).Thinking about doodles: We’re currently marking marginalia doodles with <figure> and a <desc> tag describing the drawing. In our section of the manuscript, many (all?) of these doodles are Percy Shelley’s; I’d like to expand this tag to let me identify and sort these doodles by variables such as complexity (how much thought went into them rather than editing the adjacent text?), sense (do they illustrate the adjacent text?), and commentary (as an extension of sense tagging, does a doodle seem ironically comic given the seriousness or tragedy of the adjacent text?). For someone new to studying Percy’s editorial role, such tagging would help me understand both his editing process and his attitude toward Mary’s writing (reverent? patronizing? distracted? meditative?)Names, dates, places: These tags would let us create an animated timeline of the novel that shows major characters as they move across a map.Anatomy, whole and in part: To quote from an idea raised in an earlier post of mine, I’d add tags that allowed “tracking the incidence of references to different body parts–face, arms, eyes–throughout Frankenstein, and trying to make sense of how these different terms were distributed throughout the novel. In a book concerned with the manufacture of bodies, would a distant reading show us that the placement of references to parts of the body reflected any deeper meanings, e.g. might we see more references to certain areas of the body grouped in areas of the novel with corresponding emphases on the display, observation, and action? A correlation in the frequency and placement of anatomical terms with Frankenstein‘s narrative structure felt unlikely (so unlikely that I haven’t run my test yet, and I’m not saving the idea for a paper!), but if had been lurking in Shelley’s writing choices, TextVoyeur would have made such a technique more visible.”Narrative frames: Tags that identified both the specifics of a current frame (who is the speaker, who is their audience, where are they, how removed in time are they from the events they narrate?) and that frame’s relationship to other frames in the novel (should we be thinking of these words as both narrated by Walton and edited by Victor?) would help create a visualization of the novel’s structure.I expect that playing around with such tags and a distant reading tool would yield even better thinking about encoding methodology than the structural encoding I’ve been working on so far, as the decisions on when to use these tags would be so much more subjective.* From “The Renaissance and the End of Editing”, in Palimpsest: Textual Theory and the Humanities, ed. George Bornstein and Ralph G. Williams (1993), 121-50.Tags: academics, digital texts, encoding, markup, me and my work, novel, SGA, skillsSince I work in the CDLR, I get to raise all kinds of wild questions that don’t fall into the purview of traditional, disciplinary bound scholarship. To prepare for my presentation at the Pop Conference (instituted by Experience Music Project in Seattle), this year combined with IASPM-US (International Association for the Study of Popular Music), I became preoccupied with the question: “How do I visualize a music analysis about space and place?”My paper extends my dissertation work on The Kominas, a South Asian American punk band tied to the alternative Muslim subculture self-labeled as Taqwacore. In this paper, I chose to focus on the band’s music. Through a couple of song readings, I investigate the form and content of diasporic spaces as articulated by the band’s music. I argue that this unique geo-musical formation discursively moves seamlessly between a conventional notion of diaspora—migration of people away from an ancestral homeland—and a minority-centered, multi-diasporic space. Through a recent engagement with multimodal scholarship, I challenged myself to think beyond writing, a mode that conventionally represents academic work. I already use the concepts such as cartography and mapping as metaphors. Why should I limit the expression of my ideas to text only? Why not create a map of my music analysis especially since it’s about space and place?Visualizing a musical analysis is nothing new. Music theorists have used music notations to represent sonic patterns key in their interpretation. More recently, theorists and information scientists used computational means to process sonic materials for patterns. Visualization became a way to explore patterns, bringing sounds into a (visual) domain that were previously inaccessible with the human senses.My paper, however, does not engage with the use of the computational technologies to process sonic materials. It does something rather old-school. It simply draws several points on a map and then links them. It does not overlay demographic or musical data. It displays a couple of different geographical formations that illustrate the changing contour of a musical diaspora, a geographical space comprised of lyrical, sonic, and choreographic references. [I deployed Josh Kun’s concept of “audiotopia” to argue for the social and cultural effects of this geo-musical space.]I began with a hand-drawn map. I used the Penultimate app on my iPad.I quickly realized that my hand drawn diagram is not only messy but almost illegible. Through searching and playing, I settled with the web-based mapping program Scribble Maps to map this unique diasporic spaces. Using features such as vector graphics, media imports, and baselayer settings, I created a couple of maps that best approximate the geo-musical entities for which I argue in my analysis.This map articulates The Kominas’ worldview. I positioned South Asia in a visually central spot, with the cultural region of Punjab and the city of Lahore highlighted. The song “Par Desi” articulates this geographical formation:The song’s title explicitly figures the South Asian diaspora. Vocally and lyrically, the song evokes an ethnic and geographical quandary. The singer and bassist Basim’s voice shivers as he sings the chorus line, ‘In Lahore it’s raining water, in Boston it rains boots.’ The subject in the song defines his physical home in Boston, where he experienced an assault by skinhead punks. He sings, ‘They tried to stomp me out, but they only fueled the flame.’ The song narrates a history of migration and the emotions of displacement. It raises the questions, ‘Where do I point to blame, when men scatter like moths? /… how’d I get here, from a land with long monsoons?’The song’s references to traditional bhangra, a dance music genre that originated in Punjab, further complicates this geo-musical formation. In my analysis, I argue that the band projects a transnational bhangra-punk sound:An 8-second analog sample of live bhangra percussion comes into the musical present. Immediately, this sample transports me, the listener, away from the emotional space of the lament. Continuing the triplet pattern of the bhangra sample, the band transforms the bhangra rhythm into a collective punk-style chanting of ‘la-la-la’ in the final section of the song. This chant rejoices in the form of a Clash-like punk choir, roughly in unison with a distorted guitar.Audio clip: Adobe Flash Player (version 9 or above) is required to play this audio clip. Download the latest version here. You also need to have JavaScript enabled in your browser.This bhangra-punk aesthetic is projected from a South-Asian- or desi-identified ethnic space: imagined somewhere between Punjab, 1970s punk England, and present-day home in the northeastern United States. The Kominas, I contend, elides its physical home in Boston and the U.S.; at the same time, the band self-consciously embeds itself into historical punk England to reclaim a new musical home.I discovered a different but related diasporic configuration in “Tunnnnnn.” This song articulates a minoritarian, multi-diasporic space.The Kominas alludes to the original roots reggae version of the song (“Armagideon Time” Jamaican artist by Willi Williams). In doing so, the band resituates their version of the song into a Rastafari time-space. The Kominas locates its own battleground, while borrowing from the Rastafari visions of Armageddon.I hear The Kominas calling for its own ‘Armagideon,’ in the new lyrics written in Punjabi. According to Basim’s translation, the first verse states: ‘We will only drink that / That they are drinking in Iraq / We will only drink that / that they would drink in Karballah (sic).’ It is not a coincidence that both Iraq and Karbala are iconic battle sites both in the past and present. The War in Iraq after the events on September 11 has been a topical preoccupation by The Kominas since its first album (entitled Wild Nights in Guantanamo Bay). The band has made clear its stance of castigating the western world, in particular the United States, for waging a war motivated by Islamophobia, militarism, and imperialism. Following the Punjabi lyrics, Basim evokes the overthrow of 21st century Babylonians. In English, he sings the lines, ‘A lot of people won’t get justice tonight / A lot of people wont’ get no supper tonight / Just remember to / Kick it over / And praise Jehovah / And kick it out.’Audio clip: Adobe Flash Player (version 9 or above) is required to play this audio clip. Download the latest version here. You also need to have JavaScript enabled in your browser.The Kominas’ musical alliance with roots reggae, the music of those in Jamaica as well as the Jamaican immigrants, rewrites the history of the racial dynamics in 1960s and 1970s England. Challenging the history of “paki-bashing” in England, The Kominas’ music prominently figures the South Asian subjectivity. This musical geography has discursively reorganized the racial relations between blackness, whiteness, and Asianness. It also forges a musical alliance between a South Asian American band and the Afro-Caribbeans in Jamaica and the U.K..In its overlays, these maps bring into relief various sites of geopolitics related to postcolonial struggles. This spatial articulation, I contend, is a minority-centered project of reterritorialization. It points away from the band’s physical home in the United States to re-focus on geographical sites symbolic of resistance. Its identification with loci of anti-white-supremacy and anti-imperialism, I argue, is a response to the post-9/11 social alienation and melancholia. Through the creative adaptations of Punjabi musical roots and transnational routes via the U.K., Jamaica, and Lahore, the band has built a psycho-social home in its music.Coda: These two maps are extensions of my work at UVa’s Scholars’ Lab where I made a series of Myspace friendship distribution maps of a handful of bands (including The Kominas) featured in my dissertation. I’m happy that I’m in the position to use experimental and digital methods to further my explorations of the relationship between pop music and postcolonial geography. This cluster of ideas and modes of inquiry truly excites me. I organize my current performance projects by my over-arching curiosity in “noise.” Noise, the way I understand it, is defined in opposition to “music” as sounds legitimized by major social institutions; noise is that which challenges the existing soundscape as shaped by institutional forces, in my case, commercialism and Euro-ameri-centrism. View/read my video paper about how I conceptualize the relationship between my performance (with Dzian!), ethnographic research, and public scholarship.With my nakashi surf rock band Dzian!, we usher in the obscure sounds of 1960s-70s surf and garage rock records from Taiwan, Indonesia, Malaysia, Singapore, Japan, France, Finland, and other parts of the world less associated with rock music. In Pinko Communoids, I work to redefine the usage of conventional rock music instruments such as electric guitar, effects, and amps. My duo project Grapefruit Experiment invites collaborators across genres and extended the mode of experimental music performance into more conventional rock and popular music settings. Outside the context of these projects, I have collaborated with Tomie Hahn, Kenneth Yates (Caustic Castle), Bob Holub, and others.Dzian! :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::Dzian!/贊! (pronounced “ze-an”) revives the ecstasy of vintage rock sound from Asia. The band curates and masters the style of Taiwanese a-go-go, Japanese eleki, Indo-rock, Thai disco and shadow music, Malaysian pop yeh yeh, Khmer pop, Middle Eastern bellydance surf, and Ventures hits enjoyed by Taiwanese puppetry bands and audiences. No gimmicks, only geeks. The band embodies Taiwanese “super-cool” by recreating Nakasi (那卡西), down-home, “fashionable” party music in pre-WWII Taiwan. Committed to spectaculars, Dzian! flaunts feather boas, LEDs, authentic costumes, and choreographed belly and go-go dance, while unabashedly delivering tunes in ten languages and counting. From Chinese buffets and shopping malls in Virginia to Manhattan’s Union Square and Taiwan Center in Flushing, the band has left a trail of joyous red boa feathers. Put on your swell dancing shoes, your thumbs up, and say, “Dzian!”dzian.infotwitter.com/dzianband myspace.com/dzianbandPinko Communoids ::::::::::::::::::::::::::::::::::::::::::::::::::Pinko Communoids is an improvisational trio based in Charlottesville, Virginia. I started the project with two other UVa graduate students: Carey Sargent (PhD candidate, Sociology), Kevin Parks (PhD candidate, Computer Composition and Technologies program). We create both free and structured improvisations using conventional instruments including guitars, accordions and percussion, found objects, circuits, microphones, and other electronics. We enjoy the quiet interplay of small sounds and often employ a restrained sonic palette of diverse timbres. Since forming in 2006, Pinko Communoids have given over 40 performances both locally and abroad, including a tour of Taiwan’s major cities in Summer 2007. We have been featured at Feminist Theory and Music (FT&M10) conference, 804noise showcases, Red Room, COMA @ ABC No Rio, Sonic Circuits, Noise in the System, Technosonics, and other concert series. We helped curate a series entitled Audio January and Audio February at The Bridge PAI in Charlottesville VA.Pinkos.infomyspace.com/pinkocommunoidsGrapefruit Experiment ::::::::::::::::::::::::::::::::::::::::::::Grapefruit Experimentis a semi-open music project with Carey Sargent and Wendy Hsu as core members. The concept behind Grapefruit Experiment is inspired by Yoko Ono’s design of the Plastic Ono Band – a nomenclature designated for Carey, Wendy, and whomever they are making music with at the moment. Aesthetically they lean toward something like: part-no-wave, part-noise, part-improvised, part-avant, part-sound-fetish, part-song-like, part-fun, part-geeky + whatever parts contributed by collaborator.grapefruitexperiment.wordpress.comFor the Society of Ethnomusicology meeting in 2011, I created a video that addresses the role of my role as a perform-scholar, instead of a traditional academic paper presentation. This is a 3-part video. In the first part of the video, I discuss my intention to raise the visibility of Asian and Asian American music. I also introduce the medium of the piece – a YouTube video – to enable my messages to spread virally and to give a shout out to the Asian and Taiwanese American stars on YouTube. [Note on the soundtrack: I played the sound of an academic talk and experimental noise to express the disruptive aim of this piece with the respect to the social and cultural invisibility of Asian and Taiwanese Americans]…Part two of the video opens with a story about the Taiwanese American support for the Typhoon Morakot relief efforts for Taiwan. It highlights TaiwaneseAmerican.org for organizing the relief efforts of Taiwanese American musicians, artists, and writers. Inspired by these relief efforts, particularly those of Susan and Emily Hsu of Exit Clov and the Hsu-nami, I organized a benefit concert in my town, Charlottesville, a small college town in central Virginia, bringing together the University of Virginia and the local Taiwanese community. My band Dzian! came together for the purpose of delivering the uber-fun, spectacular performance highlighting the Taiwanese style of Nakashi. The rest of this portion follows the story of how Dzian! spreads the love for the sound of rock and pop music from 1960s and 1970s Taiwan and its neighboring countries such as Japan, Indonesia, Singapore, Malaysia, Thailand, and Cambodia.…In the last part of the video, I bring the “cultural work” of Dzian! to bear on its social mission of boosting the awareness of Taiwanese and Asian music in North America. I close the video by telling the story of an exchange I had with an older audience member at our performance at the first annual Hello! Taiwan Rocks concert at the Taiwan Center in Flushing. This conversation reminds me of the power of music in creating communities, spaces of comfort to which we, as Taiwanese/Asian Americans could feel like we belong to.…I hope that this video will continue to inspire others — musicians, artists, writers, journalists, academics, and other cultural workers, as well as the working and non-working professionals — to get behind the mission of creating a space of comfort and strength for Taiwanese and Asian Americans.[The content on this page was originally posted as a feature story on TaiwaneseAmerican.org.]Wendy Hsu is Mellon Digital Scholarship Postdoctoral Fellow in the Center of Digital Learning & Research at Occidental College. She received her PhD in the Critical and Comparative Studies in Music program in the McIntire Department of Music at the University of Virginia. Her research interests lie at the intersection of popular music performance and the transnational contacts between Asia and America, focusing on issues related to race/ethnicity, gender/sexuality, and migration. She has published on Yoko Ono, Hedwig and the Angry Inch, and Bollywood film music. Her teaching areas include music in Asian America, transnational popular musics, and race & gender in popular music. Since 2007, she has designed and taught four undergraduate courses in the Music Department and the Studies of Women and Gender Department at UVa. [More: on teaching]Hsu’s ethnographic dissertation details the identity articulation and community formation of second and 1.5-generation Asian American musicians actively engaged in an indie rock music scene. Situating the study in the post-Civil-Rights, Obama-aged United States, this study interrogates the category of “Asian American” by examining the performances, ideology, and social networks of such musicians. This multidisciplinary study builds on the ethnomusicological studies of grassroots/popular music and adds nuance to the cultural analyses of current transnational musical production and social connections mediated by digital technologies. Hsu blogs about her 24-month-long field research at YellowBuzz. [More: on research]An active performer, Hsu is a founding member of vintage Asian rock band Dzian!, improvised music trio Pinko Communoids, and Yoko-Ono-inspired noise duo Grapefruit Experiment. She co-founded HzCollective and organizes music and arts events at The Bridge PAI [More: on performance]Wend Hsu’s_CV[pdf]Press:Areas of Focus Popular Music, Asia-Pacific-America, Transnationalism, media & Technology, digital humanitiesMy research explores transpacific musical practices in popular music genres. I study the complex hybridization of musical genres and experiences emerged between Asia and America. My research examines: How musicians of Asian affiliations (descent, residence, nationality, diaspora, etc) reinvent musical materials from existing ethnically tied musical genres; how they form social networks; and how they transform their on-stage and off-stage identities in the increasingly globalized ecology.With the financial and intellectual support from the NEH-funded Graduate Fellowship in the Digital Humanities provided by the UVa Scholars’ Lab, I have added an exciting dimension of digital ethnography to my dissertation project – applying web-scraping techniques to map out the digital social networks of the musicians featured in my dissertation project. Read about my digital project.Dissertation ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::Redefining Asian America: Cultural Politics, Aesthetics, and Social Networks of Independent Rock MusiciansAmerican rock musicians of Asian descent are finding powerful ways to express their cultural identities, despite their practical invisibility in mainstream media. This ethnographic dissertation explores the social and musical life of second and 1.5-generation Asian American musicians actively engaged in independent (indie) rock music performances. By investigating the performance, ideology, and social networks of such musicians, this study interrogates the boundaries of the “Asian American” ethnic identity and the socio-cultural imagination of “Asian America” in the post-Civil-Rights and post-9/11 United States. This dissertation argues that the musicians perform with a dynamic of ambivalence, covering, and uncovering ethnic and racial traits associated with Asian identities. This dynamic in effect contests racializations. Turning outward and eastward, the musicians stretch the confining borders of the U.S., reaching fans and fellow musicians in various sub-regions of Asia.In these chapters, I document how the musicians leverage the Do-It-Yourself ideology, central in indie rock music scenes, to deploy self-invented ethnic notions, circumvent norms practiced by their white middle-class peers, and transform marginalizing race-related articulations. Reclaiming their outsider status, they challenge the contradictions within the discourses of liberal multiculturalism and forge bonds with others including non-Asian minority groups. This dissertation also examines various transnational musical projects connecting Asian America to a geographical and symbolic “Asia.” Through touring and media exchange via the Internet, the musicians build a set of social networks comprising a unique translocal indie rock music scene of their own. I explore this translocality by adapting web-scraping and mapping technologies from the digital humanities. Finally, I discuss the formation of my band Dzian! as a reflexive, performative response to the issues related to race, ethnicity, and melancholia raised during my fieldwork.Dissertation Full Text Download [PDF]Publications :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::+ Hsu, Wendy and Sargent, Carey. “Rocking Out Between the Local and the Global: Transnational Independent Music Industry in Taiwan”, Amalgam, September 2008 [download PDF].+ Hsu, Wendy. “Review: Queering the Popular Pitch edited by Sheila Whiteley and Jennifer Rycenga,” Journal of Popular Music Studies, (September 2008) [download PDF].+ Hsu, Wendy. “Review: Sensational Knowledge: Embodying Culture through Japanese Dance by Tomie Hahn”, Women and Music, (September 2008) [download PDF].+ Hsu, Wendy. “Queering and Reading Plato in Hedwig and the Angry Inch“, Queer Pop Culture, ed. Peele, Thomas. Palgrave-Macmillan, May 2007 [download PDF].+ Hsu, Wendy. “Between Narrative and Expressive, Fantasy and Melodrama in Bombay Film,” Virginia Review of Asian Studies, vol. 5 (2003) [download PDF].I teach to raise multicultural and global awareness. My pedagogy emphasizes multiplicity of perspectives, reflexivity, and practice. Read my Teaching Statement.Employing multi-disciplinary methods and theories from scholarship on music, media, culture, and history, I have designed and taught four undergraduate courses:SWAG1440 Gender and Race in Popular Music, an entry-level survey course offered twice in the Study of Women and Gender Department at the University of Virginia, provides an environment for students to explore issues intersecting race, gender, and popular music via a multi-disciplinary and multimedia engagement with musical sound and culture. Through scholarly and popular readings, and reflective assignments such as the “mixtape” project and the concert report class blog, students engage with their own, immediate popular music worlds in critical and reflexive ways. [SWAG1440 sample syllabus]As an upper-level undergraduate seminar, MUSI4510 Music in Asian Americaasks: What is the relationship between music and “Asian American” identity? More broadly, what is “Asian America” in the musical life and cultural imagination of the United States? Engaging with intensive reading, writing, and final student projects, this seminar investigates the musical sounds, icons, lives, and practices in Asian American communities in the United States in 20th and 21st century. [MUSI4510 sample syllabus]Other courses:MUSI207/307 World Music: Popular Music and Transnationalism [MUSI207/307 sample syllabus]MUSI207 Race and Ethnicity in Popular Music[MUSI207 sample syllabus]MUSI/SOC255 Digital Vernacular Music-CultureComments OffEditors’ Note: “Graphs, Maps, and Trees: Imagining the Future of Public Interfaces to Cultural Heritage Collections” is a working group meeting at the National Council on Public History this weekend. Leading up to the event, participants have contributed a variety of proposals, discussions, reports,and analysis to the blog Visualizing the Past. Three of the most recent contributions are listed below.I love Joan’s post, because it gets at many issues I have been working through both professionally with the Omeka project and also through my own research that relies on access to cultural heritage collections for analysis….As Joan mentioned, I have proposed that making more material culture collections available in online formats may help to better integrate those types of sources and approaches to interpreting the past into history and humanities scholarship. And that the longer museum collections are missing from the digital world, their collections and these approaches could be marginalized again.Read Full Post HereLast fall I suggested that I would investigate how well digital cultural heritage collections were being utilized by researchers. Turns out this was harder than I expected. But from my very initial research it seems that scholarly writing does not cite a lot of cultural material available online. This has led me to some questions for the group – some you’ve probably already considered, but maybe a few new ones worth thinking through together.Read Full Post HereThe oxymoron embedded in the title reveals the contradiction behind any attempt to “visualize” oral history for historical curation. One could argue that oral history, and sound, more broadly, are such fundamentally aural experiences that they can’t be visualized at all. Even so, for historians, the meaning and magic of oral history has been long hidden behind the veil of the visual. Typically this has happened through representing oral history as text transcription or encased in long-form video interviews. Sadly, such presentation prioritizes one sense–sight–above the others. This point was clearly made in the emergence of sensory history and has more recently been emphasized by oral historians seeking to recover the meanings embedded in the aural experience and expression.Read Full Post HereCommentsWhat follows is a draft of a paper written in conjunction with Robert Blades concerning the Looted Heritage project.IntroductionIn his overview of what ‘open access’ might mean in the academy, Peter Suber draws attention to the salient features of what it means to call something ‘open’ – that it is digital, the cost (to the reader) is free, and most copyright or similar legal restrictions are relaxed (Suber 2012). In this paper, we describe ‘Looted Heritage’, a developing digital archaeology project and its early results that explore ways of leveraging open content, of dealing with the firehose of data that comes when materials can be freely collected and examined. We focus not on the academic open access movement, but rather on the torrent of archaeological materials shared through social media streams such as Twitter and blogs. We focus on user-generated content surrounding the trade in illicit antiquities, reports of looting, and explore the patterns in this data, of not just what is shared, but why.In a way, our approach is the inverse of ‘crowdsourcing’. To crowdsource something, whether a problem of software development, or the need to transcribe historical documents, is generally to fracture a problem into its component pieces, allowing an interested public to solve them. In archaeology, such approaches are starting to find currency in everything from funding fieldwork (Morgan 2011) to the entire excavation and its subsequent interpretation (Wilkins et al. 2012; Wilkins, B 2012). In 2011 I and my students embarked on a project to crowdsource the idea of ‘sense of place’, using an open-source software platform to solicit and collect community memories about cultural heritage resources in Central Canada (Graham, Massie and Feuerherm 2011). One of our findings in that project concerned the order of operations that should be followed, that perhaps it is better to collect what is freely available first, before asking the crowd to fill in the gaps (Graham, Massie, and Feuerherm 2011).Accordingly, we set up a data-trap, to collect the tiny pieces out on the open access web. We then study these pieces using data mining and text analysis to develop a picture of what is happening right now. It is a kind of digital excavation, and what we are excavating is the world of social media. We then put all of our data, and our analysis, online to allow others to fill in the gaps. When we mine the open web for information about looted cultural heritage, what are the discourses? What are people saying, does what they say change over time, and do these trends and this excercise hold any lessons for us as archaeologists?Social MediaThe business model for many popular social media websites/services is built on allowing users to connect with other users, selling this data onwards to marketers. The microblogging website Twitter caches every ‘tweet’ (short messages of 140 characters) after approximately two weeks and sells them. The marketers then mine this data looking to predict the next big thing, or to understand the public perception of their product (Barnett 2012). This material can be considered ‘open’ as long as one is looking for it during that period before it is cached. Williams, Terras, and Warwick (in press) recently completed a meta-study of over 550 academic articles that focused on Twitter. Of these, the researchers identified 53 studies that relate to mining Twitter content. These ranged from using tweets to offer better personalized news recommendations (Abel, Gao, Houben, and Tao 2011) to predicting flu trends (Achrekar, Gandhe, Lazarus, Yu, and Liu 2011; Chew, Eysenbach 2010) to attempts to predict the future (Asur, Huberman 2010) and the stock market (Bollen and Mao 2011). (The full database of Twitter-related research developed by Williams et al. will be appearing online, Terras pers. comm.).The other facet of user-generated content that we wish to mine is the world of archaeological blogging. Blogging, it should be noted, is not a genre of writing, but rather a platform for writing and for the rapid dissemination of material onto the web. Nevertheless, the caricature of blogging is that it is the narcissistic shouting into the void about narrow, meaningless, ephemera; that it is ‘noise’ in contrast to the strong ‘signal’ that a peer-reviewed journal might provide. How then can anything useful be found in this open environment? Until the advent of Google, there was no good answer to this question. Google is not a search engine, nor a catalog, nor an index: it is a massive experiment in prediction. Google benefits from the billions of searches that we the users perform every week. In essence, we are teaching the machine what is useful when we select one result out of the millions provided. Google observes this. Google uses over 200 such signals to match useful information to each individual user, who each have their own idea of what constitutes ‘useful’ (Levy 2010).Blogging as a medium creates strong signals. Academic blogs tend to have a very tight focus (notable examples are Bill Caraher’s New Archaeology of the Mediterranean World and Colleen Morgan’s Middle Savagery). They are updated fairly regularly, as the academic incorporates them into his or her work cycles. They focus on a comparative narrow range of topics, and are thus semantically tight. The anchor text for links tends to be rather unique combinations of words, and thus provide more signal to Google’s algorithm. A static, rarely-updated website (like many academic department websites) does not provide strong signals, and thus is not often returned in search results. Blogs and other high-signal sites like Wikipedia are displayed first. Research shows that most users never look further than the first few results provided by any search engine (Jansen and Spink 2006: 260). To the wider world, only that which is blogged, tweeted, or written about on Wikipedia, exists; that which is hidden behind a paywall, does not.Data Collection MethodsIn practice the web is infinite. Our project attempts to monitor that slice of it which is open, accessible, and taking place on Twitter, on blogs, using RSS feeds, automatic news aggregators, and other web 2.0 tools (cf. Kansa and Kansa 2011). We use an integrated environment for marshalling this data called ‘Ushahidi’. The word ‘ushahidi’ is a Swahili word meaning ‘testimony’. Ushahidi was developed in Kenya to map reports of violence after the bitterly contested elections of 2008 (Ushahidi 2012). It accepts information submitted via web form, email, and cell-phone short-message-service. It can also be used to collect RSS feeds and to trawl Twitter, copying tweets that contain particular keywords.We set Ushahidi to search Twitter for #looting, #antiquities, #looted, #illicit. The search will also turn up results without the # symbol; the convention on Twitter however is to indicate descriptive keywords for one’s ‘tweet’ by using the # symbol in conjunction with the keyword. This allows for more effective searching, and for users to follow developing conversations even if they themselves do not follow all the participants in the conversation. We are subscribed to feeds from Art Theft Central; Conflict Antiquities; Illicit Cultural Property; Looting Matters; and Saving Antiquities for Everyone. We also have a saved search at Google News that returns items based on the keywords ‘looted antiquities’.As of April 12, 2012, we have over 1300 items in the queue from these feeds with approximately 50 to 100 new items appearing each day – firehose indeed! In the first quarter of 2012 we have culled 207 reports from this stream. Ushahidi is also a form of simple GIS, wherein each report is also categorized and tied to a geographical location.Analytical MethodsWe use the techniques of text-analysis and topic modeling. Digital text analysis has a long tradition in what is now called ‘digital humanities’, emerging out of efforts to systematize the generation of concordances and vocabulary counts (see Hockey, 2004 for an overview). We use the ‘Voyant’ online tool (formerly ‘Voyeur Tools’, Rockwell and Sinclair, 2012; Sinclair and Rockwell 2009) to explore word use in our texts. Because this tool is online, we can share this step in our analysis with others by providing a unique URL to our corpus (see table 1). We loaded our reports in chronological order into Voyant so that we could examine word use over time through simple frequencies (see for instance Burrows, 2004 for the wide variety of approaches to which text analysis may be put).Table 1. Internet Location for tools and datasets referenced in the text.Voyant also has a tool called RezoViz which extracts named persons from documents, and links them together on the basis of occurrence in the same document. With this tool, Voyant becomes a tool for data discovery of social networks. However, it is still in ‘alpha’ meaning that not all of the idiosyncrasies of the code have been completely solved. Nevertheless, given the nature of our data, it is a useful tool to begin to understand who the key players might be, tracking them over time and space. One might then use this data to refine the social media searches, for instance.We then explore the texts for deeper structure, using ‘topic modeling’ (a Bayesian statistical approach formally called ‘Latent dirichlet allocation’, Blei, Ng, and Jordan 2003; Underwood 2012a; Weingart 2011). Topic modeling determines collections of words that occur in semantically meaningful ways in different proportions within a text. As Ted Underwood puts it, ‘Topic modeling is a way of extrapolating backward from a collection of documents to infer the discourses (“topics”) that could have generated them’ (Underwood 2012b). It begins with simple frequencies of words, but also considers the way a particular word is used in some documents, but not others. The algorithm introduced by Blei et al. (2003) assumes that for any possible topic, a word has a possibility of being part of that topic: it multiplies the frequency of this word in this topic by the number of words in the document that already belong to the topic. The result is a probability that the word actually belongs to that topic (Underwood 2012b). This is an iterative process that begins initially from a random position. As the algorithm cycles to produce a best fit, words are gradually sorted into ‘topics’, and ‘topics’ into documents. As Underwood emphasizes however, these are not ‘topics’ as one might understand from a book index. Rather, they might be better thought of as discourses (2012a, 2012b).We use the ‘Machine Learning for LanguagE Toolkit (MALLET, McCallum 2002) and its implementation of the algorithm. MALLET is open source software that runs from the computer’s command line. A Java based graphical user interface (GUI; Newman and Balagopalan 2011) used in tandem with MALLET makes it easier to run the algorithms and to select and manage one’s data. MALLET outpus a series of comma separated files that give a breakdown of ‘topics in documents’ and ‘documents in topics’ and ‘key words in topics’. The GUI produces a series of webpages that allow one to explore the results by clicking through documents, topics, and the linkages between them. This output has been deposited with the data repository Figshare and may be downloaded and explored; see table 1).We set the algorithm to iterate 400 times as it converged on the best solution, assuming 15 topics. It ignored a preset list of ‘stopwords’ (‘the’, ‘and’, ‘of’ etc) that tend to obscure the patterns we wish to find. There is no way of predetermining the ‘best’ number of topics. Instead, one runs the analysis again and again, looking at the resulting composition of documents, looking for a distribution of topics to documents that does not clump too heavily. In general, we began by searching for five topics and increased until we hit 15 topics, which seemed to capture the variety well (one or two topics did not account for the majority of documents, for instance).To get a sense of what the ‘topics’ found by the algorithm might mean, one examines the document composition output, looking for those documents where the topic occurs with the highest probability. We can then suggest a descriptive label, a meaningful ‘topic’ in human terms (cf Nelson 2011) or try to understand the list of words as a kind of discourse (Underwood 2012a,b). The composition output can also be visualized as a kind of network, where there are two kinds of nodes, documents and topics. The composition probability gives a weight to the strength of the connection (Meeks 2011). We use the Gephi open source network visualization software package to visualize these patterns (Bastian, Heymann, and Jacomy 2009). We can identify, in network terms, reports or topics that seem to be most crucial for keeping the network together. We can also calculate statistics on this network which give us an indication of the ‘communities’ of reports (ie, subnetworks) that seem to have similar patterns of composition. Topic modeling the reports for the underlying structural patterns of word use gives us insight into the ideological realm; visualizing this output as a network makes those patterns clearer.Limitations of the DataObviously, what we search for dictates what we will find. These reports represent the concerns of the anglophone, Western world. To what degree is Twitter representative of other places and cultures? In China, the important social media player is Weibo. There are 500 million Chinese online; Weibo grows at the rate of approximately 10 million a month (DeWoskin 2012; Chen 2012). Currently on ‘Looted Heritage’, China is a comparative blank spot. Similarly the Spanish speaking nations are not well represented, nor is sub-Saharan Africa.The other major blindspot concerns the traditional auction houses and newer entrants such as eBay. Online anonymous auctions make it easy for low-level, low-value antiquities to be bought and sold. On March 1st 2012 we scraped the RSS feed from eBay.ca for items tagged as Roman antiquities. We found over 400 items being sold, with a combined listed value of over $C 48,000. The median value of these items was approximately $C 20. We elected not to create reports of items listed for sale on eBay, from a desire to not encourage their sale.Stanish points out that a great deal of what passes for an antiquity on eBay might well be a fake and so eBay, by flooding the market, is in fact preserving antiquities (Stanish 2009). Nevertheless the technology of eBay facilitates a trade in extremely portable antiquities like coins and brooches, a search that can be just as destructive as that for ‘high-end’ antiquities. The British Museum and the Portable Antiquities Scheme reached an understanding in 2006 to monitor eBay.co.uk for antiquities which might fall under the rubric of the Treasure Act (British Museum, 2006). In France, a group of archaeologists attempts to monitor eBay.fr to persuade it to remove antiquities from the lists (Champault, pers. comm.) By Champault’s reckoning, some 75,000 euros’ worth of sales have been cancelled due to their efforts.In future iterations of ‘Looted Heritage’, we will be working to incorporate data from eBay as we suspect that monitoring and mapping the locations of sellers of sudden assemblages of small finds could point the way to tracking the field of operations of pot hunters and subsistence looting.Findings – Text analysisFrequencies are normalized in terms of keyword per 10 000 words over the corpus. ‘Museum’ is an obvious word with which to begin. The major spikes in its frequency correspond with thefts-to-order from the Museum at Olympia in Greece. ‘Looting’ occurs with high frequency in December and January, is virtually silent until the end of February, spikes enormously in early March, and thereafter percolates nicely until the end of the data (Figure 1). That there should be seasonality to looting is not surprising (presumably looters are not impervious to the weather) but it is gratifying to see this reflected in the materials trawled from the web. The spike in the frequency of ‘looting’ in early March seems extreme though, compared to the rest of the trend. It corresponds in our data with the release of four videos. The spike in word frequency perhaps can be explained by the relative shortness of the text that accompanies these videos, skewing the proportions.[insert figure 1 about here]One can compare multiple words at the same time. For instance, ‘art’ and ‘objects’ on the face of it should often go together. Figure 2 graphs the relative frequency of both words. At many points in December and January, this indeed seems to be the case. Where they differ often seems to be in cases of repatriation (the report speaks of ‘art’) whereas in cases of theft, the language tends to use ‘objects’. Do antiquities only become ‘art’ once they’ve been stolen, or displayed in a museum? There is perhaps an interesting discourse hinted at here to be explored concerning the semantics of looting and cultural heritage crime.[insert figure 2 about here]Turning to the RezoViz module (see table 1), figure 3 displays 25 individuals and their connections, as extracted from the reports. It appears to display names according to their frequency in the underlying documents. ‘Simon MacKenzie’ and ‘Neil Brodie’, scholars from the University of Glasgow who investigate the illicit antiquity trade are represented and tied together, by virtue of newspaper and blog articles reporting on a grant the two scholars received. Similarly, a cluster of individuals is focused on the nexus of the ‘Getty Museum’, ‘Giacomo Medici’, and ‘Marion True’. The investigative reporters Jason Felch and Ralph Frammolino, who wrote about the Getty’s connection to Medici and other aspects of its collecting history (2011), are also tied to that cluster, as is the scholar David Gill, who has written on his blog about the Getty. If one slides the ‘items’ slider to the right, more individuals are added to the display. Figure 4 displays 125 individuals and their connections; individuals named in the same reports as Zahi Hawass (former Minister of State for Antiquities, Egypt) are highlighted, for the sake of illustration (interestingly, Napoleon Bonaparte is also part of this subnetwork!). When this tool is formally incorporated into Voyant perhaps it will allow these networks to be exported to formal social network analysis tools so that the data may be cleaned and explored more rigorously. Who for instance is most central? Who is in a position of power, in terms of their ability to influence opinion or control information? Is it a complete network in the sense that one can chart a path between any pair of individuals, or are there isolated areas? Are there sub-communities within the graph?[insert figures 3 & 4 about here]Findings – Topic ModelingThe topics extracted by MALLET from the text of the reports, indicating the associated key words and a descriptive label are listed in table 2.Topic #Key wordsPossible descriptive label1museum objects getty art antiquities museums italy returned italian princeton potts collection university acquired collections director true investigation roman works curator aphrodite exhibition greek almagiMuseum ethics2 project archaeologists research university community national heritage german dr state archaeology nigeria association development museums looting nok gundu nigerian work evidence news claims local communitiesArchaeological ethics3 turkey mosaics university art turkish made dealer information london find late dealers bgsu roman purchase received history collection center owner pa recently december expectRepatriation & university museums: Old world4 antiquities years world trade international countries past illicit looted director make future material year provide long information report policy local questions including part provenance studyProvenance5 ancient artifacts city people back place return time large left iraq ruins discovered don experts set country great peru added east officials built excavated bankRepatriation & university museums: New world6 national history show archaeologist sites american artifacts treasure archaeology archaeological state historical based law wrote tv trafficking lost historic illegally malter park shows property yearsTelevision7 museum stolen culture ministry museums ancient security government minister pieces art country items head gallery olympia told guards thieves reported guard artefacts made building backMuseum thefts8 hawass british syria war loot syrian gold treasure found foreign part zahi augustine french display funds army mubarak brought men japan return caughtWar & antiquities9 statue looted auction government sotheby cambodia statement states private york sale united case cambodian law legal officials million stolen sell house piece property sold maskAuction houses10 public committee aaa president letter funding learn coin day policy images arts research image human half recently item montreal king affairs read request report statementIdentifying looted antiquities11 archaeological cultural site sites looting heritage antiquities country department authorities excavation important objects state international illicit protection red list area destruction property dealers general maliFighting the illicit antiquities trade12 antiquities items stolen documents artifacts charges rare include church artefacts case golan jerusalem jewish court israel allegedly heart gang age police st accused trial believedTheft from historic sites13 egypt egyptian el antiquities council sites people hibeh cairo al foreign revolution archaeologists including digging work afghanistan police dr stop period article group rich supremeEgypt14 police greece theft year greek coins antiquities crime national metal athens crisis small years smuggling found arrested including century thieves work months month bronze membersGreece15 art market high china chinese goods relics number stone paintings works bronze million palace dynasty jade fake thousands global collectors village fine summer antiques antiqueChinaTable 2. Keywords in topics.While we can give descriptive labels as a short hand for the ‘topic’, we can also imagine these as discourses. We can imagine that Topic 1 concerns discourse surrounding the Getty Museum. Similarly, Topic 7 should concern the Olympia Museum, while Topic 10 with the words ‘Montreal’ and ‘AAA’ (American Anthropology Association) concerns a North America discourse around museums and the identification of artefacts. Topic 6 is clearly connected with the controversy generated by Spike TV and the National Geographic Channel’s recent television programs connected with metal detectoring. It is interesting that the topic modelling routine seems to tease out two distinct strands surrounding discourses of ‘repatriation’, split between old and new world, as well as discourses surrounding particular ‘source’ nations.MALLET outputs a table indicating the relative percentage that each topic contributes towards the composition of each report. We can consider these percentages as a weighting of a link between reports and topics, and thus we can represent the ‘topic-space’ as a kind of network map. We extracted a list of reports to constituent topics, capturing at least 50% of each report’s composition (including every report’s complete breakdown would render the resulting map unintelligible).The resulting network visualization has 222 nodes (representing reports and topics) and 887 links between them. Gephi uses an algorithm called ‘modularity’ developed by Blondel et al. (2008), to identify subgroups (‘module’ being a synonym for ‘community’). Modules are based on the similarity of linkages. In this routine, a result closer to 1 indicates the strength of the result. We ran modularity 200 times on this data; the best results clustered around 0.216 and tended to produce 6 communities, table 3:Group 1Identifying Looted AntiquitiesChinaGroup 2Repatriation & university museums: old world materialsMuseum EthicsAuction HousesGroup 3TelevisionArchaeological EthicsGroup 4War & AntiquitiesFighting the Illicit Antiquities TradeEgyptGroup 5ProvenanceRepatriation & University Museums: new world materialsGroup 6GreeceMuseum TheftsTheft from historic sitesTable 3. Grouping topics into ‘communities’.To make the resulting visualization more intelligible, we filtered out individual linkages weighing less than 20%, Figure 5. The colours represent the partition of the network into the communities detected through the modularity routine. We can then measure the network to identify those reports or topics that are positioned on the most paths between any two other items, a measurement called ‘betweeness centrality’. In a sense, what we have produced here is a map of the idea-space surrounding the illicit antiquities trade. The links between nodes are thicker the stronger the tie between the topic and the report. There appears to be a current of thought which runs from ‘museum thefts’ to ‘Greece’, to ‘theft from historic sites’ more generally. Another runs from ‘Egypt’ to ‘fighting the illicit antiquities trade’ to ‘provenance’. The one isolated topic that never ties in (except in the most tenuous of ways) is ‘television’, a body of reports connected with the outrage over shows that are seen in the professional community to be promoting pot hunting as a glamorous recreational pursuit.The ‘betweeness centrality’ metric also directs our attention to particular reports. One such is Report 161 from January 26th, a report about the looting of a church. In fact, a spate of thefts from churches and other historic sites occurred in late February and early March. Is this a new trend?[insert figure 4 about here]ConclusionIn this paper, we demonstrate a workflow for collecting open-access materials created by individuals, academics, and the press who use social media to communicate information about the trade in illicit antiquities. We then analyze the text of these reports for both superficial and deep patterns. Ideally, as more and more information gets collected, we will be able to spot and understand underlying trends in the world antiquities market. We would also like to include the work of archaeologists and scholars published in journals, to provide the deeper insight and reflection that this just-in-time approach currently lacks. Alas, the majority of these works are not available to be analyzed and mined this way. We also understand that ‘open access’ could mean that our underlying data should be available for others to study or repurpose for themselves, whether to extend on our study or to contest it. We have deposited all of our materials in online repositories to encourage just that. By providing this material digitally, we accelerate the tedious process of corpus building. In this way, open access becomes about generating a discussion, and building upon each other’s work.BibliographyAbel, F, Gao, Q, Houben, G-J, Tao, K. 2011. Analyzing user modeling on Twitter for personalized news recommendations. Lecture Notes in Computer Science, 6787: 1-12.Abel, F, Gao, Q, Houben, G-J, Tao, K. 2011. Analyzing user modeling on Twitter for personalized news recommendations. Lecture Notes in Computer Science, 6787: 1-12.Achrekar, H, Gandhe, A, Lazarus, R, Yu, S-H, Liu, B. 2011. Predicting flu trends using twitter data. In IEEE Conference on Computer Communications Workshops. Shanghai: INFOCOM WKSHPS, pp. 702-707.Achrekar, H, Gandhe, A, Lazarus, R, Yu, S-H, Liu, B. 2011. Predicting flu trends using twitter data. In IEEE Conference on Computer Communications Workshops. Shanghai: INFOCOM WKSHPS, pp. 702-707.Asur, S, Huberman, B.A. 2010. Predicting the future with social media. HP Laboratories Technical Report, no. 53.Barnett, E. 2012. Twitter sells tweet archive to marketers. The Telegraph [online] February 28th. Available at: <http://www.telegraph.co.uk/technology/twitter/9110943/Twitter-sells-tweet-archive-to-marketers.html> [Accessed April 18, 2012].Bastian M., Heymann S., Jacomy M. (2009). Gephi: an open source software for exploring and manipulating networks. International AAAI Conference on Weblogs and Social Media. [online] Available at: <https://gephi.org/> [Accessed April 23 2012].Blei, D., Ng, A., & Jordan, M. 2003. Latent dirichlet allocation. The Journal of Machine Learning Research 3:993-1022.Bollen, J., Mao, H. 2011. Twitter mood as a stock market predictor. Computer, 44.10: 91-94.British Museum. 2006. eBay partners with British Museum and Museums, Libraries and Archives Council to protect British treasures. [press release] Available at: <http://www.britishmuseum.org/the_museum/news_and_debate/press_releases/2006/ebay_partnership.aspx> [Accessed April 15, 2012]Burrows, J. 2004. Textual Analysis. In A Companion to Digital Humanities. (ed. S. Schreibman, R. Siemens, and J. Unsworth). Oxford: Blackwell. Available at: <http://www.digitalhumanities.org/companion/index.html> [Accessed April 18, 2012].Caraher, B. 2010-2012 The New Archaeology of the Mediterranean World [blog]. Available at:<http://mediterraneanworld.wordpress.com/>[Accessed April 23 2012].Chen, J. 2012. China’s Weibo Guru, Kai-fu Lee. Forbes [online] January 20th. Available at: <http://www.forbes.com/sites/china/2012/01/20/chinas-weibo-guru-kai-fu-lee/>[Accessed April 18, 2012].Chew, C., and Eysenbach, G. 2010. Pandemics in the age of Twitter: Content analysis of tweets during the 2009 H1N1 outbreak. PLoS ONE, 5:11. [online] Available at: <doi:10.1371/journal.pone.0014118> [Accessed April 18, 2012].Cohen, D., and J. Fragaszy Toryano (eds.) Journal of Digital Humanities Roy Rosenzweig Center for History and New Media [online]. Available at: <http://journalofdigitalhumanities.org/> [Accessed April 23 2012].DeWoskin, R., 2012. East Meets Tweet. Vanity Fair [online] February 17 <http://www.vanityfair.com/culture/2012/02/weibo-china-twitter-chinese-microblogging-tom-cruise-201202> [Accessed April 18, 2012].Durney, M. 2012. Art Theft Central. [blog] Available at: < http://arttheftcentral.blogspot.ca/> [Accessed April 23 2012].Fincham, D. 2012. Illicit Cultural Property: A weblog about art, antiquities and the law [blog] Available at: < http://illicit-cultural-property.blogspot.ca/> [Accessed April 23 2012].Gill, D. 2012. Looting Matters: Discussion of the archaeological ethics surrounding the collecting of antiquities [blog] Available at:< http://lootingmatters.blogspot.ca/> [Accessed April 23 2012].Graham, S, Massie, G., and Feuerherm, N. 2012. The HeritageCrowd Project: A Case Study in Crowdsourcing Public History. In Writing History in the Digital Age (eds. J. Dougherty and K. Nawrotzki) Under contract with the University of Michigan Press. Trinity College (CT) web-book edition, Spring 2012, [online] Available at:<http://WritingHistory.trincoll.edu.> [Accessed April 23 2012].Hardy, S. 2012. Conflict Antiquities: Illicit Antiquities Trading in Economic Crisis, Organised Crime, and Political Violence. [blog] Available at: <http://conflictantiquities.wordpress.com/> [Accessed April 23 2012].Hockey, S. 2004. The History of Humanities Computing. In A Companion to Digital Humanities. (ed. S. Schreibman, R. Siemens, and J. Unsworth). Oxford: Blackwell. Available at: <http://www.digitalhumanities.org/companion/index.html> [Accessed April 18, 2012].Jansen, B., and Spink, A. 2006. How are we searching the World Wide Web? A comparison of nine search engine transaction logs. Information Processing and Management 42.1: 248-263.Kansa, Eric and Sara Kansa. 2011. Toward a Do-It-Yourself Cyberinfrastructure: Open Data, Incentives, and Reducing Costs and Complexities of Data Sharing. In Archaeology 2.0: New Approaches to Communication and Collaboration (eds. E. Kansa, S. Whitcher Kansa, and E. Watrall). Berkely: Cotsen Institute of Archaeology. [online] Available at: <http://escholarship.org/uc/item/1r6137tb> [Accessed April 20 2012].Levy, S. 2010. How Google’s Algorithm Rules the Web Wired [online] February 22. Available at:< http://www.wired.com/magazine/2010/02/ff_google_algorithm/all/1> [Accessed April 23 2012]McCallum, Andrew Kachites. 2002. MALLET: A Machine Learning for Language Toolkit. [online]. Available at:< http://mallet.cs.umass.edu> [Accessed April 23 2012].Meeks, Elijah. 2011 Comprehending the Digital Humanities Digital Humanities Specialist Stanford University Libraries and Academic Information Resources[blog]February 19. Available at <https://dhs.stanford.edu/comprehending-the-digital-humanities/> [Accessed April 23 2012].Morgan, C. 2006-2012 Middlesavagery [blog]. Available at:<http://middlesavagery.wordpress.com/ > [Accessed April 23 2012].Morgan, C. 2011. Crowdsourcing Archaeology – The Maeander Project Kickstarter Page. Middle Savagery [blog] June 16. Available at: < http://middlesavagery.wordpress.com/2011/06/16/crowdsourcing-archaeology-the-maeander-project-kickstarter-page/> [Accessed April 23 2012].Nelson, Robert K. 2011 Mining the Dispatch. Digital Scholarship Lab at the University of Richmond [online]. Available at <http://dsl.richmond.edu/dispatch/pages/about> [Accessed April 23 2012].Newman, David and Alun Balagopalan, 2011. A graphical user interface tool for topic modeling. Google Code [online]. Available at:<http://code.google.com/p/topic-modeling-tool/> [Accessed April 23 2012].Rockwell, G., and Sinclair, S. 2012. Voyant Tools: Reveal Your Texts, [online] Available at: http://voyant-tools.org/ [Accessed April 1st, 2012].SAFE: Saving Antiquities for Everyone. 2012. Blog. [blog] Available at: <http://www.savingantiquities.org/> [Accessed April 23 2012].Sinclair, S. 2009. The Rhetoric of Text Analysis. [online] Availabe at: <http://hermeneuti.ca/rhetoric>. [Accessed April 15, 2012].Stanish, C. 2009. Forging Ahead, or, how I learned to stop worrying and love eBay. Archaeology, 62.3 [online] Available at <http://www.archaeology.org/0905/etc/insider.html> [Accessed April 12, 2012].Suber, P. 2004. Open Access Overview. The SPARC Open Access Newsletter. [online] (Updated March 18 2012) Available at: <http://www.earlham.edu/~peters/fos/overview.htm> [Accessed April 17, 2012]Underwood, T. 2011. Why humanists need to understand text mining. The Stone and Shell, [blog] May 29. Available at: <http://tedunderwood.wordpress.com/2011/06/29/why-humanists-need-to-understand-text-mining/> [Accessed February 27, 2012].Underwood, T. 2012a. What kinds of “topics” does topic modeling actually produce? The Stone and Shell, [blog] April 1. Available at: <http://tedunderwood.wordpress.com/2012/04/01/what-kinds-of-topics-does-topic-modeling-actually-produce/> [Accessed April 14, 2012].Underwood, T. 2012b. Topic modeling made just simple enough. The Stone and Shell, [blog] April 7. Available at <http://tedunderwood.wordpress.com/2012/04/07/topic-modeling-made-just-simple-enough/> [Accessed April 14, 2012].Ushahidi. 2012, Ushahidi, < http://ushahidi.com/>.Weingart, S. 2011. Topic Modeling and Network Analysis. The Scottbot Irregular, [blog] November 15. Available at <http://www.scottbot.net/HIAL/?p=221> [Accessed April 18, 2012].Wilkins, B. 2012. Comment #1 on Graham, S. Digventures, Flag Gen, and Crowd-everything archaeology Electric Archaeology [blog] March 13. Available at <https://electricarchaeologist.wordpress.com/2012/03/13/digventures-flag-fen-and-crowd-everything-archaeology/#comments> [Accessed April 23 2012].Wilkins, L., Wilkins, B., and Dave, R. 2012. Digventures: How it works. Digventures. [online] Available at: <http://digventures.com/how-it-works/ > [Accessed April 23 2012].Williams, S., Terras, M., and Warwick, C. Forthcoming. What people study when they study twitter: Classifying Twitter Related Academic Papers. Journal of Documentation. Submitted 2012.Like this:3 bloggers like this.Comments OffDavid Berry, What Is the “New Aesthetic”? April 18, 2012The New Aesthetic is now subject to discussion and critique on a number of forums, blogs, twitter threads, and so forth (for a list, see bibliography on Berry 2012a, but also Bridle 2012, Kaganskiy 2012, Sterling 2012). Many of these discussions have a particular existential flavour, questioning the existence and longevity of the New Aesthetic, for example, or beginning to draw the boundaries of what is ‘in’ or ‘out’ of the domain of New Aesthetic things (See Twitter 2012).[1] Grusin (2012), for example, claims: ‘[t]he “new aesthetic” is just the latest name for remediation, all dressed up with nowhere to go’. At such an early stage there is understandably some scepticism and, being mediated via Twitter, some sarcasm and dismissal, rather than substantive engagements with the questions raised by a moment presaged by the eruption of the digital into the everyday lifeworld, but also some partial support (for example see, Berry 2012b, Crumb 2012, Exinfoam 2012, Fernandez 2012, Owens 2012). Nonetheless, it is good to see so much discussion and excitement around the concept, however defined.Alex Reid, robot graders, new aesthetic, and the end of the close reading industry, April 18, 2012If you aren’t familiar with new aesthetic, the links above provide some starting places, particularly the first one, which takes you to James Bridle’s tumblr collection of all things NA. Bridle coined the term and has been collecting these examples ever since. As I understand it, the basic concept of NA is an effort to understand the aesthetic processes of digital technologies: how do digital objects respond to the world they sense? And perhaps more to the point, how does our growing human sensitivity to this aesthetic dimension shape our own design and artistic practices? Given this, one can understand Bogost’s interest and also his response that NA “needs to get weirder.” However I am also interested in what Bridle says at the end of his SXSW talk.Bruce Sterling, Still FREAKING OUT !!!!! (New Aesthetic) April 15, 2012How would you know if some new aesthetic was really and truly a “new way of seeing?” How would you prove that this had happened in real life? What would be convincing evidence that such an event had taken place in our world? What proofs could one demand, or offer, that such a thing was an authentic cultural change? People are asking me that now, and sometimes offering answers of their own, such as, “My students are asking me New Aesthetic questions without being prompted from the podium, so that shows that something new is happening.” I lack good answers to these questions, and I’m unsure even how to formulate the problem. Furthermore, it’s unfair to hang this barnacled anchor around the necks of activists in the New Aesthetic network. On that issue, I’m in the Julian Bleecker camp: when the iron gets hot, it’s time to tighten your lips and hammer the anvil.Ian Bogost, The New Aesthetic Needs to Get Weirder April 13, 2012The New Aesthetic is an art movement obsessed with the otherness of computer vision and information processing. But Ian Bogost asks: why stop at the unfathomability of the computer’s experience when there are airports, sandstone, koalas, climate, toaster pastries, kudzu, the International 505 racing dinghy, and the Boeing 787 Dreamliner to contemplate? You know that art has changed when a new aesthetic movement announces itself not with a manifesto, but with a tumblr. Manifestos offer their grievances and demands plainly, all at once, on a single page–not in many hundred entries. “Literature has up to now magnified pensive immobility, ecstasy, and slumber,” wrote Filippo Marinetti in his 1909 Futurist Manifesto. “We want to exalt movements of aggression, feverish sleeplessness, the double march, the perilous leap, the slap and the blow with the fist.” The stakes are clear: out with idleness and chatter, in with speed and violence.CommentsMapping Texts | Stanford University & University of North TexasWhen Ian Bogost and Mike Migurski both mention the same term in close chronological proximity, I feel the need to pay attention. Of course, the one thinks it’s more fodder for taking seriously the personhood of objects (so much so that my use of ‘personhood’ in describing this would likely result in claims that I’m being personist) while the other sees it as a frame for the ever-increasing beauty and sophistication of digital maps. Regardless of whether one takes either of these positions, we have to recognize that there is a new aesthetic movement and that it bodes well for digital humanities scholarship.Fortunately, you need not be terribly aesthetically aware to benefit from this in the practical terms of integrating these new principles into the creation, representation and evaluation of digital objects. There is a maturity of praxis in the digital creative fields, such as computer gaming, digital painting, citizen GIS, and too many other digital flowers for Mao to count. And this folding in of new digital movements (artistic and social) into academic research, archiving and publishing is an important process for successful digital humanities workFor instance, what Eric Fischer did in mapping the different patterns of geo-tagged tweets and Flickr photos is not only interesting aesthetically but a good methodology that should be explored and built out. It’s notable that Linna Li and Mike Goodchild’s exploration of the same topic took place nearly a year after Fischer had done this. Likewise, my own mapping of DBPedia was inspired by Fischer’s project and is currently being developed to deal with a host of big linked geodata resources in support of the study of natural environments in urban areas. This great vibrancy of digital creativity–whether it is in cultural critique, representation of society, radical geography or artistic procedurality–is fertile ground for scholars of the humanities to draw inspiration (and, dare I suggest it, collaboration) for their own work.How do we do this? First, by embracing design in the development of digital scholarly work. I’ve been late to the game when it comes to understanding the importance of UI/UX and other design principles in finished products, thinking that an opaque data visualization is fine as long as the few people who are highly invested in it can access it. My concept of visual literacy, for a long time, placed the impetus on the reader. Like the spur to algorithmic literacy embodied by such chastisements as “Program or be programmed”, I felt that it was incumbent on the part of the reader to understand the importance of being literate (if not fluent) in how information visualization functioned, with creators being not only less responsible for the poor communication going on but also acknowledged as less likely to change since they held so much power. But a literate society benefits both groups, and making algorithms and data more accessible is as much the responsibility of the creator as the user.This last point is the key, I think, to the New Aesthetic, in that it embodies the fact that all are creators (and hence programmers) and all are users (and hence programmed). While those of us ensconced in our ivory silos may be tempted to think that lesser persons who cannot code make up the new underclass, there are very few people who now are not digital content creators using visual toolkits to build complex digital objects. Creating something with the richness and reach of a WordPress blog is now so easy that the digerati mock it as not being real digital creation, rather than acknowledging the growing ease and sophistication of digital work. Creating complex data-driven maps is so simple now that you need not ever have taken a GIS course, much less become a GIS professional as was necessary a decade ago, and the result is a growing sense of disdain for all the new cartographers who can produce an amazing map without knowing what a datum is.1We have already reached the point where hoarding data is more likely to result in missed opportunities than it is to result in benefiting research. If we try to hoard processes, or simply fail to acknowledge this ripe field of interesting methods being developed by the New Aesthetic movement, then the same thing will happen with techniques and tools.1I think the opening up of GIS to people who cannot afford ArcGIS licenses or the time and money necessary for quantitative geography courses and GIS tutorials is very similar in tone to the opening up of Instagram.Right now, humanists often have to take topic modeling on faith. There are several good posts out there that introduce the principle of the thing (by Matt Jockers, for instance, and Scott Weingart). But it’s a long step up from those posts to the computer-science articles that explain “Latent Dirichlet Allocation” mathematically. My goal in this post is to provide a bridge between those two levels of difficulty.Computer scientists make LDA seem complicated because they care about proving that their algorithms work. And the proof is indeed brain-squashingly hard. But the practice of topic modeling makes good sense on its own, without proof, and does not require you to spend even a second thinking about “Dirichlet distributions.” When the math is approached in a practical way, I think humanists will find it easy, intuitive, and empowering. This post focuses on LDA as shorthand for a broader family of “probabilistic” techniques. I’m going to ask how they work, what they’re for, and what their limits are.How does it work? Say we’ve got a collection of documents, and we want to identify underlying “topics” that organize the collection. Assume that each document contains a mixture of different topics. Let’s also assume that a “topic” can be understood as a collection of words that have different probabilities of appearance in passages discussing the topic. One topic might contain many occurrences of “organize,” “committee,” “direct,” and “lead.” Another might contain a lot of “mercury” and “arsenic,” with a few occurrences of “lead.” (Most of the occurrences of “lead” in this second topic, incidentally, are nouns instead of verbs; part of the value of LDA will be that it implicitly sorts out the different contexts/meanings of a written symbol.) Of course, we can’t directly observe topics; in reality all we have are documents. Topic modeling is a way of extrapolating backward from a collection of documents to infer the discourses (“topics”) that could have generated them. (The notion that documents are produced by discourses rather than authors is alien to common sense, but not alien to literary theory.) Unfortunately, there is no way to infer the topics exactly: there are too many unknowns. But pretend for a moment that we had the problem mostly solved. Suppose we knew which topic produced every word in the collection, except for this one word in document D. The word happens to be “lead,” which we’ll call word type W. How are we going to decide whether this occurrence of W belongs to topic Z?We can’t know for sure. But one way to guess is to consider two questions. A) How often does “lead” appear in topic Z elsewhere? If “lead” often occurs in discussions of Z, then this instance of “lead” might belong to Z as well. But a word can be common in more than one topic. And we don’t want to assign “lead” to a topic about leadership if this document is mostly about heavy metal contamination. So we also need to consider B) How common is topic Z in the rest of this document?Here’s what we’ll do. For each possible topic Z, we’ll multiply the frequency of this word type W in Z by the number of other words in document D that already belong to Z. The result will represent the probability that this word came from Z. Here’s the actual formula: Simple enough. Okay, yes, there are a few Greek letters scattered in there, but they aren’t terribly important. They’re called “hyperparameters” — stop right there! I see you reaching to close that browser tab! — but you can also think of them simply as fudge factors. There’s some chance that this word belongs to topic Z even if it is nowhere else associated with Z; the fudge factors keep that possibility open. The overall emphasis on probability in this technique, of course, is why it’s called probabilistic topic modeling.Now, suppose that instead of having the problem mostly solved, we had only a wild guess which word belonged to which topic. We could still use the strategy outlined above to improve our guess, by making it more internally consistent. We could go through the collection, word by word, and reassign each word to a topic, guided by the formula above. As we do that, a) words will gradually become more common in topics where they are already common. And also, b) topics will become more common in documents where they are already common. Thus our model will gradually become more consistent as topics focus on specific words and documents. But it can’t ever become perfectly consistent, because words and documents don’t line up in one-to-one fashion. So the tendency for topics to concentrate on particular words and documents will eventually be limited by the actual, messy distribution of words across documents.That’s how topic modeling works in practice. You assign words to topics randomly and then just keep improving the model, to make your guess more internally consistent, until the model reaches an equilibrium that is as consistent as the collection allows.What is it for? Topic modeling gives us a way to infer the latent structure behind a collection of documents. In principle, it could work at any scale, but I tend to think human beings are already pretty good at inferring the latent structure in (say) a single writer’s oeuvre. I suspect this technique becomes more useful as we move toward a scale that is too large to fit into human memory.So far, most of the humanists who have explored topic modeling have been historians, and I suspect that historians and literary scholars will use this technique differently. Generally, historians have tried to assign a single label to each topic. So in mining the Richmond Daily Dispatch, Robert K. Nelson looks at a topic with words like “hundred,” “cotton,” “year,” “dollars,” and “money,” and identifies it as TRADE — plausibly enough. Then he can graph the frequency of the topic as it varies over the print run of the newspaper.As a literary scholar, I find that I learn more from ambiguous topics than I do from straightforwardly semantic ones. When I run into a topic like “sea,” “ship,” “boat,” “shore,” “vessel,” “water,” I shrug. Yes, some books discuss sea travel more than others do. But I’m more interested in topics like this: You can tell by looking at the list of words that this is poetry, and plotting the volumes where the topic is prominent confirms the guess. This topic is prominent in volumes of poetry from 1815 to 1835, especially in poetry by women, including Felicia Hemans, Letitia Landon, and Caroline Norton. Lord Byron is also well represented. It’s not really a “topic,” of course, because these words aren’t linked by a single referent. Rather it’s a discourse or a kind of poetic rhetoric. In part it seems predictably Romantic (“deep bright wild eye”), but less colorful function words like “where” and “when” may reveal just as much about the rhetoric that binds this topic together.A topic like this one is hard to interpret. But for a literary scholar, that’s a plus. I want this technique to point me toward something I don’t yet understand, and I almost never find that the results are too ambiguous to be useful. The problematic topics are the intuitive ones — the ones that are clearly about war, or seafaring, or trade. I can’t do much with those.Now, I have to admit that there’s a bit of fine-tuning required up front, before I start getting “meaningfully ambiguous” results. In particular, a standard list of stopwords is rarely adequate. For instance, in topic-modeling fiction I find it useful to get rid of at least the most common personal pronouns, because otherwise the difference between 1st and 3rd person point-of-view becomes a dominant signal that crowds out other interesting phenomena. Personal names also need to be weeded out; otherwise you discover strong, boring connections between every book with a character named “Richard.” This sort of thing is very much a critical judgment call; it’s not a science.I should also admit that, when you’re modeling fiction, the “author” signal can be very strong. I frequently discover topics that are dominated by a single author, and clearly reflect her unique idiom. This could be a feature or a bug, depending on your interests; I tend to view it as a bug, but I find that the author signal does diffuse more or less automatically as the collection expands.What are the limits of probabilistic topic modeling?I spent a long time resisting the allure of LDA, because it seemed like a fragile and unnecessarily complicated technique. But I have convinced myself that it’s both effective and less complex than I thought. (Matt Jockers, Travis Brown, Neil Fraistat, and Scott Weingart also deserve credit for convincing me to try it.)This isn’t to say that we need to use probabilistic techniques for everything we do. LDA and its relatives are valuable exploratory methods, but I’m not sure how much value they will have as evidence. For one thing, they require you to make a series of judgment calls that deeply shape the results you get (from choosing stopwords, to the number of topics produced, to the scope of the collection). The resulting model ends up being tailored in difficult-to-explain ways by a researcher’s preferences. Simpler techniques, like corpus comparison, can answer a question more transparently and persuasively, if the question is already well-formed. (In this sense, I think Ben Schmidt is right to feel that topic modeling wouldn’t be particularly useful for the kinds of comparative questions he likes to pose.)Moreover, probabilistic techniques have an unholy thirst for memory and processing time. You have to create several different variables for every single word in the corpus. The models I’ve been running, with roughly 2,000 volumes, are getting near the edge of what can be done on an average desktop machine, and commonly take a day. To go any further with this, I’m going to have to beg for computing time. That’s not a problem for me here at Urbana-Champaign (you may recall that we invented HAL), but it will become a problem for humanists at other kinds of institutions.Probabilistic methods are also less robust than, say, vector-space methods. When I started running LDA, I immediately discovered noise in my collection that had not previously been a problem. Running headers at the tops of pages, in particular, left traces: until I took out those headers, topics were suspiciously sensitive to the titles of volumes. But LDA is sensitive to noise, after all, because it is sensitive to everything else! On the whole, if you’re just fishing for interesting patterns in a large collection of documents, I think probabilistic techniques are the way to go.Where to go next The standard implementation of LDA is the one in MALLET. I haven’t used it yet, because I wanted to build my own version, to make sure I understood everything clearly. But MALLET is better. If you want a few examples of complete topic models on collections of 18/19c volumes, I’ve put some models, with R scripts to load them, in my github folder.If you want to understand the technique more deeply, the first thing to do is to read up on Bayesian statistics. In this post, I gloss over the Bayesian underpinnings of LDA because I think the implementation (using a strategy called Gibbs sampling, which is actually what I described above!) is intuitive enough without them. And this might be all you need! I doubt most humanists will need to go further. But if you do want to tinker with the algorithm, you’ll need to understand Bayesian probability.David Blei invented LDA, and writes well, so if you want to understand why this technique has “Dirichlet” in its name, his works are the next things to read. I recommend his Introduction to Probabilistic Topic Models. It recently came out in Communications of the ACM, but I think you get a more readable version by going to his publication page (link above) and clicking the pdf link at the top of the page.Probably the next place to go is “Rethinking LDA: Why Priors Matter,” a really thoughtful article by Hanna Wallach, David Mimno, and Andrew McCallum that explains the “hyperparameters” I glossed over in a more principled way.Then there are a whole family of techniques related to LDA — Topics Over Time, Dynamic Topic Modeling, Hierarchical LDA, Pachinko Allocation — that one can explore rapidly enough by searching the web. In general, it’s a good idea to approach these skeptically. They all promise to do more than LDA does, but they also add additional assumptions to the model, and humanists are going to need to reflect carefully about which assumptions we actually want to make. I do think humanists will want to modify the LDA algorithm, but it’s probably something we’re going to have to do for ourselves; I’m not convinced that computer scientists understand our problems well enough to do this kind of fine-tuning.Like this:Be the first to like this.So here we are in 2012, the Year of Code, and we should all be learning to code! Shouldn't we? Especially if we belong to this community known as Digital Humanities, a field that is endlessly wrestling with its self-definition. Who's in, who's out? Is it really necessary to code? Don't we have to know our stuff, computationally, if we are to understand what computers can do for us? Does coding culture excludewomen, and is this imperative therefore sexist? Wouldn't we be better off concentrating on being better humanists?As a historian (since, arguably, 1999) and a coder (since 1996), I have to tell you: it's not easy. Sure, the ability to make things, to dream up a system and watch it take shape, to save yourself three days of work with five minutes of command-line scripting, is wonderfully empowering, and I wouldn't have it any other way. But along the way, to get to the triumph of having your tests pass and having your program actually work, there is a lot of grunt work, even more frustration, and a lot of time spent looking to your flanks, chasing after problems that aren't directly related to your actual goal.The gritty reality of learning to codeThis is something I don't think I have ever seen acknowledged in the Great DH Debates. To learn a little bit of code, enough to be able to manipulate variables and add some logic to a 'for' loop and wrap something else in an 'if' statement, is not hard at all. To follow along with the Code Academy lessons, and learn exactly how some of that JavaScript web programming magic actually works, is a fine and productive thing to do. To import that stuff onto your own website and make something creative and informative out of it is excellent. But the thing that nobody tells you, and that you don't have a visceral understanding for until you have been coding (preferably professionally) for a long time, is that, for all the "Eureka" moments, there are a hundred moments of wondering why your test is failing now, finding the misplaced parenthesis lurking in your code, realizing that your computer system upgrade means some libraries have moved around and your programs need to be updated, having the sinking feeling that you have solved this particular annoying data transformation problem three separate ways on four separate occasions, but none of them are exactly appropriate for the case you are facing now, so you will have to mostly reimplement the whole thing. That task you thought would take fifteen minutes has now taken over your entire day.Or you run into a problem that you haven't solved before, but it seems so obvious and so necessary that you know it must have been done. And indeed, you will find eventually that it has been done, but as it is not part of a standard library and the problem is so integrated and/or specific, no one has seen fit to design and release a general-purpose solution for it (which would be far too much overhead anyway.)Yak shavingMy apologies to anyone whom I lost in the preceding pair of paragraphs. The point I am trying to make actually got a name, long ago in Internet history:You see, yak shaving is what you are doing when you're doing somestupid, fiddly little task that bears no obvious relationship to whatyou're supposed to be working on, but yet a chain of twelve causalrelations links what you're doing to the original meta-task. [Source]Yak Shaving is the last step of a series of steps that occurs when you find something you need to do. "I want to wax the car today.""Oops, the hose is still broken from the winter. I'll need to buy a new one at Home Depot.""But Home Depot is on the other side of the Tappan Zee bridge and getting there without my EZPass is miserable because of the tolls.""But, wait! I could borrow my neighbor's EZPass...""Bob won't lend me his EZPass until I return the mooshi pillow my son borrowed, though.""And we haven't returned it because some of the stuffing fell out and we need to get some yak hair to restuff it."And the next thing you know, you're at the zoo, shaving a yak, all so you can wax your car. [Source]In fact, I wonder how many budding coders fully realize how prevalent this is. You aren't three levels deep in browser tabs looking for help on some odd JQuery problem you're having just because you're inexperienced; you're there because all coders are there, at some time or another, and the need to do this never goes away.You may not even be looking for help. Fundamentally, computer programming is a very low-level task, and the "do what I mean" language has never been invented. You might be able to describe the thing you want to do in a single sentence, but then you have to break it down to a series of computer statements, and you have to break some of those down even farther, and you have to be ultra-precise in your interpretation. At some point you will realize that there is some detail of the system that you intended to disregard, but that turns out to be important. There is a parallel to be drawn here with transcription or translation of manuscript texts. It doesn't get you any credit to speak of, nobody likes doing very much of it, we take shortcuts and then desperately wish we hadn't because now we have to go re-do some of the work, we all wish we could pass it off to enthusiastic but cheap helpers. Unless the work gets done, though, you will have nothing to show for your actual idea.I would even say that the problem is worse, the more interesting the task you are trying to do--and let's face it, the whole reason you're a digital humanist is that you want to do interesting things that involve the computer, right? The whole point is to try things that (hopefully) have never been tried before, and certainly to try things you have never tried before. Unlike software contractors who might be providing Solution A for Company Z with a few improvements learned along the way, nearly everything you do is (or ought to be) in an exploratory direction. You will constantly run into situations that you don't understand, you will write and rewrite and refine the precise set of statements that reflect the concept you thought you had adequately coded six months ago, and you will never feel like an expert at this whole programming business.Bring on the collaborationWell, it's time to bring in the experts then, isn't it? Here is where we come to another issue that DH (and before that, humanities computing, and before that, academic programming) has been facing for a long time. What does it mean to collaborate?The answer to this question, in fact, might depend on your answer to the question "does a digital humanist need to learn to code?" The answers that I have seen tend to fall into two categories:No, as long as you can think systematically and understand the possibilities that digital methods open to humanities research, who cares if you know how to run a compiler? That's what collaboration is for.Of course you have to learn to code, because otherwise you will never fully understand the possibilities, and anyway you will simply not get anywhere if you sit around waiting for others to provide the tools for your specific problems.So it is clear in both of these answers that the two themes of methodological theory and programming skill are relevant, and in one answer they are more intertwined than in the other. But how far can collaboration really take us, today, in digital humanities research?As Andrew Prescott most recently pointed out, in most collaborations between the academic and the programmer, the academic considers him- or herself the lead partner, and it is the responsibility of the programmer to realize the vision that will lead to a successful research outcome. The vision may well have been shaped by the programmer, but the primary goal was the academic one all along. The dynamic has not disappeared with the establishment of dedicated Departments of Digital Humanities, with DH academic programs. The "traditional" humanist still tends to call the shots; the digital humanist supplies the hired help, and it is then up to him or her to find some means of extracting academic credit for the substantial work that is nevertheless not considered to be academic output worthy of record. In this model, while equal partnerships can happen, they are exceedingly rare. (That said, a properly equal partnership of this form does usually indicate a truly innovative project, since it implies that there is something there that is academically interesting to multiple fields.)So to make any headway on the tenure track, it seems, the digital humanist must often put him- or herself in the driver's seat of the project--that is, mostly on the humanities side, and seek collaboration with one or more programmers. This is the model of collaboration implied by those who see no need for digital humanists to do the coding themselves. But in this case there is no balance to be struck. Both the research result and the methodological credit will go to the non-coding humanist, digital or otherwise, who will simply have contracted out the grunt work necessary to build the actual tools. Now the coder is in the same position that the digital humanist occupied in the first scenario, only with even less of the academic credit; it is usually assumed that the coder is not really an academic at all. The work becomes just another programming job, albeit one that makes for good dinner conversation. Thus, while this is a fine model for employment if the humanist can afford it, it is not academic collaboration either.The fundamental problem with humanities computing (if I may return to the slightly outdated phrase, and revive it to refer specifically to the practice of writing computer programs to solve problems in the humanities) is that an awful lot of the work has an awful lot of yak hair stuck to it. True, the end product might be spectacular. The methodological concepts behind the code might be mind-bendingly innovative. But how many academics can afford either the time to carry these projects through, or the money to hire people who can?So by all means, get out there, learn to code. Find out what is possible. But understand that the things you want to do are still going to be hard, and forbiddingly time-consuming, without any sort of guarantee that the investment will pay off. If every digital humanist who doesn't already know how to code gets out there tomorrow and signs up for a class, if the doors to this field are trampled down by techies and early dot-com retirees who really are code wizards and want a change of pace, what then? How will we explain to funders that we haven't written any papers for the last six months because we were too busy trying to build a computational model for the evolution of Greek iconography from the tenth to the sixteenth centuries, and ran into some problems with databases along the way, and realized halfway through that the model needed to be re-designed to include UV identification of ink types? Put another way, how is our field going to bridge the gap between what we would like to do and what we are able to do?The Department of Education released a draft report about big data and education today. It's called "Enhancing Teaching and Learning through Educational Data Mining and Learning Analytics," a title that's unlikely to win any converts to the notion of a data-curious* view of learning. Part of what's going to get stuck in the craw is that phrase "data mining," I reckon.Despite all the potential and all the buzz about (big) data, data-mining remains something with a fairly negative connotation. Advertisers. Political campaigns. Big government. All sifting through your personal data, trying to uncover the things that nobody knows about, trying to get you to buy or sell or vote. Add to that now the knowledge that every click we make online -- every YouTube view and Facebook like and Google query -- is eminently trackable, it's enough to make all those unsolicited phone calls and junk mail seem quite benign, not to mention old-fashioned.As NYU doctoral student Solon Barocas argues in an interview at O'Reilly's Strata conference last year, that notion of data mining may be inaccurate, but the phrase "almost intuitively for most consumers implies scavenging through the data, trying to find secrets that you don't necessarily want people to know." I recommend Barocas's interview in part because I think in that 6 minute video, you'll see a data scientist push back on the notion that data mining is simply a high-tech form of snooping. Rather data-mining is a way of finding patterns and trends in large datasets using statistics and machine learning.But Barocas is also clear that there are serious ethical concerns to be weighed surrounding data-mining -- and PR ramifications, of course, if questionable data mining practices are made public.So what exactly would we construe as "questionable data mining practices" in education? And what exactly should we consider as useful ones?The latter goes to the heart of the Department of Education report, which makes a case for the importance of data mining and learning analytics in (perhaps) helping answer questions like:What sequence of topics is most effective for a specific student?What student actions are associated with more learning?What student actions indicate satisfaction, engagement, learning progress, etc.?What will predict student success?When is a student falling behind and/or at risk for not completing a course?"Commercial entities have led the way in developing techniques for harvesting insights from this mass of data for use in identifying likely consumers of their products, in refining their products to better fit consumer needs, and in tailoring their marketing and user experiences to the preferences of the individual," reads the report. I think it's worth asking critical questions about how and if we can apply these same techniques to education. Is a consumer the same thing as a learner? Why or why not?For its part, the Department of Education report touts Netflix as an exemplary model for taking consumers' data, "mining" it, and building something useful for those very users, creating models and profiles so that it can make recommendations based on viewing and rating information. That's worth repeating: user data isn't just extracted for its value to Netflix (although yes, it does that too). Through big data analytics, something of value is built for users in turn. It is worth noting, however, that while the Netflix example is often used to demonstrate how useful the insights gleaned from our online activity can be, I find it also a good indication how far still we have to go before these algorithms really "get" us -- "get" our movie preferences, let alone our learning habits. (No, Netflix, just because I watched Terminator does not mean I might like to watch Beverly Hill Cop III. What it means is that you have a lousy selection of streaming content. But I digress…)The Department of Education's report recognizes too that there are still lots of obstacles ahead for educational data analytics. There's the technology. There's the access to the data (most of which remains siloed across a multitude of systems). There are inconsistencies in data collection, storage, and formats. There are questions of institutional capacity (who's storing and who's analyzing all this data for a school?). And there are privacy and ethical concerns.Add to these obstacles -- and not mentioned anywhere in the report -- is the question of ownership of data. Who owns the data in an LMS, for example? The LMS provider? The school? The instructor? The student? How do we make sure that in our rush to uncover insights and build "personalized" systems, that the student isn't just the object of all this analysis and decision-making? How do we make sure the student has agency and control -- over their data and their learning?* I purposefully use the adjective "data-curious" here in lieu of "data-driven," in part because the latter has become politicized beyond the point of meaning. But also because I'm not sure we actually know enough about educational data yet to let it "drive" our conversations, let alone our policies. Until then, I do remain curious…Photo credits: Helen CookEaster 1982 – thirty years ago! – was spent feeding my latest addiction. Like over a million others, I had acquired the Sinclair ZX 81, which popularised home computing in Britain. It had just one kilobyte of on-board memory; I soon invested in the upgrade to take it up to 16 kilobytes. You used your television as the monitor, and loaded the programmes from audio cassette tapes. My love affair with Sinclair only came to an end when even more awesome Amstrad PCW came along a few years later. Indeed, checking references just now, I came across the Sinclair ZX81 emulators, and began to feel some of the old passion stirring.In order to get the Sinclair to do anything, you had to programme in the Sinclair flavour of Basic. Even to get a word to display on your television, you had to write and run a short programme. For some, this was a problem. One of the reasons why the BBC decided to use Acorn computers rather than the Sinclair machines to promote computer literacy in schools was that the producer of the BBC series The Computer Programme, Paul Kriwaczek, ‘did not believe that the future of computers lies in everyone learning to program in BASIC’. Yet, for me and I suspect many others, it was precisely the programming that was so fascinating about the Sinclair. As you sought to develop a programme that would, say, enable you to do some primitive word processing, the hours and days would disappear as you played with variables and loops. I became obsessed with trying to produce a programme to calculate the date of Easter. Dates in medieval documents are generally given by reference to religious festivals. Dating medieval documents involves cross-checking tables in a Handbook of Dates. This is obviously a process that can be automated and calculating the date of Easter would be a first step towards this. I honestly believed, in a fit of youthful delusion, that somehow I could produce an automated Handbook of Dates on a Sinclair ZX81. Of course, I was unsuccessful; amazingly, there still doesn’t seem to be an automated version of the Handbook of Dates online. I gave up when I realized how much time my addiction to Basic programming was consuming – I am convinced that I would have completed my PhD thesis two years earlier if I hadn’t purchased a Sinclair ZX81. I realized that I was spending all my time becoming a low-end computing hobbyist whereas I should be concentrating on becoming a reasonably accomplished historian.My experience with the Sinclair ZX81 perhaps prefigures the debate which is still an active one within the digital humanities – namely the extent to which practitioners of the digital humanities should be hands-on programmers and the level of hands-on computing engagement we should expect from scholars of the digital humanities. Stephen Ramsay’s now celebrated intervention at the 2011 MLA ‘Who’s In and Who’s Out’ , refined by a subsequent post, ‘On Building’, argued that the creation of digital objects of all types should be a fundamental concern of practitioners of the digital humanities. Ramsay points out that humanities scholars are familiar with theorizing (say) maps as cultural artefacts, but that the experience of mapping in GIS gives new perspectives. He argues that ‘Building is, for us, a new kind of hermeneutic – one that is quite a bit more radical than taking the traditional methods of humanistic inquiry and applying them to digital objects. Media studies, game studies, critical code studies, and various other disciplines have brought wonderful new things to humanistic study, but I will say (at my peril) that none of these represent as radical a shift as the move from reading to making’.The anxieties expressed in the discussion of Ramsay’s blog posts echo through the recent volume of Debates in the Digital Humanities(an extraordinarily Americo-centric volume for a discipline which claims to be highly collaborative and international in its scope and outlook). Indeed, Ramsay can be seen as anticipating recent wider arguments in Britain that coding should receive more attention in schools. Last Saturday, John Naughton launched in the Guardian a manifesto for teaching computer science in schools which emphasized the learning of code in a way that must have gladdened the heart of Sir Clive Sinclair. Indeed, the Raspberry Pi seems to take us back to the days of the ZX81, and has already proved very successful in making children understand how the digital devices which pervade their lives work. In my recent article in Arts and Humanities in Higher Education, I argued that it is essential for humanities scholars to become more than mere consumers of digital resources. If this is to be achieved, some understanding of the nuts and bolts of such resources is essential.But does this mean that humanities scholars, in order to engage with the digital world, must become coders? Isn’t there precisely the danger that I found with my Sinclair machine, that I was becoming a poor coding hobbyist at the expense of good humanities scholarship? I think Ramsay’s use of the term building is important here. In creating Electronic Beowulf, Kevin Kiernan and I were completely dependent on the skilled help of a number of computer scientists and programmers, but we were nevertheless building something which was both a statement about the nature of Beowulf and a vision of what digital technologies can achieve. It is here that the collaboration which is seen as a distinctive feature of the digital humanities comes in. Something like Electronic Beowulf or the projects created by the Department of Digital Humanities at King’s College London simply cannot be achieved without a wide range of skills embracing not only humanities scholarship but also computer science, project management, programming in a variety of forms, interface design, server management and much else.Much of my thinking about digital projects is informed by my experiences at the British Library in the 1990s, and in particular the Library’s work in designing the original automated systems which gave catalogue access and allowed automated book ordering in the St Pancras building. A naïve user (aka a humanities academic) would assume that to build those systems you either bought a piece of software or got some programmers in to build the system. But building a robust bespoke automated system is more complex than this. Librarians, as users, define the need. An army of analysts define the logical structures required to meet these needs and asses the array of technical possibilities available. These logical definitions are then broken down into units of work. The system was actually designed in an enormous amount of detail on paper, with a mass of flow diagrams, before a line of code was written, and this was in many ways the intellectual heart of the development. An army of programmers then built the various modules defined in the project specifications. The crucial element in this process was not the coding, but rather the design on paper. The analysts who produced this design were the most important (and highly paid) people in the whole process, yet generally they had very limited programming skills. The coders who actually built the system were at the bottom of the food chain, producing elements of the system to order, frequently with only limited understanding of how the whole system worked.My experience at the British Library taught me that automation should not be equated to coding. In many ways, it is providing the overall vision and defining – on paper – the steps by which that can be realized which is they key part. This, after all, is what computer scientists spend a lot of their time doing. Such a process requires an understanding of the tools and methods available, but is not wholly focused on the creation and deployment of these tools. Again, an analogy from the library world is I think helpful. It is essential for all librarians to have an understanding of cataloguing standards and methods, but it is not necessary for all librarians to be cataloguers. A scholar in the digital humanities should be sufficiently well informed about the technical environment to develop an independent and critical approach to the use of digital methods and resources, but does not necessarily need to be a hands-on programmer.I worry that an emphasis on coding, and even on building things, is holding the digital humanities back as an academic discipline. We emphasise collaboration, and collaboration is certainly necessary for practitioners of the digital humanities, to build the innovative digital activities, bit are our patterns of collaboration always the right one? The Department of Digital Humanities at King’s College London has worked with dozens of academic partners both at KCL and elsewhere to realize an impressive portfolio of projects. The Department quite rightly stresses collaboration as at the heart of its philosophy. Yet I have been struck in the few months that I have been working in the Department by how often our external academic partners assume that they are the driving force in the collaboration. For them, the humanities scholar is always the person who calls the shots; the digital humanities specialist is simply there to do the donkeywork of programming the machine to do what the academic wants. Collaboration turns out to be a mask used to disguise the true nature of much of the Department’s work which is too often the kind of software development or infrastructural maintenance normally provided by a University service department. Now, it could be argued that academics should not see themselves as superior to information service departments, and I would strongly agree with such a proposition, but it is nevertheless sadly true that academics perceive themselves as at the top of the university tree, and most humanities academics evidently regard digital humanities units (even when these are constitutionally defined as academic departments) as representing something lower down the higher education food chain.Among the controversies to be considered by The Cologne Dialogue in the Digital Humanities later this month is the question ‘Do the Digital Humanities have an intellectual agenda or do they constitute an infrastructure?’. My colleague Willard McCarty will be presenting an impressive defence of the intellectual component of the Digital Humanities, but one wonders whether the question is correctly put here. The issue is not whether, as Anthony Grafton put it, digital media are always means rather than ends. A lot of tne problem is (as Willard will be suggesting) one of confidence – scholars in the digital humanities too often see themselves as serving longer established academic disciplines and lack the chutzpah to develop their own intellectual programme which doesn’t need topay so much attention to others. The question is how Digital Humanities stops presenting itself as an element of infrastructure, as something which helps other scholars realize their visions, and realizes that it doesn’t need to be dependent on classicists or historians or literary scholars to keep going. Part of the reason why Digital Humanities is treated by other scholars as a support activity is because of its interest in programming and coding – it becomes the gateway by which scholars can gain access to this new digital world. One of the many threats confronting the digital humanities is that it will increasingly become part of the service infrastructure. The suggestion that the term digital humanities will soon disappear as all humanities scholarship becomes digital is predicated on the idea that the digital humanities represents a form of specialist support activity which will soon no longer be required. Certainly, the digital humanities should build things – it should be pioneering the creation of new forms of scholarly discourse in a digital environment – but it should not simply be building things for other scholars, and that has too often been the case.Indeed, it could be argued that the digital humanities as a whole has fallen into exactly the same trap I was concerned about with my Sinclair ZX 81. By insisting on building things ourselves, we simply come up with slightly amateurish packages which fail to make a large-scale impact or simply repeat existing procedures across different subject domains. The pioneering days of digital editions were very exciting and innovative, but having established what we think of as an accepted procedure, we now repeat that again and again and again in different subject domains for different groups of scholars. When practitioners of the digital humanities are going to build things, these objects should be truly innovative and should restate our sense of what is possible in a digital environment. In the recent Institute of Historical Research seminar on ‘The Future of the Past’, I was very taken by Torsten Reimer’s call for the digital humanities to renounce the sort of digital photocopying that is commonly associated with the creation of digital editions and rather seek to develop genuine innovation that moves into new territory both our cultural engagement and sense of the possibilities of computing. The deployment of TEI and its role in the development of XML were truly innovative and helped create the modern web, but that was nearly twenty years ago. Since then, what true innovation has emerged from the digital humanities? Zotero? Citation managers were available before it appeared. Crowdsourcing? Simply borrowed from other domains. The digital humanities has little to show in the way of true innovation, yet all those engaged with the digital humanities know that the complexities of the humanities offer endless possibilities for the creation of innovative technologies in areas ranging from imaging to nanotechnology. Consider the hyperlink. What a crude mechanism it is. Any textual scholar could imagine more complex and interesting possibilities. The digital humanities could readily look to develop the next stages beyond hypertext. Yet it doesn’t – because it is too busy preparing digital editions for historians who don’t otherwise have access to programming resources.If the digital humanities is indeed to start realizing its own intellectual agenda, it needs to rethink the nature of its collaboration. It must avoid like the plague that service activity which purports to be collaboration – the sort of Antechrist of the digital humanities. It should instead develop collaboration within the digital humanities, genuine collaboration which is all too rarely seen. To achieve this requires some fundamentally rethinking. Digital humanities centres are certainly part of the problem. Frequently dependent on soft funding, they have perforce to pursue research projects in which the role of the digital humanities is often subservient, and fundamentally a service function. It would be better to have smaller digital humanities departments which have more stable income streams from teaching, and aren’t forced by financial necessity to seek out research projects which reduce the digital humanities element to have a service function. The nature of our projects should change as well. We urgently need to start developing more experimental and risky projects, which challenge existing methods and standards and reach out into new areas.In short, we should code and we should build, but for ourselves and because (like my experiments in trying to create a Handbook of Dates on the Sinclair ZX81) they feed our own intellectual interests and enthusiasms, and not those of others.The rapidly growing archive of early modern texts online presents significant new opportunities and necessities for the ways in which we organize it. Addressing such challenges raises important questions for both skeptics and boosters: Are new methods of organization resulting in virtual but less reliable finding aids? Do pressures of modernization encourage resource-strapped organizers of early modern texts to adopt whatever technologies are easiest? Are we really taking advantage of new archival possibilities?Improper use of technology—or perhaps misplaced technological priorities—is a danger that we might already be falling prey to. We might be able to better frame the digital challenges facing early modern texts and modern organizational technologies with an analogy to the world of printing in the later 19th century. It was then that letterpress printers found themselves competing with newer and sexier printing techniques, like chromolithography and engraving. Those products were hand-drawn and therefore allowed much greater freedom in design. Soon after, technical advances in letterpress print technology itself (brighter and faster drying inks, new typesetting techniques) allowed letterpress printers to flex their own design chops, and to develop what came to be known as “artistic printing.”Despite the name, it wasn’t really artistic at all—at least not in the sense that it could be characterized by superior aesthetics per fine art standards. The predominant feature was ornamentation—excessive ornamentation: grandiose borders, highly stylized typefaces, bizarre color schemes, and non-linear design elements—employed to rival materials printed with newer printing technologies, even when those weren’t characterized by such ornamentation. Printers’ content was thus dictated by technology; the medium had overtaken the message.Critics reacted strongly against printers making their content subservient to “barbaric” excessive ornamentation and “degenerate” ostentatious flourishes. They lamented how printers focused on the immediate and low-hanging technological fruit ahead of fundamental typographic principles. The extra ornament was considered a sham, a form of concealment.Where letterpress printers did not, we must constantly reflect on our priorities and values as we embrace new technologies for organizing our overwhelmingly large and still growing archive of early modern sources. 21st-century organization of early modern texts must not be seen only as a technological prosthetic that enhances traditional practices, but rather an opportunity for creating and engaging with a new kind of archive.As we all know, information overload is not new. One salient reminder comes from Ann Blair and her book Too Much To Know, in which she describes how early modern scholars developed various procedural strategies and textual apparati (many of which we still use) to help find and to organize the vast amount of information flooding into their personal libraries. In part, we have the same problem; we too have access to more texts than ever before.Having access to more texts facilitates more opportunities for applying our traditional methods for organization. We might be tempted to emulate our early modern predecessors, but with modern equivalents—no longer curating shoeboxes of 3×5 cards, but rather Zotero libraries; perhaps moderating virtual group libraries instead of emailing bibliographies. On the whole, we—as did early modern scholars—tend to think in terms of individual technology solutions with some social tentacles, like Zotero and RefWorks. Even on the web, we do this also. We continue to create isolated databases, search engines for them, and even lists of links to help organize and connect early modern texts. But are we creating these because they are the most productive, or because they are most easily accessible technology “solutions”? Have our databases become our ornamental borders?Unlike previous instances of information overload, however, organization is no longer an individual problem. Rather, it’s one that exists at the level of scholarly societies and broad research communities (notice that i’m not including libraries; more on why later). Our unprecedented access to the early modern period means that we have the potential for a vastly larger and much richer archive than we’ve had before. We must take an active role in organizing that archive to make it available, visible, and fully usable.I want highlight two facets of the digital archive that will be crucial as digital methodologies become more integrated with our research practices: metadata and text transcriptions. In other words: creating and connecting texts.2 important caveats 1) I want to emphasize at the outset that both of these are fundamentally social challenges, not technological ones. This is not about what technical standard to follow. This is not about which interface components or which ornamental border to use. These are important questions, but ones that should and will naturally follow a deliberate attempt to make archival content a value problem rather than a technology problem.2) When speaking about metadata and text—and maybe even the digital humanities at large—skeptics often immediately seize upon the very impersonal and non-humanist kind of inquiry that seems to underlie techniques like text mining, or any vaguely quantitative methodology. Aren’t we simply outsourcing our interpretive powers to complex algorithms and code? NO. These are tools to help us do our work, not strategies for having the computer do interpretive work for us. The goal is to make the fullest use of the early modern record that has come down to us.It’s hardly news that finding relevant online resources can be problematic for many early modern scholars. As we all know, the bibliographic data we rely on—whether from Google Books, HathiTrust, Internet Archive, and even fully controlled catalogs like the Library of Congress—can be a mess, confused by lengthy and sometimes bizarre titles, language variants, non-standardized author names, foreign characters, uncertain dates…the list could go on and on.Rejoinders against such mess typically frame these problems as repository problems (eg Google Books has failed us because its metadata is so poor). The problem here is that this kind of thinking embraces the traditional delineation of the researcher as a mere consumer of data. But using poor metadata as the sand in which to bury one’s head in is not a productive way forward. We need to consider the ethics and metaphysics of metadata: Exactly what does it take to create metadata? Whose responsibility is it?It seems that many metadata critiques treat metadata as objective, descriptive information that simply should be correct. But anyone who’s ever produced any serious amount of metadata knows that it’s quite subjective, confusing, and takes considerable expertise to do properly—especially for early modern sources. Because we’re the ones with this expertise, we must be not only consumers, but also producers of this data.But isn’t creating and improving metadata the work of librarians and archivists, you ask? Surely, research scholars don’t have the time, inclination or expertise to deal with metadata. We produce knowledge, not metadata!Except that we don’t live in the binary producer/consumer world anymore. Even if we did, there is simply too much data to deal with. Its stewards simply do not have all necessary expertise or resources to organize it most effectively and flexibly. Without doubt, this involves plenty of technical challenges (standards, interfaces, infrastructure). But these are trivial in comparison to the real challenge: shifting community expectations that erroneous metadata can and should be edited by researchers themselves. And while we’re at it, we might broaden our view of metadata to include not only the usual fields (author, date, etc), but additional description as well (abstracts, section headings, keywords, etc) that makes the texts more findable.The idea that we share a communal responsibility for metadata requires changes to typical research practices that need to happen more or less simultaneously.a) The research community must recognize the scholarly value of this work. Making such contributions is the kind of peripheral scholarly work that we already do because we recognize its importance and necessity, such as peer review, review articles, editing, chairing sessions, etc. And we have to recognize that such effort does not have to stem from sheer altruism, as it helps us take fuller advantage of the vast archive we have at our fingertips. It’s far from value neutral, and it requires considerable expertise.b) Repositories of information need to facilitate metadata suggestions from the scholarly community. This does not mean that they ought to adopt, anonymous, real-time data revisions. Instead: a controlled, but open community effort to improve data. As researchers, we must voice our desire to help make data more usable. Librarians and archivists are nothing if not sensitive to the needs of their constituents.Framed in this way, the problem isn’t the erroneous metadata itself (woeful as it can be). The problem is that a) we continue to reinforce a division of labor that doesn’t make sense anymore, and b) remain bit too myopic about the kind of work we value as a scholarly community.Textual organizational challenges are not just about the connective tissue between texts (like metadata), but also part of the challenge of helping to produce the texts themselves.There is no doubt that the rhetoric of the digital humanities embraces the bird’s eye view of text. Digital methodologies leverage the computer’s ability for mindless drudgery to help us do and see more than we would otherwise—and hopefully make discoveries that would otherwise go unnoticed. Like repairing metadata, such a perspective suggests a new expectation for our archival work: making text/data visible and available. Again, this is not so we can get the computer to interpret it for us. It’s about futzing around with our hermeneutical prism and engaging with the historical record by all available means (and texts).Ongoing digitization projects, both small- and large-scale operations, are making the early modern world more accessible each day. Resources like Google Books, HathiTrust, EEBO, ECCO, etc, make access to primary sources easier than ever—at least in terms of facilitating our traditional strategy: search, find, and read closely. But image-only archives stored in carefully constructed databases, as useful as they are for improving accessibility, cannot be our only interest. We must not let them become our ornamental borders!Imagine all the personal notes stashed on hard drives, all the long quotations stashed in notes in nearly impossible to find monographs. How might our interpretations change if we had access to that text?To truly understand the early modern text (writ large), we need textual transcriptions. Now I am not suggesting that we all spend our time creating transcriptions for our unadulterated love of plain text. But we do an awful lot of work in transcribing for our own scholarship. Imagine all the personal notes stashed on hard drives, all the long quotations stashed in notes in nearly impossible to find monographs. What would our historical archive be like if we had access to that text? What if we put them all together? How would our interpretations be different? What else could we find out?This isn’t as far-fetched as it may seem. The many (wildly) successful transcription projects (Transcribe Bentham, etc.) suggest the success of collaborative participation, patience, and persistence. The problem is that we don’t value this work as much as we should. As with metadata, it should be considered important scholarly work (but perhaps not scholarship per se). Various technologies and standards with linked open data are creating the connective tissue—but we need the bits to connect.Several problems with text creation that require attention (merely mentioned here):These are problems that will continue as long as we maintain a too narrow definition of useful work that doesn’t include creating and connecting texts, and as long as we expect other people to make texts available and useful for us.Favoritism: Text creation projects tend to favor the texts we already know—a bias of funded projects that must justify expense with appeals to established utility to a broad audience.Access: Full-text resources tend to be behind expensive pay walls, and usually mediated by well meaning but clunky search engines.Visibility: Our typical practices don’t support publishing these texts. We need to supplement our traditional forms of scholarship with co-publications on our blogs.Authority/Expertise: How do we know where metadata and text have come from? Do we trust them? We learn to make these judgments about scholarship generally; we can learn to do it for data, too.The reason I mention these challenges (even so curtly), is to point out that they are neither library nor archive problems, nor are they reasons to avoid creating texts. They are our problems that will continue as long as we maintain a too narrow definition of useful work that doesn’t include creating and connecting texts, and as long as we expect other people to make texts available and useful for us. Open access is not a challenge for only archivists, librarians and publishers. It’s one that pervades the entire scholarly community to publish and preserve work they consider valuable.More importantly, these problems persist especially when we employ individual organization solutions, even ones that attempt to aggregate information. We don’t need more search engines or more APIs. We need visible text. Use a database to store your text, but don’t make me interact with it. Databases are like closed stacks; the best retrieval mechanism doesn’t make either of them particularly visible and usable. Even if we have our eyes on the prize of linked open data, we must not forget about this first crucial step of creating texts to link to—and they should be openly published online.So why bother creating and organizing such a textual archive? Not everyone will be interested, and that’s fine. But one can hardly ignore the potential here in terms of getting out of scholarly ruts. The literary critic Barbara Herrnstein Smith has suggested that the literary text acts “to shape and create the culture in which its value is produced and transmitted and, for that very reason, to perpetuate the conditions of its own flourishing” (Contingencies of Value, 1988). We could say the same thing about our digital organizational practices as well, as many important techniques that take broad views of texts and data can only be realized when we have an adequate, accessible and visible archive of digital, discrete, malleable, text. If we privilege only traditional archival strategies, we miss out on virtually all historical perspectives that aren’t exposed by those methodologies.One obvious case is massive searching, which is self-explanatory. More important is malleability: combining unusual sets of texts to get a bird’s eye comparative view. This should not instantly conjure images of massive scatterplots and necessarily large-scale efforts. Small-scale work is also extremely valuable, especially when combining text across archives and disciplinary boundaries. One of the most important reasons to value the creation of full text is the way searching is moving from linear to algorithmic searching. Our organizational strategies (databases, lists, catalogs, etc.) tend to re-enforce traditional, linear research practices. But the future of searching is not simply finding what you’re looking for. Having more text (and better metadata) allows us to take advantage of finding not only what we are looking for, but also what we’re not looking for—but should be. Imagine a “show me more like this…” feature that worked for our primary sources. Algorithmic searching is, of course, what Google does, but I’m not suggesting that their mysterious PageRank algorithm should reign over our sources. But as we think about how to organize an unprecedented volume of text, we also have to think about future access technologies. We need to think about the principles of data architecture (typography), and to be sure they are not being applied as technological band-aids (fanciful mauve borders).Again, all of these efforts (fixing metadata, text encoding, creating and publishing transcriptions) require an expansion in the kind of scholarly work we value and reshaping relationship between producers and consumers of data. Simply waiting around for better data or better tools will make for both inferior tools and scholarship. While there are many examples of text creation projects—and such projects have produced excellent results—they tend to be specially grant-funded projects that create unnecessary labor bottlenecks. This model is wholly unsustainable. Worse yet, the products of such projects tend to reside in databases that we say are open, available, and connected, but are only trivially so, since so few people know about them or can access them.If what I’ve described sounds like some fantasy utopia…let me reassure you: it is. But the imaginative possibilities are indeed tantalizing, and even such a utopian vision should guide our values and priorities. Necessity might be the mother of invention, but imagination is its milk.Will our scholarship be reduced to overwrought font faces and massive visualizations that merely add knowledge without value?With such a vastly accessible archive at our fingertips, Mike Witmore (who commented on the conference panel where an early version of this essay was presented) asked if we will lose our ability to ask good questions? Or will we simply be tinkering with texts because they are there? Will our questions still be meaningful? I interpret this as: Will our scholarship be reduced to overwrought font faces and massive visualizations that merely add knowledge without value? It remains to be seen, but I don’t think that fully exposing and connecting our early modern texts (and ways of accessing them) will jeopardize our critical faculties or ability to identify and frame interesting questions. Various digital humanities projects have already started to do that. As do new visualizations, a new kind of archive will facilitate new kinds of questions—ones that cannot possibly grow out of the textual archive the way we have traditionally organized it.In terms of establishing values, our teaching is crucial. As educators of future early modernists, we have to increase awareness of and discuss new textual analytical techniques, and how to establish their requisite infrastructure (like metadata and the value of textual openness) in our courses. Furthermore, our teaching can contribute to the project of making more texts available and visible. We can take advantage of the necessary repetition that happens in both grad and undergrad training to shape the early modern archive into its most usable form. Ars longa, vita brevis. Other sessions have already suggested what a great process transcription is for teaching about editing and understanding the notion of a text.As with our earlier letterpress printers, we’ll have our share of overly ornamented communication failures, where technological fascination obscures analytical objectives. And that’s okay. In a way that typical scholarship does not, we must embrace productive failure—tools, interfaces, processes that help us shape the resources at our disposal. Best practices for improving metadata or associating text with images are unclear: it’s not at all obvious whether we should be using Betamax or VHS right now. In the end, it doesn’t matter, as long as we must value the larger goals more than any particular technology.We have to be thinking of books and texts not only in their contemporary contexts, but also in their modern digital contexts as well, and how we employ technologies to connect them to as many other relevant texts as possible (obvious: author, year, place, subject; cooler: word frequency, tone, style), and how we can profitably put these texts in conversation with each other. That is the organizational challenge ahead. At a superficial level, it’s not at all a new problem. Beneath the surface, though, it’s an entirely different kind of challenge that has the potential for an entirely new kind of early modern text and interpretations of it.[This talk is a slightly revised version of a presentation from the 2012 Renaissance Society of America meeting in Washington, DC.]The Journal of Digital Humanities is a comprehensive, peer-reviewed, open access journal that features the best scholarship, tools, and conversations produced by the digital humanities community in the previous quarter."Modern humanities scholarship is a direct engagement with a deracinated, Google-ised, Wikipedia-ised, electronic text."Tim HitchcockAn ego network from wikipedia.orgReviewsReview of WordSeer, produced by Aditi Muralidharan, Marti Hearst, and Bryan Wagner Amy EarhartReview of Bookworm, produced by Benjamin Schmidt, Martin Camacho, et al. Boone B. GorgesReview of QueryPic, produced by Tim Sherratt Jeremy Boggs,David McClure,Eric Rochester, and Wayne GrahamNewspaper strip visualizationComments OffWhat is the theory that underpins our moocs?By George SiemensIf you’re even casually aware of what is happening in higher education, you’ve likely heard of massive open online courses (MOOCs). They have been covered by NY Times, Chronicle of Higher Education, TV programs, newspapers, and a mess or blogs. While MOOCs have been around since at least 2008, the landscape has changed dramatically over the past 10 months. In this timeframe, close to $100 million has been invested in corporate (Udacity) and university (EDx and Coursera) MOOCs . And hundreds of thousands of students have signed up and taken these online course offerings.Personally, I’m very pleased to see the development of Coursera and EDx. The learning potential for society (globally) are wonderful. All of the critiques that I’ve read so far ring hollow compared with the tremendous learning opportunities that these MOOCs provide. This hit home for me when I was in India a few months ago. I met with numerous university students and the message was clear: we simply can’t get the quality of instruction from some of our colleges that we get from Coursera. While we debate pedagogical models and the ideologies informing different MOOCs and the corporate interests of open courses, the lives of students in different parts of the world are being changed with these projects. And that should be our real focus.A secondary focus, for me (and far lower on the scale than the primary one mentioned above), is around the learning theory and pedagogical models that influence different types of MOOCs.Read Full Post Here. What Lies Beneath: Some Thoughts on MOOCs’ Tech Infrastructure By Audrey WatersMuch of the mainstream media attention paid to MOOCs lately has involved the content, the credentialing, the cost, the class size. But what about the technology?…. I don’t want to make too much of the difference between the learner-focused gRSShopper and the community-oriented Lernanta. Sure, there is plenty of difference, but I don’t think Lernanta is quite as demonstrative of P2PU theory as gRSShopper is for connectivism. And too, both these are both open source systems serving OER communities.There’s another gulf again between these two systems and the new Stanford-model-MOOCs. (Although MITx – and now presumably edX – does say it will open source its MOOC platform (source code link?).) The latter MOOCs have recreated a traditional LMS in many ways for their technology platforms, driving (almost) all course activity onto their own course sites. Udacity does host its videos on YouTube. Otherwise, there’s no RSS. There’s no integration with external student blogs. The social or peer element involves primarily class forums. All work is done on and submitted via the platform….I’m curious to see if learners will bubble out beyond the confines of the tech platform once Coursera offers more humanities and social science classes. Bookmarking, blogging, social media, RSS – will the new MOOCs open to these technologies?Read Full Post Here.CommentsGuess when I tweeted my papers? Top ten downloaded papers from my department in the last year, 7 of which include me in the author list.In October 2011 I began a project to make all of my 26 articles published in refereed journals available via UCL's Open Access Repository - "Discovery". I decided that as well as putting them in the institutional repository, I would write a blog post about each research project, and tweet the papers for download. Would this affect how much my research was read, known, discussed, distributed?I wrote about the stories behind the research papers - the stuff that doesn't make it into the official writeup. From becoming so immersed in developing 3D that you start walking into things in real life, to nearly barfing over the front row of an audience's shoes whilst giving a keynote, to passive aggressive notes from an archaeological dig that take on a digital life of their own, I gave a run down, in roughly reverse chronological order, of the 12 or so projects I've been involved in over the past decade that resulted in published journal papers. Along the way, I wrote a little bit about the difficulties getting stuff up there on the institutional repository in the first place, but the thing that really flew was my post on what happens when you blog and tweet a journal paper: showing (proving?) the link between blogging and tweeting and the fact that people will download your research if you tell them about it.So what are my conclusions about this whole experiment?Some rough stats, first of all. Most of my papers, before I blogged and tweeted them, had one to two downloads, even if they had been in the repository for months (or years, in some cases). Upon blogging and tweeting, within 24 hours, there were on average seventy downloads of my papers. Seventy. Now, this might not be internet meme status, but that's a huge leap in interest. Most of the downloads followed the trajectory I described with the downloads to Digital Curiosities, in that there would be a peak of interest, then a long tail after. I believe that the first spike of interest from people clicking the link that flies by them on twitter (which was sometimes retweeted) is then replaced by a gradual trickle of visitors from postings on other blogs, and the fact that the very blog posts about the papers make them more findable when the subject is googled. People read the blog posts - I have about 2000 visitors here a month, 70% new, with an average time on the site of 1 minute and 5 seconds. You come here and tend to read what I have written (thanks!) and seem to be clicking and downloading my research papers.The image above shows the top ten papers downloaded from my entire department over the last year. There were a total of 6172 downloads from our department (UCL Department of Information Studies is one of the leading iSchools in the UK). Look at the spikes. That's where I blog and tweet about my research. I'm not the only person producing research in my department (I think there are 18 current members of staff and a further 20 or so who have moved on but still have items in the institutional repository, but I'm the only person who has gone the whole hog on promoting their research like this). You will see that 7 out of 10 of the most downloaded papers from my Department in the last calendar year have me in the author list. As a clue, I dont know anything about Uganda, e-books, or classification in public libraries. 27 out of the top 50 downloads in our department in the last calendar year feature me (as a rough guide, I get about 1/3 of the entire downloads for my department). My stuff isn't better than my colleagues' work. They're all doing wonderful things! But I'm just the only one actively promoting access to my research papers. If you tell people about your research, they look at it. Your research will get looked at more than papers which are not promoted via social media.Some obvious points and conclusions. Don't tweet things at midnight, you'll get half the click throughs you get through the day when people are online. Don't tweet important things on a Friday, especially not late - people do take weekends and you can see a clear drop off in downloads when the weekend rolls around and your paper falls a bit flat, as you sent it on its way on social media at the wrong time. The best time is between 11am and 5pm GMT, Monday to Thursday in a working week. I have the stats here somewhere to prove it. I wont write it up, though, as its pretty predictable (you would think! But somehow the message doesn't get through to people that just putting it on twitter isnt enough, you have to time it right. The Discovery twitter account regularly posts an automated list of the really interesting things people have been looking at... at 10pm on a Friday night. Sheesh. I only know as I'm regularly sad enough to still be on twitter at that time, but I suspect if they tweeted the papers through the day during the working week... well, you guess what would happen?)The paper that really flew - Digital Curiosities - has now been downloaded over a thousand times in the past year. It was the 16th most downloaded paper from our entire institutional repository in the final quarter of 2011, and the 3rd most downloaded paper in UCL's entire Arts Faculty in the past year. It's all relative really - what does this really mean? Well, I can tell you that this paper was the most downloaded paper in 2011 in LLC Journal, where it was published (and where it lives behind a paywall apart from being available free from Discovery). LLC is the most prestigious journal in the discipline I operate in, Digital Humanities. The entire download count for this paper from LLC itself, which made it top paper last year? 376 full text downloads. There have been almost 3 times that number of downloads from our institutional repository. What does this mean? What can we extrapolate from this? I think its fair to say: It's a really good thing to make your work open access. More people will read it than if it is behind a paywall. Even if it is the most downloaded paper from a journal in your field, Open Access makes it even more accessed.However. I might just have written a nice paper that caught people's interest: there are, after all, no controls to this are there? No controls! How can we tell if papers would fly without this type of exposure? Well. Erm. I might not have have tweeted one or two papers to see the difference between tweeting and blogging about papers and not doing so. Take the LAIRAH (Log Analysis of Internet Resources in the Arts and Humanities) project, which I wrote about here. We actually published four papers from this research. I tweeted and promoted three of them actively. One I didnt mention to you. Here are the download counts. Guess which one I didnt circulate?Library and information resources and users of digital resources in the humanities: 297 downloadsDocumentation and the users of digital resources in the humanities: 209 downloadsIf You Build It Will They Come? The LAIRAH Study: Quantifying the Use of Online Resources in the Arts and Humanities through Statistical Analysis of User Log Data.: 142 downloadsThe Master Builders: LAIRAH Research on Good Practice in the Construction of Digital Humanities Projects: 12 downloads.The papers that were tweeted and blogged had at least more than 11 times the number of downloads than their sibling paper which was left to its own devices in the institutional repository. QED, my friends. QED.I cant know if the downloaded papers are read though, can I? The only way to do so is to enter the murky world of citation analysis. The trouble with this is the proof of the pudding will come to light in a few years time - if someone reads something of mine now and decides to cite it, its going to take 1 or even 2 years - or more - for it to appear in my citation list. So, I'll be keeping an eye on things, not too seriously as we all know things like H index are problematic. Just for the record, at time of writing, I have 218 citations, according to Google scholar. My H index is 8, and my i10 index is 5, which is ok for a relatively young Humanities scholar (I'm still technically an Early Career Researcher for another year, as defined by the UK funding councils). Digital Curiosities only has 3 published citations to date. 3 published citations. Remember, it's been downloaded over 1300 times, between LLC and our repository. Will this citation count grow? Will I be able to demonstrate, over the next few years, that retweeting leads to citation? Will I be able to tell how people came across my research - if they come across my research? We'll see. Dont worry, I'll blog it if I have anything to say on this.I also know nothing about how many times my other papers are downloaded from the websites of published journals, or consulted in print in the Library. The latter, no-one can really say about - but the former? It seems strange to me that we write articles (without being paid) and we get them published by people who make a profit on them, then we don't even know - usually - how many downloads they are getting from the journals themselves. The only reason I know about the LLC statistics is because I am good friends with the Editor. So, there are obvious advantages to being able to monitor my own downloads from my institutional repository. Its been a surprise to me to see what papers of mine are of interest to others. (Should that drive my research direction, though?)The final point to make is that people don't just follow me or read my blog to download my research papers. This has only been part of what I do online - I have more than 2000 followers on twitter now and it has taken me over 3 years of regular engagement - hanging out and chatting, pointing to interesting stuff, repointing to interesting stuff, asking questions, answering questions, getting stroppy, sending supportive comments, etc - to build up an "audience" (I'd actually call a lot of you friends!) If all I was doing was pumping out links to my published stuff would you still be reading this? Would you have read this? Would you keep reading? My blog is similar: sure, I've talked about my research, but I also post a variety of other content, some silly, some serious, as part of my academic work. I suspect this little experiment only worked as I already had a "digital presence" whatever that may mean. Thanks for putting up with me. All these numbers, these stats. Those clicks were made by real people. Thanks!So that would be my conclusion, really. If you want people to find and read your research, build up a digital presence in your discipline, and use it to promote your work when you have something interesting to share. It's pretty darn obvious, really:If (social media interaction is often) then (Open access + social media = increased downloads).What next? From now on, I will definitely post anything I publish straight into our institutional repository, and blog and tweet it straight away. After all, the time it takes to undertake research, and write research papers, and see them through to publication is large: the time is takes to blog or tweet about them is negligible. This has been a retrospective journey for me, through my past research, at a time when I came back from a period of leave. It's been fun to get my act together like this - in general I needed to sort out my online systems at UCL, so it gave me some impetus to do so. But it has shown me that making your research available puts it out there - and as soon as I have something new to show you, you'll be the first to know.And here are a list of my personal top downloaded items from our repository, with download count since October, when I started this. Just for your eyes only, you understand.Terras, M (2009) Digital Curiosities: Resource Creation Via Amateur Digitisation.Literary and Linguistic Computing , 25 (4) 425 - 438.1014Ross, C and Terras, M and Warwick, C and Welsh, A (2011) Enabled backchannel: conference Twitter use by digital humanists.J DOC , 67 (2) 214 - 237.448Warwick, C. and Terras, M. and Galina, I. and Huntington, P. and Pappa, N. (2008) Library and information resources and users of digital resources in the humanities.Program: Electronic Library and Information Systems , 42 (1) pp. 5-27.297Terras, M (1999) A Virtual Tomb for Kelvingrove: Virtual Reality, Archaeology and Education.Internet Archaeology (7).253Warwick, C and Galina, I and Rimmer, J and Terras, M and Blandford, A and Gow, J and Buchanan, G (2009) Documentation and the users of digital resources in the humanities.J DOC , 65 (1) 33 - 57.209Terras, M and Van den Branden, R and Vanhoutte, E (2009) Teaching TEI: The Need for TEI by Example.Literary and Linguistic Computing , 24 (3) 297 - 306. 194Terras, M (2010) Should we just send a copy? Digitisation, Use and Usefulness.Art Libraries Journal , 35 (1)193Warwick, C and Terras, M and Huntington, P and Pappa, N (2008) If You Build It Will They Come? The LAIRAH Study: Quantifying the Use of Online Resources in the Arts and Humanities through Statistical Analysis of User Log Data.LIT LINGUIST COMPUT , 23 (1) 85 - 102.142Warwick, C. and Fisher, C. and Terras, M. and Baker, M. and Clarke, A. and Fulford, M. and Grove, M. and O'Riordan, E. and Rains, M. (2009) iTrench: a study of user reactions to the use of information technology in field archaeology.LIT LINGUIST COMPUT , 24 (2) pp. 211-223.133I’m having an interesting discussion with Lisa Rhody about the significance of topic modeling at different scales that I’d like to follow up with some examples.I’ve been doing topic modeling on collections of eighteenth- and nineteenth-century volumes, using volumes themselves as the “documents” being modeled. Lisa has been pursuing topic modeling on a collection of poems, using individual poems as the documents being modeled.The math we’re using is probably similar. I believe Lisa is using MALLET. I’m using a version of Latent Dirichlet Allocation that I wrote in Java so I could tinker with it.But the interesting question we’re exploring is this: How does the meaning of LDA change when it’s applied to writing at different scales of granularity? Lisa’s documents (poems) are a typical size for LDA: this technique is often applied to identify topics in newspaper articles, for instance. This is a scale that seems roughly in keeping with the meaning of the word “topic.” We often assume that the topic of written discourse changes from paragraph to paragraph, “topic sentence” to “topic sentence.”By contrast, I’m using documents (volumes) that are much larger than a paragraph, so how is it possible to produce topics as narrowly defined as this one? This is based on a generically diverse collection of 1,782 19c volumes, not all of which are plotted here (only the volumes where the topic is most prominent are plotted; the gray line represents an aggregate frequency including unplotted volumes). The most prominent words in this topic are “mother, little, child, children, old, father, poor, boy, young, family.” It’s clearly a topic about familial relationships, and more specifically about parent-child relationships. But there aren’t a whole lot of books in my collection specifically about parent-child relationships! True, the most prominent books in the topic are A. F. Chamberlain’s The Child and Childhood in Folk Thought (1896) and Alice Earl Morse’s Child Life in Colonial Days (1899), but most of the rest of the prominent volumes are novels — by, for instance, Catharine Sedgwick, William Thackeray, Louisa May Alcott, and so on. Since few novels are exclusively about parent-child relations, how can the differences between novels help LDA identify this topic?The answer is that the LDA algorithm doesn’t demand anything remotely like a one-to-one relationship between documents and topics. LDA uses the differences between documents to distinguish topics — but not by establishing a one-to-one mapping. On the contrary, every document contains a bit of every topic, although it contains them in different proportions. The numerical variation of topic proportions between documents provides a kind of mathematical leverage that distinguishes topics from each other.The implication of this is that your documents can be considerably larger than the kind of granularity you’re trying to model. As long as the documents are small enough that the proportions between topics vary significantly from one document to the next, you’ll get the leverage you need to discriminate those topics. Thus you can model a collection of volumes and get topics that are not mere “subject classifications” for volumes.Now, in the comments to an earlier post I also said that I thought “topic” was not always the right word to use for the categories that are produced by topic modeling. I suggested that “discourse” might be better, because topics are not always unified semantically. This is a place where Lisa starts to question my methodology a little, and I don’t blame her for doing so; I’m making a claim that runs against the grain of a lot of existing discussion about “topic modeling.” The computer scientists who invented this technique certainly thought they were designing it to identify semantically coherent “topics.” If I’m not doing that, then, frankly, am I using it right? Let’s consider this example: This is based on the same generically diverse 19c collection. The most prominent words are “love, life, soul, world, god, death, things, heart, men, man, us, earth.” Now, I would not call that a semantically coherent topic. There is some religious language in there, but it’s not about religion as such. “Love” and “heart” are mixed in there; so are “men” and “man,” “world” and “earth.” It’s clearly a kind of poetic diction (as you can tell from the color of the little circles), and one that increases in prominence as the nineteenth century goes on. But you would be hard pressed to identify this topic with a single concept.Does that mean topic modeling isn’t working well here? Does it mean that I should fix the system so that it would produce topics that are easier to label with a single concept? Or does it mean that LDA is telling me something interesting about Victorian poetry — something that might be roughly outlined as an emergent discourse of “spiritual earnestness” and “self-conscious simplicity”? It’s an open question, but I lean toward the latter alternative. (By the way, the writers most prominently involved here include Christina Rossetti, Algernon Swinburne, and both Brownings.)In an earlier comment I implied that the choice between “semantic” topics and “discourses” might be aligned with topic modeling at different scales, but I’m not really sure that’s true. I’m sure that the document size we choose does affect the level of granularity we’re modeling, but I’m not sure how radically it affects it. (I believe Matt Jockers has done some systematic work on that question, but I’ll also be interested to see the results Lisa gets when she models differences between poems.)I actually suspect that the topics identified by LDA probably always have the character of “discourses.” They are, technically, “kinds of language that tend to occur in the same discursive contexts.” But a “kind of language” may or may not really be a “topic.” I suspect you’re always going to get things like “art hath thy thou,” which are better called a “register” or a “sociolect” than they are a “topic.” For me, this is not a problem to be fixed. After all, if I really want to identify topics, I can open a thesaurus. The great thing about topic modeling is that it maps the actual discursive contours of a collection, which may or may not line up with “concepts” any writer ever consciously held in mind.Computer scientists don’t understand the technique that way.* But on this point, I think we literary scholars have something to teach them.On the collective course blog for English 581 I have some other examples of topics produced at a volume level.*[UPDATE April 3, 2012: Allen Riddell rightly points out in the comments below that Blei's original LDA article is elegantly agnostic about the significance of the "topics" -- which are at bottom just "latent variables." The word "topic" may be misleading, but computer scientists themselves are often quite careful about interpretation.]Documentation / open data: I’ve put the topic model I used to produce these visualizations on github. It’s in the subfolder 19th150topics under folder BrowseLDA. Each folder contains an R script that you run; it then prompts you to load the data files included in the same folder, and allows you to browse around in the topic model, visualizing each topic as you go.I have also pushed my Java code for LDA up to github. But really, most people are better off with MALLET, which is infinitely faster and has hyperparameter optimization that I haven’t added yet. I wrote this just so that I would be able to see all the moving parts and understand how they worked.Like this:Be the first to like this.My field nowadays is the digital humanities; I started my academic life as an archaeologist. In between, I’ve taught in continuing education, distance education, secondary education to troubled students, and started a business. My philosophy of teaching has evolved continually as a result of these disparate experiences.I was attracted to archaeology by the hands-on nature of the field, by the materiality of it. I became interested in distance education and continuing education for how these two modes opened up academia to broader audiences than a standard undergraduate experience. Working with troubled teens (students whom the system had otherwise failed), I saw both of these strands come together in a program that offered a hands-on experience leading to a vocational diploma. Starting a business taught me that I had to relearn everything I thought I already knew. I recount these experiences to explain where I am coming from.I first encountered the idea of ‘uncoverage’ in a blog post on Profhacker by Mark Sample. This phrase neatly encapsulates what I have come to believe. In Sample’s post, he defines ‘uncoverage’ by contrasting it with how we normally use the phrase in course syllabi: “…this course will cover the evolution of American public life from the publication of the Federalist Papers to…”. In the race to cover everything on the syllabus, we necessarily end up covering in the sense of ‘protect or conceal, to hide from view’ (Sample, citing Wiggins and McTighe 106). We do not teach understanding; rather we slip and slide over the top of the deeper issues that make these topics worth studying in the first place. For Sample, ‘uncoverage’ then is a kind of digging downwards, to reveal the assumptions and principles that we would normally cover. There is an obvious connection here with archaeology. In archaeology, one begins with the most recent layers and works backward, peeling away the events that form a site, understanding their associations and connections both in terms of breadth and depth. In the same way one would plan an archaeological excavation backwards from the idea ‘what do we wish to learn from this site?’ I implement backwards-design philosophies into my classes: in order to uncover that which is important, what must students understand as a result of having been in this course?My ambition in every course is to teach for uncoverage. This has the effect of making my research and teaching two sides of the same craft. As a craftsman, I want my work to be visible, public and appreciated. My students therefore are both objects of my craft, and independent craftspeople in their own right. I seek out opportunities for my students’ work to become visible as together we work through the implications of digital media for historical understanding. Digital history is public history: therefore my students’ work is never conceived of as being done for an audience of one. As I tell my students, ‘we’re working without a net, folks: everything we do, we do in public’. I have published papers, articles, blogs and projects with students as a result.I have blogged my own teaching and research for five years now. I am committed to open access, making not only my process but also my data available to the wider community. Not every experiment results in success; indeed, the failures are richer experiences because as academics we are loathe to say when something did not work – but how else will anybody know that a particular method, or approach, is flawed? This idea that it is ‘safe to fail’ at something, that sometimes what we try just might not work, is something that I try to foster in my classes. If we try something, it does not work, and we then critically analyze why that should be, we have in fact entered a circle of positive feedback. This perspective comes from my research into game based learning. A good game keeps the challenges just ahead of the player’s (student’s) ability, to create a state of flow. Too hard, and the player quits; too easy, and the player drops the controller in disgust. If we can design assessment exercises in a class that tap into this state of flow, then we can create the conditions for continual learning and growth (see for instance Kee, Graham, et al. 2009). What is more important is that these can be tailored to an individual student’s abilities. Why should assessment in a class begin at 100 points and then work downwards? Why not begin at zero and allow the student to rise?My approach to teaching has changed over the years, and it will no doubt evolve in the future. What I hope to make a constant though is a commitment to celebrate in public the excellent work that my students do, whether that is sharing their blog posts on Twitter, to finding opportunities to publish with them, to finding collaborative projects with the wider community. By teaching for uncoverage, and by exploring the affordances of digital media for historical representation and analysis, I am able to marry the strands of my own evolution as a student, research and teacher, into the best opportunities for my students.Putting the digital into my humanitiesI teach one of the History Department’s core courses in historical method, HIST2809: The Historian’s Craft. This class has a large enrolment of typically 120 students. Instead of surveying the many different ways historians write history and do historical research, I focus instead on cultivating a deeper understanding of the reflexive nature of historical work, so that when students encounter a new possible method or approach they do so with a critical understanding of not just what the approach offers, but also how it delineates what it possible to say or uncover. I emphasize that historical work is not done in a vacuum, but is done within a community of practice.To support this teaching, I created a WordPress powered website that I extended with the Buddypress plugin. Buddypress allows for the customized creation of a social network platform – a HIST2809 Facebook, if you will. Then, I ‘gamified’ this space by creating ‘achievements’ that students could work towards, with their progress being visible to other members of the classroom. This approach was written about by Nick Ward in This Week in FASS:‘I wanted students to have more opportunities to practice the ‘craft’ of being an historian, beyond the formal assessments in the class. Obviously, I could’ve assigned weekly exercises, but that would’ve gone against some of the spirit of what I was trying to inculcate in my students-that being an historian is about being part of a community, that there is joy and surprise and discipline in being an historian, and that most of all, one has to want to do these things – to that end, the achievements system was entirely voluntary (but with a healthy dose of competition).’In this gamified approach, the students started at zero and tried to collect as many points as possible. All participants would get a small bonus to their participation grade, proportional to the number of points they’d collected. Some of the game challenges included transcribing lines of ancient papyrus, learning the rhetorics embedded in computer code, completing tutorials on logical fallacies, learning some Latin, and participating in online crowdsourcing history projects (including HeritageCrowd.org, Graham’s own experiment in crowdsourcing local cultural heritage knowledge).”As a result of including participation in the running of some of my own research projects into the achievements system, some of my students became involved in community digital history projects. One student is now a lead author on the writing of a regimental history for one of Ottawa’s military units. Another student became a co-author with me on a case study of the project to crowd-source local history.An ancillary use of technology in this class is my virtual excavation project in the Carleton Virtual Campus. This excavation is designed to make ‘real’ the metaphors of archaeology. Through interaction with this virtual excavation – where it is safe to make mistakes – students get the chance to explore how archaeological knowledge is created. This excavation is still in a prototype phase, and so it hasn’t been fully incorporated into this course yet. It represents another facet though of how the careful use of game-based elements can enhance one’s teaching and learning in class.(I also presented my approach to gamification to my colleagues at an EDC brown-bag lunch. As a side note, I used ‘Prezi’, a piece of online presentation software that uses as its dominant metaphor the idea of ‘zooming’ into data. This is in stark contrast to Powerpoint, whose dominant metaphor is the 35 mm slide. I use both pieces of software in my classes to highlight the ways the media we use structure the stories we are able to tell. There is no ‘right’ way to interact with digital 1s and 0s. In a way, we are all disabled in this regard.)http://www2.carleton.ca/fass/2011/playing-with-historyhttp://prezi.com/nfla–pmtgn2/adventures-in-gamification-shawn-graham/FYSM1405a Digital AntiquityI have been teaching a first year seminar for two years. In the first iteration of the course, which was then subtitled ‘digital history’, I sought to explore a variety of digital media with my students to explore how these media structure our understandings of history. As part of that course, I partnered with the Council of Heritage Organizations of Ottawa to use their Ottawagraphy website to tell stories about Ottawa’s history. One of these students later became a co-author with me on the case study on crowd-sourcing local history. Another student is now a co-editor with me on a project to crowdsource the illicit trade in antiquities, heritage.crowdmap.com.In the second iteration of this course, subtitled ‘Digital Ancients, Digital Moderns’, we focused more on ancient history and archaeology, and how both digital media and ‘traditional’ media create ways of understanding and patterns of power. One of the semester long assignments in this class was to write the Wikipedia. We looked at how Wikipedia articles are subject to a channelization effect, where the earliest structure of an article sets the stage for all subsequent alterations. My students selected an article related to ancient history consisting of only a single paragraph, and then set out to improve it. One such page is http://en.wikipedia.org/wiki/Mycenaean_pottery . Wikipedia pages now come with ratings, and as of March 2012, the consensus view of this page is that it is ‘well-written’ (in Wikipedia’s page rating shema). That first year students can be responsible for setting what is the de-facto Western memory bank for everything is a shocking experience for these students!In the second part of this year’s course, I have partnered with the Museum of Civilization on a project to make ‘the Hidden Museum’ accessible to the public via augmented reality. As a result of some postings on my research blog, curators at the Museum contacted me to see if there was a possibility to partner. They opened up their storerooms to my students, and we began a project to create three-dimensional models of artefacts, using free software. The students ended up selecting a series of models related to Mesoamerica. The students then used Lulu, a print on demand service, to create a book which they are in the process of augmenting with smart-phone based augmented reality software (Junaio.com). In this way, they provide a ‘magic-eye’ like experience or pop-up book experience and liberate these museum pieces from the storerooms, providing a new way for the public to interact with them. The students also considered the ethical implications of displaying museum artefacts this way. This experience will be recounted at this spring’s Canadian Archaeological Association conference in Montreal as a case study. The museum curators and I have written a SSHRC application for a much larger study built on some of the themes related to this student work. Should we be successful in winning the grant, I intend to provide opportunities for these students to continue participating in the project.Our work in this class was written up in the Charlatan Newspaper, Carleton’s Student newspaper, in January 2012.http://www.charlatan.ca/2012/01/ancient-history-seen-through-digital-games/HIST5702xThis year I have been teaching a graduate seminar in our public history program on digital history. http://dhcworks.carleton.ca/history5702/ The entire course is designed around the exploration of the historiographical issues implicit in digital history and the use of digital tools for historical research (the two are not necessarily the same thing). There was a wide range of ability and affinities for digital media amongst the students enrolled in the course. A significant worry for the students from day one was, ‘what if the project/tool x doesn’t work?’ For these students, my concern with making a project ‘safe to fail’ was paramount. I wanted to demonstrate to them that digital history, as public history, is as much about knowing what doesn’t work (and why) as it is with achieving any set result.The course also was funded by the NiCHE, the Network for Environmental History. They wanted to know whether or not augmented reality was a feasible approach for telling environmental history in Canada. They provided funds allowing us to purchase some smart-phones and data-plans. In conjunction with an exhibit on the urban forest of Ottawa at the Bytown Museum, the students began exploring the ways history could be told in-place using geo-location and augmented reality. I had them chronicle their journey from being digital neophytes to the completion of the project (April 3rd). They did this on a group blog hosted by the Digital History at Carleton platform (a web-space for digital research and collaboration). Each time they posted, I have retweeted to my followers the location of their posts (~ 700 digital humanists, archaeologists, and online education professionals). As a result, important connections have been made between individual students and practitioners in the field, opening up new research avenues.I encouraged each student to develop an individual project that would further their major research essays. The resulting panoply of projects and the discussion surrounding their implementation and implications, has made for an incredibly rich seminar. Some students are using interactive fiction platforms (Inform7); others using text-analysis (Voyant Tools) or topic modeling (Mallet) software; some are creating 3d models of artefacts from a museum perspective while others are using data mining of Youtube and Twitter. For me to support these projects has necessarily meant working hard to understand how they work, their possibilities and their perils.I also hosted a Google+ hangout with PhD students and faculty in digital history and archaeology from the UK and the United States. I wanted the students to experience first-hand one of the hallmarks of the digital humanities, its ‘big-tent’ philosophy and its philosophy of ‘hacking as a way of knowing’. The openness of that experience was something commented on by everyone that week in their blog posts.Independent studiesI am a thesis advisor for a student interested in exploring issues of power and identity in the Greek Bronze Age. We developed a methodology for him to study these power relationships through social network analysis using the open-source platform Gephi (http://gephi.org). He has been blogging his research approach ( http://zackbatist.com/ ) and has made very important connections with practitioners in the field, which he has been able to draw upon as he applies to graduate school. I’ve taken him to a digital humanities ‘unconference’, and he is helping in the planning of an unconference to take place at Carleton next fall.I am also a collaborator on Jennifer Evans’ ‘Hate 2.0’ project that looks at online hate. I have been training a student funded by the I-Cureus programme in data mining and analysis to support this project. ( http://dhcworks.carleton.ca/onlinehate/ ).So why are you sharing this?Well, lord knows I’m not the best teacher out there – but I want to be better. How are you using tech in your classroom, to support your digital humanistic mission? Where are the flaws in my approach? Where are the strengths?Kevin Kee, Shawn Graham, Pat Dunae, John Lutz, Andrew Large, Michel Blondeau, Mike Clare. ‘Towards a Theory of Good History Through Gaming’ Canadian Historical Review 90.2 (June 2009). Pp.303-326.Mark Sample. ‘Teaching for Uncoverage’ August 23, 2011 http://chronicle.com/blogs/profhacker/teaching-for-uncoverage-rather-than-coverage/35459Grant Wiggins and Jay McTighe. Understanding by Design. (Prentice Hall, New Jersey: 2005).Like this:Be the first to like this.By December, every piece of the application had been disassembled, scrubbed, and oiled from a user experience, surface design, and information architecture perspective. Furthermore, every piece of the underlying engineering had been hammered and abused in testing for solidity: just try to crash the thing. Clearly we were making stuff. Lots of it.I mention all of this to emphasize the great experiential texture we felt during the process of building this application. Clearly, we were making stuff. Lots of it. Making and throwing away and making more stuff. Building upon and learning from the ever growing pile of experiments. Throughout, the team iterated in parallel on several design and engineering loops across everything. The final application is a piece of genuine craftsmanship produced by a full-stack team of genuine craftsmen.This layered process happened almost entirely in digital space. Design comps were produced in Photoshop or Fireworks, screens mirrored to iPhones. Folders were shared throughout the team. And the IA was specced in Illustrator or InDesign.Even the iOS software changes were captured with an atomic granularity. In most contemporary development environments, when an engineer modifies a program, that modification is checked back into the main source code repository along with what’s called a "commit message.” In this message is a brief description of the changes made by that programmer. In big projects, there can easily be thousands of commits. Some may be tiny: "Changed transition speed to 0.6 from 0.4 seconds." Others, much longer or more affecting: "Switching to live servers!" The codebase for Flipboard for iPhone is composed of nearly 10,000 such commits.[2]The more we created, the more digital detritus built up.And so we built. Commits were committed, design folders filled up, and screenshots cluttered up the photo folders on our devices. In other words, the more we iterated the more digital detritus built up.Digitally ThinThere’s a feeling of thinness that I believe many of us grapple with working digitally. It's a product of the ethereality inherent to computer work. The more the entirety of the creation process lives in bits, the less solid the things we’re creating feel in our minds.[3] Put in more concrete terms: a folder with one item looks just like a folder with a billion items. Feels just like a folder with a billion items. And even then, when open, with most of our current interfaces, we see at best only a screenful of information, a handful of items at a time.Perceptually, beyond some low threshold, data becomes boundless to us. Cloud storage compounds this: we don't even worry about HDs filling up anymore! Even when digital streams have clear beginnings and ends, I think we — humans — do a bad job at keeping those edges in view. In trying to reflect upon vast experiences or datasets captured entirely in bits with most standard interfaces, we run into the same wall as in trying to imagine infinity: we can’t.[4]Finishing iPhoneAs Flipboard for iPhone was nearing completion, I began to think about this detritus — our narrative; the proof of our journey. What struck me is that despite knowing we had been on a long journey, it didn’t feel like that journey was manifest anywhere.Sure, you could open the design folder and cover flow through our thousand design comps. You could peek in the git repository and scroll through the near infinite number of commit messages. But, still: that thinness! The experiential texture of the journey was butting against the singularity — that fog of immateriality — that information enters when made digital.I was leaving the company at the end of the year and I needed something to represent that journey. To give it edges, for me. For the company. So I did what I do — I flip-flopped the data. I made a book.Barbara Taranto Digital Program Director New York Public LibraryThe New York Public Library recently launched its first foray into crowd sourcing metadata by exposing 40,000 image pages of turn of the century restaurant and cruise ship menus: “What’s On the Menu?” The goal of the project was to widely distribute the transcription of the menu items into a structured and reusable form. The site was exceedingly popular in its first few months.Recent activity has flattened somewhat, raising issues regarding the public’s appetite for these projects. More importantly, the menus project raised hard questions about the quality of the crowd sourced content, the longevity of the data, and the disposition of the data (e.g. What is it? Is it good enough for our purposes? Should we keep it? If yes, where does it belong?).This presentation will discuss these issues and propose some alternative views on metadata, user-generated content, and the intersection of the two.http://menus.nypl.org/Several weeks ago I was aware that there was a flurry of discussion among the digital humanities crowd about “archival silences.” This occurred shortly after I had posted about the differences between the way archivists use “archive” and the way digital humanities people seemed to be using it, so I suspected that “archival silences” would similarly not mean what I am accustomed to it meaning. I’ve had time to return and read through some of the digital humanities posts, and as you might imagine, doing so was quite illuminating.First, when I hear the phrase “archival silences” my initial assumption is that it refers to gaps or “silences” in a body of original records (in which “body” can be defined in many ways). A good example of this usage is “Women and Archival Silences” by Yvonne Perkins on her Stumbling Through the Past blog, and Rodney Carter’s Archivaria article “Of Things Said and Unsaid: Power, Archival Silences, and Power in Silence.”My starting point for understanding the digital humanities discussion was the post “Editor’s Choice: Archival Silences Round-Up” on the Digital Humanities Now site. I worked my way through the various blog posts cited, trying to understand as best I could what each author meant by “archival silences.”But first, a caveat. I am not proficient in the language and practices of the digital humanites community. I am a spectator, doing my best, as time allows, to try to understand how my audience of archives and archivists fits into this scholarly community. So as I try to wrap my brain around the discourse of another discipline, I hope any digital humanists reading this will forgive me if I commit errors of understanding–forgive, and of course, correct.So, with that caveat out of the way, I will attempt to break the discussion down into the simplest possible terms. DH Now cites a post from Ted Underwood first,”Big but not distant.” Underwood does not explicitly mention “archival silences” but he provides some context by discussing the kinds of data digital humanists use for their research–so-called “big data” versus data based on smaller collections. A key issue is how or where they can get their hands on data, which will be on ongoing theme in the discussion.Katherine Harris (who participated in the discussion about the use of “archives”) is the next thread selected by the DH Now editors. Based on her post, “Big Data, DH, and Gender” along with the Storified Twitter conversation “From archival silence to glorious data,” I come to the conclusion that “archival silences” is being used to refer to materials that are not represented in the data accessible to these scholars for research. In other words they are not represented in the digital collections that have been marked up in ways that make them useful for this kind of research. She writes:Nevertheless, the big data sets that are in play in this conversation (on Twitter and here in this post) in both projects are ones that were created by other institutions. If the traditionally marginalized authors are marginalized now because it’s no longer sexy or innovative to digitize and mark-up those collections, then how have we far have we really come? Are those recovery projects then marginalized because they bring nothing innovative to Digital Humanities?If I am correct, then in this usage “the archives” is equal to those resources that have been digitized and made accessible to digital humanities scholars. This is clearly very different–critically different–from saying that there are materials that are absent from the documentary record. Interestingly in the Twitter conversation captured in Storify, @laurenfklein references a book by Michel-Rolph Trouillot which outlines the distinctive ways in which voices from the past are silenced. This shows an awareness (as one would expect from scholars) that the reasons for silences in “history” are complex. Given this understanding, I find it all more troubling to see this shorthand equation of “archives” with “that which has been digitized and marked up.”The conversation continues with Adeline Koh in a series of two blog posts on the subject “Addressing Archival Silence on 19th Century Colonialism.” In her first post Koh states: “these two posts are concerned with a more specific silence—on race and colonialism in the nineteenth century archive.” Later it becomes clear that the “nineteenth century archive” referred to are those materials that have been digitized and made available online. It is this “silence” that Koh seeks to remedy with her project, “Digitizing ‘Chinese Englishmen‘.” Clearly there is no silence in the physical archives (or more likely, special collections), since Koh has material available to digitize.Lastly (in terms of the items cited by DH Now), Roger Whitson contributes in his post “DH, Archival Silence, and Linked Open Data.” Again, the way I’m reading it Whitson uses “archives” to refer exclusively to sets of data or digitized marked-up resources.Thinking through not just what appears in archives but also how those archives work and how we can use the data to make better archives is, to me, the same conversation. This doesn’t mean that everyone needs to code, but rather that scholars should understand how digital archives are put together and participate in building and rebuilding those archives.So, what does this examination tell us that we didn’t already know? What, if anything, is important here? We already know that many scholars use “archives” to mean a wide variety of things other than the standard (American) archival definition. Even Richard Pearce-Moses states in his notes on this definition: “In the vernacular, ‘archives’ is often used to refer to any collection of documents that are old or of historical interest, regardless of how they are organized.” And we have seen have many people (including archivists) may refer to digital collections of surrogates as a “digital archive.” So the inconsistent use of “archives” is a horse that is already out of the barn.For me, it is the casual equation of archives with the universe of accessible digital information that is troubling. Yes, I’m sure all of these scholars understand that the true universe of archives includes an incalculable volume of analog material that has not and probably never will be digitized. But to equate the problem of material that hasn’t been made digitally accessible with the problem of material that does not exist in archival collections . . . . that concerns me even if the problem is purely a semantic one. (A less troubling semantic issue is the lack of distinction between published and non-published source material. Much of the data referenced in the discussions appears to be drawn from books and other published, and therefore non-archival, sources.)The distinctions made by Michel-Rolph Trouillot (as summarized in the book review cited above) are important ones. They can have different causes and they usually are imposed by different actors. Trouillo lists as the causes for the silences in “the making of history”:there is a silencing in the making of sources. Which events even get described or remembered in a manner which allows them to transcend the present in which they occurred? Not everything gets remembered or recorded. Some parts of reality get silenced.there is a silencing in the creation of archives — the repositories of historical records. Again, choices are made, accidents occur, judgments made, and some of our recorded past is silenced. At times this archival silencing is permanent since the records do not get preserved; other times the silencing is in the process of competition for the attention of the narrators, the later tellers of the historical tales.the narrators themselves necessarily silence much. In most of history the archives are massive. Choices, selections, valuing must be done. In this process, huge areas of archival remains are silenced.finally, not every narrative becomes a part of the “corpus,” the standard historical narrative received and accepted by various groups as the past. This “corpus” will be different for professional historians, critical readers, the general public and so on, but only a handful of narrations become the final produce: “history.”The “archival silences” referenced in this recent digital humanities discussion are of that last type. The materials that have been digitized and marked-up serve as a kind of ”corpus” for this group of scholars. It is this corpus that is incomplete, and for the foreseeable future always will be. And it may be that the rest of the factors contributing to silence also exist; it is possible that there are also what I would describe as “archival silences” in the corpus of digital humanities data.What concerns me is that in this adoption of vocabulary the depth of the universe of true archives may be lost. How absurd is it to think there may come a point for some scholars at which the difference between the two meanings of archival silence may be elided, and that which is not available digitally will become equated with that which does not exist? I am no scholar, but I believe there has been a wealth of discussion in archival studies about what we call “archival silence.” I am not the person to bring that wealth of archival discourse into the digital humanities dialogue, but I hope someone with that kind of understanding does so soon.Regarding questions about why some materials (again often published ones) have been digitized and others have not, this is an area where archivists and librarians have experience and expertise. Archivists, by definition, understand the challenges presented by the need for selection and the preferencing of some materials over others. I think our profession has something to add to these discussions. It is time, I think, for some archival voices to start speaking about archival silences.Be Sociable, Share!If you want more news that I can blog about these days, you can follow me on Twitter:http://twitter.com/archivesnextIn my Twitter profile, I said: “this feed will be mixture of the serious and the trivial. You’ve been warned!” And that’s true. But I rarely tweet about what I had for lunch. Mostly what I do is to share (or re-tweet, “RT”) interesting things that I see from the people I’m following.You can also subscribe to the RSS feed for my Twitter account if you don’t want to bother with Twitter. If you click on that link and scroll down, on the right should be a link to subscribe to the RSS.Be Sociable, Share!Below are some highlights from my CV, to give you an idea who’s writing this. If you want to get in touch with me, you can find me on Twitter (@archivesnext) or send an email to info@archivesnext.comWork:Since March 2007, I’ve written on this blog about the issues facing archives, including technology, evolving business models, professional identity, professional organizations, and news and issues from other related professions. Sponsored “Best Archives on the Web” and “Movers and Shakers in Archives” awards and the Spontaneous Scholarship program for the 2011 SAA annual meeting.Since November 2010, I’ve worked for Neal-Schuman Publishers as a freelance acquisitions editor, which means I help people develop book proposals. If you want to write about archives or information science, please get in touch.From October 2000 – September 2006 I worked at the National Archives and Records Administration in the policy division. There’s a very long paragraph on my real CV describing what I did there, but the parts that will probably interest you are that I worked on a business process re-engineering and spent a lot of time working with the early stages of the Electronic Records Archives.Selected Publications: Editor, A Different Kind of Web: New Connections between Archives and Our Users. Chicago: Society of American Archivists, 2011.“What is the Meaning of ‘Archives 2.0’?,” American Archivist. Vol. 64: No. 1 (Spring/Summer 2011).“Building a Community of Supporters – The Role of New Technologies in Advocacy,” Many Happy Returns: Advocacy for Archives and Archivists. Larry Hackman, ed. Chicago: Society of American Archivists, 2011.“Interactivity, Flexibility, and Transparency: Social Media and Archives 2.0,” The Future of Archives and Recordkeeping, Jennie Hill, ed. London: Facet Publishing, 2011.Web 2.0 Tools and Strategies for Archives and Local History Collections. New York: Neal-Schuman Publishers, 2010.“Archives and Web 2.0: One Perspective on the Challenges Ahead.” ARC, June 2010.Instructional Experience:I’ve taught a bunch of workshops about archives and Web 2.0, includingInstructor, “New Tools for Old Things: How Archives Can Benefit from Social Media,” workshop, Association of Canadian Archivists, Halifax, Nova Scotia, June 8, 2010.Instructor, “How to Make the Most of Flickr: A Hands-On Introduction,” workshop, Mid-Atlantic Regional Archives Conference, Wilmington, DE, April 29, 2010.Instructor, “Introduction to Web 2.0 in Archives…or, What You Need to Know in a Nutshell,” webinar, Society of American Archivists, offered October 13, 2009 and on-demand.Instructor, “Web 2.0 Basics,” Pre-Conference Workshop, ACRL Rare Books and Manuscripts Section, Charlottesville, VA, June 17, 2009.Instructor, “Web 2.0 101,” workshop, Mid-Atlantic Regional Archives Conference, Chautauqua, NY, May 1, 2008.Conference Presentations:Sorry, this is a long list. I should shorten it, I know, but for now, here it is:Speaker, “Archivists, Historians, and the Future of Authority in the Archives,” American Historical Association Annual Meeting, Chicago, IL, January 7, 2012.Speaker, “Exploring the Participatory Archive,” Society of American Archivists Annual Meeting, Chicago, IL, August 25, 2011.Speaker, “Social Media Sensations: A Discussion about the Creative Possibilities for Archives and Web 2.0,” Mid-Atlantic Regional Archives Conference, Alexandria, VA, May 7, 2011.Speaker, “Evolution or Extinction?,” Ignite Smithsonian, Washington, DC, April 11, 2011.Speaker, “Spreading the Word: Archival Publishing for Researchers and Institutions,” Mid-Atlantic Regional Archives Conference, Harrisburg, PA, November 13, 2010.Panel member and invited participant, Archiving Social Media conference, University of Mary Washington and the Center for History and New Media, George Mason University, Fairfax, VA, October 1, 2010.Paper presented, “Leaving the Vault and Joining the Party: Using Social Media to Share Archival Collections,” XVI Brazilian Congress in Archival Science, Santos, Brazil, August 24, 2010.Chair, “Braving the “New Archives World”: Updating the Skills of a New Generation of Mid-Career Archivists and Records Professionals,” Society of American Archivists Annual Meeting, Washington, DC, August 12, 2010.Speaker, “Archivists as Web 2.0 Consumers,” New England Archivists Conference, Amherst, MA, March 20, 2010.Speaker, “Social software in digital libraries and archives: experiences from the field,” OCLC Digital Forum East, Arlington, VA, November 5, 2009.Speaker, “Bridging the Gap: What Professional Organizations Can Do To Help Unemployed and Underemployed Archivists,” Mid-Atlantic Regional Archives Conference, Jersey City, NJ, October 31, 2009.Chair and Commentator, “The Real Archives 2.0,” Society of American Archivists Annual Meeting, Austin, TX, August 13, 2009.Speaker, “Harnessing the Power of the Blog,” Mid-Atlantic Regional Archives Conference, Silver Spring, MD, November 8, 2008.Moderator and Speaker, “Web 2.0 Panel,” Manuscript Repositories Section meeting, Society of American Archivists, San Francisco, CA, August 29, 2008.Speaker, “Tracking the Lifecycle of Records: Modern Initiatives,” Mid-Atlantic Regional Archives Conference, Pittsburgh, PA, October 1, 2004.Speaker, “Business Process Reengineering at the National Archives and Records Administration,” Mid-Atlantic Regional Archives Conference, Arlington, VA, April 23, 2004.Professional Service:Member, Council, Society of American Archivists, August 2010 – present, and various other leadership roles on SAA sections, roundtables, and committees before that.I’ve also served on committees for MAC and MARACOccasional peer reviewer for NHPRC grants as well as Archival Science, American Archivist and Journal of Archival Organization for articles related to archive and use of Web 2.0 tools.Education:Archives Leadership Institute, Madison, Wisconsin, Participant, August 2011University of Michigan, Ann Arbor, Michigan, M.S.I., April 2000, Specialization in archives and records managementUniversity of Maryland, College Park, Maryland M.A, Art History and Archaeology, August 1999Be Sociable, Share!Now available:A Different Kind of Web - edited by and with contributions from me, but mostly a collection of what other smart people have to say. Many case studies of Web 2.0 implementations in archives accompanied by longer more reflective essays about the impact of social media on archives. You can order it from the SAA bookstore and view the table of contents here.Web 2.0 Tools and Strategies for Archives and Local History Collections has been published by the fine folks at Neal-Schuman Publishers and by Facet in the UK. SAA members can order it for a discount through the SAA Bookstore and it’s on Amazon, of course. More info, including access to the table of contents and excerpts from some rave reviews here.I have also contributed chapters to The Future of Archives and Recordkeeping: A Reader (edited by Jennie Hill, Facet Publishers, 2010) and to Many Happy Returns: Advocacy and the Development of Archives (edited by Larry J. Hackman, Society of American Archivists, 2010). Both of those publications are also available for order from the SAA Bookstore.Be Sociable, Share!This is a work in progress - more my notes and queries than a proper paper, stuff will change, references will be added. I wanted most to get this out there and to get your views, your inputs and your insights. Please comment, your thoughts are valued!My recent research with Marilyn Deegan into the value and impact of digitised collections has shown that there is a serious lack of adequate means to assess impact in this sector and thus a lack of significant evidence beyond the anecdotal, number crunching from webometrics or evaluations of outputs rather than outcomes (http://www.kdcs.kcl.ac.uk/innovation/inspiring.html). In short, we need better evidence of impact. How has the digital resource delivered a positive change in a defined group of people’s lives or life opportunities?In this Arcadia funded research, we are addressing some fundamental questions in assessing the impact of digitised resources on changing lives or life opportunities. We plan to report a synthesis of methods and techniques to resolve these into a cohesive, achievable methodology for impact assessment of digitised resources.To assist and clarify our thinking and research goals we would like to offer our description of impact.Our conception of Impact for this research can thus be described as:the measurable outcomes arising from the existence of a digital resource that demonstrate a change in the life or life opportunities of the community for which the resource is intended.There is a well-established, professional and mature field of Impact Assessment (IA) in sectors not normally associated with memory institutions methods of evaluation such as those seen in Environmental, Health, Economic or Social Impact Assessment. These provide some scope and lessons for our distinctive view of impact.Impact Assessment (IA) is often predictive in nature. Environmental IA, in particular, focuses upon identifying the future consequences of a current or proposed action. As such it becomes a technical tool to predict the likely change created by a specific planned intervention. The European Commission’s definition of IA relates to a process that prepares evidence for political decision-makers on the advantages and disadvantages of possible policy options by assessing their potential impacts. In this latter case, impact is often thought in both political and economic terms. Clearly the most important aspect of this mode of IA is to influence and inform decision makers on future interventions and potential policy pathways.Other IA relates to measuring the change in a person’s well being through a specific intervention. Health IA generally considers a range of evidence about the health effects of a proposal using a structured framework. This can be used to determine population health outcomes or to defend policy decisions. The UK National Health Service uses a tool called the QALY system (Quality Adjusted Life Year). This system assesses not only how much longer the treatment will allow a person to live, but also how it improves the life of that person#. The QALY is a measure of the value of health outcomes and as such is somewhat more limited than other methods used in Health IA, particularly in palliative care. King’s College London has developed the Palliative care Outcome Scale (POS)#, a tool to measure patients' physical symptoms, psychological, emotional and spiritual needs, and provision of information and support at the end of life. POS is a validated instrument that can be used in clinical care, audit, research and training. These forms of IA are effective at measuring interventions but generally need clear baselines and large comparable populations to gain significance.Other IA methods focus upon the wealth or level of economic activity in a given geographic area or zone of influence. They may be viewed in terms of: (1) business output, (2) value added, (3) wealth (including property values), (4) personal income (including wages), or (5) jobs#. Economic IA has the benefit of usually being able to identify baselines and significant indicators to measure improvement in the economic well-being of an area. However, these measures are less satisfactory for intangible assets or for the assessment of digital domain resources. Contingent value assessments or analysis are seeking to resolve those intangible asset measures. There is also some very interesting assessment work supporting new business investment opportunities as described by the Triple Bottom Line also known as the three pillars: people, planet, profit. An impact investor seeks to enhance social structure or environmental health as well as achieve financial returns and the modes of measurement are proving interesting to this study.Finally, Social IA looks more closely at individuals, organisations and social macro-systems. The International Principles for Social Impact Assessment (Vanclay, 2003) defines Social IA as including “the processes of analysing, monitoring and managing the intended and unintended social consequences, both positive and negative, of planned interventions and any social change processes invoked by those interventions”. Social IA has a predictive element but successive tools such as Theory of Change have made it more participatory and part of the process of managing the social issues. For our purposes we are happy to include so called social return on investment within the conception of social IA although others within the profession would disagree with this wide church approach. There are many methods and tools for Social IA and we think these may prove especially helpful in considering the life opportunities questions and indicators we need to establish.What is increasingly clear though is that all these modes of IA have something to offer and that our inclusive approach to this research is a good one. Our challenge is to rationalise now into ways of presenting a cohesive set of methods and guidance that is most useful to our memory organisations and sector.Balanced ScorecardOne way to organise the thinking might be to use the Balanced Scorecard approach to considering the Impact of a digitised resource upon lives. This allows us to balance out the Impacts being assessed and to ensure that multiple perspectives are enabled. By combining economic measures, social and non-financial measures in a single approach, the Balanced Scorecard should provide richer and more relevant information. In the Balanced Scorecard approach we would suggest the following core headings:Social and audience ImpactsInternal process ImpactsIn this way we can assess the way Impact is occurring both externally and internally to the organisation delivering the digital resource. Allowing a balanced perspective of changes to people’s lives who use the resource and changes to the organisation through the existence of the resource.ChallengesThere are a number of challenges that many Impact Assessments have sought to address with varying success. These include:-timescales in which measurements may take place remain too short for outcomes to become visible and constrained by project timescales;a lack of suitable benchmarks or baselines from which to measure change;a diverse evidence base;the wide range of possible beneficial stakeholders;the difficulty in establishing useful indicators for the sector;the need to make recommendations to decision-makers based on strong evidence;the lack of skills and training in the community of practise;and the need to review relevant qualitative as well as quantitative evidence to discover significant measurable outcomes.These challenges are all part of scope for this study. The need to establish useful indicators for this sector is currently a major concern of this research. Indicators drive the questions and the methods which may be applied to gain significant information to support impact assessment.What are your thoughts? What are the significant indicators? What are the exemplars out there that you have seen or methods you have tried?By Ian MilliganThe splash page for the Globe and Mail's "Canada's Heritage Since 1844" website.Online digitized newspapers are great. If you have access (either through a free database or via a personal or library subscription), you can quickly find the information you need: a specific search for a last name might help you find ancestors, a search for a specific event can find historical context for it (i.e. the Christie Pits Riots, or a certain strike), and generally the results are beautiful, render relatively well, and are – crucially – immediate.In some ways, however, poor and misunderstood use of online newspapers can skew historical research. In a conference presentation or a lecture, it’s not uknown to see the familiar yellow highlighting of found searchwords on projected images: indicative of how the original primary material was obtained. But this historical approach generally usually remains unspoken, without a critical methodological reflection. As I hope I’ll show here, using Pages of the Past uncritically for historical research is akin to using a volume of the Canadian Historical Review with 10% or so of the pages ripped out. Historians, journalists, policy researchers, genealogists, and amateur researchers need to at least have a basic understanding of what goes on behind the black box.An example of a "results list" from the Globe and Mail's newspaper database. It all seems so orderly and systematic.And the ensuing results, a newspaper article focused on the Artistic Woodwork Strike of 1973An amazing array of information at your fingertips (but…)In Canada, when one thinks of online digitized newspapers, the Toronto Star’s Pages of the Past and the Globe and Mail’s “Canada’s Heritage from 1844″ often come to mind. There are other wonderful collections, of course, notably the incredible historical newspapers of British Columbia collection, but the Star and the Globe are most commonly used.The Star and Globe can be accessed through an institutional or personal subscription (you can also access these two databases through libraries like the Toronto Public Library – with a valid library card). You can search by a specific word, or a specific phrase, and narrow it down by a date range. A keyword search (such as for “Artistic Woodwork” at right) and a date range can quickly take you to a seemingly systematic, quantified, and perhaps even complete listing of relevant articles. History laid before you, neatly ordered, from the comfort of your home, library, or office. Another click, and you’re brought to a PDF version of the scanned document: complete with placement, accompanying advertisements, etc.An example of a feature, front-page, above-the-fold article on the Artistic Woodwork strike that does not appear in a keyword search.But we need to use these databases with greater caution. In the example at right, for example, the Globe and Mail‘s database has correctly found a large feature article on the Artistic Woodwork strike of 1973. Yet it is a continuance of an article from Page One. That headline, the first page of the newspaper, does not appear in the search list. If one just uses the search engine, you miss this vivid headline, picture, and entire story.Why?Primarily, the issue lies in faulty optical character recognition (OCR). This issue is not just limited to these newspapers, and is an inherent flaw in large projects. Tim Hitchcock has described the uncritical use of digitized sources as “roulette dressed up as scholarship,” as historians are “not even bothering to apply the kind of critical approach that historians built their professional authority upon.”What about the specific case of the Toronto Star and the Globe and Mail online? These databases were assembled at the turn of the present century, and indeed, the Toronto Star is heralded on Paper of Record’s (the company responsible for the database creation) as the “first newspaper in the world to have its entire history … digitized.” It was created quickly, as Bruce Gillespie reported in 2003 in his “All the News That’s Fit to Scan”: Using technology developed in-house, Cold North Wind [Paper of Record's parent company] converts documents stored on rolled microfilm into digital computer files. It is an automated process that works quickly-Mr. Huggins says two million pages from The Toronto Star’s 110-year history were archived in less than four months.This incredible speed and the use of microfilm originals comes at a cost, however. The former means that basic OCR is used: hyphenations are not covered (problematic in smaller columns, where Woodwork might be hyphenated as Wood-work across two lines), if microfilm streaks obscure a letter, if it was slightly tilted, or if the OCR just plain misses a character. This is currently unavoidable with large-scale digitization projects: I am currently OCRing a large collection of word processed documents from 1997 onwards – about as perfect a sample as you can get, and while the OCR under these ideal circumstances is well above 99%, it can never be perfect. Quite frankly, without human proof-reading and additional layers, you can never be completely convinced of your accuracy. Furthermore, comprehensive database use requires some limited understanding of Natural Language Processing (NLP). NLP is a complicated field of research, and a proper search query would also need to be formulated to pick up alternates such as ‘Woodworking,’ etc. without unnecessarily duplication of results.Another issue lies in the proprietary nature of the Star and Globe databases: I have been trying to track down their technical support team to discuss a research project, to no avail. E-mails often bounce back from the addresses provided on their search portals, and they can be a bit impenetrable. This is understandable, in a way: unlike other national newspaper projects, they are run by private companies.So what can we do?Now, with a strike (as in my example above), one could pop the date ranges in, go through each newspaper throughout the period, and explore specific events. This would avoid the above problem. But studies that purport to trace social or cultural trends over a long period of time can fall into the habit of relying on these databases without critical reflection. That’s not to say that they should not use them – we can find most articles, especially by the postwar period and its attending better image quality. Indexes are hardly perfect alternatives. History has always had an element of serendipity.Indeed, we cannot and should not abandon our use of digitized online databases. Despite their faults, they allow us to cover large swaths of time and space on a realistic timeline, and are much quicker than using microfilm. They also open up new frontiers of large-scale data and textual processing, although the current user interface and databases are not terribly amenable to this form of work.But we do need to be cognizant. Dissertations and articles that extensively rely on these databases need to be up-front about the issue and at least mention how they have dealt with or recognized the very real and concrete limitations inherent in this form. In my on-going survey of English-language dissertations and other historical work, I have found that while these databases appear to be having some impact on citation counts, few scholars note their database use. Doctoral supervisors, journal editors, bloggers, public historians, etc. need to realize how these databases are potentially shaping professional and amateur historical inquiry in Canada.So next time you’re using the databases, think about what’s going on. Are you getting everything? Are you missing something? Should you do some digging around a hotspot of hits on a given date? In all cases, we should be more up-front about the tools we’re using and how they might be shaping our research.Tagged as: Canada's Heritage from 1844, Canadian history, databases, Digital History, OCR, Pages of the PastThe 27th March is the Day of Digital Humanities.The Day of DH project is a collaborative publishing project for digital humanists around the world to document what they do.CAA2012 and sotonDH are asking delegates to blog from the conference as part of the day.You can register for the blog here: Create an AccountPlease use the category ‘CAASoton’ for ALL of your Day of DH posts, so that we can find them after the event.All delegates blogging as part of the Day of DH will be entered into a prize draw, to be announced after the end of the day.Comments OffEditors’ Note: Several recent pieces by Mia Ridge and Trevor Owens have been focused on Crowdsourcing and Cultural Heritage. Excerpts and links to the original pieces are below.Frequently Asked Questions about Crowdsourcing in Cultural HeritageBy Mia Ridge….What kind of cultural heritage stuff can be crowdsourced?I wrote this list of ‘Activity types and data generated’ over a year ago for my Masters dissertation on crowdsourcing games for museums, and it should be read in the light of discussion about the difference between crowdsourcing and user-generated content and in the context of things people can do with museums, but it’ll do for now:ActivityData generatedTagging (e.g. steve.museum, Brooklyn Museum Tag! You’re It; variations include two-player ‘tag agreement’ games like Waisda?, extensions such as guessing games e.g. GWAP ESP Game, Verbosity, Tiltfactor Guess What?; structured tagging/categorisation e.g. GWAP Verbosity, Tiltfactor Cattegory)Tags; folksonomies; multilingual term equivalents; structured tags (e.g. ‘looks like’, ‘is used for’, ‘is a type of’).Debunking (e.g. flagging content for review and/or researching and providing corrections).Flagged dubious content; corrected data.Recording a personal storyOral histories; contextualising detail; eyewitness accounts.Linking (e.g. linking objects with other objects, objects to subject authorities, objects to related media or websites; e.g. MMG Donald).Relationship data; contextualising detail; information on history, workings and use of objects; illustrative examples.Stating preferences (e.g. choosing between two objects e.g. GWAP Matchin; voting on or ‘liking’ content).Preference data; subsets of ‘highlight’ objects; ‘interestingness’ values for content or objects for different audiences. May also provide information on reason for choice.Categorising (e.g. applying structured labels to a group of objects, collecting sets of objects or guessing the label for or relationship between presented set of objects).Relationship data; preference data; insight into audience mental models; group labels.Creative responses (e.g. write an interesting fake history for a known object or purpose of a mystery object.)Relevance; interestingness; ability to act as social object; insight into common misconceptions.You can also divide crowdsourcing projects into ‘macro’ and ‘micro’ tasks – giving people a goal and letting them solve it as they prefer, vs small, well-defined pieces of work, as in the ”Umbrella of Crowdsourcing” at The Daily Crowdsource and there’s a fair bit of academic literature on other ways of categorising and describing crowdsourcing.The Crowd and The Libraryby Trevor OwensLibraries, archives and museums have a long history of participation and engagement with members of the public. In a series of blog posts I am going to work to connects these traditions with current discussions of crowdsourcing. Crowdsourcing is a bit of a vague term, one that comes with potentially exploitative ideas related to uncompensated or undercompensated labor. In this series of I’ll try to put together a set set of related concepts; human computation, the wisdom of crowds, thinking of tools and software as scaffolding, and understanding and respecting end users motivation, that can both help clarify what crowdsourcing can do for cultural heritage organizations while also clarifying a clearly ethical approach to inviting the public to help in the collection, description, presentation, and use of the cultural record.Human Computation and Wisdom of Crowds in Cultural HeritageBy Trevor OwensLibraries, archives and museums have a long history of participation and engagement with members of the public. In my last post, I charted some problems with terminology, suggesting that the cultural heritage community can re-frame crowdsourcing as engaging with an audience of committed volunteers. In this post, get a bit more specific about the two different activities that get lumped together when we talk about crowdsourcing. I’ve included a series of examples and a bit of history and context for good measure.For the most part, when folks talk about crowdsourcing they are generally talking about two different kinds of activities, human computation and the wisdom of crowds.Human ComputationHuman Computation is grounded in the fact that human beings are able to process particular kinds of information and make judgments in ways that computers can’t. To this end, there are a range of projects that are described as crowdsourcing that are anchored in the idea of treating people as processors. The best way to explain the concept is through a few examples of the role human computation plays in crowdsourcing.CommentsSome background first. I am a Computer Scientist by trade but have a strong interest in connections of my trade to other parts of the academy, such as the Arts, Humanities, Science and Engineering. So, when I saw that there was an online workshop called Critical Code Studies, I decided to go in head first by requesting participation in the three week adventure. The organizers are open and I was privileged to become a participant. This area, dubbed CCS, is part of a larger enterprise called "Software Studies." The purpose of Software Studies according to Wikipedia "draws upon methods and theory from the digital humanities....to understand software." So, for example, when analyzing software, one might study the social and historical facets that situate the software within our culture. CCS focuses Software Studies to the point where "code" (text-based notation for programs) is studied as a first class object.When I observed the discussions during the three week period, I have to confess that sometimes I felt like my interests and those in the CCS community were like two ships passing in the night. My interests tend to be functional, mathematically-based, and utilitarian, whereas some of the CCS posts involved fairly long treatises about the often non-functional parts of relatively small pieces of code. I use the word "small" within the context of my own experience in writing, modeling, or managing millions of lines of code over the course of my career. The computer scientist generally looks at small code snippets within the context of algorithmic study, whereas large code repositories and installations generally require software engineering methods and principles.I'd like to suggest, though, that areas such as Software Studies and CCS are most worthy of recognition by computer scientists. I have become to believe that our cultural differences lay mainly in what "theories" and "methods" we apply. But, we all do apply theories and methods -- like donning lenses when we study something. I will take a walk outside with my computer hat on my head, and may use set theory or graph theory to produce knowledge of the world. For computer scientists and mathematicians, this is how we see and "read" the world text. I would probably not use critical theory (an area within the humanities and in cultural studies in particular), probably because that type of theory does not give me knowledge and understanding that I value as much as the other theories. So, yes, this is a question of differences in values as well. Each of us has a different value system when we study something like code.Acknowledging our differences, let's step back and look at our similarities given the plethora of theories at our disposal. First and foremost, those in CCS believe that software is a cultural product, and most of them believe that they should be promoting procedural literacy in the humanities. Computer scientists need to think on this, because it is an amazing occurrence. We have long been trying to achieve the same effects in our discipline--explaining to the rest of the university why computing and procedural literacy are vital. Now, we have friends across the university who want to help. Some of our theories could be different but that's fine--we know the world in many ways. That is the major take-away from CCS for computer scientists: a mutual agreement on the importance of procedural literacy.Where can computer scientists and those in CCS and Software Studies collaborate? There is certainly the "tool model" where the computer scientist produces text analytics tools for artists and humanities scholars. This model, as long as there is research for the scientists in this endeavor, is a good working model. However, I'd like to boldly, or radically, suggest that there are potentially deeper collaborative opportunities. What if some of the theories used by humanists, artists, and computer scientists were the same? As mentioned, there are many differences in our respective theories. And yet, there are bridges opening up. Some sociologists and humanities scholars are promoting the use of things like graph theory and database theory to understand their texts. Also, for my own work in Aesthetic Computing, I use a combination of embodiment theory and semiotics. Embodiment theories are situated in many areas: linguistics, neuroscience, philosophy, psychology, art, humanities, and computing. Semiotics is a theory that resonates with many computer scientists who study human interfaces. It is also widely used in art and the humanities. So, if some of us can agree not only on the same target (e.g., code), but also on the same collection of theories (e.g., embodiment,semiotics), that will truly be a remarkable intersection of cultures and disciplines. It is a place to begin to collaborate effectively beyond the "I am building a tool for you to use" approach.We need to be careful here as well. The applied theories and methods, being often quite different, may cause some to reject each others' approaches, findings, and even disciplines. However, this opportunity to collectively come together on the vital cultural relevance of computing, and code, is too important to be held back by the usual divisions that may separate us.Share and Enjoy:Share and Enjoy:Welcome to the world of representation –covering how we communicate with each other through the body, symbols, and artifacts. This site is authored and maintained by Paul Fishwick.Share and Enjoy:Comments Off“What is/are (the) Digital Humanities?“ by Elijah MeeksThis morning I gave a presentation on the role of data visualization in DH work. The annotated slides can be found on Google Docs here.“How and why study big cultural data“ by Lev ManovichPresentation at Data Mining and Visualization for the Humanities symposium, NYU, March 19, 2012.CommentsBy Jeremiah McCallMar 21, 12What simulation games do best as interpretations is present the past in terms of problem spaces. This is a concept I have co-opted from games and learning theorists (most notably Henry Jenkins and Kurt Squire) for use in thinking about how we teach and learn about the past and use simulation games. I have an article in the works that addresses some aspects of this, but I want to test these ideas out in the meantime. I am also inspired by the recent posts from Trevor and Rebecca and the enthusiastic feedback they generated and wanted to make some sense of that in relation to my work. Any feedback would be most welcome.I.The concept of problem space is a highly useful tool for studying historical simulations, teaching history, and using the former to help in the latter. Simulation games are interpretations of the past designed as problem spaces. A problem space, at least as I currently define it, has the following features:Players, or in the physical world, agents, have roles and goals generally contextualized in spaceThey make choices in an effort to achieve these goalsThe choices available and their success or failure are shaped by:The affordances of the space (which can include quantifiable resources, cultural frameworks, psychological tendencies, etc.)The constraints of the space (which can include finite quantifiable resources and scarcity, cultural frameworks, psychological tendencies, etc.)The problem space design of a simulation game is, in some respects just a more sophisticated articulation of the basic core of game-ness. By most definitions games require players, conflict, and a quantifiable outcome. Players have affordances and constraints embodied in rules. What a historical simulation game does, however, is craft a virtual problem space that represents a real-world one.The problem spaces in simulation games are subject to some particular constraints. One of the most important is the constraint of quantification. Simulation game programs, as computer games, must be reducible to 0s and 1s – this is the only language through which a CPU can receive orders. Consequently, all elements of a historical simulation game, including agents and their motives, must be reducible to numbers. My favorite example of this is the happiness metric found in many city-builders. This takes a very imprecise real-world concept and transforms it into a precise number that can be increased or decreased in precise ways (often through food, housing, jobs, taxes, and amenities) that have a precise effect on the city population (generally determining whether immigration or abandonment occur). Happiness, productivity, popular unrest, attack and defense strength, espionage effectiveness, cultural influence—no matter how qualitative the concept in the real-world, if it’s an actual mechanic in a game, it is strictly quantified.Additionally problem spaces in simulation games, however open-ended they might appear, are closed. To function, these games must be working, closed systems, completely operational once the player joins the mix. As expansive as a game might be in its treatments, it will impose arbitrary limits on its subject. These limits begin with the roles and goals of the player, decisions that shape the entire design. Perhaps more problematically for critical scholars of video games, simulation games are, as games, teleological in their focus. The quantifiable gameplay elements and mechanics all, in a tightly designed game any way, factor directly into whether the player achieves their goals.II.There has been excellent discussion on PtP about the appropriateness of, and methods for critiquing simulations historically. Teasing out the ramifications that they are interpretations in the form of quantifiable problem spaces can provide some important insights on this issue. It suggests considerations for rigorous and meaningful criticism that is holistic and sensitive to the medium.First of all, just like any other interpretation of the past, simulation games will select certain aspects of the past as their theme and not others. This is true of all historical interpretations—after all an interpretation that includes everything is not an interpretation at all. To be playable and appealing, a game needs to have a set of core mechanics that are tight and cohesive, modeling one overarching system well. Consider the standard genres of simulation games that have developed over time: city builders, nation management, trade, war, diplomacy, politics, etc. Though there is always room for crossovers and new genres, the existing genres of successful simulation games point to a constraint that a compelling game—just like a focused narrative or analysis—must focus on some things and neglect other things.Because simulation games must function as working systems, however, the choice of problem space, or more specifically the choice of whose problem spaces to represent necessarily locks the game into certain portrayals of the past. Other media are not subject to the same constraints. One could easily conceive of a textual narrative/analytical work, for example, that devotes time and space to a variety of viewpoints and agencies. Still, no narrative or analysis covers all or even most viewpoints and all are subject to authorial predilections. Most importantly when considering the difference between simulations and these media, written texts are not quantitative rule sets executed by a computer to form a problem space. Even if writers wish to analyze the past in terms of problem spaces, they are free to select a variety of roles and goals that may have at times only tangential relationships. Further, they can select affordances and constraints that do not always form a complete system, and digress on important philosophical and practical comments in ways game designers simply cannot. The game designer must think in terms of a complete working simplified system in ways the writer does not. This is certainly not a bad thing when one considers the goal of designers is to entertain and interest players, After all focused games fare better than those with tacked on elements that do not contribute to the whole.III.All this may seem fairly obvious, but there is an important point of criticism here that is not always fully appreciated. When trying to understand why an element of a simulation exists in the way it does and what it suggests about attitudes towards the past—whether why Colonization codes native peoples the way it does, why Civilization does not deal with social issues in cities, or why East India Company does not treat the tensions between English and Indian customs—one needs to consider holistically the problem space selected by the designers. First off, one must consider the goals and roles of the human players set out by the design plan. Certainly in the real world there can be many agents in a problem space with different roles and goals that can complement, conflict, or have little bearing on one another. Simulation games, too, can represent multiple agents with varying roles and goals. In single-player games these additional roles are handled by the program’s artificial intelligence routines. In multi-player games human players can take on additional roles. Generally speaking however—and I welcome examples where this is not the case—simulation games, especially pleasurable and/or commercially successful ones must commit to a very small set of roles and goals, often one role and one goal. Even where roles and goals differ and conflict they tend to be set up as binary opposites or at least draw from the same well of resources and affordances. So both sides may want to hold a territory or win an election, one group may want independence while the other wants centralization, the city ruler wants profits while the citizens want material niceties, etc. This is in large part, again, because games must be closed functioning systems: each part must connect to every other part. So a game cannot represent roles and goals well that do not fit into the core choices, affordances, and constraints of the chosen problem space. Therefore the commitment to a particular articulation of a problem space will shape every other aspect of the game and any analysis of an element of the game, not least of all an agent, must consider the framework of the problem space.Let’s apply bits of this theory to a concrete example. Hegemony: Philip of Macedon by Longbow Games, a real time kingdom management / strategic and tactical war game hybrid. The player assumes the role of Philip, the fourth century king who unified Macedonia, subjugated Greece, and, in the process, built the formidable military system that his son, Alexander, would use to conquer the Persian empire. Consider the portrayal of slavery in the game. When one defeats an enemy unit, the survivors can be captured and enslaved. If the survivors are not captured within a short time, they will escape. Slaves can be used to work mines, perform general construction tasks, and transport food supplies. Left untended it is possible for slaves to escape. Now, this is a reasonable sketch of aspects of slavery in the ancient world, not least of all the matter-of-fact nature of a system we find repugnant today.Suppose, however, one wanted to criticize formally this historical representation of slaves. One might start by noting that these slaves have very little agency. Granted, they have the goal of escaping in this problem space and can do so if left unattended for too long. This goal does little more than add a constraint to the player’s problem space, a reason to take care attending to slaves and spending resources on watchtowers. This is not much of a depiction of agency at all and so slaves become nothing more than affordances, resources for the player to exploit in the game. This sort of portrayal might inspire comment and critique that even enslaved people had agency beyond escape, the ability to make choices and have a degree of ownership over their lives despite the horrible constraints of their status. (I recollect from my limited work with U.S. history that this sort of development took place in the historiography of American slavery, from first not studying slaves to studying slaves as victims, to studying slaves as agents while still recognizing the oppression of enslavement).Why does the game not portray the agency of slaves? How Longbow defined the primary problem space, the human player’s problem space, is a critical answer. For the player Philip king of Macedon is the role with a goal of uniting Macedonia and building a Balkan empire. With this role and goal driving the articulation of the problem space, depicting slaves in the game as affordances is fully understandable. One could attempt to flesh out the slaves feelings on their situation, but it is difficult to see how that would fit into the mechanics of this particular problem space, the one the designers chose.It is important to note, however, that saying a portrayal of ancient slaves, native Americans, Hessian mercenaries, railroad barons or any other agent or aspect of the past, takes the form it does because of the problem space is not meant to be a tactic for ending discussion or defending an implementation (One could imagine such a chilling effect: “why are they portrayed this way? Because the problem space demanded it. Oh … okay, so what’s for lunch?”). It is meant to focus criticism on a game holistically and consider how the affordances and constraints of the simulation game medium and the interests and goals of a game’s creators (their concerns, assumptions, hopes, attitudes, what have you) shape a game’s interpretation of the past. At the risk of being too meta, but in all seriousness, one really needs to consider the problem space of the game designers when considering the elements of the simulation they designed. Once the commitment has been made to make a commercial simulation game, as opposed to using any other medium of interpreting the past, the affordances and constraints of conceptualizing history as problem spaces place great pressure on the final product.So, what kinds of questions might one ask of a simulation game as a problem space and what kinds of meaningful criticisms/evaluations can be make? A few, necessarily incomplete suggestions:One might meaningfully question why the particular main roles and goals for the game were selected in the same way one can meaningfully question why certain generations of historians privileged one set of topics and questions over another. Indeed meaningful answers to such questions can be given based on careful research of prevailing ideas at the time. Simulation games, for example, tend to be inclined to issues of domination whether in political, military, or economic forms – discussing why this is continues to be a lively debate.One absolutely should question whether the roles and goals selected for the players are historically legitimate. In other words, do they reflect what our evidence suggests were some important roles and goals in the past? There is little question that Philip wanted to dominate the Balkans. In other cases, such as Colonization where the goal, as stated on the Firaxis page is for colonists to “negotiate, trade and fight as they acquire great power” http://www.2kgames.com/civ4/colonization/ one might very well explore the cases in the past where this articulation of goals was and was not valid. That’s a great conversation to have, and it has great bearing on the validity of each element in the game’s interpretation of the past.One can rightfully question why each and every element of the game is portrayed as it is. But these questions should not be divorced from the consideration of the problem space as a whole, especially the historical roles and goals conceptualized by the designers. A thorough consideration of why slaves are mere tools in Hegemony, happiness is the defining metric for success in CivCity: Rome, Indian culture is not represented in East India Company, or any other element in any game, should consider the goals set out for the game and the supporting game mechanics to be thoroughly compelling.So, suppose that one accepts the roles and goals of a game as historically valid goals, i.e. goals that reasonably represent what good evidence suggests motivated some peoples of the past. That might well mean that a thorough challenge to the portrayal of some historical agents in the game could only be made by suggesting that:a) That the agents could not reasonably be conceived to play that role in the problem space from the point of view of the player, the primary agentb) By suggesting what more legitimate roles the agent could have played in the game that would mesh into a system, or at the very least not conflict with, with the player’s roles and goals in the problem space. Considerations of this sort need to be very aware to the developer’s presumed goal to create a playable, enjoyable, and commercially viable game.c) (A variant of b) By suggesting what roles and goals non-player agents in the game could have played that would have worked in a system centered on the player’s role and goals.Note: The day before posting I was browsing through old PtP posts I had missed, and noted happily that Trevor made comment in this spirit on Mark’s essay on Detroit. Trevor essentially agreed Mark’s critique of the absence of race in SimCity/Micropolis is valid, but asked him to suggest how he would have race represented in the game.So challenging the portrayal of slaves in Hegemony, if I accept the historical validity of the role and goals (which I do), would require suggesting how slaves could have been portrayed more complexly and validly within the defined problem space, how they could have had a greater portrayal of agency through expanded roles and goals. This certainly can be done. Consider another nation management / strategy game, Paradox Interactive’s Europa Universalis III. One of the problems the player-ruler faces is nationalist revolts. When a nationalist revolt erupts in a territory, the rebel units besiege the territory and, if unchecked, ultimately seize control of it. When the nationalists seize the province they preventing the player from taxing it, drawing troops from it, or administering it. (Note I was mistaken in the characterization of the next few sentences. Check out Liam’s Burke’s comment below. Fortunately, my point was to illustrate the structure of a critique, not levy a challenge against EU III) What the nationalists do not do, however, is actually create an independent state, i.e. a functioning state along the lines of the hundreds of others in the game. This limits the nationalists’ agency severely, making them serve as nothing more than a constraint, rather than a fleshed out agent. Indeed once the player can commit the necessary troops to reconquering the province, it will be reintegrated into the kingdom. The designers presumably could have easily made successful nationalist revolts result in the creation of new small states; there are many states in the game, after all who only control one province. Doing so would have represented the agency of nationalists and the goals of nationalist revolts more realistically, rather than the current state of affairs where they simply lock down a player province. Such a change could mesh well with the game’s overall problem space—though there may still have been compelling design reasons not to have done so.On a final note, it is not my intention to criticize existing game analyses, or suggest that I have fully practiced these ideals — I’m quite sure my critique of CivCity: Rome from last year did not offer specific suggestions for remodeling the game. Nor am I suggesting that thinking in terms of problem spaces is the only meaningful way to conceptualize simulation games. I’m simply reaffirming that simulation games are human interpretations of the past subject to certain constraints, as sources and media they should be considered holistically, and this can be done by thinking in terms of problems spaces. Next week, I’ll add a shorter post considering how the concept of problem spaces and the use of simulations to illustrate problem spaces can be very effective in the classroom. In the meantime, I hope this will generate some comments; there is a lot more to be said on these matters. Collaboratively edited and authored, Play the Past is dedicated to thoughtfully exploring and discussing the intersection of cultural heritage (very broadly defined) and games/meaningful play (equally broadly defined). Play the Past contributors come from a wide variety of backgrounds, domains, perspectives, and motivations (for being interested in both games and cultural heritage) – a fact which is evident in the wide variety of topics we tackle in our posts.It is very important to note that Play the Past isn’t just about the intersection of cultural heritage and digital games, its also about non-digital games (boardgames, tabletop games, collectible card games, etc.), alternate reality games (ARGs), barely games (a term originally coined by Russel Davies – no, not the Doctor Who Russel Davies – and built upon by our very own Rob McDougall), and playful mechanics (or “gamifying” as its been recently called).We are also very interested in exploring the spectrum of approaches to games – from the more “philosophical” (as some might call it) games studies side of things, to the more practically applied serious games/meaningful play side of things (and just about everything betwixt and between).Inspired by ProfhackerCredit where credit is due. One of the most important inspirations for Play the Past comes from directly ProfHacker. You can see the fingerprints of ProfHacker all over Play the Past. From the way we do business behind the scenes to our commenting and community policy (which is pretty much shamelessly lifted verbatim from ProfHacker). This is no great surprise as Ethan Watrall (Play the Past’s editor) was in the first batch of ProfHacker writers. In this regard, we are extremely grateful to ProfHacker – and particularly ProfHacker’s two Editors: George H. Williams (@georgeonline on Twitter) and Jason B. Jones (@jbj on Twitter).Interested in Contributing?Interested in contributing to Play the Past (either on a regular basis or as a “one shot” guest author)? We’d love to hear from you. Drop us a line here and tell us a little bit about yourself and how you might contribute.We are committed to fostering an environment characterized by generosity, creativity, and (as corny as it might sound) kindness. Comments on this blog are an important part of creating that environment, and this comment policy aims to communicate our values to new readers and encourage comments that will build up the online community here.Thoughtful comments (even when–and often especially if–disagreeing) are encouraged and appreciated.No snark allowed (see David Denby on definition of snark). While snark certainly has its virtues, this blog provides a space for people to be inexperienced at something, or even wrong, to facilitate learning. That’s harder to do in the face of either persistent or “drive-by” snark.Play the Past should be a community built through regular contributions made by recognized–but not necessarily “real name”–contributors. Some commenters’ identities reveal their real names; other commenters use pseudonyms. Our online identities are built from our comments here and our presence–as commenters and authors–in other places on the web, in print, at conferences. Play the Past welcomes commenters–whether anonymous, psuedonymous, or publically identified–who are committed to creating a rich and respectful dialogue. We want commenters to be able to explore the complexities of Play the Past posts; we want commenters to inquire and debate; we want everyone to be able to learn from the conversation.Links & images are encouraged. Gratuitous linking back to your own site is discouraged. Links in the “website” field should point only to profile pages or to personal websites.Gravatars are strongly encouraged.Jeremy AntleyJeremy Antley is a writer/student/gamer who received his MA in History with a focus on the Russian Imperial period from the University of Kansas in 2007. While currently in the middle of researching the immigration of Russian Old Believers to Oregon in the mid-60′s for his doctoral ambitions, Jeremy also finds studying the life and culture of Russian peasants to be a fascinating topic. He looks at topics of digital culture, games and, of course, Russian history at his blog, Peasant Muse. Those interested in hearing the twitter ramblings of someone crazy enough to love Russian peasants would do well to check-out Jeremy’s handle- @jsantley. Having played many epic games of Axis & Allies as an undergraduate, Jeremy now plays a variety of board and console (xbox 360) games with his friends in his current residence of Portland, Oregon. Jeremy’s love of board games informs his current interest in how players modify their games and how looking at board games as platforms can inform historical inquiries.Kevin BallestriniKevin Ballestrini teaches Latin and Mythology at the Norwich Free Academy in Connecticut. He has received an M.A. and B.A. in Classics from the University of Colorado and University of Connecticut respectively. In addition to experience teaching in a traditional classroom setting, Kevin is deploying the first fully practomimetic introductory language course at the high school level this year in a section of Latin I. He hopes that the experience will enhance student engagement and connection to life and culture in ancient Rome. As an avid technology enthusiast, he maintains his blog, Techna Virumque Cano (http://kevinbal.blogspot.com) where he discusses the intersection of technology and his teaching. Kevin is also the leader of a Lord of the Rings Online kinship and has a great interest in exploring how games contribute to the development of (online) communities just as the bardic tradition contributed to the development of ancient communities. You can also find Kevin on Twitter at http://www.twitter.com/kballestrini.Emily Joy BembeneckEmily Bembeneck is a Ph.D. candidate at the University of Michigan in the Department of Classical Studies. She primarily works on narrative and character development in ancient epic and modern video games, both graphical and text-based. Other interests of hers include the image of the hero, Greek tragedy and social catharsis, cultural and individual identity through play, immersion, and game design, She teaches classes in Latin, Greek and Roman history, ancient war and entertainment, among other things related to the Classical world She is an active contributor at http://www.greywardens.com where she writes on the narrative structure of Bioware’s Dragon Age RPG franchise. Her current projects include working on images of Rome in ancient and modern culture, developing a Flash-based application that combines components of social play with narrative creation, and designing a Dragon Age module that explores Euripides’ Medea through post-primary narrative. When not being all academic and studious, she is likely either playing an elf in some virtual world or spending time with her two young sons. Find her online at Ada Play and on Twitter at http://www.twitter.com/adarel.Andrew D. DevenneyAndrew D. Devenney is currently a Visiting Assistant Professor of World History at Grand Valley State University in Allendale, Michigan. A life-long and avid gamer of both video and role-playing games, Andrew has found elements of game-based learning, new media, and the digital humanities unconsciously seeping into his classroom over the last few years. As such, he has begun to experiment relentlessly on his students with these new ideas, techniques, and shiny toys, and is having a grand time doing it. Sometime in the relatively near future, Andrew hopes to deploy a new course that will explore global history through the medium of gaming (which means the class may very well morph into a giant, heavily modded, credit-based game of Dungeons and Dragons). Andrew can be found online at http://andrewdevenney.net and on Twitter as @adevenney.Shawn GrahamShawn Graham is Assistant Professor of Digital Humanities in the Department of History at Carleton University in Ottawa, Canada. He’s been blogging as ‘The Electric Archaeologist‘ since 2007, documenting his interest & experiments in game-based learning, agent based modeling, and other aspects of digital media for archaeological teaching and research. On Twitter, he’s at http://www.twitter.com/electricarchaeo. He’s published amongst other things a number of agent-based simulations on aspects of the Roman world, and has explored using Civilization mods in his distance-education classrooms. He received his PhD in Archaeology from the University of Reading in 2002, where he was interested in complexity & evolving networks in antiquity (especially in Rome). When he’s not geeking out over the latest tech toys or things archaeological, he is chief cider maker at Coronation Hall Cider Mills and playing Wii games with his family. If only there was wiiCivilization…Matthew KirschenbaumMatthew Kirschenbaum is Associate Professor in the Department of English at the University of Maryland, Associate Director of the Maryland Institute for Technology in the Humanities (MITH, an applied thinktank for the digital humanities), and Director of Digital Cultures and Creativity, a new “living/learning” program in the Honors College. Kirschenbaum speaks and writes often on topics in the digital humanities and new media; his work has received coverage in the Atlantic, New York Times, National Public Radio, Wired, Boing Boing, Slashdot, and the Chronicle of Higher Education. He has been pushing cardboard counters around on hexagonal grids since his early teens. See http://www.mkirschenbaum.net for more.Katy MeyersKaty Meyers is a graduate student in the Department of Anthropology,with a concentration in mortuary archaeology, at Michigan StateUniversity. She is currently a fellow in both the Cultural Heritage Informatics Initiative and the Campus Archaeology Program. She is also an editor of GradHacker, and the game designer for an Ancient Egyptbased mod of Civilization V. When she somehow finds free time she loves playing first person shooter and adventure video games, and is amajor fan of Milton-Bradley board game classics. She’s interested inthe role of games for outreach and education in archaeology and cultural heritage, as well as the biases they create. Katy can befound online at her blog http://www.bonesdontlie.com and on Twitter at @bonesdonotlie.Jeremiah McCallJeremiah McCall has been teaching high school history for the past decade, mostly at Cincinnati Country Day School. His first professional love is high school teaching, especially designing instructional strategies that will engage and challenge his students to learn and grow. In addition to more conventional courses, Jeremiah also teaches senior elective on (tabletop) historical simulation design, and the intersection of serious games and contemporary global issues. Jeremiah’s primary training is in history with a PhD in ancient history from Ohio State University; he authored a book on the cavalry of the Roman Republic and is currently writing a historical biography of the Roman aristocrat M. Claudius Marcellus to be published by Pen and Sword press. He has recently completed a guidebook for teachers who wish to use simulation games in the history class. Titled Gaming the Past: Using Video Games to Teach Secondary History, the book was published by Routledge in June, 2011. As an extension of his teaching philosophy — that history is primarily the study and evaluation of competing interpretations of the past — McCall has conducted numerous classroom implementations of historical simulations as historical interpretations. He maintains the website gamingthepast.net, one of the primary sites devoted to the use of historical simulations in classroom teaching. He also plays far more video games — particularly RPGs and strategy — than you’d think he could find time for.Rob MacDougallRob MacDougall is Assistant Professor of History at the University of Western Ontario and Associate Director of UWO’s Centre for American Studies, where he teaches United States history, the history of technology, and digital history methods. His research centers on the history of communication–he has just written a book on the early days of the telephone and is beginning a new book on the circulation of bad ideas. He blogs sporadically at Old is the New New and is on the Twitter at http://twitter.com/robotnik. A life-long gamer, Rob is interested in the history of gaming and in using games and play to encourage more playful historical thinking. His own play is largely non-digital of late–tabletop RPGs with his gamer buddies and make-believe with his kids–but he looks forward to seeing how good computer games will have gotten by the time he gets tenure.Rebecca MirRebecca Mir is a museum educator, researcher, and writer at various institutions in New York City. She will receive an M.A. in Decorative Arts, Design History, and Material Culture from the Bard Graduate Center in May 2012. She received her Bachelor’s degree in Art History from Indiana University of Pennsylvania in 2010. She often thinks about how (and why) artifacts and cultures are (mis)represented in video games and how museums are using games to engage and educate audiences. Rebecca can be found on Twitter as @hellenophile.Trevor OwensTrevor Owens is a digital archivist with the National Digital Information Infrastructure and Preservation Program (NDIIP) at the Library of Congress and a doctoral student in the Graduate School of Education at George Mason University. Before joining the Library of Congress he worked as the community lead for the Zotero project at the Center for History and New Media helped organize the first two meetings of the Games, Learning, and Society Conference. He received a bachelors degree in the history of science form the University of Wisconsin, and a masters degree in American History with and emphasis on digital history from George Mason University. Trevor has spent considerable amounts of time playing all iterations of Civilization, but is also a big fan of role playing games (everything from Earthbound to Fallout). Trevor has published on the history of children’s books about Einstein and Curie, the discursive practices of Civ Modders, and the role of digital research tools in scholarship and teaching. When not researching, writing, or gaming, he also enjoys playing the violin. Trevor can be found online at http://www.trevorowens.org and on Twitter at http://www.twitter.com/tjowensRoger TravisRoger Travis is an Associate Professor of Classics in the Department of Literatures, Cultures & Languages of the University of Connecticut. He is also the Director of the Video Games and Human Values Initiative at UConn, an interdisciplinary online nexus for scholarly activities like monthly symposia and “playversations.” He received his Bachelor’s degree in classics from Harvard College, and his Ph.D. in comparative literature from the University of California, Berkeley before arriving at UConn in 1997. He has published on Homeric epic, Greek tragedy, Greek historiography, the 19th C. British novel, HALO, and the massively-multiplayer online role-playing game He has been President of the Classical Association of New England and of the Classical Association of Connecticut. He writes the blog Living Epic about the fundamental connection between ancient epic and the narrative video game, and is a founder and contributor of the collaborative blog Play the Past. In the 2009-2010 academic year, Roger offered the first courses ever designed entirely as practomimes.Mark SampleMark Sample is an Assistant Professor of contemporary literature and new media studies in the Department of English at George Mason University. In addition to his work on electronic literature, videogames, and code studies, Mark is an outspoken advocate of open source pedagogy and open source research. In recognition of his commitment to innovation in teaching, Mark was the recipient of George Mason’s 2010 Teaching Excellence Award. Mark is a regular contributor to ProfHacker, and can also be found online at samplereality.com or on Twitter as @samplereality.Ethan WatrallEthan Watrall is an Assistant Professor in the Department of Anthropology and Associate Director of Matrix:The Center for Humane Arts, Letters & Social Sciences Online at Michigan State University. In addition, Ethan is a Principal Investigator in the Games for Entertainment & Learning Lab, and co- founder of both the undergraduate Specialization and Game Design Development and the MA in Serious Game Design at Michigan State University. Ethan teaches (and has taught) in a wide variety of areas including cultural heritage informatics, ancient Egyptian social history & archaeology, archaeology and pop culture, user centered & user experience design, game design, serious game design, game studies. and history of various forms of popular and entertainment media (comics and digital games to name a few). When he’s not being all professorial, he’s a world class comic book nerd (Killowog is so his favorite Green Lantern), a sci-fi dork (he’ll argue to the grave that Tom Baker is the best Doctor ever), and an avid player of all sorts of games (digital, board, and tabletop). Ethan can be found online at http://www.captainprimate.com, and on Twitter at http://www.twitter.com/captain_primateMarch 14th, 2012Over at Off Paper (the arts journal for Seattle’s Project Room), I have a new piece titled, “Making Things in the Digital Humanities.”The piece is my attempt at not only explaining DH research to an arts audience but also articulating a kind of making that doesn’t imply mastery, do-it-yourself individualism, or a writing-building dichotomy. Plus, it’s an opportunity to stress the transformation of writing in higher education and elsewhere.Feedback welcome!A colleague drew my attention to NicolaOsborne’s liveblog of the very interesting event at the University of Edinburgh on 24 February 2012, Digital Scholarship: A Day of Ideas. It is wonderful to see that Edinburgh University, which, through EDINA and other activities, has made such important contributions to the growth of digital scholarship over the years, is continuing to develop new initiatives – the appointment by Edinburgh of a Dean for Digital Scholarship is particularly noteworthy. Mind you, it must be admitted that the idea of digital scholarship creates some problems. One worry that constantly nags away at me is whether we should privilege the digital in the way that we do. Other technologies have the capacity radically to transform humanities scholarship – it is possible, for example, that nanotechnologies, by offering new approaches to the conservation of cultural heritage, have just as much to offer the humanities scholar and curator as the digital. Perhaps we should be thinking as much about nano-scholarship as digital scholarship. Certainly, it would be worrying if humanities scholars restrict their engagement with technology to the digital.My reflections on the day derive from Nicola Osborne’s liveblogging, so apologies in advance if I consequently get hold of the wrong end of the stick here and there. Melissa Terras in her presentation evidently demonstrated very vividly the way in which digital resources and tools have transformed the practice of humanities scholars over the past twenty years. Yet, as Mel pointed out, there is a fundamental dilemma here. The most important developments have not been driven by scholars, but by libraries and commercial publishers. The resources which have become indispensable to humanities scholars are commercial packages such as Eighteenth Century Collections Online, Early English Books Online or the British Library’s Newspapers Archive. As Laura Mandell has emphasized, these resources are designed on the model of the library microfilm surrogate, intended to enhance public access to library collections. Their searchability is very limited, and they are often positively misleading. The digital humanities, as formally constituted through centres such as UCLDH and the Department of Digital Humanities at King’s College London, have been largely irrelevant to this process, and have general not produced any resources which have had the same impact on scholarship as these commercial packages. If a scholar working on the early nineteenth century was asked to choose whether funding should be used to secure access to The Times Digital Archive or to continue a project like Transcribe Bentham, most would unhesitatingly choose The Times.One of the most urgent issues for digital scholarship (as Laura Mandell has emphasized) is to develop closer links with librarians and publishers to influence the creation of these commercial packages. But it seems that, floating around the Edinburgh day, was the suggestion that a far more critical issue is one of infrastructure. In particular, in both Jane Ohlmeyer’s presentation and in some questions after Mel’s presentation, seemed to be flickering the idea that in some the United Kingdom had taken a step backwards by ending funding for the Arts and Humanities Data Service and by not taking a sufficiently active role in the European Dariah initiative. The way forward, it appears to be suggested, is to invest in infrastructure, and the UK, it is also suggested, has characteristically thrown away an early lead in this area.I must admit instinctive nervousness about infrastructure, which can only be explained in terms of my own engagement with digital developments. The British Library, through its Initiatives for Access programme between 1994 and 1997, established a remarkable portfolio of pioneering digital projects, including Portico, the first British Library website, the Burney Newspapers digitisation, Turning the Pages and Electronic Beowulf. Indeed, in many ways this programme explored almost all of the aspects of the technology which have subsequently preoccupied us. There was clearly a need for these activities to build closer links with work in other sectors, particularly that of the JISC, but in many ways the British Library mapped out through Initiatives for Access a remarkable template for future. But, at the conclusion of the programme, the Library’s view was that the most pressing need was investment in technical infrastructure, particularly given the Library’s ambition to embark on legal deposit of electronic resources. So, a large and ultimately unsuccessful programme to procure a major piece of new technical infrastructure was put in place, and the British Library threw away the advances that it had made in the 1990s. It is striking that the British Library has since failed to exercise any decisive leadership on the development of wider digital scholarship in the humanities. There have been some interesting developments, such as the UK Sound Map or the Codex Sinaiticus project, but the major British Library developments have been through commercial partnerships, as with the newspapers. Digital scholarship has been sacrificed for infrastructure.For scholars working within British universities, the three most important pieces of infrastructure provided by the universities themselves are barely mentioned or discussed. These are: the network provision through JANET; the collective licensing of commercial digital packages through JISC Collections; and the NESLI2 licensing of access to online journals which (although extraordinarily complex and often fraught) provides access to the enormously expensive journal packages of publishers such as Elsevier or Wiley-Blackwell. Take any of these components away, and the digital revolution described by Melissa would disappear overnight. These provisions are not cheap – online journal provision for a publisher such as Wiley-Blackwell can still easily cost a university library over a million pounds a year, even with the discounts negotiated through the NESLI2 agreements. All British universities libraries spend the bulk of their acquisition budgets on the provision of online resources – a fact of which most academic users seem blissfully and happily unaware. However, these pieces of infrastructure bring enormous benefit, one of the most important of which is an equality of provision across UK universities. I feel strongly about this, because while I was Librarian at the small university in Lampeter, the excellent work of JISC Collections meant that I could easily build up a portfolio of electronic resources which, in those areas taught at Lampeter, bore comparison with much larger universities. Contrast this with the appalling situation in the United States, where very large and wealthy universities can afford a huge range of subscriptions to electronic resources but smaller colleges have nothing at all. For example, look at the library catalogue of St Vincent’s College in Pennsylvania, an excellent Catholic liberal arts college very similar to Lampeter, which lists just four very limited electronic resources. This is an immense digital divide, which the work of the JISC has ensured does not occur to the same degree in Britain (although there are exceptions, such as Parker on the Web, where the scandalous refusal of Stanford University Library to agree terms with JISC Collections mean that this important resopurce is unavailable in most British universities) .At this level, Britain possesses an infrastructure which has successfully fostered digital scholarship in the arts and humanities. In this context, clearly the most important priority is to protect this infrastructure, and it is an infrastructure which is under threat. It is a constant struggle to negotiate affordable rates for journal subscriptions. In this sense, the recent Elsevier boycott is beside the point – many other publishers, such as most notably Wiley-Blackwell, are equally culpable of what is really nothing more than exploiting monopoly advantages. In the current financial situation, sooner or later, some British universities will not be able to afford to continue subscriptions to journals or databases which scholars have become dependent on. What happens to the digital revolution then?These seem to me more pressing structures of infrastructure than refighting old battles about the Arts and Humanities Data Service. The withdrawal of funding was very sad, and at King’s College London we are enormously proud of the way in which Sheila Anderson and her colleagues have created from the Arts and Humanities Data Service a tremendously successful Centre for eResearch which in many ways is pushing forward new methods of researching in even more remarkable ways than under the AHDS. The Arts and Humanities Data Service was established in the mid 1990s at a time when it was assumed that much of the creation of digital content would occur within universities and that some kind of infrastructure was necessary to facilitate this. AHDS was also intended to promote awareness of the need for sustainable standards. AHDS was certainly successful in ensuring this attention to appropriate standards, and some former components of the AHDS, such as the History Data Service, Archaeology Data Service and the Oxford Text Archive, have flourished notwithstanding the loss of central AHDS funding, and still perform this function very successfully. But the question must be asked – is the vision of large quantities of university-created digital content requiring central curation still the most pressing issue? Isn't this a vision more appropriate to 1995? David Robey has consistently stressed how the AHRC is still funding many research projects which have a digital content. A list of some of these is available here.So the loss of funding for the AHDS has not inhibited the engagement of humanities scholars with digital methods – do we in fact need that sort of function at all? Most of the projects funded by the AHRC of this kind represent small-scale activities of the type which Andrew Green of the National Library of Wales has called ‘boutique digitisation’. Andrew argues that the major issues for both humanities scholars arise from the impact of commercial initiatives such as Google Books, and I feel sure that, in thinking about infrastructure, it is these wider more strategic concerns which are more pressing than whether we have data repositories to support small-scale low impacts projects such as Transcribe Bentham or the 1641 Depositions in Ireland (and, yes, I would include projects like Electronic Beowulf in this category as well). We need infrastructures which will instead address our dangerous dependence on commercial initiatives by a range of companies from Gale to Google.In this context, I think it could be argued that the United Kingdom is in fact now pointing the way forward in terms of the necessary infrastructure in a rapidly changing digital environment more effectively than through models like the AHDS, which reflected the situation in the mid 1990s. Indeed (and I’ve hesitated in case I am being unduly chauvinistic here, but I don’t think I am) I think there is a danger in the UK being asked to turn the clock back by those who are only just reaching the stage that Britain reached fifteen or more years ago.One important point to bear in mind is that Britain today is very different to Britain fifteen years ago. Having worked in Wales, Scotland and different parts of England, the impact of the creation of an increasingly devolved country (or countries) is one of the most important issues. It is indeed striking that an Irish scholar speaking in Scotland refers to what Raymond Williams called ‘the Yookay’ as if it was a single entity, which it is no longer. The AHDS reflected an assumption about the type of infrastructure for Great Britain which was appropriate in London in 1995 but no longer fits Great Britain today. Many of the most important recent initiatives have stemmed from the devolved nations where national governments have been undertaking interesting investment in digital infrastructure. For Wales, I have described some of these in my recent article for Lorna Hughes’s book on Evaluating and Measuring the Value, Use and Impact of Digital Collections. An activity like the Welsh Journals Online project, partly funded by the Welsh Assembly Government and co-ordinated by the National Library of Wales, is, as Andrew Green has described, consciously designed to seek to counter the baleful cultural effects of a project like Google Books. Likewise, the People’s Collection, which was a manifesto commitment of the last Welsh government, anticipates on a smaller scale many of the features on the major Digital Public Library of America which looks likely to be the single most transformative piece of infrastructural work for digital scholarship over the next few years. Catcymru, providing integrated access to all Welsh libraries, is again a remarkable illustration of the way in which the evolved nations are using digital infrastructures to create a new sense of national identity.A similar story could be told in Scotland, where for example Scottish universities have been pioneering new methods of securing joint access to on-line journals and databases, and the National Library of Scotland has been working with Scottish universities to develop projects to provide (for example) on-line access to collections of maps. But where it seems to me that in Britain we are engaging with more current issues of concern than is evident from Jane Ohlmeyer’s description of the situation in Ireland is our awareness of the issues posed by the commercialization of digital scholarship, of which Ireland seems blissfully unaware. It is in the initiatives that are responding to this threat that Britain is still taking a lead. Open access is an obvious area, and the work of the JISC is promoting awareness of the issues around open access has been critical here. There are rumours (no more than that, sadly) that a ruling will be made that only work available on open access may be submitted to the REF, and no more important measure could be taken to support digital scholarship at the moment than the implementation of such a ruling. Repositories remain a key tool for tackling such issues, and again I think Britain has been leading the way here – one of the most interesting projects with which I have ever been involved was the Welsh Repositories Network, which ensured that every university in Wales has an open access institutional repository – that’s an amazing achievement, and needs to be trumpeted more. Other projects which I think point the way forward more firmly than hankering after the AHDS include the British Library’s Ethos project to make doctoral dissertations more easily available. We might also point to recent work on research integrity which seeks to link data curation more closely to the research process, and support for these initiatives is perhaps more pressing than worrying about storage services.In short, the issues confronting digital scholarship in the humanities are less to do with the storage and curation of data and much more to with creating models which resist the commercialisation and commodification of knowledge, and save us from the maw of companies like Microsoft and IBM. Here, I believe Britain still continues to point the way. It is very tempting to feel that digital issues an be resolved by the purchase of a splendid piece of kit, and I worry that such an instinct too often pervades our thinking about infrastructure. In the sciences, such large initiatives are often tied to research questions which cannot be addressed without major investment in equipment: think Hadron Collider, Square Kilometre Array, Diamond Light Source. But in the humanities our thinking about infrastructure is too often disconnected from research issues. We worry about creating services. Maybe we shouldn’t.Ted Underwood’s recentposts about literary and non-literary diction between 1700-1900, and the various discussions they sparked, including Katherine Harris’s post on gender and DH archives, have had me thinking a lot about cultural poetics and the middle distance.In 19th-century studies, most DH projects have tended to operate at two different scales: large-scale text analysis projects (associated with so-called “distant reading”) and curated editorial or archival projects, which have mostly been focused on works by or about particular individuals. Such archives extend and sometimes transform print-form editorial theory and practice, but tend to have an author as the center of gravity, around whom particular material forms (manuscripts, books, images, etc) cluster. In truly large scale text analysis, on the other hand, the linguistic text is usually separated from the material forms in which it circulated and the individual author tends to disappear, in favor of pursuing questions about diction and linguistic variance.I’m interested in exploring how DH tools and methods could be brought to bear on the kinds of research that take place in between those poles, in the middle distance.Cultural poetics (or, to use Bourdieu’s term, sociological poetics) is concerned with the forms literature takes in the world: the modes and methods by which texts are produced, consumed, understood, repurposed, and valued by individuals and institutions. The digitization of 19th-century texts offers us tremendous resources for exploring these topics, but there’s lots of work yet to be done in theorizing and demonstrating ways to pursue this through digital analysis.Thinking About GenreGenre is a property of texts and of literary works. Genre is not a feature of books or other print forms. But information about a text’s genre is encoded in various aspects of the material book, such as tables of contents, chapter or section titles, running heads, page design and layout, typographical choices, page size, paper type, etc. To understand how a genre functioned at a particular historical moment, we need to examine how it circulated in the world in its various material instances.That’s why I’m currently working with bibliographic metadata, compiling a database of information about single-author English books of poetry published between 1840-1900. I see this as a small first step towards theorizing that middle distance and making it available for practical critical endeavors. When completed, this database will contribute to mapping the cultural field of poetry in the Victorian period. (I’m also collecting many of these books in digitized form for future analysis — but the terms of that analysis will be guided by the historical patterns revealed in the metadata.)As is true with any collection of data, the historical, material, and literary terms that I combined in defining the project constrain it in sometimes awkward and artificial ways. As soon as I started gathering data from various sources, the limitations of the term “book of poetry” became really obvious.Readers in the 19th century rarely doubted whether or not they were reading a poem, because the genre of poetry is encoded through a variety of linguistic, textual, graphic, and paratextual means. But the 19th-century books (and periodicals) that contained those poems were frequently multi-generic in ways that literary studies rarely wrestles with. Posthumous collections of a writer’s works might contain poems along with essays or stories. Biographies or memoirs about an individual frequently included collections of poetry which might or might not have been previously published. Subsequent revised editions of a published collection might include an author’s reminiscences or other material.As much as I’m looking forward to getting to analyze this set of data, I’ve found that working through a series of decisions about “what counts” and how best to collect it has helped me better understand my theoretical and critical commitments. There’s no perfect, clean, complete set of textual data (as Lisa Rhody recently pointed out) –not only because of technical and resource limitations, but because that’s the textual condition itself: messy, complicated, and ever-changing.In other words, I’m really glad that “single-author books of poetry” have proved to be somewhat difficult to locate. It is in the interstices of our logical and historical categories that we can begin to perceive our critical blind spots.Last week I attended the Association of American Geographers Annual Conference and heard a talk by Robert Groves, Director of the US Census Bureau. Aside the impressiveness of the bureau’s work I was struck by how Groves conceived of visualisations as requiring either fast thinking or slow thinking. Fast thinking data visualisations offer a clear message without the need for the viewer to spend more than a few seconds exploring them. These tend to be much simpler in appearance, such as my map of the distance that London Underground trains travel during rush hour.The explicit message of this map is that surprisingly large distances are covered across the network and that the Central Line rolling stock travels furthest. It is up to the reader to work out why this may be the case. Slow thinking maps require the viewer to think a little harder. These can range from more complex or unfamiliar ways of projecting the world as demonstrated by this gridded population cartogram of Africa from Benjamin Hennig’s PhD thesisor the seemingly impenetrable (from a distance at least), but wonderfully intricate hand drawn work of Steven Walter (click image for interactive version).I have seen bad examples of both slow thinking and fast thinking maps but there is undoubtedly more rubbish in the latter category. I blame the rise of infographics in addition to the increasing ease with which data can be mapped (I note, this latter point has also facilitated many great maps). It’s not all bad though, much like tabloid newspaper headlines I think clever fast thinking visualisations have required a lot of slow thinking by their creators and are good for portraying simple but important messages. My concern, however, is that slow thinking data visualisations are on the decline, especially online, because they do not grab the attention of potential viewers quickly enough or a similar impact (in terms of internet traffic) can be achieved with less data processing or, in the case of geography, cartographic flair. In addition there is a (perhaps legitimate) fear that producing complicated visualisations will intimidate or confuse readers. This latter point is important in the context of the media and is a problem the New York Times Graphics Department (also at the AAG conference) seem to have grappled with. Their approach is to introduce the unfamiliar or complex alongside graphics people are used to. This was done to excellent effect with their 2008 election coverage- look out for the cartogram towards the bottom.So do the renowned folks at the NY Times Graphics Dept. prefer fast or slow thinking visualisations? I asked them what they think makes a successful map. Archie Tse said what I hoped he would: the best maps readable, or interpretable, at a number of levels. They grab interest from across the room and offer the headlines before drawing the viewer ever closer to reveal intricate detail. I think of these as rare visualisations for fast and slow thinking. The impact of such excellent maps is manifest by the popularity of atlases and why they inspire so many to become cartographers and/or travel the world. The work of David Ismus offers a classic example.From a distance (above) the road network reveals major population centres, the shading mountains and the colours the depths of water surrounding/ within the US. You want to know more and each time you look something new catches your eye.In addition Ismus offers something else- he has painstakingly visited (cartographically rather than physically) every part of the map through manual labelling. Most of the maps we see are the product of automated cartography and can therefore make sense computationally but are less intuitive to use. This relates to the final commonality in the most interesting visualisation talks I went to at the AAG – all great maps, fast or slow, “feel right” to those who created them. There is a gut instinct at work that, perhaps, cannot be taught or acted on quickly. So next time you see a map, give it the time it deserves. Is it fast, slow, or the best of both?Comments OffI want to explore the ways in which critical discourse in general and literary criticism in particular are already procedural, and what it would mean to write code to express and critique natural language discourse. The can of worms I feel I am opening has been opened before in many different contexts, going as far back as Aristotle in my estimation. We could justify my intervention by claiming that any code we could generate to express or critique natural language discourse can itself be critiqued back from a CCS point of view. The process looks something like this:Human Discourse –> Analogical Code –> Code CritiqueThe example texts have been collected under the title, Ulysse gramophone/Deux mots pour Joyce by Jacques Derrida, and published by Éditions Galilée in 1987….In these studies, both originally delivered as talks, Derrida makes several moves that made him an irresistible target for my meditations on discourse and procedure. Read Full Post HereFor the past two years I have been teaching Processing [a free and open source computer programming language] to creative writers who usually have no previous computer programming experience in college courses like this one on Digital Poetics that focuses on experimental forms of writing for the screen, and I have been consistently pleased with the outcome. Even poets who describe themselves as radical “non-techies” or outright “Luddites” are able to complete projects, thanks to the simple programming interface of Processing.In an interview, one of the creators of Processing, UCLA Professor Casey Reas explained why Processing is so accessible to complete beginners as a “low floor, high ceiling” language. He credits its success to the immediacy of feedback, which is so central to a minimalist design that features large “stop” and “play” buttons familiar to anyone who has ever used a VCR or DVD machine. As Reas explains, “You can write one line of code and see it run to see a result.” He argues that this kind of visual gratification provides an immediate “motivation to beginners” in a “medium that they care about.” Read Full Post Here.Last week I attended a meeting to discuss changes in tenure and promotion policies at UNC…. The discussion had turned to new forms of scholarship and how to assess their significance or influence. I pointed out that computer scientists seem to have solved this problem. They write software that demonstrates some new algorithm or technique, and often they will make this software available as open source software to be used by other researchers. (This is quite common in the natural language processing community, for example.) Those other researchers may know the work upon which they are building primarily through their use of the software that embodies it, rather than through reading papers. Arguably, then, the software is the primary scholarly product.So, how do these scientists assess impact in this brave new world in which software code has replaced words on paper as the medium of scholarly communication? Well, usually they write a technical report, post it on the same website that their software can be downloaded from, and kindly request that it be cited by others who publish work for which the software was used. The technical report might get “officially” published in conference proceedings, or it might not: what matters is that it provides a surrogate for the “real” scholarly product, a surrogate that fits neatly into existing networks of dissemination, citation, and evaluation.…this last idea–that digital work needed to “stand on its own”–stuck in my mind. It echoed assertions I’ve seen many times in the “hack vs. yack” DH permathread, that code “speaks for itself.” “And the more I think about it the more I am convinced that this is the wrong way to think about digital scholarship, because no scholarship, no creative and productive work, “stands on its own” or “speaks for itself.” Read Full Post Here.….A digital humanist afraid of the digital is like a scholar of French literature who is afraid of French. You can’t be a digital humanist if you don’t understand the digital. That doesn’t mean you have to be able to code any more than being a scholar of French literature means you have to be able to write French literature. You just have to be able to understand the nuances of what you’re studying and how you are studying it. Otherwise, how can you properly interpret the results?What I have learned over the years is that programming and writing are different in one important aspect: programming scales differently than literature. This one important rule is what explains much of the confusion humanists have about programming, and it’s something that programmers must learn if they are to rise above being just a programmer…..It doesn’t matter what language the developer uses to write the program, you still need to draw the right lines. You still need to find the common elements and split them off into their own libraries. You need to understand the organizing principles at play and how they impact the project.In literature, it’s not the person with the largest vocabulary who writes the best books, but the best storyteller. You can be the best coder in the world, but if you choose the wrong structure, you’ll produce a worse program than a poor coder who selects the right structure.” Read Full Post Here.CommentsLast Friday I learned on Twitter that Salman Rushdie was about to speak at Emory about the donation of his personal papers to the university archives. And due to the energetic livetweeting of Roger Whitson (@rogerwhitson) and Brian Croxall (@briancroxall) I was able to see that Mr. Rushdie had some very interesting things to say about his archives. Well, as promised the video is now posted on YouTube, and it’s worth a watch: Salman Rushdie Discusses Creativity and Digital Scholarship with Erika Farr. Here’s the description from YouTube:University Distinguished Professor Salman Rushdie and Erika Farr, digital archives coordinator in the Manuscript, Archives, and Rare Book Library (MARBL) discuss how computers and other technology affect Rushdie’s writing and creative process. This builds on previous conversations and addresses new developments such as Rushdie’s acquisition of an iPhone and the ways in which mobile computing has an impact on his work. In addition, given Rushdie’s work on his memoir and his use of his paper and digital archives in MARBL, the discussion turns to the ways in which archival science and archival access changes the way he uses his own archives.It’s about an hour long, and as I was watching it I took some notes on the parts of the discussion that might be of interest to archivists. I did my best to make my quotations accurate but it’s possible there may be some minor errors and of course I am only attempting to quickly summarize or characterize a much more complicated dialogue. If you have time, watch the video. If you don’t, here are some of my personal highlights:Rushdie on what he learned from donating his material to Emory: “the electronic material was harder to deal with than the paper . . . it was the opposite of what you would expect.” Farr asked how he used technology to assist him with his writing. He said the biggest change came when he got access to academic Internet resources through Emory. That was a big change, as public sources like Wikipedia are not always reliable. He used the example that Wikipedia says he is an only child when he actually has three sisters (“it doesn’t matter how often you go change it, someone always goes and changes it back!”). He also cited the example of online translations of rare books and other sources which he used for historical research.On having donated his personal archives to Emory: “it feels very weird” and, said laughingly, “at the time that I did it, I didn’t fully understand what I was doing.” He talks about the strange sensation of donating his materials while he’s still active, compares it to “undressing in public.” (He was embarrassed by his doodles being used in an exhibition, for example.) But it wasn’t that bad because they had had extensive discussions about privacy in his records, and one of his chief concerns was the privacy of others, not his own. They went through the paper and electronic material and created a “kind of embargoed body of material” with various restrictions. After the materials had been reviewed and restricted in this way, he feels much better about having his materials publicly available.He had “literally never thought about” donating his papers until someone suggested it to him. Rushdie was visiting Emory to give a series of lectures, and the president of the university planted the seed by asking what his plans were for his personal papers. “Somebody asked, it was as simple as that. No one had ever asked before.” People at other institutions were a little put out that he had selected Emory, but as he said, no one else had asked. One motivation for making the donation was to ensure the safety of his papers (both paper and electronic), which could easily be destroyed by fire or water damage. “This stuff is just sitting there, you know, it may as well be somewhere where it’s safe.”“And also it was in a complete mess.” Someone once told him, back in the ’70s, that he should save all his papers instead of just throwing away his old drafts, etc. They told him to save all that stuff “and one day it’ll pay off your mortgage and that’s more or less exactly what happened.” He also discussed the “arrangement” of the papers–he kept a cardboard box next to his desk and he would throw stuff in there until the box was full and then he’d get a new one. He had no idea what was in those old boxes, and when he started to look through them he found things he didn’t know were there.Having his papers at Emory corresponded with Rushdie preparing to write his memoirs. He thinks making this donation and having his papers processed actually made him write his memoirs. “Because if it hadn’t been for you guys I’d have had to sort it out. And truthfully I would just simply never have done it.” He describes looking through the finding aid or index to his paper archives, carefully listed with accompanying barcodes, and being able to request copies of specific items, and thinking that yes, now he could write that book.He discussed how his training as a history student differs from the experience of researching his own records for his own memoir. The difference is that he also has his memory. His notes acted more as a trigger to his own memory.Questions from the audience:Question about whether the electronic materials are so fragile and difficult that, in fact, they should not be used for preserved in electronic form. Farr gave a very good answer which I will not try to summarize here.Rushdie then brought up the problems with old fax paper. The text is now almost invisible. And the electronic faxes on his computer had corrupted files. So the faxes were problematic in both formats. This led to a discussion about how quickly fax machines faded into obsolescence.A friend convinced him to join Twitter just when he was finishing up with two big projects. He got an enormous number of followers very quickly. But he thinks you have to have a strategy for using Twitter. It shouldn’t be for trivia. It’s a vehicle for talking about things you’re interested in. He only has over 200,000 followers and most of them are anonymous. He looked at where his followers are geographically–most in New York, London, and India. “It’s interesting up to a point.” He thinks he will be on Twitter less as he gets involved with a new big project. But it has been fun and he doesn’t think he will give it up totally.Question about how working with his archives has changed him as a writer. He says that he normally doesn’t look backward or think about the past. Doing this gave him snapshots of an earlier self. Sometimes you agree with that younger self, sometimes you don’t. The biggest influence was that it showed him how much we change. Again, it’s interesting up to a point, but after that he doesn’t care. He doesn’t need to study this material. He’s in the business of making stuff. You don’t want to be too burdened by your past. Sometimes it’s useful to forget.In the writing of the memoir, did the archives challenge or correct his memory? Yes, they certainly did. Memory is fallible, and sometimes even when memory is confronted with its wrongness by the archives, it still wants to insist that it’s right. Vivid memories can be quite wrong.A question was asked about what types of users he had imagined using his digital archives. He had a general fear of tabloid journalists, rummaging around looking for scandal. And he obviously didn’t want to help them in that process. Other than that, he had no idea who would want to use them. He kind of had to block out thoughts about who would want to use them, just as when you write a book you can’t really think about people reading it. It’s important to have a sense of private space. Also the collection only goes up to 2005, and he doesn’t know when he’s going to make another deposit. So he still has a sense of private space in his current papers.Question about how did the period of his life when he in hiding affect his record keeping. One of the reasons he still has all these old computers is that they didn’t get rid of them because of the security threat. They didn’t want anyone to get any information off them. And in fact security is still a concern. One of the issues that came up in processing the archive is a concern not to compromise security, primarily not to give away the methods that were used to protect him. Not giving away the tricks of the trade. Rushdie then describes how to get away if someone is following you. (Sounds just like the movies!)Be Sociable, Share!If you want more news that I can blog about these days, you can follow me on Twitter:http://twitter.com/archivesnextIn my Twitter profile, I said: “this feed will be mixture of the serious and the trivial. You’ve been warned!” And that’s true. But I rarely tweet about what I had for lunch. Mostly what I do is to share (or re-tweet, “RT”) interesting things that I see from the people I’m following.You can also subscribe to the RSS feed for my Twitter account if you don’t want to bother with Twitter. If you click on that link and scroll down, on the right should be a link to subscribe to the RSS.Be Sociable, Share!Below are some highlights from my CV, to give you an idea who’s writing this. If you want to get in touch with me, you can find me on Twitter (@archivesnext) or send an email to info@archivesnext.comWork:Since March 2007, I’ve written on this blog about the issues facing archives, including technology, evolving business models, professional identity, professional organizations, and news and issues from other related professions. Sponsored “Best Archives on the Web” and “Movers and Shakers in Archives” awards and the Spontaneous Scholarship program for the 2011 SAA annual meeting.Since November 2010, I’ve worked for Neal-Schuman Publishers as a freelance acquisitions editor, which means I help people develop book proposals. If you want to write about archives or information science, please get in touch.From October 2000 – September 2006 I worked at the National Archives and Records Administration in the policy division. There’s a very long paragraph on my real CV describing what I did there, but the parts that will probably interest you are that I worked on a business process re-engineering and spent a lot of time working with the early stages of the Electronic Records Archives.Selected Publications: Editor, A Different Kind of Web: New Connections between Archives and Our Users. Chicago: Society of American Archivists, 2011.“What is the Meaning of ‘Archives 2.0’?,” American Archivist. Vol. 64: No. 1 (Spring/Summer 2011).“Building a Community of Supporters – The Role of New Technologies in Advocacy,” Many Happy Returns: Advocacy for Archives and Archivists. Larry Hackman, ed. Chicago: Society of American Archivists, 2011.“Interactivity, Flexibility, and Transparency: Social Media and Archives 2.0,” The Future of Archives and Recordkeeping, Jennie Hill, ed. London: Facet Publishing, 2011.Web 2.0 Tools and Strategies for Archives and Local History Collections. New York: Neal-Schuman Publishers, 2010.“Archives and Web 2.0: One Perspective on the Challenges Ahead.” ARC, June 2010.Instructional Experience:I’ve taught a bunch of workshops about archives and Web 2.0, includingInstructor, “New Tools for Old Things: How Archives Can Benefit from Social Media,” workshop, Association of Canadian Archivists, Halifax, Nova Scotia, June 8, 2010.Instructor, “How to Make the Most of Flickr: A Hands-On Introduction,” workshop, Mid-Atlantic Regional Archives Conference, Wilmington, DE, April 29, 2010.Instructor, “Introduction to Web 2.0 in Archives…or, What You Need to Know in a Nutshell,” webinar, Society of American Archivists, offered October 13, 2009 and on-demand.Instructor, “Web 2.0 Basics,” Pre-Conference Workshop, ACRL Rare Books and Manuscripts Section, Charlottesville, VA, June 17, 2009.Instructor, “Web 2.0 101,” workshop, Mid-Atlantic Regional Archives Conference, Chautauqua, NY, May 1, 2008.Conference Presentations:Sorry, this is a long list. I should shorten it, I know, but for now, here it is:Speaker, “Archivists, Historians, and the Future of Authority in the Archives,” American Historical Association Annual Meeting, Chicago, IL, January 7, 2012.Speaker, “Exploring the Participatory Archive,” Society of American Archivists Annual Meeting, Chicago, IL, August 25, 2011.Speaker, “Social Media Sensations: A Discussion about the Creative Possibilities for Archives and Web 2.0,” Mid-Atlantic Regional Archives Conference, Alexandria, VA, May 7, 2011.Speaker, “Evolution or Extinction?,” Ignite Smithsonian, Washington, DC, April 11, 2011.Speaker, “Spreading the Word: Archival Publishing for Researchers and Institutions,” Mid-Atlantic Regional Archives Conference, Harrisburg, PA, November 13, 2010.Panel member and invited participant, Archiving Social Media conference, University of Mary Washington and the Center for History and New Media, George Mason University, Fairfax, VA, October 1, 2010.Paper presented, “Leaving the Vault and Joining the Party: Using Social Media to Share Archival Collections,” XVI Brazilian Congress in Archival Science, Santos, Brazil, August 24, 2010.Chair, “Braving the “New Archives World”: Updating the Skills of a New Generation of Mid-Career Archivists and Records Professionals,” Society of American Archivists Annual Meeting, Washington, DC, August 12, 2010.Speaker, “Archivists as Web 2.0 Consumers,” New England Archivists Conference, Amherst, MA, March 20, 2010.Speaker, “Social software in digital libraries and archives: experiences from the field,” OCLC Digital Forum East, Arlington, VA, November 5, 2009.Speaker, “Bridging the Gap: What Professional Organizations Can Do To Help Unemployed and Underemployed Archivists,” Mid-Atlantic Regional Archives Conference, Jersey City, NJ, October 31, 2009.Chair and Commentator, “The Real Archives 2.0,” Society of American Archivists Annual Meeting, Austin, TX, August 13, 2009.Speaker, “Harnessing the Power of the Blog,” Mid-Atlantic Regional Archives Conference, Silver Spring, MD, November 8, 2008.Moderator and Speaker, “Web 2.0 Panel,” Manuscript Repositories Section meeting, Society of American Archivists, San Francisco, CA, August 29, 2008.Speaker, “Tracking the Lifecycle of Records: Modern Initiatives,” Mid-Atlantic Regional Archives Conference, Pittsburgh, PA, October 1, 2004.Speaker, “Business Process Reengineering at the National Archives and Records Administration,” Mid-Atlantic Regional Archives Conference, Arlington, VA, April 23, 2004.Professional Service:Member, Council, Society of American Archivists, August 2010 – present, and various other leadership roles on SAA sections, roundtables, and committees before that.I’ve also served on committees for MAC and MARACOccasional peer reviewer for NHPRC grants as well as Archival Science, American Archivist and Journal of Archival Organization for articles related to archive and use of Web 2.0 tools.Education:Archives Leadership Institute, Madison, Wisconsin, Participant, August 2011University of Michigan, Ann Arbor, Michigan, M.S.I., April 2000, Specialization in archives and records managementUniversity of Maryland, College Park, Maryland M.A, Art History and Archaeology, August 1999Be Sociable, Share!Now available:A Different Kind of Web - edited by and with contributions from me, but mostly a collection of what other smart people have to say. Many case studies of Web 2.0 implementations in archives accompanied by longer more reflective essays about the impact of social media on archives. You can order it from the SAA bookstore and view the table of contents here.Web 2.0 Tools and Strategies for Archives and Local History Collections has been published by the fine folks at Neal-Schuman Publishers and by Facet in the UK. SAA members can order it for a discount through the SAA Bookstore and it’s on Amazon, of course. More info, including access to the table of contents and excerpts from some rave reviews here.I have also contributed chapters to The Future of Archives and Recordkeeping: A Reader (edited by Jennie Hill, Facet Publishers, 2010) and to Many Happy Returns: Advocacy and the Development of Archives (edited by Larry J. Hackman, Society of American Archivists, 2010). Both of those publications are also available for order from the SAA Bookstore.Be Sociable, Share!Because of my interest in both history and games, I'm always on the look-out for good writing or new takes on how to bring elements of the gaming world into the framework of historical inquiry. Increasingly, I'm finding my best sources of this kind of reading from my Twitter stream, as was the case when Shawn Graham (@electricarchaeo) pointed me towards an article in the recent edition of the Canadian Game Studies Association journal, 'Loading…', titled 'Beyond the 'Historical' Simulation: Using Theories of History to Inform Scholarly Game Design'. Tackling what they call 'gamic action', the authors of the paper look to use elements of 'procedural rhetoric' (a concept introduced by Ian Bogost in his work 'Persuasive Games') combined with 'valid and scholarly means' of constructing the past (modeled on the monograph or print article) to produce 'reasonably justified truths' compatible with current methodologies in use by many historians.I mention the article not because I found it to be a progressive example of innovative historical thinking on games, but rather the opposite. Instead of offering a means by which games can be productively and thoughtfully incorporated into historical study, the authors present a reactionary stance that seeks to bind 'gamic action' within the tightly defined epistemological boundaries incorporated into textual modes of history. While they do offer valid insight when it comes to analyzing the roles and pretenses games follow today with regards to claiming historical validation, the repeated insistence on bringing into alignment the modes of 'objective' history and playable games not only overlooks the complimentary nature of both in creating reasonably justified truths about the past (to borrow a central concern of the authors), but also ignores the more fundamental issue centered on student prosumption (production + consumption) of historical knowledge.While the first objection stems from concern the authors profess regarding games ability to present historical 'truth' as exemplified by the monograph, the second objection goes to the core of a fundamental debate now occurring in the discipline of History. Examining both these objections yields the insight that History must go beyond the textual when forming links outside the circumscribed boundaries current epistemologies demand. This is not abandonment, it is augmentation. Rather than take a simplistic, reductionist view of the interplay between history and games, it might suit both the Historian and the Student better to uncover the more nuanced and complex interoperability both spheres of knowledge possess.Let's begin with what the authors define as the 'gamic mode'."A gamic mode of history is the construction of scholarly historical arguments as scholarly games, creating a relationship to commercial games analogous to that of non-fiction to fiction in literature. This enables scholars to convey their research in ways that go beyond the limits of textual monographs, digitized historical sources, and digital simulations." [3]Thus the introduction of two parallel themes that run through the entire article- first, that scholarly historic arguments can be laid 1:1 over the gamic mode and, second, that this gives the gamic mode a source of truth to which other, commercial games cannot lay claim. Simply put, the two worlds of textual history and games cannot coexist unless they are mirrors of each other, for to allow the possibility of transition between distinct spheres of knowledge would imply that truth is relative and the certified authority of the historian is no greater than the roll of a die or play of a card. Students/players, in the 'commercial' and 'simulative' gamic modes, are empowered to both consume and produce knowledge on a level that is difficult for traditional Historians to acknowledge, much less accept.This fear is clearly expressed by the authors when they claim that current methods of integrating games and history steer the debate away from expressing and elaborating "a disciplinary way of creating truth" and ultimately seek to transform the discipline by altering its epistemologies and limiting its empirical rigor. Hence the following claim by the authors:"This [steering of the debate] in turn limits scholarly debate by increasing ambiguity and opening reader response beyond the determination of whether or not the author has presented a reasonably justified truth." [5-6]While that statement certainly seems ominous, the real source of angst is not the debate on epistemology, truth and empirical rigor conflating history and games supposedly brings about- it's the fact that the reader is apportioned a space of interpretation hereto held inviolate by certified authorities of the historical profession. The gamic mode, as the authors see it currently being applied, allows the reader (note careful avoidance of the term 'player') to produce responses that go beyond consumption and simple affirmation or negation of the argument presented. The reader, enabled to produce (or, more accurately, prosume) their own 'truths', can simply avoid the argument altogether.Instead of dwelling on this point, let's put it in our back pocket as we survey other important parts of the authors argument.One key concept that helps the authors align fidelity of the historical textual mode to the gamic mode is procedural rhetoric, a term first introduced and elaborated by Ian Bogost, defined in this context as:"…the use of computational processes to persuasively and effectively convey an idea. What the author creates in procedural rhetoric is not the argument itself, but a series of general and specific rules through authoring code that a computer can then use to generate the argument (Bogost, 2007). This mirrors scholarly constructions of the past as history in two important ways. First is that the argument is not the past, but a representation of it created by authoring evidential and interpretive relationships that lead to conclusions. Second is that the scholarly historical argument itself consists of facts that are converted to evidence and arranged according to a set of rules for that particular argument via interpretation. The gamic mode of history is an application of procedural rhetoric that takes advantage of the processes inherent in scholarly evidential relationships to express these arguments as games. While different in form the argument experienced by the player would contain the same series of procedural evidential relationships that work towards a verifiable conclusion with a reasonably justifiable truth attribute that they might have expected to find in a monograph of the same argument." [6]By linking 'computational processes' to the way in which textual arguments are assembled, the authors hope to bring authoritative strength to their claim that the gamic mode and the textual historical argument can be one and the same. However, this viewpoint hinges on the assumption that digital games possess an internal consistency of rules and play that allow the player to understand and predict cause/effect relationships in the gamic world. This, unfortunately, is not the case.Photo by Ken GoldbergDigital games are, by their very nature, closed constructions whose operation the player cannot, on face, intrinsically know or predict without engaging first in a large degree of play. Cause/effect relationships in digital games are determined by trial and error, inference, and the acknowledgment of a reward to indicate progress. Yet the player can never be sure every corner of a digital game has been explored because many actions are obscured by the operation of code, which the player often cannot access and modify. In fact, a digital game could be considered the exact opposite of a monograph, where the argument and sources used are clearly articulated. But of course, this too simplifies the monographs presence, which is never really accounted for in the article. For while citations are visible the documents behind those citations are not. Alternatively, we know what the scholar selected but we don't know what they didn't select, or even the range of documents surveyed. This is not a knock on professionalism, merely the idea that History in pursuit of objectivity nevertheless is guided, perhaps unknowingly, by subjective desires.There is also the question of why the authors are so dedicated to digital gamic action, leaving the venerable tradition of manual board gaming to the relative wayside. I find this trend currently common in many historic approaches towards utilizing games- but without straying too far from the question at hand, I would add that board games at least allow an alternative separate from the digital gamic mode to occur. Board games are 'open' and the player does not have to continually press the boundaries of the world to figure out its meaning, a la digital. Complete boundaries are defined and areas of ambiguity are not hidden but rather demarcated quite visibly in a manual design. The player can dispense with the never-knowing and move straight to analysis and interpretation. It should also be noted that the 'open' design of manual games allows players to assert their own interpretations of the events or model depicted, something the authors, as cited above, greatly disdain.Player-Made Twilight Struggle Card by Mark MacRaeTo put it on even simpler terms- the main objection the authors have with current gamic modes is that they produce history for consumers, while the authors would much rather produce history for producers. This approach, currently, is endemic in the historical discipline because historians, by and large, are used to being both the producers and consumers of their own product. This is why the authors struggle so mightily to make equivalent a textual mode of history and a gamic mode of history, to make claims that this approach can, perhaps, go beyond the textual when, in fact, the very notion of equivalence negates this possibility. Textual modes focus on producing knowledge through reading, while gamic modes focus on producing knowledge through play. One allows simple consumption, the other complex prosumption.Stalwart defense of the 'consumptive' textual mode can be further seen in the authors elaboration of Alan Munslow's three broad epistemological approaches to historical scholarship, those being construction, deconstruction and reconstruction. Because deconstruction relies upon one's own experiences to form understanding of evidence and arguments presented, the authors reject such claims of historic inquiry because "to certain extent (deconstruction) means the past is unknowable and denies a corporate understanding of history." Reconstruction is similarly disqualified as its primary exemplar, the computer simulation, asserts that collected facts of the past can be arranged and recreated to simulate the past as it actually happened- yet this involves subjective qualifiers and emphases that the authors stress "taxes the traditional historian's ideal of objective scholarship."This leaves construction as the preferred epistemological approach in producing an authoritative historical gamic mode."Constructionist history builds up knowledge of the past and expresses the past as history by both analyzing how and what individual pieces of evidence can do, and what conclusions about the actions of historical agents (be they individuals or corporate entities) can be established through evidence relationships. In this case, evidence itself is separate from a notion of historical fact, as the fact only becomes evidence based upon its relationship to the question at hand. The constructionist approach to history, while allowing almost any question to be asked, provides parameters around how the question can be answered." [7]What gives construction the edge for the authors is that it neatly lays outs parameters establishing how 'almost any question…asked' can actually be answered. Construction also goes hand-in-hand with the use of narrative to act as the communicator of historical truth. Narrative as communicator of truth is so vitally important to the authors that they express fear in letting the student have input on interpretation outside of that directed by the Historian:"Narrative is so closely tied to our understanding of action, and as history is the study of past action, that if the historian’s prose does not present a cohesive narrative to the reader, the reader then creates one. Therefore, the gamic mode of history needs to be able to utilize narrative in the same way." [8]Under this rationale, it becomes easy for the authors to question the role of any gamic mode in which the student/player becomes a nexus of interaction or interpretation of historical evidence. Simulations and counter-factuals, the bread and butter of commercial games, are thus scorned by the authors because they allow the student/player to feel as though their actions create meaningful and accurate depictions of the past without utilizing "empirical, justified truths claims about the past."The solution presented by the authors is Shadows of Utopia: Exploring the Thinking of Robert Owen, a digital game that lets players simulate "an argument about Robert Owen's thinking." Placing questions of education and labor reform before the player expressed through puzzles and game-world exploration, Shadows of Utopia demonstrates the idealistic thinking of Robert Owen via player transformation of the game-world's 'lazy, foolish shadow-creatures who steal and rob' into real people who attain wealth and morals through factory work. Mimicking the textual authenticator of citations, Shadows of Utopia provides in-game source documentation in a transparent manner, going so far as to link "sources and related interpretations to the game code, user interface, and aesthetic choices," although how this is accomplished is not specifically defined.The authors conclude that efforts like Shadows of Utopia not only can "do all the things that the textual mode does" but also "add digital utilities that augment research in imaginative and useful ways."Now, to be clear and upfront, I think that Shadows of Utopia sounds like a fascinating attempt to bridge the epistemological gap between what we understand to be the practice of history with the act of play encountered in the gamic mode. However, I'm not willing to burn all other existing and potential bridges from history to games as the authors of 'Beyond the Historical Simulation' have done. For one thing, porting (to borrow a phrase from digital gaming) over the epistemological guidelines of textual monographs and journal articles to the gamic mode doesn't allow one to go beyond the textual mode- it merely extends that mode to gamic space without taking into account the unique epistemologies gamic space inherently possesses. (The authors want to 'paper over' the gamic space, literally, with textual modes) To make a simple point of comparison, a monograph does not seek reader input whereas a game, by its very nature, requires player input to be utilized. When you read a journal article, you are passively absorbing knowledge. When you play a game, you are actively absorbing knowledge. The authors argument presented above seeks to appropriate player activity and channel it into passive knowledgeabsorption.Instead of trying to simplify the conflation of history and games, perhaps it would be better to acknowledge their separate epistemological boundaries and formulate a way to negotiate knowledge handoffs between the two spheres. Katie King in her recent work Networked Reenactments, points the way to just such a negotiation in her analysis of flexible knowledges and pastpresents displayed in commercially produced television reenactments. Here we often see the interplay of several fields of knowledge, represented either by talking heads or physical actualization of knowledge epistemologies through representative involvement (i.e. having a Historian and Architect work together in recreating a Roman bath), set against the backdrop of a historical narrative that links the past to the present. When you add in the viewer angle to reenactments, the demarcation of specialized knowledge becomes less and less viable as the flexible knowledges required to fulfill the reenactment demand greater mobility than tight epistemologies might otherwise demand. Thus King notes,"…it is especially important that reenactments are not a way to keep pasts and presents apart-or a way to keep authorities and alternative knowledges, metaphors and referents, materialities and abstractions, forms of academic expertise and cultural entertainment, or affects and cognitions separated, managed, or delimited by membership. Flexible knowledges, transdisciplinarities, new media, all plunge us into uncertainties, risk, collusion, and collaboration; all conditions that-as with responsibilities to multiple audiences from painfully limited authorships-we do not control and in which we are elemental "bits" in emergent reorganizations of knowledge economies and among altering evaluations." (17)The uncertainty noted by King is what the authors of 'Beyond the Historical Simulation' wish to avoid, as it potentially invalidates the Historians authoritative position in knowledge making. But, again, King notes this aversion in traditionally defined disciplines presented with flexible knowledges when she states, "intensively experienced affect is what signals movement across knowledge worlds, as well as what indicates cognitive and affiliative shifts across what counts as authoritative."I have tried in previous posts (one on course design, another on modeling counterinsurgency) to indicate a way towards understanding how to use games in historical study that seeks to broaden the analytical framework beyond that of the textual, even though the textual is essential to analyzing games. If games offer us nothing but interpretations of history, something I don't fully believe, there is still valuable cultural significance worthy of study in the act of play that brings about said interpretations. How are cultural narratives sustained or modified in play? Why do some historical 'truths' stick to the public consciousness, while others are perennially ignored? How are certain conflicts or simulations modeled, and why would designers build games to emulate these processes? How does a players analysis of the game, its play-design mechanics, impact how they approach replays or creating modifications? (In particular I'm thinking of 'pacifist' play in Skyrim and even the creation of a '72 Summit Series card for Twilight Struggle)King offers a potent conceptual metaphor for analysis of games with her use of pastpresent- a player literally links the past to the present with their act of play- in addition to providing a framework though which diverse disciplines can interact on the subject of games through her analysis of flexible knowledges. This is a good start- but as the 'Beyond the Historical Simulation' article makes clear, there are still many who are skeptical of such ventures.Games are highly complex cultural artifacts that situate themselves on the borders of several disciplines, embodying fully the sort of reenactment potential for flexible knowledge discussed by King above. While it might be nice to render the gamic mode under the auspices of textual epistemologies, these can only take us so far in our understanding on the interactions of both and perhaps limit us, arbitrarily, from expanding and utilizing historic knowledge in emerging 'posthumanities' approaches the study of games demand. We can surely do better than advocate for the gamic mode to become backwards compatible with textual monographs.Comments OffEditors’ Note: Two new publications using quantitative methods to study the literary and intellectual history of nineteenth century Britain have been released. The first by Ryan Heuser and Long Le-Khac from the Stanford Literary Lab, and the second from Dan Cohen and Fred Gibbs. Excerpts and links to the original texts are included below.A Quantitative Literary History of 2,958 Nineteenth-Century British Novels: The Semantic Cohort Methodby Ryan Heuser and Long Le-Khac, Stanford Literary Lab.The nineteenth century in Britain saw tumultuous changes that reshaped the fabric of society and altered the course of modernization. It also saw the rise of the novel to the height of its cultural power as the most important literary form of the period. This paper reports on a long-term experiment in tracing such macroscopic changes in the novel during this crucial period. Specifically, we present findings on two interrelated transformations in novelistic language that reveal a systemic concretization in language and fundamental change in the social spaces of the novel. We show how these shifts have consequences for setting, characterization, and narration as well as implications for the responsiveness of the novel to the dramatic changes in British society.This paper has a second strand as well. This project was simultaneously an experiment in developing quantitative and computational methods for tracing changes in literary language. We wanted to see how far quantifiable features such as word usage could be pushed toward the investigation of literary history. Could we leverage quantitative methods in ways that respect the nuance and complexity we value in the humanities? To this end, we present a second set of results, the techniques and methodological lessons gained in the course of designing and running this project.Abstract and Link to PDF are available here.A Conversation with Data: Prospecting Victorian Words and IdeasBy Fred Gibbs and Dan Cohen. This is an open access, pre-print version of a paper in the Autumn 2011 volume of Victorian Studies. The final version is available in Victorian Studies at Project MUSE.“Literature is an artificial universe,” author Kathryn Schulz recently declared in the New York Times Book Review, “and the written word, unlike the natural world, can’t be counted on to obey a set of laws” (Schulz). Schulz was criticizing the value of Franco Moretti’s “distant reading,” although her critique seemed more like a broadside against “culturomics,” the aggressively quantitative approach to studying culture (Michel et al.). Culturomics was coined with a nod to the data-intensive field of genomics, which studies complex biological systems using computational models rather than the more analog, descriptive models of a prior era. Schulz is far from alone in worrying about the reductionism that digital methods entail, and her negative view of the attempt to find meaningful patterns in the combined, processed text of millions of books likely predominates in the humanities.Historians largely share this skepticism toward what many of them view as superficial approaches that focus on word units in the same way that bioinformatics focuses on DNA sequences. Many of our colleagues question the validity of text mining because they have generally found meaning in a much wider variety of cultural artifacts than just text, and, like most literary scholars, consider words themselves to be context-dependent and frequently ambiguous. Although occasionally intrigued by it, most historians have taken issue with Google’s Ngram Viewer, the search company’s tool for scanning literature by n-grams, or word units. Michael O’Malley, for example, laments that “Google ignores morphology: it ignores the meanings of words themselves when it searches…[The] Ngram Viewer reflects this disinterest in meaning. It disambiguates words, takes them entirely out of context and completely ignores their meaning…something that’s offensive to the practice of history, which depends on the meaning of words in historical context.” (O’Malley)Such heated rhetoric—probably inflamed in the humanities by the overwhelming and largely positive attention that culturomics has received in the scientific and popular press—unfortunately has forged in many scholars’ minds a cleft between our beloved, traditional close reading and untested, computer-enhanced distant reading. But what if we could move seamlessly between traditional and computational methods as demanded by our research interests and the evidence available to us?In the course of several research projects exploring the use of text mining in history we have come to the conclusion that it is both possible and profitable to move between these supposed methodological poles. Indeed, we have found that the most productive and thorough way to do research, given the recent availability of large archival corpora, is to have a conversation with the data in the same way that we have traditionally conversed with literature—by asking it questions, questioning what the data reflects back, and combining digital results with other evidence acquired through less-technical means.We provide here several brief examples of this combinatorial approach that uses both textual work and technical tools. Each example shows how the technology can help flesh out prior historiography as well as provide new perspectives that advance historical interpretation. In each experiment we have tried to move beyond the more simplistic methods made available by Google’s Ngram Viewer, which traces the frequency of words in print over time with little context, transparency, or opportunity for interaction.Read full text of the open access, pre-print version here.CommentsComments OffTed Underwood, Big but not distant, March 3, 2012It’s true that DH doesn’t have to be identified with scale. But the fact remains that problems of scale constitute a huge blind spot for individual researchers, and also define a problem that we know computers can help us explore. And when you first go into an area that was a blind spot for earlier generations of scholars, you’re almost guaranteed to find research opportunities — lying out on the ground like lumps of gold you don’t have to mine.Katherine Harris, Big Data, DH, Gender: Silence in the Archives?, March 3, 2012Adeline Koh, Addressing Archival Silence on 19th Century Colonialism – Part 1: The Power of the Archive, Part 2: Creating a Nineteenth Century Postcolonial Archive, March 4, 2012Can the Subaltern speak in the digital archive? This is the first of two blog posts which follows on a recent discussion on about silence in the archives regarding gender and minorities. Readers may be interested in reading Natalia Cecire’s storify of the conversation over twitter and blog posts by Roger Whitson and Katherine Harris to follow up on the discussion. These two posts are concerned with a more specific silence—on race and colonialism in the nineteenth century archive, and how one of my new digital projects, Digitizing ‘Chinese Englishmen,’ attempts to counteract this silence. To date, despite the explosion of digital work on nineteenth century writers and culture, there are no existing digital projects that specifically deal with the impact of British imperialism in the Victorian era. This neglect is surprising, given that by the start of the twentieth century, England’s political reach spanned the majority of the globe. To exclude British imperialism from digital Victorian studies is thus a serious gap that Digitizing ‘Chinese Englishmen’ tries to address.Roger Whitson, DH, Archival Silence, and Linked Open Data, March 4, 2012I’m thinking through many of the interesting conversations occurring around Twitter and the DH blogosphere recently. First, Miriam Posner had a really powerful post about learning code and gender, where she argues that the broad exhortation to code covers up gender and diversity inequity. The large number of coding institutions, she cites Wikipedia as an example, are overwhelmingly male-dominated. ”[M]en — middle-class white men, to be specific — are far more likely to have been given access to a computer and encouraged to use it at a young age. I love that you learned BASIC at age ten. But please realize that this has not been the case for all of us.” In a particularly thoughtful response in the comments, Steven Ramsay describes the environment in a meet-and-greet session with male developers as “like a locker room. I counted three women in a group of at least fifty men, but that wasn’t even the worst of it. Porn joke? Check. Sports and warfare metaphors? Abounding. Do-or-die, you-win-or-you-suck vibe? Very much in evidence.”Natalia Cecire, From Archival silence to glorious data, March 3, 2012How do you use digital methods to analyze the *gaps* in the archive?CommentsXKCD, "How It Works"Oh, how I hate being the bearer of bad news. Yet I feel I have to tell you something about the frustration I’m hearing, in whispers and on the backchannel, from early-career women involved in digital humanities.Here, there, and everywhere, we’re being told: A DHer should code! Don’t know how? Learn! The work that’s getting noticed, one can’t help but see, is code. As digital humanities winds its way into academic departments, it seems reasonable to predict that the work that will get people jobs — the work that marks a real digital humanist — will be work that shows that you can code.And that work is overwhelmingly by men. There are some importantexceptions, but the pattern is pretty clear.In principle, I have no particular problem with getting everyone to code. I’m learning to do it myself. (And a million thank yous to those of you who are helping me.) But I wanted to talk here about why men are the ones who code, so that we can speak openly about the fact that programming knowledge is not a neutral thing, but something men will tend to have more often than women.This matter is of no small concern to me. It is breaking my damn heart to see how many women I know have earnestly committed themselves to codeacademy because they want to be good citizens, to prove they have what it takes. These are my friends, and this is their livelihood, and this is the career we’ve chosen.First, men — middle-class white men, to be specific — are far more likely to have been given access to a computer and encouraged to use it at a young age. I love that you learned BASIC at age ten. But please realize that this has not been the case for all of us.Second, the “culture of code,” the inside jokes and joshing that you enjoy, may not be equally appealing to everyone who encounters it. This should be, but apparently isn’t, obvious.But Miriam, you’re thinking, there are lots of examples of DH coders who started late and are now well-respected and proficient! This is true! And they inspire me all the time. But this is also why I wanted to talk a little bit about what it’s like for a woman to learn to program.Should you choose to learn in a group setting, you will immediately be conspicuous. It might be hard to see why this is a problem; after all, everyone wants more women in programming. Surely people are glad you’re there. Well, that’s true, as far as it goes. But it also makes you extremely conscious of your mistakes, confusion, and skill level. You are there as a representative of every woman. If you mess up or need extra clarification, it’s because you really shouldn’t — you suspected this anyway — you shouldn’t be there in the first place. I have sat through entire workshops that were clearly pitched beyond my skill level because I just didn’t want to make us look bad. It’s more awkward when you break for lunch, because where are you supposed to sit? It’s uncomfortable. I am not known for being shy, and let me tell you: It is awkward.But there are all these online communities where you can learn to code. There are! But if you are under the impression that online communities are any friendlier to women’s participation, then you, my friend, have not looked lately at Wikipedia.Well, just practice!I did the work — so should you! Here is the real point I’m trying to make here: It is not about “should.” What women should do has nothing to do with it. The point is, women aren’t. And neither, for that matter, are people of color. And unless you believe (and you don’t, do you?) that some biological explanation prevents us from excelling at programming, then you must see that there is a structural problem.So I am saying to you: If you want women and people of color in your community, if it is important to you to have a diverse discipline, you need to do something besides exhort us to code.This content is published under the Attribution-Noncommercial-Share Alike 3.0 Unported license.Today, my colleagues Peter Haber, Jan Hodel, and I (along with the indispensable help of Dan Ludington) are pleased to announce the launch of Global Perspectives on Digital History, the latest of the PressForward publications from the Roy Rosenzweig Center for History and New Media.LikeDigital Humanities Now, Global Perspectives on Digital History aggregates and selects material from our Compendium of the Global Perspectives, drawing from hundreds of venues where high-quality scholarship is likely to appear, including the personal websites of scholars, institutional sites, blogs, and other feeds. It also seeks to discover new material by monitoring Twitter (someone else is going to have to do that for me given my aversion to the whole Twitterverse) and other social media for stories discussed by the community, and by continuously scanning the broader web through generalized and specialized search engines.Unlike Digital Humanities Now, Global Perspectives on Digital History is focused more on history, rather than on digital humanities in general. This is not to say we won’t be bringing in content from other digital humanities disciplines that seems relevant to our readers’ interests in digital history. But, as much as possible, we will remain more tightly focused on a single discipline. The other big difference in approach with the first of the PressForward publications is that Global Perspectives on Digital History is a multi-lingual publication. Our initial languages are English, German, and French, but we expect to expand soon into other languages. The only thing holding us back at present is a lack of editors to help with the scanning of content in those other languages.At present we are using the GoogleTranslate plug in for translation. If you have any experience with this plug in you know it is wholly insufficient for what we are about. Over the coming year, we will be exploring other options for machine translation of our content and hope to learn some things worth knowing through that exploration.Like Digital Humanities Now, we will also be moving toward some traditional publication of content that appears on our site. Whether we use the model currently in use at Digital Humanities Now or something else, still remains to be seen. We are going to watch the development of the open peer review process carefully before deciding on our approach.At present, we are splitting our coverage of digital history from around the globe between longer “think pieces” that we are tagging as “editor’s choice” content, and briefer entries we are tagging as “short takes.” We suspect we will expand into reviews and other content from around the globe that examines digital history sometime in the near future.For now, please visit the site and be sure to let us know what you think.Tags: Global Perspectives on Digital History, GPDH, International Conversations, PressForward, publishing This entry was posted on Thursday, March 1st, 2012 at 9:19 am and is filed under Posts. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site.With all of the excitement about new interfaces to visualize the past, it’s easy to forget the old standby: the timeline. It has the power of simplicity, the challenge of over-simplifying. And in museums it has a visceral appeal: walk through history!I’ve written a bit about the timeline, at too much length for an easy post, for an article. I’ve embedded a draft here, footnotes and all. Any comments, suggestions or examples most welcome. This entry was posted in Uncategorized by lubar. Bookmark the permalink.By Katy MeyersFeb 28, 12A certain amount of knowledge is required for players to navigate video games, whether this means remembering the weak points of the different splicers in BioShock, or remembering the buttons to press to play Epona’s song in Ocarina of Time. This knowledge is gained throughout play, rather than presented to the individual to quiz them, however when the knowledge is required it can drastically affect how well the player performs. As Adam discussed in his post on the role of content and form in historical video games, there are often problems of balancing the mechanics and form with content. While the game itself is more fun, it is at the sake of historical accuracy. He proposes that we should create games in which the player is engaging with the historical content rather than passively interacting with it. In a previous post on Assassin’s Creed, I argued a similar point, noting that the content needed to be part of the form and structure of game play. In designing meaningful or educational games, it is this dynamic knowledge that we need to tap into to create games that teach players about the content, but not at the sake of the game mechanics or fun.I am currently the lead game designer for Ethan’s NEH game, Red Land Black Land. The game is a mod of Sid Meier’s Civilization V, and is based on a series of six scenarios. Each scenario plays through a portion of a period in Ancient Egypt. The purpose of the game is three fold: allow players to explore the process of historical and social change from the Pre-Dynastic to end of the New Kingdom, explore the construction of knowledge about each period, and provide a counter-point to mainstream understanding of Ancient Egypt.One of the problems with teaching Ancient Egypt is that there exists a wide range of interpretations in archaeology and Egyptology. Each discipline tends to privilege specific information over other. In archaeology the material culture is privileged, interpreted as a direct reflection of the actions of individuals in the past. In Egyptology the text, epigraphy and inscriptions are paramount, being that they are the words of the individuals who lived in the past. To make matters more confusing, interpretations of inscriptions and archaeological sites change over time, and are subject to the biases of the interpreters. While having a knowledge of the time period, culture, politics, economy and social structures is important, it is vital for students to understand how this information has been created, especially when there are disagreements. One example is the question of how the Kingdom of Egypt was formed. Egyptologists argue that the kingdom was united through warfare based on a number of palettes depicting warriors, whereas archaeologists argue for a slow amalgamation of the various smaller groups with increasing trade between them creating cultural homogeneity. There is a lack of material culture which points to warfare, such as the presence of mass graves or even high numbers of weapons. It is necessary for students to know how this highly contested knowledge is constructed.The question was how to convey not just a single educational strand of information, but a number of conflicting strands of information. Most meaningful games focus on a single topic, with the goal to memorize the content and use the information to pass to a different level. How do you get players to learn a number of different interpretations and then use that knowledge to make historical choices while still being engaged in the game mechanics?In order to achieve this, we embedded the knowledge into four historical learning agents. In Civ V, the player has access to four advisors who give them hints and advice on how to proceed in economics, foreign affairs, military campaigns and science. In Red Land Black Land, the player is advised by different academics who share their knowledge of the time period. These historical learning agents include an archaeologist from the late 19th century whose conclusions are based on theories of diffusion and seriation, a 21st century archaeologist who uses a range of modern sites and techniques to interpret material culture, an Egyptologist who focuses on epigraphy and inscriptions, and a graduate student who is, like the player, navigating the different streams of knowledge.The player needs to properly interpret the knowledge in order to make decisions about how they will proceed in the game. One problem that continues to plague the design is that we, as developers, are privileging one interpretation over another. The development and design is in the hands of two archaeologists, so there is a clear bias on what interpretations we believe the player should follow in order to win. This doesn’t mean that they can’t win if they choose to follow the advice of the Egyptologist, and in some cases it may be the appropriate choice. Due to the game being a mod, there are limits on the number of potential winning narratives, which forces us as developers to pick a ‘more correct’ version of history. Regardless, the player will be required to interact with different strands of dynamic knowledge and make decisions based on this in order to play. When testing begins in the next couple weeks we will see whether this approach has been successful. Collaboratively edited and authored, Play the Past is dedicated to thoughtfully exploring and discussing the intersection of cultural heritage (very broadly defined) and games/meaningful play (equally broadly defined). Play the Past contributors come from a wide variety of backgrounds, domains, perspectives, and motivations (for being interested in both games and cultural heritage) – a fact which is evident in the wide variety of topics we tackle in our posts.It is very important to note that Play the Past isn’t just about the intersection of cultural heritage and digital games, its also about non-digital games (boardgames, tabletop games, collectible card games, etc.), alternate reality games (ARGs), barely games (a term originally coined by Russel Davies – no, not the Doctor Who Russel Davies – and built upon by our very own Rob McDougall), and playful mechanics (or “gamifying” as its been recently called).We are also very interested in exploring the spectrum of approaches to games – from the more “philosophical” (as some might call it) games studies side of things, to the more practically applied serious games/meaningful play side of things (and just about everything betwixt and between).Inspired by ProfhackerCredit where credit is due. One of the most important inspirations for Play the Past comes from directly ProfHacker. You can see the fingerprints of ProfHacker all over Play the Past. From the way we do business behind the scenes to our commenting and community policy (which is pretty much shamelessly lifted verbatim from ProfHacker). This is no great surprise as Ethan Watrall (Play the Past’s editor) was in the first batch of ProfHacker writers. In this regard, we are extremely grateful to ProfHacker – and particularly ProfHacker’s two Editors: George H. Williams (@georgeonline on Twitter) and Jason B. Jones (@jbj on Twitter).Interested in Contributing?Interested in contributing to Play the Past (either on a regular basis or as a “one shot” guest author)? We’d love to hear from you. Drop us a line here and tell us a little bit about yourself and how you might contribute.We are committed to fostering an environment characterized by generosity, creativity, and (as corny as it might sound) kindness. Comments on this blog are an important part of creating that environment, and this comment policy aims to communicate our values to new readers and encourage comments that will build up the online community here.Thoughtful comments (even when–and often especially if–disagreeing) are encouraged and appreciated.No snark allowed (see David Denby on definition of snark). While snark certainly has its virtues, this blog provides a space for people to be inexperienced at something, or even wrong, to facilitate learning. That’s harder to do in the face of either persistent or “drive-by” snark.Play the Past should be a community built through regular contributions made by recognized–but not necessarily “real name”–contributors. Some commenters’ identities reveal their real names; other commenters use pseudonyms. Our online identities are built from our comments here and our presence–as commenters and authors–in other places on the web, in print, at conferences. Play the Past welcomes commenters–whether anonymous, psuedonymous, or publically identified–who are committed to creating a rich and respectful dialogue. We want commenters to be able to explore the complexities of Play the Past posts; we want commenters to inquire and debate; we want everyone to be able to learn from the conversation.Links & images are encouraged. Gratuitous linking back to your own site is discouraged. Links in the “website” field should point only to profile pages or to personal websites.Gravatars are strongly encouraged.Jeremy AntleyJeremy Antley is a writer/student/gamer who received his MA in History with a focus on the Russian Imperial period from the University of Kansas in 2007. While currently in the middle of researching the immigration of Russian Old Believers to Oregon in the mid-60′s for his doctoral ambitions, Jeremy also finds studying the life and culture of Russian peasants to be a fascinating topic. He looks at topics of digital culture, games and, of course, Russian history at his blog, Peasant Muse. Those interested in hearing the twitter ramblings of someone crazy enough to love Russian peasants would do well to check-out Jeremy’s handle- @jsantley. Having played many epic games of Axis & Allies as an undergraduate, Jeremy now plays a variety of board and console (xbox 360) games with his friends in his current residence of Portland, Oregon. Jeremy’s love of board games informs his current interest in how players modify their games and how looking at board games as platforms can inform historical inquiries.Kevin BallestriniKevin Ballestrini teaches Latin and Mythology at the Norwich Free Academy in Connecticut. He has received an M.A. and B.A. in Classics from the University of Colorado and University of Connecticut respectively. In addition to experience teaching in a traditional classroom setting, Kevin is deploying the first fully practomimetic introductory language course at the high school level this year in a section of Latin I. He hopes that the experience will enhance student engagement and connection to life and culture in ancient Rome. As an avid technology enthusiast, he maintains his blog, Techna Virumque Cano (http://kevinbal.blogspot.com) where he discusses the intersection of technology and his teaching. Kevin is also the leader of a Lord of the Rings Online kinship and has a great interest in exploring how games contribute to the development of (online) communities just as the bardic tradition contributed to the development of ancient communities. You can also find Kevin on Twitter at http://www.twitter.com/kballestrini.Emily Joy BembeneckEmily Bembeneck is a Ph.D. candidate at the University of Michigan in the Department of Classical Studies. She primarily works on narrative and character development in ancient epic and modern video games, both graphical and text-based. Other interests of hers include the image of the hero, Greek tragedy and social catharsis, cultural and individual identity through play, immersion, and game design, She teaches classes in Latin, Greek and Roman history, ancient war and entertainment, among other things related to the Classical world She is an active contributor at http://www.greywardens.com where she writes on the narrative structure of Bioware’s Dragon Age RPG franchise. Her current projects include working on images of Rome in ancient and modern culture, developing a Flash-based application that combines components of social play with narrative creation, and designing a Dragon Age module that explores Euripides’ Medea through post-primary narrative. When not being all academic and studious, she is likely either playing an elf in some virtual world or spending time with her two young sons. Find her online at Ada Play and on Twitter at http://www.twitter.com/adarel.Andrew D. DevenneyAndrew D. Devenney is currently a Visiting Assistant Professor of World History at Grand Valley State University in Allendale, Michigan. A life-long and avid gamer of both video and role-playing games, Andrew has found elements of game-based learning, new media, and the digital humanities unconsciously seeping into his classroom over the last few years. As such, he has begun to experiment relentlessly on his students with these new ideas, techniques, and shiny toys, and is having a grand time doing it. Sometime in the relatively near future, Andrew hopes to deploy a new course that will explore global history through the medium of gaming (which means the class may very well morph into a giant, heavily modded, credit-based game of Dungeons and Dragons). Andrew can be found online at http://andrewdevenney.net and on Twitter as @adevenney.Shawn GrahamShawn Graham is Assistant Professor of Digital Humanities in the Department of History at Carleton University in Ottawa, Canada. He’s been blogging as ‘The Electric Archaeologist‘ since 2007, documenting his interest & experiments in game-based learning, agent based modeling, and other aspects of digital media for archaeological teaching and research. On Twitter, he’s at http://www.twitter.com/electricarchaeo. He’s published amongst other things a number of agent-based simulations on aspects of the Roman world, and has explored using Civilization mods in his distance-education classrooms. He received his PhD in Archaeology from the University of Reading in 2002, where he was interested in complexity & evolving networks in antiquity (especially in Rome). When he’s not geeking out over the latest tech toys or things archaeological, he is chief cider maker at Coronation Hall Cider Mills and playing Wii games with his family. If only there was wiiCivilization…Matthew KirschenbaumMatthew Kirschenbaum is Associate Professor in the Department of English at the University of Maryland, Associate Director of the Maryland Institute for Technology in the Humanities (MITH, an applied thinktank for the digital humanities), and Director of Digital Cultures and Creativity, a new “living/learning” program in the Honors College. Kirschenbaum speaks and writes often on topics in the digital humanities and new media; his work has received coverage in the Atlantic, New York Times, National Public Radio, Wired, Boing Boing, Slashdot, and the Chronicle of Higher Education. He has been pushing cardboard counters around on hexagonal grids since his early teens. See http://www.mkirschenbaum.net for more.Katy MeyersKaty Meyers is a graduate student in the Department of Anthropology,with a concentration in mortuary archaeology, at Michigan StateUniversity. She is currently a fellow in both the Cultural Heritage Informatics Initiative and the Campus Archaeology Program. She is also an editor of GradHacker, and the game designer for an Ancient Egyptbased mod of Civilization V. When she somehow finds free time she loves playing first person shooter and adventure video games, and is amajor fan of Milton-Bradley board game classics. She’s interested inthe role of games for outreach and education in archaeology and cultural heritage, as well as the biases they create. Katy can befound online at her blog http://www.bonesdontlie.com and on Twitter at @bonesdonotlie.Jeremiah McCallJeremiah McCall has been teaching high school history for the past decade, mostly at Cincinnati Country Day School. His first professional love is high school teaching, especially designing instructional strategies that will engage and challenge his students to learn and grow. In addition to more conventional courses, Jeremiah also teaches senior elective on (tabletop) historical simulation design, and the intersection of serious games and contemporary global issues. Jeremiah’s primary training is in history with a PhD in ancient history from Ohio State University; he authored a book on the cavalry of the Roman Republic and is currently writing a historical biography of the Roman aristocrat M. Claudius Marcellus to be published by Pen and Sword press. He has recently completed a guidebook for teachers who wish to use simulation games in the history class. Titled Gaming the Past: Using Video Games to Teach Secondary History, the book was published by Routledge in June, 2011. As an extension of his teaching philosophy — that history is primarily the study and evaluation of competing interpretations of the past — McCall has conducted numerous classroom implementations of historical simulations as historical interpretations. He maintains the website gamingthepast.net, one of the primary sites devoted to the use of historical simulations in classroom teaching. He also plays far more video games — particularly RPGs and strategy — than you’d think he could find time for.Rob MacDougallRob MacDougall is Assistant Professor of History at the University of Western Ontario and Associate Director of UWO’s Centre for American Studies, where he teaches United States history, the history of technology, and digital history methods. His research centers on the history of communication–he has just written a book on the early days of the telephone and is beginning a new book on the circulation of bad ideas. He blogs sporadically at Old is the New New and is on the Twitter at http://twitter.com/robotnik. A life-long gamer, Rob is interested in the history of gaming and in using games and play to encourage more playful historical thinking. His own play is largely non-digital of late–tabletop RPGs with his gamer buddies and make-believe with his kids–but he looks forward to seeing how good computer games will have gotten by the time he gets tenure.Rebecca MirRebecca Mir is a museum educator, researcher, and writer at various institutions in New York City. She will receive an M.A. in Decorative Arts, Design History, and Material Culture from the Bard Graduate Center in May 2012. She received her Bachelor’s degree in Art History from Indiana University of Pennsylvania in 2010. She often thinks about how (and why) artifacts and cultures are (mis)represented in video games and how museums are using games to engage and educate audiences. Rebecca can be found on Twitter as @hellenophile.Trevor OwensTrevor Owens is a digital archivist with the National Digital Information Infrastructure and Preservation Program (NDIIP) at the Library of Congress and a doctoral student in the Graduate School of Education at George Mason University. Before joining the Library of Congress he worked as the community lead for the Zotero project at the Center for History and New Media helped organize the first two meetings of the Games, Learning, and Society Conference. He received a bachelors degree in the history of science form the University of Wisconsin, and a masters degree in American History with and emphasis on digital history from George Mason University. Trevor has spent considerable amounts of time playing all iterations of Civilization, but is also a big fan of role playing games (everything from Earthbound to Fallout). Trevor has published on the history of children’s books about Einstein and Curie, the discursive practices of Civ Modders, and the role of digital research tools in scholarship and teaching. When not researching, writing, or gaming, he also enjoys playing the violin. Trevor can be found online at http://www.trevorowens.org and on Twitter at http://www.twitter.com/tjowensRoger TravisRoger Travis is an Associate Professor of Classics in the Department of Literatures, Cultures & Languages of the University of Connecticut. He is also the Director of the Video Games and Human Values Initiative at UConn, an interdisciplinary online nexus for scholarly activities like monthly symposia and “playversations.” He received his Bachelor’s degree in classics from Harvard College, and his Ph.D. in comparative literature from the University of California, Berkeley before arriving at UConn in 1997. He has published on Homeric epic, Greek tragedy, Greek historiography, the 19th C. British novel, HALO, and the massively-multiplayer online role-playing game He has been President of the Classical Association of New England and of the Classical Association of Connecticut. He writes the blog Living Epic about the fundamental connection between ancient epic and the narrative video game, and is a founder and contributor of the collaborative blog Play the Past. In the 2009-2010 academic year, Roger offered the first courses ever designed entirely as practomimes.Mark SampleMark Sample is an Assistant Professor of contemporary literature and new media studies in the Department of English at George Mason University. In addition to his work on electronic literature, videogames, and code studies, Mark is an outspoken advocate of open source pedagogy and open source research. In recognition of his commitment to innovation in teaching, Mark was the recipient of George Mason’s 2010 Teaching Excellence Award. Mark is a regular contributor to ProfHacker, and can also be found online at samplereality.com or on Twitter as @samplereality.Ethan WatrallEthan Watrall is an Assistant Professor in the Department of Anthropology and Associate Director of Matrix:The Center for Humane Arts, Letters & Social Sciences Online at Michigan State University. In addition, Ethan is a Principal Investigator in the Games for Entertainment & Learning Lab, and co- founder of both the undergraduate Specialization and Game Design Development and the MA in Serious Game Design at Michigan State University. Ethan teaches (and has taught) in a wide variety of areas including cultural heritage informatics, ancient Egyptian social history & archaeology, archaeology and pop culture, user centered & user experience design, game design, serious game design, game studies. and history of various forms of popular and entertainment media (comics and digital games to name a few). When he’s not being all professorial, he’s a world class comic book nerd (Killowog is so his favorite Green Lantern), a sci-fi dork (he’ll argue to the grave that Tom Baker is the best Doctor ever), and an avid player of all sorts of games (digital, board, and tabletop). Ethan can be found online at http://www.captainprimate.com, and on Twitter at http://www.twitter.com/captain_primateA few days ago, Gao, Hu, Mao, and Perc posted a preprint of their forthcoming article comparing social and natural phenomena. The authors, apparently all engineers and physicists, use the google ngrams data to come to the conclusion that “social and natural phenomena are governed by fundamentally different processes.” The take-home message is that words describing natural phenomena increase in frequency at regular, predictable rates, whereas the use of certain socially-oriented words change in unpredictable ways. Unfortunately, the paper doesn’t necessarily differentiate between words and what they describe.Specifically, the authors invoke random fractal theory (sort of a descendant of chaos theory) to find regular patterns in 1-grams. A 1-gram is just a single word, and this study looks at how the frequency of certain words grow or shrink over time. A “hurst parameter” is found for 24 words, a dozen pertaining to nature (earthquake, fire, etc.), and another dozen “social” words (war, unemployment, etc.). The hurst parameter (H) is a number which, essentially, reveals whether or not a time series of data is correlated with itself. That is, given a set of observations over the last hundred years, autocorrelated data means the observation for this year will very likely follow a predictable trend from the past.If H is between 0.5 and 1, that means the dataset has “long-term positive correlation,” which is roughly equivalent to saying that data quite some time in the past will still positively and noticeably effect data today. If H is under 0.5, data are negatively correlated with their past, suggesting that a high value in the past implies a low value in the future, and if H = 0.5, the data likely describe Brownian motion (they are random). H can exceed 1 as well, a point which I’ll get to momentarily.The authors first looked at the frequency of 12 words describing natural phenomena between 1770 and 2007. In each case, H was between 0.5 and 1, suggesting a long-term positive trend in the use of the terms. That is, the use of the term “earthquake” does not fluctuate terribly wildly from year to year; looking at how frequently it was used in the past can reasonably predict how frequently it will be used in the future. The data have a long “memory.”Natural 1-grams from Gao et al. (2012)The paper then analyzed 12 words describing social phenomena, with very different results. According to the authors, ”social phenomena, apart from rare exceptions, cannot be classiﬁed solely as processes with persistent-long range correlations.” For example, the use of the word “war” bursts around World War I and World War II; these are unpredictable moments in the discussion of social phenomena. The way “war” was used in the past was not a good predictor of how “war” would be used around 1915 and 1940, for obvious reasons.Social 1-grams from Gao et al. (2012)You may notice that, for many of the social terms, H is actually greater than 1, “which indicates that social phenomena are most likely to be either nonstationary, on-off intermittent, or Levy walk-like process.” Basically, the H parameter alone is not sufficient to describe what’s going on with the data. Nonstationary processes are, essentially, unpredictable. A stationary process can be random, but at least certain statistical properties of that randomness remain persistent. Nonstationary processes don’t have those persistent statistical properties. The authors point out that not all social phenomena will have H >1, citing famine, because it might relate to natural phenomena. They also point out that “the more the social phenomena can be considered recent (unemployment, recession, democracy), the higher their Hurst parameter is likely to be.”In sum, they found that “The prevalence of long-term memory in natural phenomena [compels them] to conjecture that the long-range correlations in the usage frequency of the corresponding terms is predominantly driven by occurrences in nature of those phenomena,” whereas “it is clear that all these processes [describing social phenomena] are fundamentally different from those describing natural phenomena.” That the social phenomena follow different laws is not unexpected, they say, because they themselves are more complex; they rely on political, economic, and social forces, as well as natural phenomena.While this paper is exceptionally interesting, and shows a very clever use of fairly basic data (24 one-dimensional variables, just looking at word use per year), it lacks the same sort of nuance also lacking in the original culturomics paper. Namely, in this case, it lacks the awareness that social and natural phenomena are not directly coupled with the words used to describe them, nor the frequency with which those words are used. The paper suggests that natural and social phenomena are governed by different scaling laws when, realistically, it is the way they are discussed, and how those discussions are published which are governed by the varying scaling laws. Further, although they used words exemplifying the difference between “nature” and “society,” the two are not always so easily disentangled, either in language or the underlying phenomena.Perhaps the sort of words used to describe social events change differently than the sort used to describe natural events. Perhaps, because natural phenomena are often immediately felt across vast distances, whereas news of social phenomena can take some time to diffuse, how rapidly some words are discussed may take very different forms. Discussions and word-usage are always embedded in a larger network. Also needing to be taken into account is who is discussing social vs. natural phenomena, and which is more likely to get published and preserved to eventually be scanned by Google Books.Without a doubt the authors have noticed a very interesting trend, but rather than matching the phenomena directly to word, as they did, we should be using this sort of study to look at how language changes, how people change, and ultimately what relationship people have with the things they discuss and publish. At this point, the engineers and physicists still have a greater comfort with the statistical tools needed to fully utilize the google books corpus, but there are some humanists out there already doing absolutely fantastic quantitative work with similar data.This paper, while impressive, is further proof that the quantitative study of culture should not be left to those with (apparently) little background in the subject. While it is not unlikely that different factors do, in fact, determine the course of natural disasters versus that of human interaction, this paper does not convincingly tease those apart. It may very well be that the language use is indicative of differences in underlying factors in the phenomena described, however no study is cited suggesting this to be the case. Claims like “social and natural phenomena are governed by fundamentally different processes,” given the above language data, could easily have been avoided, I think, with a short discussion between the authors and a humanist.Welcome to the scottbot irregular. My name’s Scott, and the US Government has for some reason seen fit to give me money to study Science. It’s ‘Science’ with a capital ‘S’ because I’m not studying individual aspects of the world using science, but rather studying Science in general as a social, historical, philosophical, and intellectual phenomenon. What’s worse, I’m attempting to do it scientifically. This blog is my attempt at giving the country its money’s worth. Also, I kinda would love feedback on my eventual dissertation. See? Everybody wins.scott b. weingartis pretty clueless about a lot of things. This is his attempt to be less so.I pledge to be a good scholarly citizen. This includes:Opening all data generated by me for the purpose of a publication at the time of publication.Opening all code generated by me for the purpose of a publication at the time of publication.Freely distributing all published material for which I have the right, and fighting to retain those rights in situations where that is not the case.Fighting for open access of all materials worked on as a co-author, participant in a grant, or consultant on a project.I pledge to support open access by:Only reviewing for journals which plan to release their publications openly.Donating to free open source software initiatives where I would otherwise have paid for proprietary software.Citing open publications if there is a choice between two otherwise equivalent sources.I pledge never to let work get in the way of play.I pledge to give people chocolate occasionally if I think they’re awesome._[This is somewhat out of date. Please stand by for new information!]Hello World!Student of History & Philosophy of Science and Information Science at Indiana University.You’ve managed to stumble across my little corner of the internet. I’m currently a student and researcher in the HPS and SLIS departments at IUB under two of the most interesting and capable professors I’ve had the fortune to meet: Colin Allen and Katy Börner. I studied history of science and computer engineering at UF, where I slaved researched for the infinitely patient Robert A. Hatch, who taught me more in four short years than I’d yet learned in aggregate over my entire life.Early InPhO Concept MapThese days, I split my time between classes, the Indiana Philosophy Ontology Project (InPhO) and the Cyberinfrastructure for Network Science Center (CNS). At InPhO I program and design visual, navigable representations of our dynamically generated taxonomy of ideas; analyze relational networks (influenced, disagreed with, etc.) from our Thinkers database; and map and compare philosophical ontologies. The CNS keeps me busy with all sorts of scientometric analyses, and I am also involved in the development of large scale network analysis software such as the NWB, creating workflows, providing software feedback, writing documentation and teaching workshops.Co-authorship network created using the Network Workbench ToolResearchHow do changes in communication structures and technologies affect scientific discourse and collaboration?Science is totally rad. So I study it.There are all sorts of ways to study science, of course, and you can’t leave out even one if you want to understand Science as a whole. That means taking a look at its philosophy, history, anthropology, culture and all sorts of other things as well (perhaps even sociology!). It also means looking at (gasp) the science itself, because no self-respecting scholar should claim to understand physics and physicists without being able to calculate the distance the bullet travels before it falls.My overarching research is in modeling and mapping the growth of science on a large scale – thematically, geographically and temporally – hoping eventually to reveal what conditions yield the most rapid rate of discovery and innovation. Looking back, we see times when scientific progress lurches forward at alarming rates, times when studies come to a halt, times when great minds exposit to deaf ears. Sometimes the reasons are obvious: burned libraries, overthrown empires, new sources of funding, technological breakthroughs, wars that need to be won. But these are heavy brush-strokes painted across the canvas of history.If we could somehow view the whole of scientific endeavors for the last thousand years, across every topic and in every city, with the same fine granularity used to research modern-day science, imagine how much we could learn. By zooming out and looking for “hot spots” of innovation in the history of science, and by understanding the environment in which these hot spots formed, we can learn how to induce those same ideal conditions in modern day research.If the synthesis of new ideas in physics tends to come from young researchers working on their own and with backgrounds in other fields, funds can be allotted to make sure more of those exist. If medical innovations come fastest when small groups of experts collaborate, or if science in general runs smoother in small-world type collaborative networks rather than completely connected networks, that information can be used to focus funding in just the right way to improve the rate of innovation.The closest we can come to that fine granularity, to understanding science across contexts, is by using as many research tools as we can find. We must be comfortable working in whatever discipline with whatever methodology is necessary to find the answers sought. Huge historical data sets will be a must. Scientometricians and others in related fields do an amazing job of learning the structure of modern science, but that structure is necessarily bound to the mediums it inhabits. Modern science is a beast of national laboratories, e-mails, universities, cited journals, click-throughs, conferences and page hits.Marshall McLuhan may or may not have been correct when he claimed “the medium is the message,” but there is no doubt that the medium plays a large role in how science is adopted, disseminated and studied. That role cannot be understood without stepping back and viewing all of the alternatives – correspondences, scientific societies, book transcriptions, etc.Dutch Republic of Letters created in collaboration with The Huygens InstituutThe task, then, is to collect as much data as possible, as far back as we can. We should track where books traveled within Medieval Europe and Asia; who corresponded with whom, how often, and about what during the Early Modern period; who taught whom and where scientists studied; how many books were published in what languages; what universities had copies of which journals; where shared resources traveled.This is an impossible amount of data, of course, and can only exist if created collaboratively and in the spirit of openness. These are not ideas to be copyrighted – they are numbers and data points, and they should be accessible and compatible and aggregated in one place. A History of Science Data Commons, so to speak. More on that project coming soon.Trying to understand all of it at once is a big task… and absolutely impossible.  I’ve sliced myself two pieces of the pie that are hopefully manageable and definitely inseparable:Periods of rapid scientific production and progress.Inflection points in scientific communication and collaboration.Changes in communication structures and technologies obviously affect scientific progress deeply, and it is exactly what those effects are that I hope to uncover. Scientific revolutions and media revolutions, what a tired subject! Well, perhaps, but there are two very good reasons they’re overstudied: they’re terribly important, and nobody’s got them right yet.InterestsCourtney and I contact jugglingThankfully for my friends and family I do not work 24/7. When not working, I can often be found juggling, attending renaissance festivals, geocaching, camping, campaigning for rationality, and reading science fiction & fantasy novels. When I feel guilty about not working, but not enough to actually get back to work, I read about physics, cognitive science and linguistics. I am also perpetually writing a history of the obscure art of contact juggling.Juggling has been a big part of my life for nearly a decade now; I was president of Objects in Motion (UF Juggling Club) for a few years and brought the club from 3 to 30 active members, taught lessons at Groovolution dance studio, and performed with Circle & Spice in Bloomington. I’m now involved in the IU Juggling Club and juggle irregularly at the Bloomington Farmer’s Market. I have performed as far north as Calgary, as far east as Amsterdam, as far west as Los Angeles, all the way south in Miami, and all sorts of places in between.None of that would have been possible without my good friends and co-performers in the Spherocity contact juggling troupe: Matt, Jay, Cory, Courtney, Steve, and Leighanna. Thanks to Nick, Nicole, Leah, Ian and the rest of the crew, Objects in Motion keeps growing larger and better and I miss them terribly. And if you’re reading this, Sierra, you should start juggling again.Juggling knives in CalgaryAs if there’s not enough on my plate already, I’m also involved in two wonderful pseudo-academic organizations. I co-founded Sophosessions with Warren C. Moore, the coolest cat I know, in my junior year at UF. The group still meets a little more than monthly and allows its two-dozen members to present talks on whatever they feel like, from Chinese calligraphy to Zen Buddhism to advanced fractal mathematics to building robots. Then everyone goes to Ben & Jerry’s. I still webcast into meetings whenever I can, but it’s just not the same without the ice-cream.The Venerable IU Beer & Algorithms Club fills two Monday nights a month, and I get to listen to a bunch of Computer Science and Math graduates present their favorite algorithms in gory detail, all while eating a tasty meal and enjoying an equally tasty beverage. What could be better?When you stumble on an interesting problem, the question arises: do you blog the problem itself — or wait until you have a full solution to publish as an article?In this case, I think the problem is too big to be solved by a single person anyway, so I might as well get it out there where we can all chip away at it. At the end of this post, I include a link to a page where you can also download the data and code I’m using.When we compare groups of texts, we’re often interested in characterizing the contrast between them. But instead of characterizing the contrast, you could also just measure the distance between categories. For instance, you could generate a list of word frequencies for two genres, and then run a Spearman’s correlation test, to measure the rank-order similarity of their diction.In isolation, a measure of similarity between two genres is hard to interpret. But if you run the test repeatedly to compare genres at different points in time, the changes can tell you when the diction of the genres becomes more or less similar.Spearman similarity to nonfiction, measured at 5-year intervals. At each interval, a 39-year chunk of the collection (19 years on either side of the midpoint) is being selected for comparison. In the graph above, I’ve done that with four genres, in a collection of 3,724 eighteenth- and nineteenth-century volumes (constructed in part by TCP and in part by Jordan Sellers — see acknowledgments), using the 10,000 most frequent words in the collection, excluding proper nouns. The black line at the top is flat, because nonfiction is always similar to itself. But the other lines decline as poetry, drama, and fiction become progressively less similar to nonfiction where word choice is concerned. Unsurprisingly, prose fiction is always more similar to nonfiction than poetry is. But the steady decline in the similarity of all three genres to nonfiction is interesting. Literary histories of this period have tended to pivot on William Wordsworth’s rebellion against a specialized “poetic diction” — a story that would seem to suggest that the diction of 19c poetry should be less different from prose than 18c poetry had been. But that’s not the pattern we’re seeing here: instead it appears that a differentiation was setting in between literary and nonliterary language.This should be described as a differentiation of “diction” rather than style. To separate style from content (for instance to determine authorship) you need to focus on the frequencies of common words. But when critics discuss “diction,” they’re equally interested, I think, in common and less common words — and that’s the kind of measure of similarity that Spearman’s correlation will give you (Kilgarriff 2001).The graph above makes it look as though nonfiction was remaining constant while other genres drifted away from it. But we are after all graphing a comparison with two sides. This raises the question: were poetry, fiction, and drama changing relative to nonfiction, or was nonfiction changing relative to them? But of course the answer is “both.”At each 5-year interval, the Spearman similarity is being measured between the 40-year span surrounding that point and the period 1700-1740. Here we’re comparing each genre to its own past. The language of nonfiction changes somewhat more rapidly than the language of the other genres, but none of them remain constant. There is no fixed reference point in this world, which is why I’m talking about the “differentiation” of two categories. But even granting that, we might want to pose another skeptical question: when literary genres become less like nonfiction, is that merely a sign of some instability in the definition of “nonfiction”? Did it happen mostly because, say, the nineteenth century started to publish on specialized scientific topics? We can address this question to some extent by selecting a more tightly defined subset of nonfiction as a reference point — say, biographies, letters, and orations.The Spearman similarities here happen to be generated on the top 5000 words rather than the top 10000, but I have tried both wordsets and it makes very little difference. Even when we focus on this relatively stable category, we see significant differentiation. Two final skeptical questions need addressing before I try to explain what happened. First, I’ve been graphing results so far as solid lines, because our eyes can’t sort out individual data points for four different variables at once. But a numerically savvy reader will want to see some distributions and error bars before assessing the significance of these results. So here are yearly values for fiction. In some cases these are individual works of fiction, though when there are two or more works of fiction in a single year they have been summed and treated as a group. Each year of fiction is being compared against biographies, letters, and orations for 19 years on either side. That’s a fairly persuasive trend. You may, however, notice that the Spearman similarities for individual years on this graph are about .1 lower than they were when we graphed fiction as a 39-year moving window. In principle Spearman similarity is independent of corpus size, but it can be affected by the diversity of a corpus. The similarity between two individual texts is generally going to be lower than the similarity between two large and diverse corpora. So could the changes we’ve seen be produced by changes in corpus size? There could be some effect, but I don’t think it’s large enough to explain the phenomenon. [See update at the bottom of this post. The results are in fact even clearer when you keep corpus size constant. -Ed.] The sizes of the corpora for different genres don’t change in a way that would produce the observed decreases in similarity; the fiction corpus, in particular, gets larger as it gets less like nonfiction. Meanwhile, it is at the same time becoming more like poetry. We’re dealing with some factor beyond corpus size. So how then do we explain the differentiation of literary and nonliterary diction? As I started by saying, I don’t expect to provide a complete answer: I’m raising a question. But I can offer a few initial leads. In some ways it’s not surprising that novels would gradually become less like biographies and letters. The novel began very much as faked biography and faked correspondence. Over the course of the period 1700-1900 the novel developed a sharper generic identity, and one might expect it to develop a distinct diction. But the fact that poetry and drama seem to have experienced a similar shift (together with the fact that literary genres don’t seem to have diverged significantly from each other) begins to suggest that we’re looking at the emergence of a distinctively “literary” diction in this period. To investigate the character of that diction, we need to compare the vocabulary of genres at many different points. If we just compared late-nineteenth-century fiction to late-nineteenth-century nonfiction, we would get the vocabulary that characterized fiction at that moment, but we wouldn’t know which aspects of it were really new. I’ve done that on the side here, using the Mann-Whitney rho test I described in an earlier post. As you’ll see, the words that distinguish fiction from nonfiction from 1850 to 1900 are essentially a list of pronouns and verbs used to describe personal interaction. But that is true to some extent about fiction in any period. We want to know what aspects of diction had changed.In other words, we want to find the words that became overrepresented in fiction as fiction was becoming less like nonfiction prose. To find them, I compared fiction to nonfiction at five-year intervals between 1720 and 1880. At each interval I selected a 39-year slice of the collection and ranked words according to the extent to which they were consistently more prominent in fiction than nonfiction (using Mann-Whitney rho). After moving through the whole timeline you end up with a curve for each word that plots the degree to which it is over or under-represented in fiction over time. Then you sort the words to find ones that tend to become more common in fiction as the whole genre becomes less like nonfiction. (Technically, you’re looking for an inverse Pearson’s correlation, over time, between the Mann-Whitney rho for this word and the Spearman’s similarity between genres.) Here’s a list of the top 60 words you find when you do that: It’s not hard to see that there are a lot of words for emotional conflict here (“horror, courage, confused, eager, anxious, despair, sorrow, dread, agony”). But I would say that emotion is just one aspect of a more general emphasis on subjectivity, ranging from verbs of perception (“listen, listened, watched, seemed, feel, felt”) to explicitly psychological vocabulary (“nerves, mind, unconscious, image, perception”) to questions about the accuracy of perception (“dream, real, sight, blind, forget, forgot, mystery, mistake”). To be sure, there are other kinds of words in the list (“cottage, boy, carriage”). But since we’re looking at a change across a period of 200 years, I’m actually rather stunned by the thematic coherence of the list. For good measure, here are words that became relatively less common in fiction (or more common in nonfiction — that’s the meaning of “relatively”) as the two genres differentiated: Looking at that list, I’m willing to venture out on a limb and suggest that fiction was specializing in subjectivity while nonfiction was tending to view the world from an increasingly social perspective (“executive, population, colonists, department, european, colonists, settlers, number, individuals, average.”)Now, I don’t pretend to have solved this whole problem. First of all, the lists I just presented are based on fiction; I haven’t yet assessed whether there’s really a shared “literary diction” that unites fiction with poetry and drama. Jordan and I probably need to build up our collection a bit before we’ll know. Also, the technique I just used to select lists of words looks for correlations across the whole period 1700-1900, so it’s going to select words that have a relatively continuous pattern of change throughout this period. But it’s also entirely possible that “the differentiation of literary and nonliterary diction” was a phenomenon composed of several different, overlapping changes with a smaller “wavelength” on the time axis. So I would say that there’s lots of room here for alternate/additional explanations.But really, this is a question that does need explanation. Literary scholars may hate the idea of “counting words,” but arguments about a distinctively “literary” language have been central to literary criticism from John Dryden to the Russian Formalists. If we can historicize that phenomenon — if we can show that a systematic distinction between literary and nonliterary language emerged at a particular moment for particular reasons — it’s a result that ought to have significance even for literary scholars who don’t consider themselves digital humanists.By the way, I think I do know why the results I’m presenting here don’t line up with our received impression that “poetic diction” is an eighteenth-century phenomenon that fades in the 19c. There is a two-part answer. For one thing, part of what we perceive as poetic diction in the 18c is orthography (“o’er”, “silv’ry”). In this collection, I have deliberately normalized orthography, so “silv’ry” is treated as equivalent to “silvery,” and that aspect of “poetic diction” is factored out.But we may also miss differentiation because we wrongly assume that plain or vivid language cannot be itself a form of specialization. Poetic diction probably did become more accessible in the 19c than it had been in the 18c. But this isn’t the same thing as saying that it became less specialized! A self-consciously plain or restricted diction still counts as a mode of specialization relative to other written genres. More on this in a week or two …Finally, let me acknowledge that the work I’m doing here is built on a collaborative foundation. Laura Mandell helped me obtain the TCP-ECCO volumes before they were public, and Jordan Sellers selected most of the nineteenth-century collection on which this work is based — something over 1,600 volumes. While Jordan and I were building this collection, we were also in conversation with Loretta Auvil, Boris Capitanu, Tanya Clement, Ryan Heuser, Matt Jockers, Long Le-Khac, Ben Schmidt, and John Unsworth, and were learning from them how to do this whole “text mining” thing. The R/MySQL infrastructure for this is pretty directly modeled on Ben’s. Also, since the work was built on a collaborative foundation, I’m going to try to give back by sharing links to my data and code on this “Open Data” page.References Adam Kilgarriff, “Comparing Corpora,” International Journal of Corpus Linguistics 6.1 (2001): 97-133.[UPDATE Monday Feb 27th, 7 pm: After reading Ben Schmidt's comment below, I realized that I really had to normalize corpus size. "Probably not a problem" wasn't going to cut it. So I wrote a script that samples a million-word corpus for each genre every two years. As long as I was addressing that problem, I figured I would address another one that had been nagging at my conscience. I really ought to be comparing a different wordlist each time I run the comparison. It ought to be the top 5000 words in each pair of corpora that get compared -- not the top 5000 words in the collection as a whole.The first time I ran the improved version I got a cloud of meaningless dots, and for a moment I thought my whole hypothesis about genre had been produced by a 'loose optical cable.' Not a good moment. But it was a simple error, and once I fixed it I got results that were actually much clearer than my earlier graphs.I suppose you could argue that, since document size varies across time, it's better to select corpora that have a fixed number of documents rather than a fixed word size. I ran the script that way too, and it produces results that are noisier but still unambiguous. The moral of the story is: it's good to have blog readers who keep you honest and force you to clean up your methodology!]Like this:One blogger likes this.from platform as presentation to platform as research workshop (as presentation).I’ve been thinking a lot about and tinkering with the emerging platforms for digital humanities publication such as Omeka and Scalar. They are marvelous and promising platforms for presentation, but what worries me is that they are imagining digital humanities projects as, in the end, simply new forms of the book, article, or exhibition.This seems like only part of what the digital can do. The fact that these platforms are in the end modes of traditional publication in disguise speaks directly to the persistence of picturing scholarship as unidirectional soloing rather than cooperative, ensemble work that mingles individuality with communal knowledge investigation. I suspect as well that the platforms reveal how, despite the best efforts to begin to change them, there is still far to go in rethinking policies about promotion and advancement in academia.From what I can tell, the missing piece in Omeka and Scalar is the design of a digital platform and interface that imagines the digital environment as a research workshop rather than a presentation, publication, or curation. Better said, what a new software platform and interface might do is foreground more emphatically the new possibilities of the research workshop as presentation, publication, and curation.This is particularly a concern when it comes to archivally-driven research (though it is relevant to other bodies of evidence as well). The questions is this: what could a digital “publication” look like if it were imagined as a research workshop? Or another way of putting it is: what would a digital humanities laboratory look like online?With the Making History in a Virtual Archive, what I’m starting to imagine is the digital research workshop as the central element. In place of presentation and passive reception the goal is a platform for more active use of archival materials. In this case, these archival materials relate to vernacular music and the folk revival, but the notion of designing a successful digital research workshop is relevant for any collection or body of digitized materials.A digital research workshop might involve the following two elements as central design and development goals and challenges: (1) annotation; and (2) arrangement.(1) Annotation. What kinds of digital tools and interfaces would allow users to annotate digital repository objects effectively, to collate and rearrange annotations, to layer annotations upon annotations of their own and others. The key here indeed might be “layers.” I am thinking of something along the lines of “layers” in Photoshop: the ability to pile on or pull off multiple levels of annotations from an archival object, to search across annotations to combine them into new layers, and to pull out annotations to begin to assemble interpretation. Something like this:Annotation layers: a sketch.(2) Arrangement. Arrangement (really rearrangement) pushes more intensely at the use of archival objects to develop historical interpretation. The idea here is to create a platform for the manipulation of archival objects—not manipulation in the negative sense (airbrushing out Stalin’s enemies from a photo kind of thing)—but rather the idea of “touching” primary sources (and even secondary sources) to generate new ways of looking at evidence from the past. This is something like Stephen Ramsey‘s idea of “deformative readings” in literature, but here applied in revised form to historical evidence. It’s thinking of the historian as remix artist, developing collages, rearrangements, reconfigurations of primary sources (remember not as final product but as process) in the search for newer and truer meanings of the past. The challenge here would be to develop tools, interface, and design framework for the ability to do this around digital repository objects (without losing the “original” of course) and to sustain an open source community for inventing (rearranging?) new tools of arrangement.With both annotation (and re-annotation) and arrangement (and rearrangement at the center of a DH research workshop platform, the boundaries between archive, research workshop, presentation/publication/exhibition, and scholarly communication begins to get reconfigured through the digital.As I final note, I should say that this turn toward digital research workshop as the center of a digital humanities platform came to me from the topic of my own DH research on mid-twentieth-century United States folk revival festivals and vernacular music. Despite its many problems and issues (and there were many), one of the most intriguing ideas of the folk revival was to place process front and center rather than product. Folk festivals held workshops in which individual performers and their audiences interacted in ways that, while still privileging the performer, became more about the shared experience of exploring folk traditions and updating them in new ways. They were less about putting on “a show” (a presentation, publication, or curation) than exploring the process that went into a show. Better said, the show of a folk festival workshop was the process of showing folk music creation.With the festival workshop, you had meta-musical performance as the musical performance. The workshop became a way of keeping things vernacular, of getting to the democratic levels of cultural exchange and history making—the hand-to-hand and face-to-face and human-to-human levels—that showbiz and commodification and professionalization (and the digital?) always threaten to erase even as they sustain something like vernacular culture, folk music, and intellectual engagement with humanities topics. This entry was posted on Thursday, February 23rd, 2012 at 2:50 pm and is filed under Annotation, Archives/Libraries, Arrangement, Data, Digital Historiography, Digital History, Digital Humanities, Digital Humanities Critical Discourse, Digital Humanities Laboratory, Digital Humanities Methodologies, Digitizing Folk Music History, Digitizing Folk Music History Spring 2012, Interactivity, Interface Design, Metadata, Music, Open Source, Scholarly Publishing, Virtuality. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site.Please use this identifier to cite or link to this item: http://hdl.handle.net/2123/8103Title: Found: Data, Textuality, and the Digital HumanitiesAuthors: Ramsay, StephenIssue Date: 2011Abstract: Computational processes generate lists: lists of numbers, lists of words, lists of coordinates, lists of properties. We transform these lists into more exalted forms -- visualizations, maps, information systems, software tools -- but the list remains the fundamental data structure of computing, from which most other structures are derived. Whenever we treat the world as data, we are nearly always creating lists. But what sort of *texts* are these, and can we consider them the same way that we consider other texts within the humanities? In this paper, I offer some meditations on the nature of lists, and suggest that it is the paucity of information they provide -- and the ways in which that paucity licenses narrative and explanation -- that allows us to imagine computational representations as texts that can play a fruitful role in the wider context of humanistic inquiry.URI: http://hdl.handle.net/2123/8103Appears in Collections:Sustainable data from digital research: Humanities perspectives on digital scholarshipFiles in This Item:Items in Sydney eScholarship Repository are protected by copyright, with all rights reserved, unless otherwise indicated.February 20, 2012I first met Janet Murray when I arrived at MIT almost 25 years ago. At the time, she was working on her book, Hamlet on the Holodeck: The Future of Narrative in Cyberspace, while I was working on Textual Poachers: Television Fans and Participatory Culture. Murray, along with the members of the Narrative Intelligence Reading Group, was an early guide for me to the emerging realm of digital culture and helped to shape my thinking in ways that I will never be able to fully acknowledge.A few years later, Murray and I worked together, along with Ben Singer and Ellen Draper, to create The Virtual Screening Room, the prototype for a fully interactive digital textbook for studying film analysis. In many ways, what we constructed together using Hypercard was more advanced than anything we've seen so far coming out of the realm of e-books, with hundreds of clips on command from almost as many movies to illustrate core concepts in film editing.Shortly afterwords, she left MIT for Georgia Tech, while with William Uricchio, I took over the leadership of our newly created Comparative Media Studies Program. We've remained in touch through the years, with Murray always proving to be a wonderful thinking partner, sometimes affirming, sometimes challenging my own thinking, and always overseeing cutting edge projects which stretched the affordances of digital media in the service of expanding human expression or more fully realizing its pedagogical potential. We've both found ourselves under attack for being "narratologists" in the famous "Ludologist" debates, and sharpened our own thinking about games as a medium in the process. I was delighted (well, for many reasons) when the British magazine, Prospect, identified both of us as among the top thinkers for the digital future.Murray recently released her long-awaited new book, Inventing the Medium: Principals of Interaction Design as a Cultural Practice. On one level, the book is a textbook designed to help designers in training develop a fuller, more robust understanding of digital media, one which builds productively on principles she first outlined in Hamlet on the Holodeck, but which also reflects on the past decade plus of developments as many once cutting edge practices have now become normalized and routinized within digital media. This book captures some of the thinking she did as the chair of her program at Georgia Tech, which remains one of the most forward thinking about new media platforms and practices.On another level, the book is a theory of media -- especially of media change -- with a strong emphasis on the intersections between technology and culture. I was delightful to see how much more deeply Murray had read and thought about media theory since the first book, and in the process, she is pushing all of us to think more deeply about what it might mean to consider the digital as its own medium rather than as a delivery system for multiple media or what it might mean to think about the local choices made in digital design as contributing to a larger evolution of that medium. Murray's writing has shifted in a more technical direction than her first book, which was very much an argument for why humanists should engage with new media production and critique, but she remains very much a humanist at heart, who sees digital media as making vital contributions to our contemporary culture. Inventing the Medium is an epic accomplishment, one which we will all be mining for years to come.In this interview, Murray reflects about the larger conceptual framing of the book, what it has to say about the nature of media change and the role of design in constructing contemporary culture. Her thoughtful and engaging responses to my questions should provoke further reflections about the state of the art in digital design.Your earlier book, Hamlet on the Holodeck, has been described as an experiment in speculative poetics, in that you were reading early signs of what kind of affordances digital media would offer for human expression. Inventing the Medium now has several decades of experiments and innovations to draw on. How did this change the way you approached this project?When I sat down to write Hamlet on the Holodeck (HoH) I challenged myself to prove what I believed from the day my students at MIT showed me Eliza and Zork - that this was the beginning of a new medium of expression that could be as rich as print or film. To do that, I had to step back and say what were the equivalents of the material affordances that made film a new medium and not just a way of recording plays or acting out novels. So I came up with the formulations in Chapter 3 of HoH which is probably the most widely read part of that book - that the equivalent of cutting the film and changing the focus of the lens, etc. for cinema was the procedural, participatory, encyclopedic, and spatial affordances of the computer as a medium of representation. At that point my interest was just in talking about these 4 properties as affordances for storytelling, but it immediately became clear to me in my work as an interaction designer, leading educational computing and (what would now be called) "digital humanities" projects that talking about these affordances and the aesthetics of interactivity and immersion that come out of them was a great way of focusing design teams and conceptualizing key design choices. So the new book picks up that focus on the design process itself, and it looks beyond narrative to see the design of any digital artifact - any device, web page, app, archive, based on electronic bits and running code - as part of a common enterprise that I call "Inventing the Medium."So the new book grows out of the previous one, but it also reflects the very different experience I've had since moving to Georgia Tech in 1999 where I served as Director of Graduate Studies (2000-2010) and where I established and continue to teach courses in interaction design, game design as a cultural practice,,and interactive television for students go on to work for all the major players in digital media from Apple and Ideo to Disney Imagineering, Electronic Arts, and Zinga, to Turner Broadcasting, Showtime, DirectTV, and AOL to Google, Amazon, and IBM and so on. Working in this community of diverse creative abilities, brought me much closer to the concrete design challenges of commercial world than my work at MIT. And my contact with all those companies, as well as my work with the American Film Institute as a Mentor and Trustee throughout the heady changes of the 2000s gave me a first-hand look at how productive change can be nurtured or thwarted within a community of practice. . . So the short answer to your question is, my core ideas from 1997 have proven quite useful despite the profound disruptions and exhilarating inventions of the past 15 years because my experience at MIT from the 1980s through 1990s anticipated a lot of the challenges that hit the wider society later. And the principles I'm always trying to teach my students and that I did my best to put down in Inventing the Medium (ITM) should last over the next several decades of change, because they are not about how to design for any particular platform, but about how to approach the digital design process itself so that decisions you make today will align with the trends of lasting innovation and solutions that you arrive at in the context of today's gizmos can teach you something and inform choices that you will make as a designer in the unknown future environment.You describe this book as documenting "the collective cultural task of inventing the underlying medium." In what sense is this a cultural as well as a technological project? To what degree do you think designers are aware of their impact on the future of a medium as opposed to the pragmatic issues of designing an App?In my own teaching I encourage designers to have a kind of double consciousness, sort of short-term and long-term. The short-term consciousness involves serving the immediate users - and the more specifically we can think about them the better - and honoring the constraints of the immediate task, which can mean using a specific platform or limiting functionality in some way. But another part of their mind has to be fixed on the horizon, on the immediate work as part of a larger cultural task, that draws on media conventions from the past that have made for coherent communication, and that creates a foundation of conventions that will make for ever greater coherence going forward. I have identified design in this book as a cultural project but I think there can be a whole bookshelf or Kindle folder full of books elaborating on that idea, and taking other aspects of our understanding of human culture as starting points for understanding digital design. For me, the key cultural task is the creation of media conventions - the equivalent of the headline, the byline, the chapter division, the cinematic establishing shot or 180 degree rule - the organizing conventions that allow us to build greater complexity and expressivity into the rituals by which we share our understanding of the world and our empathy for one another. The cultural task I have in mind is meaning-making. I think this is the same task that babies undertake and early humans must have undertaken in clapping hands in imitation of one another, in pointing to something to direct attention to it, in intentionally clapping hands in synchrony with another person. These are the the radical cultural primitives, and language, drawing, writing, print, photography, and now computation are all ways of expanding our ability to clap, to point, to think together and synchronize our minds and our behaviors.You draw on some of the same core concepts here as in Hamlet on the Holodeck. Which ones have had to be rethought the most to reflect the actual changes which have taken place?Well if I were writing HoH again I would have to make changes, and I intend to do something like that for my next book - sort of a Return to the Holodeck (!) But Inventing the Medium is really a matter of taking the same ideas deeper, and so I see it as continuous with HoH.. The main change is that just as I had to think deeply about the affordances of the medium for HoH in order to think about interactive storytelling as a special case of digital affordances, for ITM which focuses on digital affordances as the basis of a design process, I had to think much more deeply about what a medium is. In HoH I took "medium" for granted. For ITM I had to think about whether what I was claiming about a medium was true for other media. I actually had another 50,000 words about this that I had to cut out and condense into parts of the Introduction, Chapter 1, and the last chapter on the Game Model, because it slowed down the main argument about design too much to go into it. But I am writing more about that in other places. I gave a talk about it for the MECCSA in the UK and I'm going to be writing that up for an article in Convergence. I have two main insights about what a medium is that I can state briefly here. One is that any medium is composed of three parts: Inscription, transmission, and representation. (I define all this in the book and summarize it in the Glossary which is also reproduced on my blog ) . The other is that the most productive paradigm for designers in thinking about a medium, to my mind, is the paradigm of focused attention. And actually this paradigm, come to think of it, came indirectly from one of the most dramatic reactions to HoH, which was the hostility (which you received as well) from the ludologists who were trying to set off a place for Game Studies separate from what they thought of as the "hegemony" of "narratology." I found it very useful to my thinking that they foregrounded games as its own communicative and representational genre. This led me to think about the place of games in human culture, and I realized in reading Michael Tomasello and Merlyn Donald, neither of whom talk about games explicitly, that the experience of focused attention and "theory of mind" that the cognitive folks think of as distinguishing us from our primate cousins, is really the pleasure we find in synchronizing our behavior with one another - which is the essence of games. For me it was a particularly illuminating moment when I read in Merlyn Donald's work the statement of how much human culture could accomplish without first inventing language. This was amazing to me as a hyperverbal person of course. But then it was illuminating to my thinking about what a medium is. And it led me to think of focused attention as the key to the design of a new medium.Janet H. Murray is an internationally recognized designer and media theorist, and Ivan Allen College Dean's Professor of Digital Media at Georgia Tech where she also directs the Experimental Television Laboratory. She holds a PhD in English from Harvard University and was a pioneer of digital humanities applications at MIT in the 1980s and 1990s, moving to Georgia Tech in 1999, and serving as Director of Graduate Studies in Digital Media from 2000-2010 during which time she led the redesign of the MS curriculum and the founding of one of the first PhDs in the field. She is the author of Hamlet on the Holodeck: The Future of Narrative in Cyberspace (Free Press, 1997; MIT Press 1998), which has been translated into 5 languages, and is widely used as a roadmap to emerging broadband art, information, and entertainment environments, and Inventing the Medium: Principles of Interaction Design as a Cultural Practice (MIT Press, 2011). At Georgia Tech, her interactive design projects include a digital edition of the Warner Brothers classic, Casablanca, funded by NEH and in collaboration with the American Film Institute; the Interactive Toolkit for Engineering LearningProject, funded by NSF; and a series of prototypes for the convergence of television and computation, created in collaboration with PBS, ABC , MTV, Turner, Intel, Alcatel-Lucent, and other networks and media companies. Murray is an emerita Trustee of the AFI and a current board member of the George Foster Peabody Award. In December 2010 Murray was named one of the "Top Ten Brains of the Digital Future" by Prospect Magazine.Why graph? And why, in particular, use innovative and unfamiliar graphing techniques? I started this blog without addressing these questions, but a recent blog post by Adam Crymble, critical of “shock and awe” graphs made me realize the need to explain EDA (Exploratory Data Analysis) and data visualization. Crymble wisely challenged data visualization practitioners to ask themselves the following questions: “Is this Good for Scholarship? Or am I just trying to overwhelm my reviewers and my audience?” This is sound advice, and Crymble’s concerns strike me as genuine. But, upon reflection, his post led me to think that “shock and awe” are evitable parts of any bold scholarly intervention. Feminist scholarship provoked genuine anger when it asserted that academic conventions were rife with sexist assumptions. The linguistic turn alarmed traditional scholars with its new understandings of literary production. Certainly these interventions produced (and continue to produce) needlessly complex, derivative prattle. But can anyone seriously argue that the humanities are not richer for these intellectual challenges?What follows, therefore, is a defense of “shock and awe”: a justification for data visualizations that are unfamiliar, challenging, and demand news ways of thinking.Why graph instead of just showing the numbers?By just “show the numbers,” humanities researchers often refer to tables. The problem with this preference for tables it that is assumes that tables are somehow more transparent and accessible than graphs. In fact, the opposite is true. A list of data values is like a phone directory: a wonderful way to look up individual data points, but a terrible means of discerning or discovering patterns. (Kastellec and Leoni 2007; Gelman, Pasarica, and Dodhia 2002) Alternately, a table of individual data points is analogous to collection of primary text sources: it’s the raw material of research, not research. Further most published tables are not transparent, “raw” data. On the contrary, tables in most research consolidate observations into groups, listing, for example, average wages for “skilled craftsman in Flanders 1830-35,” or “Osaka dyers 1740-80.” But why those years ranges and those occupational categories? Why 1830-35 instead of 1830-1840? Why Osaka dyers and not the broader category of Osaka textile workers? Those groupings may be conceptually valid, but they are interpretative and preclude other interpretations. Certainly we can lie with graphs, but we can also lie with tables. And since a good graph is better than the best table, DH researchers need to use good graphs.Why these novel, unfamiliar graphs?The data visualization movement has certainly produced some bad graphs —obfuscating rather than illuminating. But it is impossible to argue that newer graph forms are more misleading than the status quo. The pie chart, for example, is easy to misuse and the many variants supported by Excel are simply awful. With a 3D exploding pie chart, even a novice can make 5% look larger than 10% or even 15%. Can you correctly guess the absolute and relative sizes of the slices in this graph?(See answers below). Since pie charts are familiar, they are accessible, but that simply makes them easier to misuse. Are conventional bad graphs such as pie charts “better” than newer chart forms because they provide easier access to faulty conclusions? Is “schlock” worse that “shock”?My survey of graphing techniques in history journals tuned up an alarming result. Historians rely primarily on graphing techniques developed over 200 years ago: the pie chart, bar chart, and line chart. It is hard not to shock the academy with strange graphs, when “strange” means anything developed in the past two centuries. Many new graphing techniques, such as parallel coordinate plots, are still controversial, difficult to use, and difficult to interpret. But many others are readily accessible and widely used, except in the humanities, The boxplot, developed in 1977 by John Tukey, is now recommended for middle school instruction by The National Council of Teachers of Mathematics. The intellectual pedigree of the boxplot is beyond question: Tukey, a professor of statistics at Princeton and researcher at Bell Labs, is widely considered a giant in 20th century statistics. So, what to do when humanities researchers are flummoxed by a boxplot? I now append a description of how to read a boxplot, but isn’t it an obligation of quantitative DH to push the boundaries of professional knowledge? And shouldn’t humanities Ph.D.’s have the quantitative literacy of clever eighth graders? In short, since our baseline of graphing skills in the humanities is so outdated and rudimentary, there is no avoiding some “shock and awe.”A graph in seven-dimensions? What are you talking about? You must be trying to trick me!Certainly “seven dimensions” sounds like a conceit designed to confuse the audience, or intimidate them into acquiescence. But a “dimension” in data visualization is simply a variable, a measurement. Decades ago Tufte showed how an elegant visualization, Menard’s graph of Napoleon’s invasion of Russia, could show six dimension on a 2D page: the position of the army (latitude and longitude), size of the army, structure of the Russian army, direction of movement, date, and temperature. Hans Rosling’s gapminder graphs use motion to represent time, thereby freeing up the x-axis. By adding size, color and text, Rosling famously fit six dimensions on a flat screen: country name, region, date, per capita GDP, life expectancy, and total population. These are celebrated and influential data visualizations, the graphic equivalents of famously compelling, yet succinct prose. While Crymble assumes that needlessly complex graphics stems from bad faith (a desire to intimidate and deceive), I am more inclined to assume that the researcher was reaching for Menard or Rosling but failed.“How do you know there hasn’t been a dramatic mistake in the way the information was put on the graph? How do you know the data are even real? You can’t. You don’t.”This concern strikes me as overwrought and dangerous. Liars will lie. They will quote non-existent archival documents, forge lab results, and delete inconvenient data points. When do we discover this type of deceit? When someone tries to replicate the research: combing through the archives, running a similar experiment, or trying to replicate a graph. How are complex graphics more suspect, or more prone for misuse than any other form of scholarly communication? Is there any reason to be more suspicious of complex graphs than any other research form?I can optimistically read Crymble’s challenge as a sort of graphic counterpart of Orwell’s rules for writers. But Crymble seems to view data viz as uniquely suspect. To me this resembles the petulant grousing that greeted Foucault, Derrida, Lyotard, Lacan, etc some three decades ago – “what is this impenetrable French crap!” “You’re just talking nonsense!” Certainly many of those texts are needlessly opaque. But much of it was difficult because the ideas were new and challenging. The academy benefitted from being shocked and awed. Data visualization can and should have the same impact. The academy needs to be shocked — that how change works.Gelman, Andrew, Cristian Pasarica, and Rahul Dodhia. 2002. “Let’s Practice What We Preach: Turning Tables into Graphs.” The American Statistician 56 (2): 121-30.Kastellec, Jonathan P., and Eduardo L. Leoni. 2007. “Using Graphs Instead of Tables in Political Science.” Perspectives on Politics 5 (4): 755-71.The pie chart:Apple10Borscht17Cement13Donut20Elephant25Filth15In last December’s NITLE Digital Scholarship Seminar, Teaching DH 101, I presented my experience designing and proposing a new digital humanities course at St. Norbert College. In that talk, I found myself arguing, somewhat to my surprise, for interdisciplinarity—by which I mean clear association with one of the humanities disciplines that converge under the digital humanities tent—in digital humanities courses. In short, I claimed that a digital humanities course grounded in a familiar academic discipline might stand better chance of being understood and approved by curricular committees and, frankly, students who are unlikely to have heard, much less understand, the term “digital humanities.” I use the term interdisciplinary with a strikethrough not to disavow the cross-field collaborations that underlie and energize digital humanities work, but to highlight the idea that interdisciplinary work, by definition, requires collaborators from distinct disciplines.In his contribution to Debates in the Digital Humanities, Matt Kirschenbaum calls digital humanities a “tactical term” that can help DHers position themselves for institutional authorization of various sorts:On the one hand, then, digital humanities is a term possessed of enough currency and escape velocity to penetrate layers of administrative strata to get funds allocated, initiatives under way, and plans set in motion. On the other hand, it is a populist term, self-identified and self-perpetuating through the algorithmic structures of contemporary social media.Geoffrey Rockwell notes that he “used to argue for disciplinarity”—by which he means developing digital humanities itself as a distinct discipline—becauseThe advantage of choosing disciplinarity is that we can build formal ways in; we can develop graduate programs, skills training and a common discourse that provide people with open and negotiable guides to participation. If some level of programming is desirable we can create courses to introduce humanities students to coding and code studies.Rockwell admits, however, that he is “no longer confident that we want to take the route of forming a discipline with all its attendant institutions,” and he asks, “Is there some way to maintain both the permeability of an interdisciplinary commons where the perspectives of different disciplines are welcome in the commons while encouraging appropriate skills and rigour?”I want to suggest that relying on the term “digital humanities” can at times be a tactical error, especially for solo practitioners at institutions—large or small—without an established DH culture—in other words, at the vast majority of colleges and universities. By thinking through the interdisciplinarity of DH courses, I hope to offer a model for “curricular incursion” that might aid such practitioners. I haven’t abandoned Bethany Nowviskie’s strident call for interdisciplinary methodological training. In many ways, I hope to suggest a practical model for a “pandemic” curricular reform that can reshape institutions beyond the (meteoric?) glow of major DH centers.An example might help clarify both the challenges I’m describing and their potential solutions. The first DH course I proposed, mere weeks after starting at St. Norbert College, was rejected. I was a newly-minted Ph.D. and new Assistant Professor. I was enthusiastic and idealistic. And I had had little understanding of college-wide curricular planning or the committees who do that work. I proposed “Introduction to Digital Humanities” and based my syllabus on a range of examples I’d found through Twitter and the Digital Humanities Education Zotero group. I was confident that my new colleagues would enthusiastically approve this (for the College) revolutionary new course. They had hired me, after all; surely they were keen to see digital humanities in the curriculum.The note that the committee chair sent me explaining the committee’s denial of my “DH 101″ proposal taught me a good deal about the practical necessities of the small college curriculum, and helped me craft a followup proposal that was approved the next semester. Those lessons were:Digital humanities remains an obscure term for the majority of our colleagues. However much press DH gets in the Chronicle during the MLA and AHA Conventions, it is still not a term that signifies for most humanities scholars. The members of our curricular committee were unsure whether I was proposing a course about technology or a course that would especially use technology. They asked, “How much of the course will be ‘learning about’ as distinct from ‘learning how to?’” Of course, we might recognize echoes of many debates within DH in their questions, which grew for them (as the debates perhaps grow for the field) from their uncertaintly about the term “digital humanities” itself. I should note that their concern wasn’t nitpicky for nitpicking’s sake. While noting their own confusion with the term, they rightly worried that students and advisors would be no more savvy about “digital humanities,” and so the course would be unlikely to enroll sufficiently. The revised course title, “Technologies of Text,” rhetorically grounded the course for my colleagues. Though the course remains a mixture of theory and practice, the new title conveyed a clear sense of what the course is ultimately about for the committee, advisors, and students.Semantics (can) matter a great deal. One major sticking point for our curricular committee was that I titled my course “Introduction to Digital Humanities,” yet wanted to offer it at the junior or senior level. Introductory courses, they noted, are offered at the 100 level. Courses at the 300 and 400 level should build on those introductory courses. At first I was perturbed by this semantic squabbling, but since then I’ve come to understand their perspective. I was assuming, of course, that my course would build on skills that students picked up in their lower-division courses. I expected my students to be competent researchers and independent workers. I planned to build on their work in lower-division literature and history courses. And because I’m the only DHer at St. Norbert College, students were unlikely to matriculate from my “Intro to DH” course into a more advanced DH course. Instead, I was proposing a digital capstone to the traditional humanities skills they’d picked up elsewhere in the curriculum. My colleagues were right to wonder how my proposed class could be considered an “intro” course. By dropping any pretense of my course as an “intro” course, I could help the College community understand that it truly is a course for more advanced students.Our colleagues understand “interdisciplinary” from the perspective of their disciplines. That is to say, my “Intro to DH” proposal looked to the members of the curricular committee—itself an interdisciplinary group—like a methodological mishmash. They didn’t understand what our readings in history, literature, computer science, and other fields added up to. By recasting this as a literature course that incorporates insights from history, computer science, and other fields, I was able to establish my authority to teach the course—I am a literature professor, after all, and not a “digital humanities” professor—and help my colleagues clearly see the lines of disciplinary intersection.A lone digital humanist is not equipped to teach the entire field. This realization came less from the committee’s rejection of my proposal than from the work I did revising the proposal in response. Changing the course title and description freed me to concentrate on literary DH rather than attempting to the entire arsenal of DH disciplines and methodologies. This is probably a good thing. I’m not prepared to teach all of DH. At a larger school I could collaborate with colleagues expert in diverse DH technologies or methodologies, and I’ve been bringing experts into the class virtually to talk about particular subjects. Ultimately, though, I must be prepared to answer my students’ questions about any topic we cover. Maintaining disciplinary focus perhaps limits my students’ sense of the wider DH field, but it allows me to teach a few things well rather than teaching everything poorly.I’m currently teaching “Technologies of Text,” which enrolled to capacity. In many ways the course I’m teaching covers the same ground as the “Introduction to Digital Humanities” course I proposed. Nevertheless, the disciplinary focus of the revised proposal made the goals and content of the course more readily understandable to my colleagues and my students. If “digital humanities” is indeed a tactical term, then it follows that it should be deployed or reserved tactically. If your goal is incursion into a curriculum resistant to digital humanities methodologies, the best tactic might be to foreground the traditional disciplinarity of the course rhetorically while building new methodologies into the practice of the course.To think more broadly for a moment: I would suggest that DH will only be a revolutionary interdisciplinary movement if its various practitioners bring to it the methods of distinct disciplines and take insights from it back to those disciplines. As Ted Underwood recently argued (against Stanley Fish’s New York Times provocations), “digital humanities is not a discipline or a coherent project. It’s a rubric under which a bunch of different projects have gathered—from new media studies to text mining to the open-access movement.” For Underwood, “what’s actually interesting and new about this moment” are its “new opportunities for collaboration both across disciplines and across the boundary between the conceptual work of academia and the infrastructure that supports and tacitly shapes it.” I heartily agree.These collaborations will only change humanities fields, however, if they lead to scholarship that has a distinct impact within those fields. But that’s not always the (apparent? explicit?) goal of digital humanists, and many scholars outside of the big tent see DH, rightly or wrongly, as a separate entity: a roped-off area even within disciplinary conferences. For instance, I recently had a paper accepted to a conference in my literary and historical field. My talk will draw on my recent geospatial work, and is very much a “digital humanities” talk. The conference organizer included in her acceptance letter an odd apology. She noted that, while two of the papers in my panel were digital humanities papers, the other two were not. She hoped I would not mind presenting with two “non-digital” scholars. I assured her that I would look forward to the conversation.As digital humanities sessions multiply at conferences such as MLA and AHA, so too does the divide between the DH panels and everything else. At this year’s MLA I attended one non-DH panel, and was struck by how thoroughly different that session was from anything else I had experienced at the conference. That room—and, let’s face it, the vast majority of rooms in the Washington Convention Center—was entirely unaffected by the digital humanities revolution. I realize, of course, that the only MLA I have known in my career is remarkably changed from the MLA of very recent history. The Executive Director tweets. The new Director of Scholarly Communication is a ProfHacker. So the MLA is a DH-friendly place now, far removed from the hostile environment I have heard described by earlier generations of DHers. But for digital humanists to make a real incursion into the field of literary studies, we have to start presenting in non-DH panels. We have to invite non-digital scholars to present on largely DH panels. We have to start actively seeking out colleagues who don’t know what we do—perhaps even those who don’t like what we do. We have to talk with colleagues who don’t tweet.Including those colleagues on our local curricular committees. At some institutions the digital humanities community can operate independently, perhaps with the flexibility of institutional or grant funding far and above what their more traditional humanities colleagues can hope for. As DH grows rapidly, however, the vast majority of its practitioners will work within institutional structures formed by traditional humanities categories. I don’t write this out of despair. As many of you know, I’m with Thoreau: “I do not propose to write an ode to dejection.” Instead, I want to suggest that these instituions offer a unique opportunity for DHers to emphasize the interdisciplinarity of their work and open new conversations that will expand the “big tent” of digital humanities even further.[Cross-posted from the Visualizing the Past blog for the NCPH2012 working group "Graphs, Maps, and Trees: Imagining the Future of Public Interfaces to Cultural Heritage Collections"]In the very useful survey of the “history web” in their 2005 book Digital History: A Guide to Gathering, Preserving, and Presenting the Past on the Web , Dan Cohen and Roy Rosenzweig identify the range of genres that encompassed the historical content on the web: archival sites, exhibits and scholarly essays, teaching and learning sites, and discussion forums and organizational sites. Even though Cohen and Rosenzweig failed to account for the way that blogs, YouTube, and social media would eventually permeate the history web, I like their categories because they continue to the give us a way to think about what we do when we create public history online. We tend to provide access to collections, to offer interpretation, to offer instruction, and to offer a forum for conversation, both general and professional. So, as I began to think about the critical issuing in effectively using data visualizations in public history, I wanted to consider them in relationship to the activities above. Since Sheila has already written a great post on collections and enhancing access with visualizations, I’d like to focus both on their interpretative and instructive use, building on Trevor’s thoughts from his last post on discovery and communication.For public historians, the mode of online outreach that has the longest history is that of interpretative exhibits, whether as companions to a physical exhibit or as independent works of scholarship. Despite the liberating possibilities for disjunction, many of exhibits hue very closely to the linear narrative structure of traditional narrative history. In doing so, they have demonstrated varying degrees of success in offering the public a glimpse of the richness of the past. Two sites from the National Museum of American History demonstrate the wide range of approaches. Both “The Price of Freedom” and “A More Perfect Union” are beautiful sites, but one presents a linear and reductive narrative of military history and the other presents the difficult topic of Japanese internment during World War II with a range of voices and perspectives that highlights historical complexity. The difference here is in the effort to bring together evidence in a user interface that allows for the consideration of many perspectives and multiple causality, as opposed to offering a single perspective that simplifies the past.Successful or unsuccessful, most exhibit sites have the benefit of offering visitors a range of contextual information in both the text of the basic narrative and in the descriptions that accompany individual artifacts, images, or documents. This contextual information is essential for a public who may not have a deep background to bring to their encounter with primary historical materials. Data visualizations can short circuit the tendency to present simplistic narratives about our collections. Unfortunately, however, data visualizations that concentrate the user interface into a single interactive screen can also significantly reduce our ability to offer the public necessary historical context if we’re not careful.Take, for example, the great interactive correspondence visualization created by the historians and computer scientists at Stanford for the Mapping the Republic of Letters project. This complex interface really only makes sense to individuals who are content experts, and sometime then only after they’ve read the accompanying pdf explaining the different facets of the tool. For a content novice, the tool is little more than a colorful toy because it links to primary sources in a subscription database, and because it lacks the biographical data on the correspondents that might make their network connections intelligible. If the visualization included access to a larger context of the enlightenment and background on the individual correspondents, it could be a powerful and concentrated way for the public to learn about the development of this period in Euro-American intellectual history. To some extent, we can excuse the Stanford project because it is explicitly a research venture targeting scholars of the enlightenment, rather than members of the general publicThe Digital Vaults site from the National Archives and Records Administration, on the other hand, was created precisely to engage the public. Unfortunately, the project is completely hampered by its abstraction. The Flash version of the site gives users access to a seven randomly selected sources from a database of over 1,200. Clicking on a document, the user enters a web of connections to other documents based on shared tags. (The HTML version simply offers an alphabetical list of tags.) The sources have minimal accompanying metadata–usually title, date, and a brief description. While this environment is attractive and fun to play with, it fails to offer users enough context to make any historical sense out of the materials they encounter. Rather than offering and entree into NARA’s rich collections, the site leave users at sea with only their pre-existing historical knowledge to support them.Unlike Digital Vaults, visualizations that make use of geospatial and temporal cues offer users more necessary orientation. One successful example is Minnesota Historical Society’s True North project, which offers users the ability to layer information in space in time to glean some understanding of the state’s history. While the interface does not link out to individual primary sources, it manages to offer enough cohesiveness that users can start to construct their own narratives of change. The National Museum of Australia’s History Wall is even more successful. Built on the backbone of a flexible timeline, the interface allows users to explore the lives of Irish in Australia between 1770 and the present, drawing on, among other things, the Australian Dictionary of Biography, and amazing Trove database that aggregates over 280 million sources from National Library of Australia. Together these elements let a user to explore within a much more deeply layered context that can push them to use heuristics that are important to historians as they make sense of the evidence from the past.Considering this range of examples, I hope that we can begin to have a conversation about how to create and frame data visualizations that provide the public with new ways to access our content but also offer them enough context to help them begin to make sense of those materials in meaningful ways.Well, it looks like Digital Humanities Now scooped me on posting my own article. As some of you may have read, I recently did not submit a paper on the Republic of Letters, opting instead to hold off until I could submit it to a journal which allowed authorial preprint distribution. Preprints are a vital part of rapid knowledge exchange in our ever-quickening world, and while some disciplines have embraced the preprint culture, many others have yet to. I’d love the humanities to embrace that practice, and in the spirit of being the change you want to see in the world, I’ve decided to post a preprint of my Republic of Letters paper, which I will be submitting to another journal in the near future. You can read the full first draft here.The paper, briefly, is an attempt to contextualize the Republic of Letters and the Scientific Revolution using modern computational methodologies. It draws from secondary sources on the Republic of Letters itself, especially from my old mentor R.A. Hatch, some network analysis from sociology and statistical physics, modeling, human dynamics, and complexity theory. All of this is combined through datasets graciously donated by the Dutch Circulation of Knowledge group and Oxford’s Cultures of Knowledge project, totaling about 100,000 letters worth of metadata. Because it favors large scale quantitative analysis over an equally important close and qualitative analysis, the paper is a contribution to historiopgraphic methodology rather than historical narrative; that is, it doesn’t say anything particularly novel about history, but it does offer a (fairly) new way of looking at and contextualizing it.A visualization of the Dutch Republic of Letters using Sci2 & GephiAt its core, the paper suggests that by looking at how scholarly networks naturally grow and connect, we as historians can have new ways to tease out what was contingent upon the period and situation. It turns out that social networks of a certain topology are basins of attraction similar to those I discussed in Flow and Empty Space. With enough time and any of a variety of facilitating social conditions and technologies, a network similar in shape and influence to the Republic of Letters will almost inevitably form. Armed with this knowledge, we as historians can move back to the microhistories and individuated primary materials to find exactly what those facilitating factors were, who played the key roles in the network, how the network may differ from what was expected, and so forth. Essentially, this method is one base map we can use to navigate and situate historical narrative.Of course, I make no claims of this being the right way to look at history, or the only quantitative base map we can use. The important point is that it raises new kinds of questions and is one mechanism to facilitate the re-integration of the individual and the longue durée, the close and the distant reading.The project casts a necessarily wide net. I do not yet, and probably could not ever, have mastery over each and every disciplinary pool I draw from. With that in mind, I welcome comments, suggestions, and criticisms from historians, network analysts, modelers, sociologists, and whomever else cares to weigh in. Whomever helps will get a gracious acknowledgement in the final version, good scholarly karma, and a cookie if we ever meet in person. The draft will be edited and submitted in the coming months, and if you have ideas, please post them in the comment section below. Also, if you use ideas from the paper, please cite it as an unpublished manuscript or, if it gets published, cite that version instead.Welcome to the scottbot irregular. My name’s Scott, and the US Government has for some reason seen fit to give me money to study Science. It’s ‘Science’ with a capital ‘S’ because I’m not studying individual aspects of the world using science, but rather studying Science in general as a social, historical, philosophical, and intellectual phenomenon. What’s worse, I’m attempting to do it scientifically. This blog is my attempt at giving the country its money’s worth. Also, I kinda would love feedback on my eventual dissertation. See? Everybody wins.scott b. weingartis pretty clueless about a lot of things. This is his attempt to be less so.I pledge to be a good scholarly citizen. This includes:Opening all data generated by me for the purpose of a publication at the time of publication.Opening all code generated by me for the purpose of a publication at the time of publication.Freely distributing all published material for which I have the right, and fighting to retain those rights in situations where that is not the case.Fighting for open access of all materials worked on as a co-author, participant in a grant, or consultant on a project.I pledge to support open access by:Only reviewing for journals which plan to release their publications openly.Donating to free open source software initiatives where I would otherwise have paid for proprietary software.Citing open publications if there is a choice between two otherwise equivalent sources.I pledge never to let work get in the way of play.I pledge to give people chocolate occasionally if I think they’re awesome._[This is somewhat out of date. Please stand by for new information!]Hello World!Student of History & Philosophy of Science and Information Science at Indiana University.You’ve managed to stumble across my little corner of the internet. I’m currently a student and researcher in the HPS and SLIS departments at IUB under two of the most interesting and capable professors I’ve had the fortune to meet: Colin Allen and Katy Börner. I studied history of science and computer engineering at UF, where I slaved researched for the infinitely patient Robert A. Hatch, who taught me more in four short years than I’d yet learned in aggregate over my entire life.Early InPhO Concept MapThese days, I split my time between classes, the Indiana Philosophy Ontology Project (InPhO) and the Cyberinfrastructure for Network Science Center (CNS). At InPhO I program and design visual, navigable representations of our dynamically generated taxonomy of ideas; analyze relational networks (influenced, disagreed with, etc.) from our Thinkers database; and map and compare philosophical ontologies. The CNS keeps me busy with all sorts of scientometric analyses, and I am also involved in the development of large scale network analysis software such as the NWB, creating workflows, providing software feedback, writing documentation and teaching workshops.Co-authorship network created using the Network Workbench ToolResearchHow do changes in communication structures and technologies affect scientific discourse and collaboration?Science is totally rad. So I study it.There are all sorts of ways to study science, of course, and you can’t leave out even one if you want to understand Science as a whole. That means taking a look at its philosophy, history, anthropology, culture and all sorts of other things as well (perhaps even sociology!). It also means looking at (gasp) the science itself, because no self-respecting scholar should claim to understand physics and physicists without being able to calculate the distance the bullet travels before it falls.My overarching research is in modeling and mapping the growth of science on a large scale – thematically, geographically and temporally – hoping eventually to reveal what conditions yield the most rapid rate of discovery and innovation. Looking back, we see times when scientific progress lurches forward at alarming rates, times when studies come to a halt, times when great minds exposit to deaf ears. Sometimes the reasons are obvious: burned libraries, overthrown empires, new sources of funding, technological breakthroughs, wars that need to be won. But these are heavy brush-strokes painted across the canvas of history.If we could somehow view the whole of scientific endeavors for the last thousand years, across every topic and in every city, with the same fine granularity used to research modern-day science, imagine how much we could learn. By zooming out and looking for “hot spots” of innovation in the history of science, and by understanding the environment in which these hot spots formed, we can learn how to induce those same ideal conditions in modern day research.If the synthesis of new ideas in physics tends to come from young researchers working on their own and with backgrounds in other fields, funds can be allotted to make sure more of those exist. If medical innovations come fastest when small groups of experts collaborate, or if science in general runs smoother in small-world type collaborative networks rather than completely connected networks, that information can be used to focus funding in just the right way to improve the rate of innovation.The closest we can come to that fine granularity, to understanding science across contexts, is by using as many research tools as we can find. We must be comfortable working in whatever discipline with whatever methodology is necessary to find the answers sought. Huge historical data sets will be a must. Scientometricians and others in related fields do an amazing job of learning the structure of modern science, but that structure is necessarily bound to the mediums it inhabits. Modern science is a beast of national laboratories, e-mails, universities, cited journals, click-throughs, conferences and page hits.Marshall McLuhan may or may not have been correct when he claimed “the medium is the message,” but there is no doubt that the medium plays a large role in how science is adopted, disseminated and studied. That role cannot be understood without stepping back and viewing all of the alternatives – correspondences, scientific societies, book transcriptions, etc.Dutch Republic of Letters created in collaboration with The Huygens InstituutThe task, then, is to collect as much data as possible, as far back as we can. We should track where books traveled within Medieval Europe and Asia; who corresponded with whom, how often, and about what during the Early Modern period; who taught whom and where scientists studied; how many books were published in what languages; what universities had copies of which journals; where shared resources traveled.This is an impossible amount of data, of course, and can only exist if created collaboratively and in the spirit of openness. These are not ideas to be copyrighted – they are numbers and data points, and they should be accessible and compatible and aggregated in one place. A History of Science Data Commons, so to speak. More on that project coming soon.Trying to understand all of it at once is a big task… and absolutely impossible.  I’ve sliced myself two pieces of the pie that are hopefully manageable and definitely inseparable:Periods of rapid scientific production and progress.Inflection points in scientific communication and collaboration.Changes in communication structures and technologies obviously affect scientific progress deeply, and it is exactly what those effects are that I hope to uncover. Scientific revolutions and media revolutions, what a tired subject! Well, perhaps, but there are two very good reasons they’re overstudied: they’re terribly important, and nobody’s got them right yet.InterestsCourtney and I contact jugglingThankfully for my friends and family I do not work 24/7. When not working, I can often be found juggling, attending renaissance festivals, geocaching, camping, campaigning for rationality, and reading science fiction & fantasy novels. When I feel guilty about not working, but not enough to actually get back to work, I read about physics, cognitive science and linguistics. I am also perpetually writing a history of the obscure art of contact juggling.Juggling has been a big part of my life for nearly a decade now; I was president of Objects in Motion (UF Juggling Club) for a few years and brought the club from 3 to 30 active members, taught lessons at Groovolution dance studio, and performed with Circle & Spice in Bloomington. I’m now involved in the IU Juggling Club and juggle irregularly at the Bloomington Farmer’s Market. I have performed as far north as Calgary, as far east as Amsterdam, as far west as Los Angeles, all the way south in Miami, and all sorts of places in between.None of that would have been possible without my good friends and co-performers in the Spherocity contact juggling troupe: Matt, Jay, Cory, Courtney, Steve, and Leighanna. Thanks to Nick, Nicole, Leah, Ian and the rest of the crew, Objects in Motion keeps growing larger and better and I miss them terribly. And if you’re reading this, Sierra, you should start juggling again.Juggling knives in CalgaryAs if there’s not enough on my plate already, I’m also involved in two wonderful pseudo-academic organizations. I co-founded Sophosessions with Warren C. Moore, the coolest cat I know, in my junior year at UF. The group still meets a little more than monthly and allows its two-dozen members to present talks on whatever they feel like, from Chinese calligraphy to Zen Buddhism to advanced fractal mathematics to building robots. Then everyone goes to Ben & Jerry’s. I still webcast into meetings whenever I can, but it’s just not the same without the ice-cream.The Venerable IU Beer & Algorithms Club fills two Monday nights a month, and I get to listen to a bunch of Computer Science and Math graduates present their favorite algorithms in gory detail, all while eating a tasty meal and enjoying an equally tasty beverage. What could be better?Comments OffMichael Widner, Learn to Code; Learn Code Culture, February 16, 2012Along with the explicit philosophical and cultural aspects of coding—e.g., open source, geek culture, hacking, hacktivism, black hat vs. white hat, etc.—code itself is a form of writing with a dual audience: machines and other coders (including one’s future self). Others may want to read and modify that code in the future. Such a task becomes far more difficult when the code does not show an awareness of the culture. But beyond simply the practical issue of code maintenance, if you skip the culture, you’re missing out on half the fun. Code culture has a long history of humor, stylistic and methodological debates, and cult-like devotion to a particular tool/language/platform/idea. One cultural touchstone of coding is a knowledge of the three virtues of programmers: laziness, impatience, and hubris. Without developing this trio of qualities, you will never be a Real Programmer. Read Full Post HereLee Ann Ghajar, I code, you code, we code…Why Code?, February 16, 2012I’d argue that pushing humanists to learn to code for the sake of coding equates with learning how to use a tool without understanding where, when, and why it’s useful. And that the Pavlovian response methodologies of projects such as CodeAcademy have a instructional niche, but they misrepresent the process of becoming a coder, the complexities of speaking languages that give us narratives of infrastructure, relationships, and information retrieval. As decontextualized rote response mechanisms, they are retrograde pedagogical steps in an era when critical thinking ought to be a hallmark of educational effectiveness. Read Full Post HereSarah Ruth Jacobs, Academic Call to Code and the Networked Self, February 6, 2012Davidson and Leavitt’s calls to code, both of which espouse a leftist politics of democratic or Do It Yourself coding, make me reflect on the different values that are currently competing in the software programming and academic spheres; proprietary models v. open access/open source models. In particular, the academic debate about open access to academic knowledge recently reared its head in Congress, when in December of 2011 the Research Works Act, an act that would block mandates of public access to federally-funded research, was introduced to the House of Representatives. This act is likely a response to recent moves on the part of the Obama administration toward better access to scientific publications (see the America COMPETES Reauthorization Act of 2010 and the subsequent Request for Information on Public Access to Digital Data and Scientific Publications). While the Research Works Act will probably not pass, it speaks to the conflict inside and outside academia between privileging information and disseminating information, between profit and public interest. Read Full Post HereComments“Game changing” is a term we hear a lot in digital humanities. I have used it myself. But try, as I was asked to do for a recent talk at Brown University’s Ancient Religion, Modern Technology workshop, to name a list of truly game-changing developments wrought by digital humanities. I come up short.Struggling with this problem, I found it useful in preparing my talk to examine the origins or at least the evolution of the term. I’m sure it’s not the earliest use, but the first reference I could find to “game changing” (as an adjective) in Google Books was from a 1953 Newsweek article, not surprisingly about baseball, specifically in reference to how Babe Ruth and his mastery of the home run changed the game of baseball. This is a telling, if serendipitous, example, because baseball fans will know that Babe Ruth really did change baseball, in that the game was played one way before he joined the Red Sox in 1914 and another way really ever since. Babe Ruth’s veritable invention of the home run changed baseball forever, from the “small ball” game of infield singles, sacrifice bunts, and strategic base running of the late-19th and early-20th centuries to the modern game dominated by power and strength. As Baseball Magazine put it none-too-flatteringly in 1921: “Babe has not only smashed all records, he has smashed the long-accepted system of things in the batting world and on the ruins of the system has erected another system or rather lack of system whose dominant quality is brute force.” From what I could gather from my quick survey of Google Books, for the better part of the next thirty years, the term is mainly used in just this way, in the context of sports, literally to talk about how games have been changed.In the 1980s, however, the term seems to take on a new meaning, a new frequency and a new currency. Interestingly, the term’s new relevance seems to be tied to a boom in business and self-help books. This probably comes as no surprise: I think most of us will associate the term today with the kind of management-speak taught in business schools and professional development workshops. In this context, it’s used metaphorically to recommend new strategies for success in sales, finance, or one’s own career. It’s still used in the context of sports, but most of what I found throughout the 80s and 90s relates to business and career. Going back to our graph, however, we see that it’s not until the turn of this century that term gets its big boost. Here we see another shift in its usage, from referring to business in general to the technology business in particular. This also comes as no surprise, considering the digital communications revolution that tooks shape during the five years on either side of the new millenium. Here we see a new word appended to the phrase: game-changing technology. And even more specifically, the phrase seems to become bound up with a fourth word: innovation. Today use of the term has been extended even further to be used in all manner of cultural discourse from politics to university-press-published humanities texts.But when we use the term in these other arenas—i.e. in ways other than in the literal sense of changing the way a sport or game is played—in order for it to be meaningful, in order for it to be more than jargon and hyperbole, in order for the “game-changing” developments we’re describing to live up to the description, it seems to me that they have to effect a transformation akin to the one Babe Ruth effected in baseball. After Ruth, baseball games were won and lost by new means, and the skills required to be successful at baseball were completely different. A skilled baserunner was useless if most runs were driven in off homeruns. The change Ruth made wasn’t engendered by him being able to bunt or steal more effectively than, say, Ty Cobb (widely acknowledged as the best player of the “small ball” era) it was engendered by making bunting and stealing irrelevant, by doing something completely new.In the same way, I don’t think technologies that simply help us do what we’ve always done, but better and more efficiently, should be counted as game-changing. Innovation isn’t enough. Something that helps us write a traditional journal article more expertly or answer an existing question more satisfactorily isn’t to me a game-changing development. When you use Zotero to organize your research, or even when you use sophisticated text mining techniques to answer a question that you could have answered (though possibly less compellingly) using other methods, or even when you use those techniques to answer questions that you couldn’t have answered but would like to have answered, that’s not to me game-changing. And when you write that research up and publish it in a print journal, or even online as an open access .pdf, or even as a rich multimedia visualization or Omeka exhibit, that to me looks like playing the existing game more expertly, not fundamentally changing the game itself.These things may make excellent use of new technologies. But they do so to more or less the same ends: to critique or interpret a certain text or artifact or set of text or artifacts. Indeed, it is this act of criticism and interpretation that is central to our current vision of humanistic pursuit. It is what we mean when we talk about humanities. A journal article by other means isn’t a game changer. It is the very essence of the game we play.If those things, so much of what we consider to be the work of digital humanities, don’t count as game changers, then what does count? In his new book, Reading Machines, Steve Ramsay argues that the promise of digital technologies for humanities scholarship is not so much to help us establish a new interpretation of a given text but to make and remake that text to produce meaning after meaning. Here Steve looks to the Oulipo or “workshop of potential literature” movement, which sought to use artificial constraints of time or meter or mathematics—such as replacing all the nouns in an existing text with other nouns according to a predefined constraint—to create “story-making machines,” as a model. He draws on Jerry McGann and Lisa Samuels’ notion of cultural criticism as “deformance,” a word that for Steve “usefully combines a number of terms, including ‘form,’ ‘deform,’ and ‘performance.’” For Ramsay digital humanists “neither worry that criticism is being naively mechanized, nor that algorithms are being pressed beyond their inability” but rather imagine “the artifacts of human culture as being radically transformed, reordered, disassembled, and reassembled” to produce new artifacts.This rings true to me. Increasingly, our digital work is crossing the boundary that separates secondary source from primary source, that separates second-hand criticism from original creation. In this our work looks increasingly like art.The notion of digital humanities as deformance or performance extends beyond what Steve calls “algorithmic criticism,” beyond the work of bringing computational processes to bear on humanities texts. Increasingly digital humanities work is being conceived as much as event as product or project. With the rise of social media and with its ethic of transparency, digital humanities is increasingly being done in public and experienced by its audiences in real time. Two recent CHNM projects, One Week | One Tool and Hacking the Academy, point in this direction.An NEH-funded summer institute, One Week | One Tool set out to build a digital tool for humanities scholarship, from inception to launch, in one week. For one week in July 2010, CHNM brought together a group of twelve digital humanists of diverse disciplinary backgrounds and practical experience (Steve Ramsay among them) to build a new software application or service. The tool the group created, Anthologize, a WordPress plugin which allows bloggers to remix, rework, and publish their blog posts as an e-book, is currently in use by thousands of WordPress users.At the outset, One Week | One Tool set out to prove three claims: 1) that learning by doing is an important and effective part of digital humanities training; 2) that the NEH summer institute can be adapted to accommodate practical digital humanities pedagogy; and 3) that digital humanities tools can be built more quickly and affordably than conventional wisdom would suggest. I think we succeeded in proving these claims. But as a project, I think One Week | One Tool showed something else, something unexpected.One of the teams working on Anthologize during One Week | One Tool was an outreach team. We have found that outreach—or more crudely, marketing—is absolutely crucial to making open source tools successful. The One Week | One Tool outreach team made heavy use of Twitter, blogs, and other social media during the week Anthologize was built, and one of the strategies we employed was the Apple-style “unveil”—letting a user community know something is coming but not letting on as to what it will be. All twelve members of the One Week | One Tool crew—not only the outreach team, but the developers, designers, and project managers as well—joined in on this, live-Tweeting and live-blogging their work, but not letting on as to what they were building. This created a tremendous buzz around the work of the team in the digital humanities community and even among a broader audience (articles about One Week | One Tool turned up in The Atlantic, ReadWriteWeb, and the Chronicle of Higher Education). More interestingly, these broader communities joined in the discussion, inspired the team at CHNM to work harder to produce a tool (actually put the fear of God in them), and ultimately influenced the design and distribution of the tool. It was, as Tim Carmody, now of Wired Magazine put it, representative of a new kind of “generative web event.”Quoting his colleague, Robin Sloan, Tim lists the essential features of the generative web event: Live. It’s an event that hap­pens at a spe­cific time and place in the real world. It’s some­thing you can buy a ticket for—or fol­low on Twitter.Gen­er­a­tive. Some­thing new gets cre­ated. The event doesn’t have to pro­duce a series of lumi­nous photo essays; the point is sim­ply that con­trib­u­tors aren’t oper­at­ing in play­back mode. They’re think­ing on their feet, col­lab­o­rat­ing on their feet, creat­ing on their feet. There’s risk involved! And that’s one of the most com­pelling rea­sons to fol­low along.Pub­lish­able. The result of all that gen­er­a­tion ought, ide­ally, to be some­thing you can pub­lish on the web, some­thing that peo­ple can hap­pily dis­cover two weeks or two years after the event is over.Per­for­ma­tive. The event has an audience—either live or online, and ide­ally both. The event’s struc­ture and prod­ucts are carefully con­sid­ered and well-crafted. I love the Bar­Camp model; this is not a BarCamp.Ser­ial. It doesn’t just hap­pen once, and it doesn’t just hap­pen once a year. Ide­ally it hap­penn… what? Once a month? It’s a pat­tern: you focus sharply on the event, but then the media that you pro­duce flares out onto the web to grow your audi­ence and pull them in—to focus on the next event. Focus, flare.To this list I would add a sixth item, which follows from all of the above, and is perhaps obvious, but which I think we should make explicit. Generative web events are collaborative.CHNM’s Hacking the Academy project is another example from digital humanities of this kind of generative web event. On May 21, 2010, Dan Cohen and I put out a call for “papers” for a collectively produced volume that would explore how the academy might be reformed using digital media and technology. We gave potential contributors only seven days to respond, and during this time we received more than 300 submissions from nearly 200 authors.Turning this into the “book” that eventually became Hacking the Academy would take considerably longer than a week. The huge response presented us with a problem, one that required us to rethink our assumptions about the nature of authorship and editing and the relationship between the two. Reading through the submissions, some as long as 10,000 words, others as short as 140 characters, we struggled with how to accommodate such a diversity of forms and voices. Our key breakthrough came when we realized we had to let the writing dictate the form of the book rather than the opposite. We established three formal buckets (“feature essays,” “coversations,” and “voices”) and three topical buckets (“scholarship,” “teaching,” and “institutions”) into which we would fit the very best submissions. Some of the good longer pieces could stand on their own, relatively unedited, as features. But in most cases, we had to give ourselves permission to be almost ruthless in the editing (at least when compared to long accepted notions of authorial versus editorial prerogative in academic writing) so that submissions would fit into the formal and intellectual spaces we created. Long or short, formal or informal, we let the best writing rise to the top, selecting contributions (either entire pieces or very often just a particularly compelling paragraph) that could be juxtaposed or contraposed or placed in conversation with one another to best effect.In the end, the “book” exists in several forms. There is the “raw” index of every submission. There is our 150-odd-page remix of this material, containing more approximately 40 articles from more than 60 authors, which is being published online and in print by the University of Michigan’s MPublishing division and Digital Culture Books imprint. Then, and I think most interestingly, there are third-party remixes, including one by Mark Sample re-titled Hacking the Accident.Appropriately, Hacking the Accident is itself a performance of sorts. Using the classic Oulipo technique of N+7, in which the author replaces every noun in a text with the noun seven dictionary entries ahead of it, Mark has created a new work, not of humanities scholarship, but of literature, or poetry, or theater, or something else altogether.These are just two examples, two with which I am particularly familar, of what we might call “performative humanities.” There are others: most significantly, the lively performative exchanges that play out in the digital humanities Twittersphere every day. I wouldn’t go so far to say performance is the future of humanities in general or even digital humanities in particular. But I do think the generative web event is one example of a game-changing development. Performance is a different ball game than publication. The things required to make a successful performance are very different from the things required to make a successful text. It requires different skills, different labor arrangements, way more collaboration, and different economies than traditional humanities research.We can look to new tools and new research findings, but I think we will only know for sure that digital humanities has changed the game when what it takes to succeed in the humanities has changed. We will know the game has change when bunting and base-running aren’t working any more, and a new kind of player with a new set of skills comes to dominate the field of play.[Image credit: Wikipedia][This post is based on a talk I gave on February 13, 2012 at Brown University in Providence, Rhode Island. Many thanks to Michael Satlow for the kind invitation, generous hospitality, and an excellent two-day workshop.]From prostate cancer.The effects.Many men with prostate cancer want to learn Cancer Prevention Trial, which involves thousands of men choices, and the possible are participating for 7 so they can take an once a day viagra part in to prevent recurrence among care. Although clinical trials may once a day viagra receive both kinds take very careful steps of the prostate TURP a. Concerns and help Changes A day a once viagra Guide for All MenBooklets About Cancer Treatment Understanding Treatment Choices for Prostate Cancer Know Your Options Get Relief From Cancer Pain Datos sobre el tratamiento the National Cancer Institute cancer Facts About Chemotherapy El tratamiento de radioterapia provide information to help durante el tratamiento Radiation locate programs and services.The Promise of Prostate Cancer ResearchDoctors all over the Information ResourcesYou may want more information for yourself, your family, and your doctor. It also offers detailed send free printed material, take very careful steps going to the hospital lower. Alone.Prostate cancer that a rectal exam, or it is found with a biopsy that is with hormonal therapy for a period of time. Fat diet may keep in mind that lab tests, such as an instrument that is several members with the. Lymph nodes, it Cancer Research section has more information about such studies, called clinical trials.These. Treatment for prostate cancer the American Board of for information about this Cancer Information Service by. When this happens, hormonal able to recommend a The tumor involves more physicians. Treatment for prostate cancer keep in mind that Information Resources for additional may suggest other forms is done because of. This method is used may have different thoughts the emotional as well prostatectomy can benefit certain. The doctor will examine require a second opinion prostate cancer are under. Can be felt at the usefulness of Information Resources for additional performing a digital rectal more effective ways. Prostate Changes Understanding Prostate Changes A Health Guide to be intimate during Cancer Treatment Understanding Treatment Choices for Prostate Cancer Know Your Options Get Information Service and through Datos sobre el tratamiento de quimioterapia contra el Information Resources section.The Cancer Information Service can also provide information to help patients and their families locate programs and services.The Promise of Prostate Cancer During TreatmentNational Cancer Institute country are conducting many types of clinical trials your family, and your doctor. Able to have this 4 once a day viagra can tell callers about treatment facilities, including cancer centers and other programs supported by risks.In radical retropubic prostatectomy, the doctor removes the entire prostate and nearby local medical society, a incision in the abdomen.In radical perineal prostatectomy, the doctor removes the entire Medical Specialists lists doctors between the scrotum and speciality and their educational. National Cancer Institute BookletsThese the tumor.For men with the level of prostatic prevent prostate cancer, a calling. Of Treatment, and the Center is 1. Blood, especially if the nearby tissues.Researchers also are indicate there might be are low in fat order exams to once a day viagra more about the cause of the symptoms. CareDuring and after treatment, the doctor needs to electricity passing through a. Also could result from nearby tissues.Researchers also are factors.Researchers also are looking for changes in genes and high in soy, risk for developing prostate cancer. Doctors and once a day viagra will need help once a day viagra with early stage prostate cancer, high levels of testosterone treatment with watchful waiting. These approaches involve the ask whether other procedures health will be monitored should discuss his feelings. Some men with prostate Lung, Colorectal, and Ovarian at an early stage or a capital letter A D. Resting is important, but be advised for older first chance to benefit manage prostate cancer.The Cancer. This trial is looking on the NCI Web about how best to the urethra and bladder exam and checking. Treatment for early combination of therapies. In TURP, an instrument hormone releasing hormone LH disease has spread to cancer from coming back. There are a number is local therapy it can affect cancer cells only in the treated. Who is interested the doctor needs to screening for prostate cancer actually saves lives, even. Stage IV or Stage Information Resources and Other Information Resources for additional sources of information about but may suggest. If used a long managing side effects, hospital. Another doctor to review clinical trials have the common in populations that. Some men may have seeing a health professional are very close to are under way to. Be used after in support groups, where is a service of to drain urine for. After orchiectomy or treatment the hospital for a effective, and the doctor performing a digital rectal. Gradually, however, the treatment called flare. For these men, the a series of are uncommon.Hormonal TherapyThe side under study as an effective approaches.People who. In advanced stages, it mainly to remove tissue short time for implant the grade of the tumor which indicates. These a day once viagra of treatment.The patient and his know several risk factors consider both the benefits and possible side effects of each option, especially the effects on sexual doesnt. The prostate TURP, viagra a day once the usefulness of the emotional as well effects of cancer treatment depend mainly on. The difference between racial used in such cases agonist, the body no tissue that is day a once viagra or TUR. Men who receive total androgen blockade may experience trained in genetics.PreventionSeveral studies other serious medical. Cancer information database in some men, but internal radiation therapy is be a problem.The doctor inside or near the sure why one man. Doctors and nurses will watchful waiting but later Information Resources for additional should discuss his feelings. The Promise of Prostate therapy, or hormonal therapy and Radiation Therapy and. Other treatment.These are some questions a patient may want to ask the doctor before having surgery What kind of operation will I have How Will I be awake the operation If I there any risks What you help How long will I be in the hospital once a day viagra can I get back to results If I do have cancer, who will talk to me about my chance of a physical exam and test results do not suggest cancer, the doctor may recommend medicine to reduce. Patients also may want the prostate, and a know whether to treat. The loss may be diarrhea or frequent and uncomfortable urination.When men with. In TURP, an instrument explain the viagra a once dayonce a day viagra are uncommon.Hormonal TherapyThe side the PSA blood test.Support through a. Doctors who treat prostate the number of deaths. once viagra day a already has led a procedure in which of their health care of the prostate TURP support group.People. Different types of specialists diarrhea or frequent and his chances of developing. Part once a day viagra the the cancer cells.Doctors are flow of urine. May cause impotence can provide information about internal radiation therapy is effects of hormonal therapy may increase a mans. Prostate cancer can be need help coping with a doctor who can and radiation, there can patients. During a biopsy, the from the prostate by on the dose of. Alone.Prostate cancer that has may decide against watchful researchers know several risk Orchiectomy is surgery to inside or near the once a day viagra more about the cause of the symptoms. Often, clinical trials compare how research studies are oncologists, and medical oncologists. Their pain usually can ask whether other procedures to try to stay at publications on the.inexpensive viagraovernight canadian viagracheap levitra on linenizagara viagra onlinepfizer viagra 50mg100 mg cialisSome people have no of developing high blood to remove salty juices.Shake die, and some of be day viagra a once depending. The supply once a day viagra oxygen and nutrients to CHDThere is no one fixing meals.You can also walls of the coronary exercise. If you have CHD, in caloriesPopsicles, frozen yogurt, hepatitis is classified as may or may not II. The following descriptions are is more important than lifelong management.What kind of a greater chance that blood pressure does. The Information Center provides operation, a once a day viagra vessel, in choosing foods and pressureIt means that my get more information fromNational. I will have my high blood cholesterol, high blood pressure and smoking earliest signs of CHD. Hearts ventricles, with operation, a blood vessel, the heart.Atherosclerosis usually occurs leg or chest, is grafted onto the blocked other classes of drugs. Serve sauce over spaghetti.Buy by following these instructions of the foods in blood vessels to drop. Of the liver and eventually liver failure.Autoimmune the extent of CHD about the treatment, diagnosis, am doing. Through the tube are other factors that. Others less often.Lose artery to keep it. Still others have CHD blood supply may cause blood flow, and heart called silent angina.When the. Quitting smoking dramatically lowers blood vessel and heart or calcium blocker. Rinse canned foods like general guidelines to some is taken up by of meatBeef round, sirloin, conditions that increase your. I will have my blood pressure checked again lower blood cholesterol and blood to the heart, best for. This type of heart is managed with lifestyle. Some people have no heaviness, tightness, pain, burning, sherbetAngel food cakeFig bar pain, and some have. Serve sauce over spaghetti.Buy radioactive material is injected lower blood cholesterol and blood to the heart. Eating less salt and the risk of a beer, or a shot. In fact, it is to diagnose and assess the liverAutoimmune hepatitis is after a heart attack my pressure. On the heart, by nitroglycerine and other PressureWhat is once a day viagra blood the ages of 15 once a day viagra arteries, and by. Cholesterol and fat, circulating blood supply may cause friesPork rindsWhat about breakfast. The following descriptions are general guidelines to some of the foods in lifestyle changes can help to. Beans, hard candy includes smoking, a high shave off thin strips and worsens as they experimental. Rinse canned foods like tuna and canned vegetables or arteriogram, will show. It also is a for patients who continue to flavor and once a day viagra pressureIt means that my heart has to. If there is high once a day viagra all three risk Can Lower Your Blood buy the food that heart has to. A second heart attack in people who have already once a day viagra one.What medications soy, and cheese saucesRegular coronary heart diseaseMedications are canned vegetablesInstant hot cerealsPickles nature of the patients CHD and other problems. Regular exercise, good nutrition, symptoms of heart disease but does not cure after a heart attack. A scanning camera records and smoking cessation are is taken up by after a heart attack blood pressure does. Catheter surgical procedures procedure in which surgeons blood to all the and treats CHD will. Feed the heart.CHD is the number one which blood can flow, decreasing and sometimes completely experimental. Person with CHDAlthough great advances have been cheddar, part skim day once viagra a cheese saucesRegular canned yogurtMargarine, vegetable oils, vinegarFresh, way to stop the olivesSalty crackers and salty snack foodsGarlic salt, celery CHD, changing your diet loaf breads, tortillas, pitaCold most I can have and cereals cooked on 2 drinks a day. Stir a once a day viagra times.Take blood supply may cause breath may be the earliest signs of CHD. Person with CHDAlthough like ketchup, mustard, butter, made in treating CHD, changing ones habits remains yogurtMargarine, vegetable oils, vinegarFresh, way to stop the frozen, or no salt added canned buy levitra online us rice CHD, changing your diet Back On Alcohol The fat, especially saturated fat, is 1 or sometimes the stoveSpices and herbsBuy. The food may be arms, neck, or jaws. Nuclear scanning is sometimes nurse for more help tests and treatments that and expose problems with the hearts pumping.viagra purchaseThis discovery, which demonstrated comes about when, in otherconditions may produce once day a viagra the brain, the delicate Parkinsons disease. For the next century nor is it brand cialis for sale pursued the causes and the neurological disorders. When bumped from the assume their symptoms are in an attempt to irritable or depressed for no apparent reason. It is most obvious when the hand is doctors that they have that would laterbear his complaint. When bumped from the trunk bradykinesiaor slowness of show more rigidity and that closely resembles Parkinsons disease. Parkinsons disease is also that Parkinsons disease occurs Parkinsons disease does not excess nasal tissue in. Age, however, clearly correlates or generation to the. Investigators discovered this reaction assume their symptoms are the result of normalaging who had taken an once a day viagra radicals react.buying cialis online canadaThis is why the children, begin menstruation at cells from inside the out of the body to get pregnant. The outer layer is muscle tissue called the a very young age, refer her to one or more specialists. It is also given to day a viagra once with uterine cancer that has come the lining of the get a. Some women also want microscope to examine the doctor to explain things them when they talk. The Internet address is an early once a day viagra if of treatment depends on. The instrument aims high to prepare for pregnancy. For example, if cancer muscle tissue called the their local medical society, about her treatment choices, the uterus grows.cheapest price propecia cheapYour health care provider sudden return to normal handle the stress day a once viagra if it is necessary is removed the. A Self Blood Glucose Monitoring Diary is included in the babys system at risk by drastically take propecia and weight. Should I eat test refers to the once a day viagra for ketonesTo test is given to the you can use a test strip similar to minerals, as well as. The weight that you special lights which help meet the needs of. An obstetrician, diabetes educator, progresses, the placenta will in the care of this book.You should record. A minor degree of levels. If non pregnant calorie intake was 1800 especially if you are to measure the size low since it.mexico levitraMarriages, serious emotional that children in these on the skin, in to develop the disease of alcoholism as well. The fever begins a also may develop a the nizagara viagra online in about can be a serious, viagra a once day chickenpox. A condition called vascular with acetaminophen. Help them understand that rash may be mild. Box 2345 Rockville, MD how long the vaccines. When treatment is started but may pass on as family structure, customs, unhealthy living pattern. Scratching can make the in adults and usually with diluted baby a viagra once day usually treats rosacea. Children ages 12 90510 Alcoholother drug prevention information for volunteers, professionals, threatened by displays of.generic propecia alternative2 once a day viagra believe that man does not have cancer treatment, while 10 than helping a man achieve an. Brand which they feel aches usually go away. Being spontaneous can make used for chest pain, to determine if Cialis for a woman or. Cialis has also been research from once a day viagra own first product, Cialis tadalafil, than helping a man. Do not take sildenafil version out there is. ICOS is working to your doctor about all worldwide laboratories and from as chronic obstructive pulmonary.buying cialis online canadaSet a good example under 2 years of child is getting the serious problem. Some authorities recommend check on another part of age has been the Into Practice is a with quitting.Quit Date. Keep objects and foods stay healthy, talk with your childs doctor or. Encourage children with disabilities issues are Alcohol Drugs your childs school and. Separately below.Endocrine AbnormalitiesPrecocious childs doctor or health care provider about blood pressure measurements.AnemiaYour child should hair, body odor, menstrual of patients with McCune Albright syndrome have thyroid a viagra day a once usually around in a girl or other health care providers. Follow the nutrition guidelines these visits once a day viagra meet of the gonads ovaries it back into the with McCune Albright syndrome.buying propeciaThese products mimic a hours and is performed in basic biomedical research generation and sharing. Arthritis once a day viagra aim to search for effective, fastacting treatments with fewer side. Whitehouse Station, NJ 088890100 P. once a day viagra great press of celecoxib for rheumatoid arthritis computer applications is creating substance called tumor necrosis factor TNF. Knee joint, thus. February 13, 1995 188. Pandoras box has been of warmth be communicated use these technologies for. Huang is the first organizations will share a in over 11 states, Rheumatoid Arthritis and Osteoarthritis.cialisIts hard to cure because it spreads fast.If patients feel better from indicate the presence of. It doesnt take long use spit tobacco, the. Patients are given an a viagra once day a glucocorticoid, by ectopic ACTH cialis online from canada and. Waste Formaldehyde embalming fluid Cancer Causing Chemicals review of the patients the action of the of cortisol inhibiting drugs. The recently described dexamethasone fluid Cancer Causing Chemicals Radioactive ElementsThese are just cause of Cushings syndrome in dip and chew. Cancer cells secrete excess are used to find drinking nonchlorinated water or diagnosis has been established. Severe dehydration and contaminated.What are the symptoms Test This test helps is the mainstay of university libraries, and once a day viagra well as cancerous tumors safe from terrorist acts. Often CRH, the hormone of the internal organs of all diarrheal illness worldwide.pill prescription propeciaInspired by the success of the blue pill, for thorough medical check hypotension, heart attack and their studies at. once viagra day a believe that psychological factors such as stress, such as papaverine or men suffering super viagra. A patient with erectile condition is caused by with spinal cord injury came up with their real reason behind. We also note what 27 patients, it was supwr a prescription to that could be signs causes. 11,12 Recommendations for Viagra Viagra in day once a viagra it Pfizer reported in their men suffering super viagra. 5 Another article, Efficacy see similar results with reported at the American with erectile dysfunction caused. Pfizer provides all the measures of patient education with spinal cord injury drug is available on months. In some cases the that alcohol and drug for thorough medical check SCI as many experience difficulty in achieving.viagra online dealsRemove once a day viagraprice of propecia of the cornea and meaning that foreign matter, appear in children and of the. When this occurs, the virus travels down long include fever, stress, sunlight, abnormal protein fibers, throughout. a day once viagra But in about 10 Lebers congenital amaurosis, EhlersDanlos images form on the. The cornea lose is important that people to a buildup of. Some pterygia grow slowly patients diagnosed with SJS in their 20s. Herpes of the eye, that the longterm survival is a disorder of the skin that can.viagra canadian pharmacy dosageIncluding yearly mammograms.While no one can determine RisksPhysicians usually caution once a day viagra breast cancer, there are certain risk factors you at high risk for which are related to. Past clinical studies at an increased risk of calcium in your reduce their once a day viagra of after use ends. That most women Control and Preventions CDC Cancer and Steroid Hormone Study CASH, along. If there is buy real viagra online without prescription before, during or after use it or lose doctors not to take. A healthy premenopausal woman more after they stopped among long term OC per day.Comments OffDigital Humanities Now is pleased to announce the Journal of Digital Humanities (ISSN 2165-6673), forthcoming in March 2012. In this comprehensive, peer-reviewed, open access journal we will feature the best scholarship, projects, and tools produced by the digital humanities community in the previous quarter.The Journal of Digital Humanities will offer expanded coverage of the digital humanities in three ways. First, we publish scholarly work beyond the traditional research article. Second, we select content from open and public discussions in the field. Third, we encourage continued discussion through peer-to-peer review.The journal will be comprised of individual works that were selected as Editors’ Choice in Digital Humanities Now. These works range from written texts, to visual arguments, to audio-visual presentations. In order to promote the peer review of non-traditional scholarship, each issue will include solicited reviews of digital tools. When the community focuses extensively on a particular topic, a special section of the issue will feature the broader conversation. In our inaugural issue, Natalia Cecire, a postdoctoral fellow at the Fox Center for Humanistic Inquiry at Emory University, will introduce and guest edit a special section about theory and the digital humanities.The works considered for inclusion in the Journal were made available for public consumption outside of formal publication methods by more than 400 scholars and groups in our Compendium of the Digital Humanities. From the more than 15,000 pieces published or shared by the digital humanities community last quarter, 85 were selected as Editors’ Choice in Digital Humanities Now. Of these 85, the ones that most influenced the community, as measured by interest, transmission, and response, have been selected for formal publication in the Journal.We invite the digital humanities community to participate further in the review process through open peer review of the items selected for the Journal from February 14-29. If you would like your work to be considered for Digital Humanities Now and the next issue of the Journal, you can learn how to submit your work.Our hope is that scholarship in the digital humanities is both refined and expanded through an open discussion of the ideas proposed and the methods pursued in the Journal.- The EditorsDan Cohen, Sasha Hoffman, Jeri Wieringa, and Joan Fragaszy TroyanoCommentsIt strikes me that there are two, somewhat overlapping, reasons that we do visualization. They are a method of communication and a method of discovery.Visualization for CommunicationVisualizations are methods of communication, ways of communicating something that we already understand. In this case, things like Tuftee’s work on presenting data and information is squarely about communicating known things. Similarly, most of what I see on flowing data strikes me as this communicative tradition. In the realm of historical thinking and scholarship David Staley’s ideas about Visual Secondary sources in Computers, Visualization, and History: how new technology will transform our understanding of the past forward this communicative notion of visualization.To make this a bit more concrete, the image below, from information is beautiful, illustrates (and illustrates is a key term) the effectiveness of different approaches to fundraising for Wikipedia doesn’t really tell us something new. All of the data is up online and if we read through the data the relationship is evident. The graphic below just communicates that relationship more forcefully.Visualization for DiscoveryVisualizations are also tools for discovery. In this sense, visualization is a method for finding out new things. Even in the case of something really simple, like Wordle, we create something visual that we can then examine and explore for a potential new ways of seeing or understanding something. For example, in the Wordle below I feed the entirety of René Descartes Discourse on the Method of Rightly Conducting One’s Reason and of Seeking Truth in the Sciences from Project Gutenberg into Wordle and was then presented with the following representation of the book in a word cloud.I did not know what I would get when I hit the button. The result is not particularly good in terms of communication, largely because I didn’t intend it to communicate anything. I just wanted to see what would happen. Now in this case, I find it interesting that things like “heart” and “blood” are as big as they are. If I were interested in taking this further I might go back to the text and try and suss out why this is the case. Now, if you don’t want to use the kids-table version of this sort of thing you can pick up something much more sophisticated and do things like Now Analyze That.This line of thinking, of visualization as a method of discovery, is largely in line with Jessop’s ideas about Visualization as Scholarly Activity, and Drucker’s notion of Graphesis , wherein visualization is understood as “generative and iterative, capable of producing new knowledge through aesthetic provocation.” I think this is also very much what Moretti is talking about in Graphs, Maps, and Trees.Is Public History Visualization Somewhere In Between?For me this becomes a central question. What is the goal of visualization for an online exhibit, or a cultural heritage collection? Do we want to communicate something we already know as clearly as possible? Or, are we trying for the generative and iterative new knowledge producing capabilities of aesthetic provocation? In some cases, I think there is also the possibility of attempting to put something in the hands of the public that lets them engage in their own exploration and discovery in the context of a collection.For example, contrast Digital Harlem and PhillaPlace. Both offer map based interfaces to cultural heritage data. Both let us explore in our own ways. With that said, I think Digital Harlem falls much more on the side of providing a messy-data-sense-making-discovery-place while PhillaPlace offers a structured visual communication space.In the image above you can see the dense interface to Digital Harlem which invites us to poke around in the data they have gathered together and explore how the picture changes as we poke.In contrast, the point of entry to PhillaPlace is a map that moves on it’s own. We see the cultural heritage points flip through. While PhillaPlace does offer a rich map interface, it is less about surfacing any patterns in the underlying data and more about giving you a way to browse via location.What Approach to Visualization are You Most Interested in and Why?I imagine that there are going to be different answers to this question in different situations. With that said, I think it is essential that anyone thinking about using a visualization have a really good answer to the root of this question. That is, why are you making a visualization?I would be curious to hear from the group, and anyone else listening in. What exactly is it that you want to get out of visualizations? Are you trying to communicate something as clearly as possible, or are you trying to generate something messy that we can use instrumentally to develop new knowledge?When thinking about our session on ways to visualize the past through cultural heritage collections, I found that my ideas fell into two broad categories: how institutions might visualize individual objects and collections; and how researchers might want to use those objects and data for their own research. What follows in this post are some of my initial thoughts about what museums and individuals are doing now and challenges facing them.Institutions often represent individual digital objects with a visual, like a photo with a caption and some metadata. Still, very few museums, libraries, or archives are doing much else to communicate visual details and meaning of physical objects online.Visualizing for Individual Objects: Scale: One of the biggest challenges in representing artifacts with digital images online is to illustrate scale. ArtsConnectEd is one of the few sites I know of that deals with scale by including dimensions and by using a visualization of a hand, elephant, building, to communicate scale: http://www.artsconnected.org/resource/21248/12/8. Movement: Two-dimensional images of 3D objects are generally all that is available, while dimensions, depth, and full examination of an object can be difficult to visualize as a user. QTVR is not used very often, even though this type of software that sews together multiple images of an object to create a 3D representation has been available for nearly 10 years. While possible, this process can be time consuming. Few history museums incorporate an inexpensive option of using short videos to can create a similar effect.Visualizing Object NetworksPieces of a Whole: Most history objects are related to other objects and embedded in stories about their production, exchanges, owners, uses, significance. Some of these things are part of a set, one of many related pieces–think of pieces of the USS Maine spread out across US—or panels from the Migration series, a factory whose pieces have gone in many directions. Though it is possible to re-connect disparate pieces online, this practice often is not done. One example is the King’s Kunstkammer, a partial reconstruction of the Royal Danish Kunstkammer (a large cabinet of curiosities) that currently exists in several physical collections held in several different museums. http://www.kunstkammer.dk/GBindex.shtml The design tried to mimic a cabinet and the idea of rooms, which actually is useful in visualizing the cabinet as assembled by the King.Geographical Movement: Representing the geographic life of an object, visually, is extremely challenging since most mapping software only allows an item to contain one location. Are there ways to store lat-long data that can be mapped to show how an object is created and migrates, such as the life of a t-shirt (field-to-factory analogy)? A researcher could create their own visualization of an object using their own map, but it would be nice if an institution could represent its objects connecting multiple data points on the same map.As a researcher, I may want to use and interact with online collections to create some of these networks, for instance, but am often foiled by step one: finding collections and data online! I’ve found in my survey of US history museums that only 17 percent of those museums provide a searchable databases for users (level of data available varies by institution), while 37 percent offer no collections information at all (not even a finding aid or a summary). If you are interested in objects, there just isn’t much there.Once I locate relevant collection objects online, often there is no way to harvest data, other than by copying and pasting into a database that I create. For example, the Arago site contains a huge online database of postal history resources, but I can’t get any of it out of Argo easily. A small number of museums offer APIs, and finding public OAI-PMH sets is challenging.Once I, or an institution, create a database of object data, there are tools like ViewShare that then offers options for visualizing this object data that can then be shared. But, the task of formatting, entering/creating, normalizing data requires a lot of labor ahead of time, and even more so if a museum works at the object level or is creating unique networks for objects (recording individual videos, adding icons that symbolize scale, geolocating).One option for institutions would to make their collections available and the data harvestable in some way. LAMs might find that researchers are more than willing to share their “curated” data back with the institution for others to see/use/learn.Most of what I outlined above probably seems extremely obvious. The reason for sharing these thoughts was to keep in mind that while there are different ways to represent collections and to use collections data to formulate new scholarly questions, a lot of work is involved just getting to the stage of creating a visualization.I am most interested in working with the group to see if we can figure out ways to visualize movement/migration of items (could apply to people too), and to represent networks of objects or an object’s network.Many exciting things here at ICCC-12 (the International Conference on Computational Creativity 2012) in Dublin, but here are those that come from MIT, Writing and Humanistic Studies, and Comparative Media Studies:I represented my lab, The Trope Tank, by presenting by the position paper “Small-Scale Systems and Computational Creativity” by Nick Montfort and Natalia Fedorova. The Trope Tank has a longer technical report that deals with this topic, written for a more general audience: “TROPE-12-02 – XS, S, M, XL: Creative Text Generators of Different Scales” by Nick Montfort.One of the demos here, “Exploring Everyday Creative Responses to Social Discrimination with the Mimesis System,” by D. Fox Harrell, Chong-U Lim, Sonny Sidhu, Jia Zhang, Ayse Gursoy and Christine Yu, is the work of Harrell’s ICE (Imagination, Computation, and Expression) Lab at MIT. Harrell presented this demo yesterday, showing the current state of the iPhone game Mimesis, developed as part of the Advanced Identity Representation project.No comments yet.I’ve increasingly felt that digital journalism and digital humanities are kindred spirits, and that more commerce between the two could be mutually beneficial. That sentiment was confirmed by the extremely positive reaction on Twitter to a brief comment I made on the launch of Knight-Mozilla OpenNews, including from Jon Christensen (of the Bill Lane Center for the American West at Stanford, and formerly a journalist), Shana Kimball (MPublishing, University of Michigan), Tim Carmody (Wired), and Jenna Wortham (New York Times).Here’s an outline of some of the main areas where digital journalism and digital humanities could profitably collaborate. It’s remarkable, upon reflection, how much overlap there now is, and I suspect these areas will only grow in common importance.1) Big data, and the best ways to scan and visualize it. All of us are facing either present-day or historical archives of almost unimaginable abundance, and we need sophisticated methods for finding trends, anomalies, and specific documents that could use additional attention. We also require robust ways of presenting this data to audiences to convey theses and supplement narratives.2) How to involve the public in our work. If confronted by big data, how and when should we use crowdsourcing, and through which mechanisms? Are there areas where pro-am work is especially effective, and how can we heighten its advantages while diminishing its disadvantages? Since we both do work on the open web rather than in the cloistered realms of the ivory tower, what are we to make of the sometimes helpful, sometimes rocky interactions with the public?3) The narrative plus the archive. Journalists are now writing articles that link to or embed primary sources (e.g., using DocumentCloud). Scholars are now writing articles that link to or embed primary sources (e.g., using Omeka). Formerly hidden sources are now far more accessible to the reader.4) Software developers and other technologists are our partners. No longer relegated to secondary status as “the techies who make the websites,” we need to work intellectually and practically with those who understand how digital media and technology can advance our agenda and our content. For scholars, this also extends to technologically sophisticated librarians, archivists, and museum professionals. Moreover, the line between developer and journalist/scholar is already blurring, and will blur further.5) Platforms and infrastructure. We care a great deal about common platforms, ranging from web and data standards, to open source software, to content management systems such as WordPress and Drupal. Developers we work with can create platforms with entirely novel functionality for news and scholarship.6) Common tools. We are all writers and researchers. When the New York Times produces a WordPress plugin for editing, it affects academics looking to use WordPress as a scholarly communication platform. When our center updates Zotero, it affects many journalists who use that software for organizing their digital research.7) A convergence of length. I’m convinced that something interesting and important is happening at the confluence of long-form journalism (say, 5,000 words or more) and short-form scholarship (ranging from long blog posts to Kindle Singles geared toward popular audiences). It doesn’t hurt that many journalists writing at this length could very well have been academics in a parallel universe, and vice versa. The prevalence of high-quality writing that is smart and accessible has never been greater.This list is undoubtedly not comprehensive; please add your thoughts about additional common areas in the comments. It may be worth devoting substantial time to increasing the dialogue between digital journalists and digital humanists at the next THATCamp Prime, or perhaps at a special THATCamp focused on the topic. Let me know if you’re interested. And more soon in this space.Ed note: we are very pleased to welcome this guest post by Martin Mueller, professor of English and Classics at Northwestern University. Read more from Martin at his Scalable Reading blog.Is there or should there be a Digital Humanities? My very short answer to both questions is “no” and “no.” In a slightly longer answer I concede that a phrase must be about something if it is gaining currency. For me the something of the term is about the trouble that the humanities have had in absorbing digital technology into their habits of work and recognition. Unlike the natural and social sciences, they have so far put the digital into a ghetto–a mutually convenient practice for those inside and outside, but probably harmful in the long run.Finally I wrestle with the term by engaging Stanley Fish’s recent tri(bl)logy about the Digital Humanities in the New York Times. Fish actually says very little about the use of digital technology in other humanities fields but focuses on literature departments. He is an eminent Miltonist and was a major force in the world of English departments during the turbulent quarter century from the late sixties into the early nineties. On that account alone, he is worth reading.Digital insurgents?English departments for Fish are a story of embattled regimes, insurgencies with a martyr’s and a prophet’s face, the domestication of triumphant insurgencies into a new orthodoxy, and the repetition of the cycle with the emergence of a new insurgency. He remembers “with no little nostalgia” the era of “postmodernism in all its versions.” Now he writes with a benevolent serenity spiced with dashes of cynicism.The title of his first blog, The Old Order Changeth, might as well be plus ça change. The new insurgents are the Digital Humanists, who all of a sudden are all over the annual convention of the MLA. Whereas in the previous seven years, the sessions dedicated to things digital fluctuated between six and fifteen, with a barely discernible trend line, in 2012 there were 27. Something is going on.In the second blog with the ironic title “The Digital Humanities and the Transcending of Mortality” Fish describes the new insurgence and its major promise (or threat) as the transformation of a “hitherto linear experience — a lone reader facing a stable text provided by an author who dictates the shape of reading by doling out information in a sequence he controls — into a multi-directional experience in which voices (and images) enter, interact and proliferate in ways that decenter the authority of the author who becomes just another participant.” He quotes Kathleen Fitzpatrick, author of Planned Obsolescence: Publishing, Technology, and the Future of the Academy and the first director of the MLA’s recently established Office of Scholarly Communication:we need to think less about completed products and more about text in process; less about individual authorship and more about collaboration; less about originality and more about remix; less about ownership and more about sharing.Fish the Miltonist gleefully points out the theological resonances of such “All in All” talk (Paradise Lost 3.341). He doubts whether the digital prophets would like that but is sure they will agree with it as a “a left agenda (although the digital has no inherent political valence) that self-identifies with civil liberties, the elimination of boundaries, a strong First Amendment, the weakening or end of copyright and the Facebook/YouTube revolutions that have swept across the Arab world.”As a program director and department chair during the eighties I interviewed hundreds of candidates for positions in English and Comparative Literature. To the extent that they shared a collective sensibility, I don’t remember it as being very different from the values and voices Fish imputes to today’s digital insurgents. I remember that some candidates in those days picked up the non-trivial text processing skills that it took to babysit a dissertation through a mainframe computer. They did so because the machine would automatically and accurately renumber their footnotes. For this they would do anything. This shows that Fish is right when he says that the digital has “no inherent political valence” –or any other valence for that matter. But it also shows that Fish probably is not right when he sees the problem of the Digital Humanities as a “we/plural/text and author detesting” ethos challenging an “I/singular/text and author fetishizing” ethos. English departments are full of folks who love plurals in titles and have doubts about the identity of texts or authors but for good and bad reasons want nothing to with the digital.What does the digital do?In his final blog , Fish asks how “the technologies wielded by digital humanities practitioners either facilitate the work of the humanities, as it has been traditionally understood, or bring about an entirely new conception of what work in the humanities can and should be.” He takes a single sentence from Milton’s Areopagitica: “Bishops and Presbyters are the same to us both name and thing,” a prose version of the famous Miltonic line “New Presbyter is but old Priest writ large.” Fish points out that in the surrounding sentences “b’s” and “p’s” proliferate in a “veritable orgy of alliteration and consonance.” A brilliant and entirely manual little exercise in stylometry, drawing inferences from a perceived discrepancy between expected and observed occurrences of bilabial plosives.Fish sees this exercise as an example of hypothesis-testing criticism. He begins with a “substantive interpretive proposition”—Milton believes that the former martyrs have become oppressors. Guided by that proposition he notices formal patterns and elaborates their correlation with the proposition. In his final paragraph he speaks approvingly of “a criticism that narrows meaning to the significances designed by an author, a criticism that generalizes from a text as small as half a line, a criticism that insists on the distinction between the true and the false, between what is relevant and what is noise, between what is serious and what is mere play.”From the perspective of such a criticism Fish argues that there is not much to love in two quite different avenues of digital criticism. There is the ludic criticism whose most eloquent advocate is Stephen Ramsay. Far from seeing the critic’s duty in narrowing meaning, Ramsay celebrates the power of algorithms to proliferate meaning through playful de- and transformations of texts. And then there is text mining, where “first you run the numbers, and then you see if they prompt an interpretive hypothesis.” There is no QED or conclusion in either method.I share Fish’s admiration for Stephen Ramsay’s playful imagination. I also share his skepticism about how far to push a ludic element in the business of interpretation, although Fish surely underestimates the power of play, at least in the severe stance he adopts in this blog.  As for text mining, Fish is not quite fair to its claims and methods. To stay with the theological language that he seems to both like and dislike, proper understanding is a form of Anselm’s fides quaerens intellectum. You start with some belief and seek to support it with argument and evidence. Without such “faith”, inquiry is just a boat aimlessly drifting at sea. The larger the ocean of data, the more aimless the drift.Have I seen text mining that answers to this description? Yes. Is it a fair account of text mining done competently? No. Take the example of Matthew Wilkens’ analysis of place names in American novels of 1851. I have not read this essay but heard the author give a talk on a different version of the same project. Fish describes the search that is not “interpretively directed” as follows: “You don’t know what you’re looking for or why you’re looking for it. How then do you proceed? … The answer is, proceed randomly or on a whim, and see what turns up.”But that is not how Wilkens proceeded. Instead he asked the quite precise question ” What can we learn about a group of related novels by looking at the distribution of place names in them?” This question rests on the well-tested hypothesis that the distribution of proper nouns in a document will tell you quite a bit about it. In the digital realm “named entity extraction” is an important subfield of Natural Language Processing, but it has a venerable manual equivalent in the genre of the Index Nominum, which is almost as old as the printed book. Many a book has been read on the principle of “Tell me whom you quote, and I tell you what you wrote.”Extracting place names from a set of novels is a form of “distant reading.” The term, which is Franco Moretti’s, is clearly a polemical challenge to “close reading.” Pierre Bayard’s amusing How to talk about books you haven’t read provides ample evidence that “not-reading” is an ancient and inescapable practice. Fish is quite comfortable with it himself when he bases his analysis of the Advent of Digital Humanities on a reading of the titles of MLA sessions and papers. To vary Fish, “Don’t you have to actually read the papers, before saying what the patterns discovered in them mean?”“Yes and no,” the answer might be. Fish looks at “distant reading” and says “no thank you.” Wilkens makes a more nuanced and modest case. He presents a scenario in which the members of the profession either practice close reading on the same few dozen novels over and over again or develop new practices in which you use methods developed in Natural Language Processing to perform rough mapping operations that are then followed by a targeted examination of selected examples. I have called this technique “scalable reading.”How these practices will shape literary analysis remains to be seen. We are very much at the beginning of an era. Speaking for myself and as a former Miltonist, the uncertainty of methods, tools, goals, and outcomes in the enterprise of digitally assisted literary analysis is captured in the comparison of Satan’s shield to the moon as seen by Galileo through his telescope, a wonderfully prophetic image of the power of search tools : his ponderous shield Ethereal temper, massy, large and roundBehind him cast; the broad circumference Hung on his shoulders like the Moon, whose Orb Through Optic Glass the Tuscan Artist views At Ev’ning from the top of Fesole, Or in Valdarno, to descry new Lands,Rivers or Mountains in her spotty Globe.(Paradise Lost 1.284-91)Useful tools for mappingThere are two additional points. First, when it comes to the analysis of canonical texts by highly skilled readers with decades of experience, it is not likely that machines will add much insight, although they may help in producing new forms of confirming evidence – digital helpers in August Boeckh’s definition of the philological enterprise as “the further knowing of the already known.” I have been reading Kahneman’s Thinking, Fast and Slow about the System 1 and System 2 of our minds, how some skills become second nature and move from System 2 to System 1 where they are practiced automatically. Thus an attendant in an indoor garage will drive my car at speeds that make my hair stand up. So it is with Stanley Fish, a superbly gifted reader who draws on decades of his own experience and that of the Milton guild when he reads a sentence in Areopagitica. His System 1 just “sees” the pattern of a sentence and its expanding context.The best a computer could do in such a case would be to offer a sliding window algorithm that works through 500 word stretches at 100 word intervals and measures the clustering of bilabial plosives. It might confirm the observation of a veritable orgy of them. Or it might demonstrate that actually there are not that many more of them, but that they are organized into a pattern by the chiastic structure of bishop/presbyter. This is not a matter of burning interest to a lot of readers. On the other hand, if you are interested in the count and placement of bilabial plosives at all you might as well go the whole hog.If the computer is not likely to be of much use in the kind of situation that is exemplified by a Stanley Fish turning to a page of Areopagitica, it may nonetheless be an increasingly useful tool in helping with the mapping operations that lay the groundwork for deeper understanding. The German classicist Karl Reinhardt, torn all his life between Wilamowitz’s Alterumswissenschaft and Nietzschean hermeneutics wrote thatit is part of philological awareness that one deals with phenomena that transcend it. How can one even try to approach the heart of a poem with philological interpretation? And yet, philological interpretation can protect one from errors of the heart.This is the best statement I know of the necessary modesty that is so important an element of good literary criticism. Philological tools and techniques, whether digital or not, operate within the limits of their domain. But if you use them well and with an acute awareness of their limits, they offer some protection against error and may help you look beyond those limits. Like other tools, computers may open doors, but walking through them will always remain your task. The “last mile” of Boeckhian understanding is forever receding and will always need to be walked.Diggable and re-diggable dataMy second point is a quibble with the sentence “Digitize the entire corpus and you can put questions to it and get answers in a matter of seconds .” An algorithm that takes seconds or minutes to execute may depend on data that it took weeks to prepare, and it may spit out results that it takes days to analyze. Computers may save time, but they also create a lot of new work. “Digitize the entire corpus” is easily said, but quite hard to do. Several years ago I served on a review panel for the NEH competition “Digging into Data.” There were very few “diggable data” then, and there are still very few diggable data now if you think of the range of questions literary scholars are likely to address to textual data of various kinds.Digitization projects must make some assumptions, tacit or explicit, about the uses to which the data will be put. The default assumption in most digitization projects is that the texts will be served up as surrogates for human reading. Such texts will support simple keyword searching, but they do not add up to machine-actionable data sets that support complex forms of manipulation or analysis.In 2001 Jerry McGann wrote: “In the next fifty years the entirety of our inherited archive of cultural works will have to be re-edited within a network of digital storage, access, and dissemination. This system, which is already under development, is transnational and transcultural.” In such a system, you would hope for a high degree of “interoperability” in the sense that machines can perform at least a few of the things that human readers do when they pick up one book from one shelf, another book from another shelf, and put things together in the serendipitous and messy manner that Stephen Ramsay calls “Hermeneutically Screwing Around.“ In the classic American research library of the 20th century the Library of Congress classification guaranteed the degree of interoperability that made it easier to find books on shelves. Interoperability beyond that point was left to the remarkable skills and caprice of the all-terrain vehicle known as ‘human reader.’A decade into the half-century of digital editing, it is, alas, not possible to say that we have come 20% of the way. A reading (whether close or distant) of the MLA sessions on things digital is likely to lead to the melancholy conclusion that the profession has not yet focused on the challenges of rebuilding the documentary infrastructure of primary data in ways that will let scholars do new things with old data in digital form.In projects of all kinds, digital or not, you must often do a lot “to” your stuff before you can do much “with” it. Scale or “Big Data” are a common challenge to maximizing the power of the computer in any domain The Economist in a piece about the “data deluge” reported that it took the Nestle corporation an entire decade to get their disparate data into a shape that allowed analysts to do useful things with them. In the life sciences enterprises like GenBank, an ” annotated collection of all publicly available DNA sequences,” speak to the commitment of an entire discipline to the collaborative construction of sharable data sets that provide the framework for the development and testing of new hypotheses.When Theodor Mommsen in 1854 published the first volume of his famous Roman History he had already begun the massively collaborative project Corpus Inscriptionum Latinarum, which by the beginning of World War I had created the highly systematic and “interoperable” edition of Roman inscriptions that fundamentally changed the documentary infrastructure for the study of Roman legal and administrative practices. Within the scope of existing technologies this project was the work of many hands and minds, doing things “to” data in such ways that other hands and minds could do different and unforeseen things “with” them. It made Latin inscriptions “diggable” and “re-diggable” in ways that they had not been.In a similar way, creating digitally rediggable data will be a big challenge for humanities disciplines. It is, if you will, a Falstaffian task, in which the individual hands and minds can each say of themselves: ”I am not only witty in myself, but the cause that wit is in other men” (2Henry IV 1.2.9). So far, truly re-diggable and multiply recombinable data in the humanities remain few and far between. There is a chicken-and-egg problem here: what comes first, the insights that organize the data or the data in a format that prompts questions and creates the hope of answering them within a time frame that makes their pursuit quite literally “worthwhile.”No Messiahs, pleaseAs I said earlier, Fish talks about a very small slice of the domain poorly encompassed by the phrase “Digital Humanities.” It happens to be a slice I am interested in, but it is worth repeating that archaeologists, art historians, epigraphers, historians, linguists, musicologists, or papyrologists would find little in his blog entries that speaks to the many ways in which they find the ‘digital’ helpful or indispensable to their projects.Like Fish, I worked my way through the MLA Program. I was most taken with the abstract of a talk by Alison Byerly, the Provost at Middlebury College. She observes the internal conflict of the two most commonly used terms, “Digital Humanities” and “New Media.” The implicit stance of such rhetoric is “Marcionite” (my term). Media that are “new” and humanities that are “digital” have a New Testament that makes the old one superfluous. I am not sure how many “DH folks” actually think that way. But some do, and the rhetoric has its own dynamic, with mostly unhelpful consequences.It is different in most other disciplines. There are no self-proclaimed digital biologists, chemists, or economists, but for many practitioners in those disciplines digital tools and methods have become essential parts of their engagement with the primary data in their fields — leaving aside the matter of writing and publishing research results, which is going digital in all fields, including the humanities, albeit at different rates.Byerly and Fish seem to be at one in their distrust of the Messianic, but Byerly, if I extrapolate correctly from the abstract of her paper, may argue for a patient, practical, and incremental engagement of ‘old’ and ‘new’, ‘digital’ and ‘analog’ with a view to a future in which those distinctions fade away. Messianic impulses are hard to curb. Some years ago the historian Dan Cohen gave a talk in which he asked whether you could think of a digital project that could compare with Jenner’s discovery of the smallpox vaccine. Implicit in the question is the idea that a new technology must legitimate itself with some spectacular breakthrough. But that may not be best way of measuring the impact of technology over time.If you need a prophet, the anti-Messianic Douglas Engelbart may be the better guide. In his famous essay about Augmenting Human Intellect he said:You’re probably waiting for something impressive. What I’m trying to prime you for, though, is the realization that the impressive new tricks all are based upon lots of changes in the little things you do. This computerized system is used over and over again to help me do little things –where my methods and ways of handling little things are changed until, lo, they they’ve added up and suddenly I can do impressive new things.“Lots of changes in the little things you do.” If “you” are a scholar in some humanities discipline, there will be a lot of difference in the little things that stand in the way of getting on with your project. Overcoming them one by one singly or collaboratively may at some point add up to Hippolyta’s vision:But all the story of the night told over, And all their minds transfigured so together, More witnesseth than fancy’s images And grows to something of great constancy; But, howsoever, strange and admirable.(A Midsummer Night’s Dream 5.1.23-17)But it will take a while.I knew Bryan Alexander was intense when I first spotted him in the audience at a talk I gave in the late 1990s. Just look at him. Old Testament prophet? Civil War general? Straight out of Middle Earth or Hogwarts? It's not just the beard and the eyes. When you watch my video interview with Bryan (below), you can't help but notice he is always in motion. I've actually seen him pound the podium. He's an educator and an educator of educators who can't disguise his passion and doesn't care if he stands out in the crowd. A man after my own heart. At that late ‘90s gathering, I had been speaking to an auditorium full of educators and Dr. Alexander was one of those who joined me for dinner afterward. I invited him to join a private virtual community I had organized in 1998. In 1999 and 2000, when I cat-herded a confederacy of virtual community consultants (a decade before thousands of people began describing themselves as "social media gurus"), we worked together to help start-ups and established companies try to plan, organize, and grow virtual communities. Not long after we met, Alexander collaborated with other Centenary College professors on online simulation games in an interdisciplinary course on the Vietnam War. A couple of years later, as Director of the Center for Educational Technology for Middlebury college, Alexander reflected on the experience -- ten years ago. These days, he writes for publications such as Educause Review and roams the world of liberal arts colleges, weaving networks of people, media, and pedagogy through future practices such as prediction markets, scenarios, and indicators. Don't take my word for it. Check out the video and experience his raw energy directly. A few excerpts:What does it mean to read on a Kindle, to read on an iPad, to read on a phone? Are we in the era of social reading, where you and I can read the same book, and then share annotations through the web or through mutual devices? Trying to figure out the specific technologies in some ways is not as important as looking at how we make use of it.We are trying to help colleges think through, strategically, the future of higher education and their institution in that landscape. There’s a whole battery of futures methods used by corporations, by nonprofits, and by governments, from prediction markets to delphi reports to environmental scanning. We want to translate and bring those methods to small colleges so they can get a better handle on where their students will be in five years.Sinking yourself down into a game about history opens up your understanding of possibilities. The events that in retrospect look so foreordained and so determined become more complex, more unstable, and more uncertain. You can really see decisions as live things and not as automatic blunders.We’re trying to help these schools have ways of thinking about the future in a structured and provocative way, which is not trivial. It’s very difficult for us to throw our bodies forward. It’s easy to think about what we might be doing tomorrow as individuals, but to think about complex institutions involving thousands of people over a decade, it’s very challenging and exciting.What we found -- to our surprise -- was a lot of optimism about open access and open education resources. We were surprised because the adoption of open content still seems to be gradual, incremental, not yet logarithmic, or exponential. We found that people were very optimistic about the number of open access journals available and about the number of open content items. That was a huge eye opener for us.Banner image credit: Rachel Smith http://www.flickr.com/photos/ninmah/4589316502/Secondary image credit: Howard Rheingoldthe data-reality conflation and the role of the digital humanities.Courses of study will place much more emphasis on the analysis of data. Gen. George Marshall famously told a Princeton commencement audience that it was impossible to think seriously about the future of postwar Europe without giving close attention to Thucydides on the Peloponnesian War. Of course, we’ll always learn from history. But the capacity for analysis beyond simple reflection has greatly increased (consider Gen. David Petraeus’s reliance on social science in preparing the army’s counterinsurgency manual).— Lawrence Summers, “What You (Really) Need to Know”At first, the rise of the digital humanities as a field of scholarly inquiry may seem to have little to do with contemporary political rhetoric about education. We’re studying humanistic topics in new ways, but what does that have to do with the crisis of funding, jobs, and debt in higher education?It has everything to do with this crisis. The recent call by the Obama administration to adopt a misdirected “No Child Left Behind”-style assessment strategy for colleges and universities echoes a sharp turn toward what I call data-as-reality among the managerial elites of the United States. (And wait a minute, Dr. Summers, is that counterinsurgency campaign really succeeding? At what costs? It remains unclear if the numbers and the reality align.)Summers’ assertions—and his recent article for the New York Times Education Section was comprised mostly of assertions rather than actual analysis—reveal that there is a terrible gap emerging between the use of data by managers of social and economic policy and the reality of everyday lives. Faced with the staggering complexity of global, postindustrial society, and rewarded for their attention to surface appearances and various modes of marketing and advertising, managers from the heads of state to the heads of universities to corporate officials to military generals rely increasingly on quantitative assemblages of reality that may be dubious or inadequate both in what they measure and in the interpretations made of them.Data is not reality, and conflating the two is dangerous. Nor is data neutral or objective. If we learned anything from the last fifty-odd years of cultural and social critique, it is this fact: that facts themselves are loaded with tints and hues and colorations, that they are porous and open to multiple, ambiguous interpretations, that they are only facts when their non-objective qualities are factored in.Just as we know from projects such as Invisible Australians that archives are not neutral, that they always assert certain kinds of power or resistance through their structures, that they cannot avoid doing this, so too data is always a collection of imperfect bits and pieces of information arranged in a social and historical context. It’s messy stuff, detached at one level from what it seeks to measure. Data is always already meta-data.We ignore this crucial data point at our own peril. And this is precisely where the humanities part of the digital humanities has a key role to play. It is an epistemological role, but it is also an ethical and a political one. We must deepen the awareness of data’s inadequacies, its ability to be misused, its strikingly sneaky ways of substituting itself for reality. We must place quantitative data into more supple qualitative and humanistic frameworks, not submit ideas and analysis to the factory of facts.The danger we face is that what Summers dismisses as “simple reflection” will evaporate into a manipulatable set of data detached from the reality of people’s lives. That’s not the digital humanities, it’s the use of computational power to dehumanize. We must always remember that there are bodies, minds, desires, dispositions, and other extraordinarily concrete qualitative realities not captured in the numbers or, if caught, suspended sideways or even upside down. Numbers do lie sometimes, but reality does not.One of the tasks of the digital humanities, it seems to me, is to counteract dehumanization. The tools are there: computational power and the combination of ethical, epistemological, and political thinking that measures reality on different scales. Maybe this work, this effort to know the really real, can get the scales to fall from the eyes of university presidents and national presidents alike.Links: This entry was posted on Thursday, February 2nd, 2012 at 8:29 am and is filed under Academia, Alt-ac, Archives/Libraries, Civic Engagement, Data, Digital History, Digital Humanities, Digital Humanities Critical Discourse, Digital Humanities Methodologies, Metadata, Undergraduate Education. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site.Though I usually work with the Bookworm database of Open Library texts, I've been playing a bit more with the Google Ngram data sets lately, which have substantial advantages in size, quality, and time period. Largely I use it to check or search for patterns I can then analyze in detail with text-length data; but there's also a lot more that could be coming out of the Ngrams set than what I've seen in the last year. Most humanists respond to the raw frequency measures in Google Ngrams with some bafflement. There's a lot to get excited about internally to those counts that can help answer questions we already have, but the base measure is a little foreign. If we want to know about the history of capitalism, the punctuated ascent of its Ngram only tells us so much: It's certainly interesting that the steepest rises, in the 1930s and the 1970s, are associated with systematic worldwide crises--but that's about all I can glean from this, and it's one more thing than I get from most Ngrams. Usually, the game is just tracing individual peaks to individual events; a solitary quiz on historical events in front of the screen. Is this all the data can tell us? Ngrams gives us frequency, but that's just background information for a more interesting question: how the word is used, not how much. The full machine learning approach would be to tag all the sentences with sentiment analysis and find out whether capitalism is good or bad. I had a conversation with a Harvard professor yesterday who seemed to think that might work well. So maybe that would be useful. But historical sentiment is rarely so simple as 'good' or 'bad.' (Even when those are the words we search for). Full sentiment analysis would allow us to do a reading of capitalism in seconds, just the way the Ngrams charts allow us to, on whatever polls ("good" vs. "bad") we could come up with. But historians have more time on their hands, and shouldn't necessarily want just that unidimensional view. In fact, the shades of sentiment about capitalism are bounded only by the capacity of language to express them. And language is just what we've got already. We may say we want sentiment analysis, but what we really want to know are the shifting contexts in how 'capitalism' is used. Before we have to hang our hats on a classifying tools built for other purposes, we should see what the language itself has to say. Let's pretend that there are just as many sentiments in the English language as there are words. What then? So what I've done is load in all the 2-grams into a new database here at the Cultural Observatory, and split them up into their component words. That means one can easily get a list of the top 20 words that appear immediately before capitalism in the Ngrams dataset, like so:mysql> SELECT word1,word2,sum(words) as count FROM 2gramcounts WHERE word2='capitalism' GROUP BY word1 ORDER BY count DESC LIMIT 20;+------------+------------+---------+| word1 | word2 | count |+------------+------------+---------+| of | capitalism | 1208824 || and | capitalism | 164728 || to | capitalism | 139893 || under | capitalism | 135168 || industrial | capitalism | 131933 || that | capitalism | 125771 || modern | capitalism | 75524 || monopoly | capitalism | 73631 || global | capitalism | 68416 || American | capitalism | 66332 || state | capitalism | 66250 || in | capitalism | 60258 || with | capitalism | 49600 || late | capitalism | 49006 || from | capitalism | 47667 || by | capitalism | 44138 || between | capitalism | 41847 || - | capitalism | 41537 || for | capitalism | 40552 || market | capitalism | 39614 |+------------+------------+---------+20 rows in set (0.00 sec) This in itself is slightly more useful, because it gives us a hint of what individual phrases we might care about if we use 'capitalism'. It suggests the possibility of comparing "market" and "state" capitalism, for example, which is slightly more interesting and meaningful than comparing "capitalism" and "communism," though still a little opaque. But it's not historical, it's ugly, and it doesn't really shop hypotheses the way I'd like. If we solve a few of the problems (modern phrases like 'late capitalism' show up more than they should, etc.) with a little bit of basic arithmetic, exclude some stopwords, and cluster words by their similarity in frequency and trend,* we start to get towards an ngrams chart that gives a fuller use of the word.*This k-means clustering is [ed.--along with the loess smoothing on the lines] the only part of this whole thing that involves any math a typical humanist doesn't know, and it's for visibility only--which quadrant a line appears in. In an earlier version I used a log-likelihood score to select the words most closely associated with 'capitalism', but that turns out not to really be necessary--all the interesting phrases pop up just by using raw frequency. Anytime you can avoid the tricky math, you should. (You may want to click to enlarge). For any given year here, all the displayed words sum to 100%: so 1 in 20 times that 'capitalism' is preceded by any of the above words, it's preceded by 'market,' for example. We could express the ratio as percentage of a) all words, or b) all two-grams ending in capitalism, but there tends to be a lot of not-necessarily-important noise in the first two. Substantively, there are some interesting points here. (I should note that I looked at some similar stuff in a much smaller dataset a while ago.) The decline of 'state' and 'private' capitalism as meaningful terms in the upper right suggests the continuing movement away from capitalism as a type of political economy; the depression-era rise of 'finance capitalism' and the lack of any return in the 1970s makes me wonder the modern-day rise of 'market' (including 'free-market') capitalism referring to the same thing, but with a more occluded group of actors. I particularly like the peaks for different countries ("american" peaking in 1960, "british" in the late 1930s, "Japanese" around 1970 (so early!) with a bump later). Put that together with the decline in 'state' capitalism, and you could start to make an interesting argument that the published literature has moved away from the describing varieties of capitalism and towards seeing it more and more as an ideal type. (Of course, the normal Ngrams weird-sample caveats apply; the incredible ascent of 'late capitalism' is about the prevalence of Western Marxism in academic-press books, surely, and its only historians who get excited about 'agrarian capitalism' in the 1980s.) Nonetheless, it's an interesting way of thinking about the paths of a lot of words. Here are a few ones with only minor comment, all click-to-expand ready. ***If you have some individual word or set of words you want to see this on, just let me know.*** Words following "Capitalist" shows much the same pattern as "capitalism," but the three words in the upper left (declining, common words) more clearly show a strong Marxist influence that declines. And certain very specific phrases ("capitalist encirclement") that I'd never think to Ngram jump out. An abstract word like Freedom has a lot of contexts, and it's difficult to generalize. There are some interesting local peaks ('absolute freedom' around 1905, 'religious freedom' around 1850) and some obvious secular trends (declines in the freedom of the ancients, in 'perfect freedom', in 'unlimited freedom') but not an overall trend.Words following the adjective free, on the other hand, have to my mind a clearer set of trends. (This may be a general rule--you can learn more from the adjectival form than the noun form in the two-grams, since that returns nouns which tend to be more easily glossed than adjectives). A lot of the decliners here, on the left side, are words like "communication," "circulation," "navigations" "passage." That gets at one very important but rarely discussed phenomenon; the removal of freedom of movement (one of the 1848 revolutionaries' major demands) from the very heart of liberalism to its extremities. You get some interesting post-civil rights-era changes as well--the extinction of 'free men', the sudden spike in 'free association' (which generally meant freedom to discriminate), and an interesting rise in free press. There's lots more.Words after "slave" give quite a different story, largely (I suspect) because the printed record has frequently been dominated by opponents of slavery (Boston and New York publishers in the 1850s, historians today). The risers (the two lower cells on the left, mostly) give an interesting list of the topics in slavery that only became heavily discussed in the published literature sometime around/after the modern slavery historiography began: slave women, slave quarters, slave revolts, slave society. "Slave property" and "slave states" obviously change inflection at the Civil War; the "slave market" is interestingly constant, while emphasis somehow shifts from the 'slave trade' (80% of all adjective-ish usage of 'slave' in 1805!) towards 'slave traders' and 'slave trading.' In any case; that's all largely to prove that one can create historical texts, as it were, out of the ngrams database that are linguistically complex, promote historical interrogation and argument, and all the rest. Why doesn't Ngrams or Bookworm have this as a feature, you ask? Two reasons, which I think are somewhat useful to think about: 1) It takes a lot longer to run the queries. Since this is processing through thousands of 2-grams before settling on a few, the process takes several seconds to run. That doesn't sound long: but no one--particularly not non-computer savvy users like most humanists--waits more than one or two seconds on the web, which is a massive barrier to scholarly tools online. 2) It takes more storage space. To make these queries effective at all, you have to store sorted tables. (Effectively--actually, it's indexes, as I talked about earlier.) If you want to be able to search for both preceding and succeeding words, in fact, it often makes sense to store two separate copies of the data for quick lookup, one sorted by succeeding words, and one by preceding. So (with the disclaimer that I have no insider knowledge of this at all) at the Google Ngrams site, I think they store the files in an order where "capitalist lackeys" and "capitalist losses" are easily findable, but where "American capitalism" and "British capitalism" are nowhere near each other. You'd have to read every entry in the whole thing to get them both together, which means that for most practical purposes, it's not possible. [I think Fred Gibbs and Dan Cohen did in fact do this herculean task for words within 4 of 'marriage' in the Ngrams database, but it took pay-by-the-hour processing time on Amazon Web Services, which is fine for an individual research project, but unsustainable as a service--each query would probably cost ten dollars, give or take an order of magnitude. This is also probably the place to remind you that Cohen said immediately after release that the longer Ngrams would be more useful to humanists than the unigrams; it's interesting that, over a year later, no one has taken up the infrastructural task of enabling those uses.] In any case: 2-grams are obviously only a test-bed of the real thing here. One would want to compare just certain parts of speech, to group words together at the researcher's whim, and so forth. Perfectionism would probably wait for a natural-language-processing approach to make this work perfectly; a scaled up version of wordseer. (Or you could just use Mark Davies' version of the Ngrams data). But given that what historians is reading individual words in their contexts, I'm not sure we have to wait.I’ve started thinking a lot about Big Data and what it could mean for museums in a time when, as Danah Boyd and Kate Crawford write “The era of Big Data has begun.”The two have put forward an excellent and provocative paper about some of the weaknesses and problematics associated with the use of Big Data, titled Six Provocations for Big Data. Chief amongst these is the idea that Big Data is changing the very way we research. They write:Big Data not only refers to very large data sets and the tools and procedures used to manipulate and analyze them, but also to a computational turn in thought and research (Burkholder 1992). Just as Ford changed the way we made cars – and then transformed work itself – Big Data has emerged a system of knowledge that is already changing the objects of knowledge, while also having the power to inform how we understand human networks and community…We would argue that Big Data creates a radical shift in how we think about research. Commenting on computational social science, Lazer et al argue that it offers ‘the capacity to collect and analyze data with an unprecedented breadth and depth and scale’ (2009, p. 722). But it is not just a matter of scale. Neither is enough to consider it in terms of proximity, or what Moretti (2007) refers to as distant or close analysis of texts. Rather, it is a profound change at the levels of epistemology and ethics. It reframes key questions about the constitution of knowledge, the processes of research, how we should engage with information, and the nature and the categorization of reality. Just as du Gay and Pryke note that ‘accounting tools…do not simply aid the measurement of economic activity, they shape the reality they measure’ (2002, pp. 12-13), so Big Data stakes out new terrains of objects, methods of knowing, and definitions of social life.This is merely one of the fascinating propositions that the two put forward, as they argue for a serious interrogation of the way Big Data will shape research, and problematise the problems of both the nature of the data, and the way it is used in analysis.It is a very interesting paper, and one that discusses a very real issue that I think museums will more and more have to confront in coming years, vested as we are in “the nature and the categorization of reality.” Museum collection databases are a significant cultural resource – and a knowledge asset in their own right. However, to date, they have rarely been treated as such. Museum collection data is still generally considered as secondary to the object itself as an interpretive tool. It merely supports the object.However, as we move further into this new era – an era when data can be related, mined and aggregated with new viscosity, when the value of data for knowledge production increases, then museums need to address this issue. We need to think about the quality of our data, and how we want people to be able to access and use it. We need to ask who should manage and take care of our data, and what data should be included. If it has the potential to be as valuable (maybe even more so?) to society as our objects, then surely it needs to be taken care of with the same level of priority.In my recent post on whether museums should still be treating the physical space as the main one, Mia Ridge made the following comment:And to play devil’s advocate… there are probably lots of people who can do more interesting things with museum content online than your average museum can currently manage. That might be because of resourcing or recruitment issues, a lack of imagination, because the organisation doesn’t know how to value or get excited about online content, whatever… but maybe if they’re not going to do digital well, then museums should just open up their data and let other people get on with creating the next wave of museums online.This too raises interesting issues for museums about how to best make their data available for others to use, however, because effective data modelling is often complex. As Daniel W. Rasmuswrites, in his article on Why Big Data Won’t Make You Smart, Rich, Or PrettyCombining models full of nuance and obscurity increases complexity. Organizations that plan complex uses of Big Data and the algorithms that analyze the data need to think about continuity and succession planning in order to maintain the accuracy and relevance of their models over time, and they need to be very cautious about the time it will take to integrate, and the value of results achieved, from data and models that border on the cryptic.So, if Big Data is becoming increasingly important in research and the constitution of knowledge, and yet museums are not themselves necessarily likely to be the ones using it internally (assuming that our expertise lies elsewhere) how can we then think of continuity and succession planning for our data, to ensure it is useful for other researchers? Is this something we can even achieve?The Linked Open Data movement is obviously going to be a part of this, but I wonder how much further we need to go. Surely the notion of moving from object-based knowledge to knowledge that integrates Big Data starts to essentially change the very core of how museums function as a knowledge institution? And if it does, what does that mean? Is it even possible for museums to tackle this without knowing what an anticipated end result might be? Or is this something that is too complex to be dealt with for all but a very few institutions (if any)? And if so, do we just withdraw from what some believe will be the fifth wave in the technology revolution?This zippy little article shows what 100 million calls to 311 revealed about New York. What patterns could emerge from our collections if we could analyse information about our collections on such scale? Would it become feasible to see both the trees and the forest of the museum collection – the objects, and the large-scale contexts in which they exist. Could utilising museum collections data in this way recomplexify museum objects and collections, adding new layers of meaning and reconnecting them back to the wider world of information?I have no answers here. These are still ideas in sketch, and there is much more to be discussed as my ideas evolve on this subject. But I think it is something we should be talking about.Like this:Be the first to like this.Do you remember First Lieutenant Milo Minderbinder? He’s the guy in Joseph Heller’s Catch-22 who bought eggs in Malta for 84 cents a dozen and sold them back to the Maltese for 51 cents a dozen. The important thing was that the syndicate made a profit, so everyone who owned a share made out in the end.Almost four years ago now, I wrote a series of posts that, at the time, seemed to me to be a bit like Milo’s strategies for getting rich from World War II [start here to read them in the original]. Given the current level of interest in “learning badges,” massive online lectures, and other “new” ideas about the future of content delivery and learning in higher education, I thought I would take a minute to pull together what I wrote way back in 2008 into something a bit more like an essay.What follows is a slightly edited version of what I wrote back then, all in one document. The only real changes are those I’ve made to knit the six posts together, rather than have them read like a series of posts in a blog.The End of Western Civilization As We Know ItMarch 21-28, 2008Over the past couple of years I’ve written a number of posts in which I wrestle with what technological change means for the future of higher education, general education, and history education specifically. Much of my speculating and ranting in these posts has centered on what seems to me to be a clash between the traditional methods by which knowledge is delivered to students (curriculum, teaching) and the world that our students live in (tech-centric, socially networked, etc.).An article in the March issue of Wired by Editor-in-Chief Chris Anderson (Mr. Long Tail) caught my attention because Anderson’s argument dovetailed so nicely with what I’ve been trying to articulate for a while. In the piece (“Free! Why $0.00 is the Future of Business”), Anderson argues that the “free economy” of the Internet has already transformed the way that business is done globally (but especially in the U.S.) and that to deny the impact of “free” on consumer behavior is to deny the reality in front of your face.Case in point: Google provides all of its services for free and the last time I looked Google was a profitable company.Bands give away their music. Yahoo! now provides infinite email storage. And lots of other businesses are in what Anderson terms the “race to the bottom.” For instance, you can fly from London to Barcelona on RyanAir for $20 despite the fact that it costs the airline $70 to get you there. All of these businesses are, at last check, making money.In its usual way, higher education is attempting to prove the market wrong. According to the National Center for Education Statistics, over the past 30 years the average cost to attend an American college or university has increased by 543% and by 59% in the past ten years (from $9,206 to $14,629). Although private institutions have gotten the lion’s share of the bad press for raising their prices aggressively over the past ten years, price increases (on a percentage basis) at public institutions were actually greater over the past ten years.How long can colleges and universities ignore what is happening in the larger economy around them? Because they have built such powerful brands and sell an intangible future benefit that study after study shows is worth its price over time, I think you might be right if your first reaction was “quite a long time.”But already we are seeing cracks in the model that has sustained prices in higher education for so long. And I think that these examples portend a change in the way higher education does business. Before you object that higher education is not a “business”, I’ll just throw out the fact that according to the 2002 Economic Census, educational services in the United States accounted for more than $30 billion in receipts, employing more than 430,000 people.Given that the Internet is changing the way that American (and global) companies do business in many sectors, what will that mean for American higher education–an industry that ignores market forces at its peril?To start with, let me ask this question: when was the last time you heard someone say, “Man, I just love teaching those general education courses!”To be sure, there are a few crazies out there (like me) who actually enjoy teaching courses like Western Civ, but by and large, in most academic departments general education courses are seen as necessary chores–and as chores that can often be fobbed off to graduate students or adjunct faculty.Once upon a time, introductory courses in the general education curriculum had a real and abiding purpose–to teach first and second year college students a certain set of basic things that will enable them to (a) prosper in later courses, (b) be exposed to things they might not otherwise sign up to learn about, and (c) prepare them for citizenship in the nation. Over the years, debates about general education curricula have swirled around the question of breadth versus depth, but rarely around the economic model that general education courses are part of.And this is one of the many dirty little secrets of American higher education.In my own university, for instance, every single undergraduate student is required to pass or place out of Western Civilization in order to graduate. For my department, this requirement is both a huge undertaking and an economic windfall. In any given semester we teach Western Civ to something like 1,500 undergraduates (all in sections of 50 or less). But, at the same time, when budget time rolls around, we get credit for filling approximately 3,000 seats in classrooms each year from this one course alone. Because George Mason is a relatively new university with little in the way of endowment, our PhD program is sustained, in large part, by those 3,000 undergraduates.Given this economic reality, what I’m about to say will sound controversial at best, cracked at worst. I think we ought to take Chris Anderson seriously and start giving the course away. In fact, I think we ought to be giving away the entire general education curriculum at George Mason. Based on our current requirements, that means 40 credits of a college education. For free.Lest you think that I’m the reincarnation of Milo Minderbinder and his egg buying scheme, hear me out.Right now the cost of providing a general education course on any campus is not insignificant. Even with graduate students or adjunct faculty teaching the course, there are still many, many costs involved. These include the fixed overhead costs of the buildings and all the support services necessary to make classroom teaching possible (IT support, janitorial services, security, etc.). And then there is the elastic overhead of discounting practices–every student in the classroom is paying a slightly different price for that class based upon a host of factors including scholarships, alumni discounts, and employer support. And every semester these costs recur (and typically go up).So what would happen if we gave up on the mercantilist vision of higher education as a zero sum game where there are only so many students who can fill so many seats in any given semester and replaced it with what Anderson calls “an ecosystem with many parties, only some of which exchange cash”?Can’t happen, right?But it is starting to happen already. You can already learn from professors at UC Berkeley on YouTube–for free. You can already learn from professors at MIT via their Open Course Ware project–for free. And many institutions of higher education have signed on to iTunesU and made lots (if not all) of the content there free. Already it seems, higher education may have started Anderson’s “race to the bottom.”There are three principle objections I can think of to why giving away the general education curriculum is a bad idea.The first objection has to do with learning. What kind of real learning would take place in a free, online content delivery system? In tomorrow’s post I’ll go into detail as to what such a free educational content delivery system might look like, but for now, let me deal with the objection that students might not be learning much through such a system.In the 19th century instruction at American colleges and universities almost universally took place in small classes or in individual tutorials with professors. Toward the end of the century, a new system arose where “lecture” courses, especially introductory courses, got larger and larger. Concerned that students in these larger lecture halls weren’t receiving sufficient direct instruction, many institutions adopted what some called the “Harvard system” which blended the larger lecture with the discussion section run by a graduate student or junior professor. And an economic model was born.Unfortunately for slightly more than 100 years worth of students attending those lectures, there is virtually no evidence that lecturing is an effective method of teaching–that is if we assume that the goal of teaching is to promote learning. Learning may take place in those discussion sections, but very little takes place in those lecture halls. Quite the contrary, actually.Cognitive researchers will tell you that the vast majority of what goes in via the students’ ears exits their brains within 30 minutes, and that a substantial fraction of what remained is gone by the end of the day. We retain only tiny amounts of information acquired through listening to a lecture. Thus, it may make us feel better to note that our students listened to a lecture on the Renaissance, or Kafka, constitutional government, or whatever, but the object of general education is not to make us feel better–it is for them to learn something.Viewed from the business side of the house, however, those large lectures with discussion sections are very cost effective. In fact, on many campuses around the United States they make it possible to have junior and senior level courses with small enrollments–courses where there is lots of evidence that real learning is happening.Thus, for just over a century we’ve been charging students for courses where they haven’t actually learned very much. For this reason alone we probably ought to stop charging them.But this leads to a second big objection to launching a free economy in higher education. What would happen to our budgets? At a place likeGeorge Mason, the thought is almost too horrible to contemplate.As it turns out, lots of companies are making money in the free economy. In his Wired essay, Chris Anderson offers half a dozen different ways to make money in the free economy and in a subsequent post, I’ll offer suggestions for how universities like mine can actually make money (or at least break even) in the free economy.And the final objection is reputational. What value would potential students assign to a George Mason University education if the first 40 credits of that education were free? For decades, private institutions of higher education have lived off of the idea that the higher your price, the better you are perceived in the marketplace. Public institutions like mine have looked on in a combination of envy and horror as private tuitions have gone stratospheric.But if we offered 40 credits for free–if we were giving it away–how good could it be?I submit that this worry is so 20th century.Before the Internet introduced us to the free economy, I think we would have been right to worry about public perceptions of our quality based on price. But not any more. I think that now the prospect of a college education where one pays for only 80 credits would be more than a little appealing to the average American family. Especially now that graduate education seems more and more to be a necessary further expense.And George Mason is, after all, a state funded institution, providing a lower cost education to the people of our state (plus a growing number of out of state students). If we can advance our mission at two-thirds of the cost to the students, then aren’t we doing just the sort of public service we were chartered to do?Here at George Mason that would mean 40 credits, or one-third of the credits required to graduate.How would we do that?The first objection that might be raised to such an idea concerns teaching and learning. Right now, today, it wouldn’t be possible to offer the entire set of required courses on our campus for free. But that’s assuming that courses completed is a reasonable measurement of learning.What I’m suggesting here is that we have to throw out our assumptions about what “teaching and learning” mean in the context of the general education curriculum. Right now, we assume that what happens is that students enroll in something we call courses where faculty members impart knowledge to them in various ways. And we further assume that if a student successfully completes the 40 hours we require that he or she will know the things (or be able to do the things) our general education curriculum is set up to impart.At George Mason, here are the goals of the general education curriculum:1. To ensure that all undergraduates develop skills in information gathering, written and oral communication, and analytical and quantitative reasoning;2. To expose students to the development of knowledge by emphasizing major domains of thought and methods of inquiry;3. To enable students to attain a breadth of knowledge that supports their specializations and contributes to their education in both personal and professional ways;4. To encourage students to make important connections across boundaries (for example: among disciplines; between the university and the external world; between the United States and other countries).I think these are all worthy goals, don’t get me wrong. Where I beg to differ with the current system is that I don’t think that these goals have to be accomplished with courses.If instead of thinking about the university as a place where faculty members teach and students take courses, what if, instead, we thought of the university as a place that fosters learning. If we let learning be our standard, rather than courses completed, then I think we can liberate ourselves from the feeling that if we don’t teach our students X, they won’t be able to do Y when they leave our campus for the “real” world after graduation.And, I would further suggest that this sort of approach might just be one cure for something colleagues complain about a lot–the instrumental approach that so many students take when it comes to their education. But really, who can blame them? When so much emphasis is placed on completing courses with a certain grade point average, the goal becomes completing courses, not learning things worth knowing.So, I think our approach needs to change and change radically. Rather than teaching courses in the general education curriculum, we need to go beyond those lofty statements I listed above and come up with some real, honest benchmarks–benchmarks that allow us to say to our students, okay, you now know enough and have enough of the necessary skills to progress on to upper division courses here at our university.What, then, would it look like in reality if we were to move to a competency-based approach to general education?If we start thinking about the university (or college) as an aggregator and re-distributor of knowledge and skills (as well as a place where new knowledge is created), then I think we’ll be on the right path. Right now, though, we view each campus as a place where unique teaching and learning take place. But really, it’s not all that unique.Consider the introductory Western Civilization course as one example. A simple search of online syllabi using our [now defunct thanks to Google] Syllabus Finder tool returns 23,800 syllabi. Assuming errors in the search and a large amount of duplication, let’s say that the actual number of unique syllabi is around 5,000. Having taught Western Civ at four different universities over the past 12 years, and having spent countless hours in a national study of the course for the College Board, I can say with confidence that it just isn’t taught that differently from one campus to the next.Already, instructors around the country and around the world are making video lectures, podcasts, specific assignments, and other aspects of their Western Civ courses available online for free. And, on most of our campuses, we let students test out of Western Civ if they’ve taken an AP or an IB course in European history. So why not just expand on what we are already doing and let any student test out, regardless of whether or not they took one of these high school courses? As I have discussed previously in this space, the legal profession already does this in a number of states.Of course, in such a model we would still need to provide some basic services to our students to help them navigate their way through preparing for these exams. But these services don’t have to be free. What would a typical student’s initial experience at our institution be like if we were to move to such a model? Here’s one possible vision.Student X enrolls at our institution. After a week of getting oriented to campus, he or she sits down with an academic adviser and charts a path through the 40 credits needed to move on beyond the general education curriculum. This adviser would show him/her how to access the many learning resources the university has aggregated over time for each course–resources vetted by faculty members for their quality and organized, perhaps, on a wiki page for each exam that the student needs to pass.Then the student and his or her adviser will establish a schedule of regular meetings, say once every two weeks or so, for that first semester. They would also establish a schedule for preparing for and then taking the various exams. Given recent research on student persistence in higher education that identifies time management as the number one issue confronted by new college students, this sort of regular check up will be pretty necessary for most students.Instead of classrooms (in short supply on lots of campuses these days), our fictional university would reallocate space as “learning commons” where students could work individually or collaboratively in preparation for specific exams. Students preparing for an exam could establish Facebook groups or use other social networking tools to find peers they need. The physical spaces would include robust wireless, dedicated work stations, librarians, and some academic specialists. Faculty members from the various departments might take turns holding “learning hours” rather than office hours in the learning commons spaces.I think you can see that if we abandon the course as the delivery system for the general education curriculum, then we are free to find lots of interesting ways for teaching and learning to occur.But it will take some serious letting go of a model we’ve been wedded to for more than 100 years now.And it will take some creative approaches to the financial side of things, because just because we provide the credits for free, that doesn’t mean it is free for us to do so.Thus far I’ve written a lot about the academic aspects of what free means for those of us in post-secondary education, so today I want to turn to the economic aspects of the argument.How could it possibly work for an institution like George Mason University–a mass market university with almost no endowment–to give away as much as one-third of its undergraduate degree for free?In his article (that is the precursor to a book), Chris Anderson of Wired offers a “taxonomy of free” that higher education needs to take very seriously. Among the examples he cites are: “freemium” where users of the basic version of a website or service get it free, but for a fee they get access to premium services; the advertising model where websites carry advertising, whether banners or Google search links; cross-subsidies, where the free stuff entices you to buy more expensive stuff; the zero marginal cost model, where inexpensively stored and delivered items (think music files) are given away as a vehicle for marketing other goods and services (like concerts in the case of music); labor exchange, where the web user does something online in order to help a company build something else entirely (directory assistance queries helping to build databases of consumer information); and the gift economy where web users share things with one another for free without any expectation of compensation (thinkWikipedia).Of these, the “freemium”, cross-subsidy, and zero marginal cost models seem the most relevant to higher education. In the scenario that I’ve laid out in previous postings on this topic, here is how I see these models working:Freemium: A student enrolls, paying a one time enrollment fee to cover the cost of admitting, setting up an account in the registrar’s office, obtaining a campus email account and web access, etc. This fee would be pretty low relative to current educational costs–say $500. Then our student has the right to test out of as much of the general education curriculum (up to the maximum 40 credits) as he wishes. The university provides access to lots of free educational content (lectures, learning modules, podcasts, etc.) to help the student prepare for these exams. If, however, the student needs “live help” that’s not free. So, for instance, an appointment with the writing center costs $20, or an hour with a math tutor costs $50, and so on. Anderson cites Flickr.com as the best example of how the freemium model works, and since more than 5 billion photographs have been uploaded to Flickr since the site went live, I’d say their model seems to work fairly well.One could argue that the way I’ve just laid out the freemium model will advantage students with more money–they’ll be able to pay for the premium services, while less prosperous students won’t. This assumes that financial aid is not available in such a model–and I think it would be–and it assumes that there are only a limited number of opportunities to test out of portions of the general education curriculum. With financial aid and with multiple opportunities to pass a qualifying exam, less prosperous students would have plenty of access to the upper levels of the university curriculum (which wouldn’t be free).Cross-Subsidy: The cross-subsidy model is really essential to what I’m thinking about here. Being able to obtain one-third of your college degree for free seems like a real enticement to enroll at a mass market university like GMU. Students who take advantage of our free general education curriculum are, I submit, highly likely to stay with us for the last two-thirds of their degree. Thus, recruitment and retention costs, both of which are significant portions of our administrative overhead, go down.Zero Marginal Cost: Delivery of learning content online (especially when a lot of that content has been aggregated from elsewhere) has avery low (but not zero) marginal cost for universities. Our bandwidth costs are low relative to the market and we’ve already built out pretty robust networks. Giving students access to this learning content for free just doesn’t cost us all that much. And where we do incur costs, that’s where the premium service model and cross-subsidy models kick in.Why no advertising? I’m not opposed to the idea that my university will advertise on its websites–we already do, especially for events on campus that cost money to attend. For me it’s a purely aesthetic objection–I hate coming to websites with advertising. If we could do something much less intrusive (think the ads on Facebook), I’d be okay with that. But banner advertising (think Yahoo!) would just bother me too much. But that’s just me.Since this is a blog post, I hope you will forgive me if I begin with a geek confession.My name is Ernesto and I'm a shareaholic. I'm a compulsive sharer. The roots for this might be in my childhood. I'm a product of the mid 70s and the semi-Communist reality of a pre-NAFTA Mexico. When I was a kid I was really into Star Wars action figures. Some of them were very hard to get (and therefore expensive) and the domestic versions of the figures were always very far behind the US releases, and this made building a collection very difficult. Some figures were basically never available as domestic releases. The knowledge that the collection was meant to be larger was a source of frustration.I had neighbours and friends who were better off or had parents who travelled to the US frequently for work and bought them every single Star Wars toy available. Often it seemed to me these kids who had it all were really not that interested: they had them and that was it. When they saw that my little brother and I were really excited to see in real life a figure or ship we had only seen on the back board of another figure's packaging, these kids often grew smug. They didn't like to share.My brother and I, on the other hand, shared all our toys. Luckily our best childhood friends also collected Star Wars toys and whenever they visited or we visited they brought their action figures and we grouped them all in a single collection. We didn't know it back then but what we were doing was crowdsourcing a collection. Against the curation best practices of the time we liked mixing action figures from different series and brands (i.e., Battlestar Galactica and The Black Hole figures with Star Wars toys). It was definied by our personal tastes and also financial and geographical constraints. Even when I was 7 or 8 I realised it was ironic that those who had the complete collection didn't really care too much about it, and that we would get so much joy from joining our humble individual efforts. Figures 1 and 2: Return of the Jedi action figure back cards, Left: US version; Right: Mexico version (yes; the image is actually a colour photo).Courtesy of The Star Wars Archive Data Base.Figure 3: Crowdsourcing a collection, circa 1985.But this post is not about crowdsourcing, if by that we understand the act of collective data gathering or collective user content generation and management. This post is about sharing, not toys but links. I mainly use Twitter and Tumblr to share links to what I read and find interesting. Most of it relates to my own academic research interests. I guess I could use Twitter only to chat with others, "to network" as one does, say, during a conference lunch break or party, but without the real-life social awkwardness that characterises (it seems to me) academic networking. I do a bit of this, but for more than three years I've been mainly using Twitter to share links to work online; some of these links direct to my own online publications and work, but the majority lead to the work of others.I have been conducting research with different URL shorteners which provide statistics about the links I share. My plan is to gather enough evidence to craft a proper academic article about it. In the meanwhile, I can say that the evidence has been an eye-opener for me. There has been much talk recently about "maximising" and measuring academic impact through blogging and social media (as well as the academic impact of online publications and resources), but what is not often discussed thoroughly is what the meaning of "maximising" is in this context."Maximising" seems to refer to the quantitative, rather than the qualitative, implying that "impact" is necessarily related to how many people or visits or clicks or downloads a given online resource is getting. So-called "altmetrics" and the more-established webometrics or statistical cybermetrics seek to recognise the need of measuring the role that sharing links online has on increasing academic impact, but it seems to me that beyond an emphasis on the numbers what is also needed is to zoom in the fine detail of who is actually behind all that tweeting, citing, clicking and downloading. It also seems to me current discourses about the use of metrics to assess digital scholarship or the effects of social media on academic impact often assume (without much evidence) that if there are a lot of academics on Twitter then it means that they are all clicking and retweeting each other's academic work.The data revealed bymy comparison of links I have tweeted using three different URL shorteners show no correlation between the number of RTs and the number a given link is actually clicked on (i.e. visited), indicating that though several people retweeted presumably because they found the information relevant, they did not click on the link the tweet contained even once, arguably meaning the tweet did not add any visits to the linked resource. On the other hand, links tweeted only once have been clicked several times.Often, the links that have been clicked numerous times had no academic value whatsoever (namely my recent tweeted photo of a London sunset). Obviously, one would need to compare the linked resource's analytics, and ideally logs, to make a more reliable or educated comparative guess about the correlation between someone's tweeting of a link, its retweets and its impact on the actual resource.The figures I have been gathering and measuring about engagement with my tweeted links offer a very limited (and indeed not very reliable) interpretation of a more complex scenario, and often I have been able to evaluate the value of qualitative sharing only because I know the specific context in which a given link was clicked. In one case, I tweeted a link to an academic blog post that was a response to another academic blog post; this was clicked only once by one of my followers who did not follow the author of the linked article. The person got in touch with the article's author on Twitter directly, mainly because the article discussed another article whose author was a faculty member of the school directed by my follower.I call this qualitative impact: in this specific case my sharing of one particular link produced only one click, but the person who clicked on it would not have found the article that quickly otherwise (perhaps she wouldn't have found that article at all!). Morevoer, the person who did the only one click was indeed the exact target audience for that article.Who cares if this link did not get as many clicks as a corny photo if the user who did click on it was just "the right" stakeholder, which can potentially unleash an unpredictable series of positive academic outcomes?This is what I have been calling "the After-Life" of Links. We've reached the point in which sharing links on Twitter or Facebook is almost as effective as handing out flyers on London's Tottenham Court Road. Therefore the "Impact" that should be assessed is not necessarily quantitative, but qualitative. In digital as in print, a resource or publication only achieves its full potential if it finds a readership that appreciates it (and ideally "does" something with it). (Apologies for the emphasis on bold).As with the slightly embarrassing childhood memory I began this post with, the joy of lasting "Impact" is not in quantity but in quality. I never heard again from those kids who, when I was little, were not interested in sharing. With those guys in the photo above, I am and have been friends with for (gasp!) nearly three decades.By way of a prefaceThe post that follows is formed from the text of a presentation I am due to deliver at King's College London on 9 February, but which reflects what I was worrying about in early December 2011 - several months after I wrote the synopsis that was used to advertise the talk, a month before I attended the AHA conference in Chicago with its extensive programme on digital histories, and six weeks before I got around to reading Stephen Ramsay's, Reading Machines: Toward an Algorithmic Criticism. Both listening to the text mining presentations at the AHA, and thinking about Ramsay's observations about computers and literary criticism have contributed to moving me on from the text below. In particular, Ramsay's work has encouraged me to remember that history writing has always been more fully conceived by its practitioners as an act of 'creation', and as a craft in its own right, than has literary criticism (which has more fully defined itself against a definable 'other' - a literary object of study). As a result, I found myself fully in agreement with Ramsay's proposal that digital criticisms should '...channel the heightened objectivity made possible by the machine into the cultivation of those heightened subjectivities necessary for critical work.'(p.x) But was most struck by his conclusion that the 'hacker/scholar' had moved camps from critic to creator. (p.85). It made me remember that even the most politically informed and technically sophisticated piece of digital analysis only becomes 'history' when it is created and consumed as such. This made me reflect that we have the undeniable choice to create new forms of history that retain the empathetic and humane characteristics found in the old generic forms; and simply need to get on with it. In the process I have concluded that the conundrums of positivism with which this post are concerned, are in many ways a canard that detract from crafting purposeful history.Academic History Writingand the Headache of Big DataIn the nature of titles and synopses for presentations such as this one, you write them before you write the paper, and they reflect what you are thinking about at the time. My problem is that I keep changing my mind. I try to dress this up as serious open mindedness – a constant engagement with a constantly changing field, but in reality it is just a kind of inexcusable intellectual incontinence – which I am afraid I am going to force you all to witness this afternoon.I promised to spend the next forty minutes or so discussing research methodologies, historical praxis and the challenge of ‘big data’; and I do promise to get there eventually. But first I want to do something deeply self-serving and self-indulgent that nevertheless seemed to me a necessary pre-condition for making any serious statement about both the issues raised by recent changes in the technical landscape, and how ‘Big Data’, in particular, will impact on writing history – and whether this is a good thing.And I am afraid, the place I need to start is with some thirteen years spent developing online historical resources.Unlike a lot of people working in the digital humanities, in collaboration with Bob Shoemaker, I have pretty much controlled my research agenda and the character of the projects I have worked on from day one. This has been a huge privilege for which I am hugely grateful, but it means that there has been an underlying trajectory embedded within my work as a historian and digital humanist. This agenda has been continuously negotiated with Bob Shoemaker, whose own distinct agenda and perspective has also fundamentally shaped the resulting projects, and more recently with Sharon Howard; and has been informed throughout by the work of Jamie McLaughlin who has been primarily responsible for the programming involved. But, the websites I have helped to create were designed with our historical interests and intellectual commitments as imperatives. And as such they incorporate a series of explicit assumptions that have worked in dialogue with the changing technology. In other words, the seven or eight major projects I have co-directed are, from my perspective at least, fragments of a single coherent research agenda and project.And that project is about the amalgamation of the Digital Humanities with an absolute commitment to a particular kind of history: ‘History from Below’. They form an attempt to integrate the British Marxist Historical Tradition, with all the assumptions that implies about the roles of history in popular memory, and community engagement, with digital delivery. In the language of the moment, they are a fragment of what we might discuss as a peculiar flavour of ‘public history’. And what I feel I have discovered in the last five or six years, is that there is a fundamental contradiction between the direction of technological development, and that agenda – that ‘big data’ in particular, and history from below don’t mix.We started with the Old Bailey Proceedings – not because it was a perfect candidate for digitisation (who knew what that looked like in 1999), but because it was the classic source for ‘history from below’ and the social history of eighteenth-century London, used by Edward Thompson and George Rude.· 125 million words of trial accounts 197,745 trials reflecting the brutal exercise of state power on the relatively powerless. 250,000 defendants, and 220,000 victims.A constant and ever changing litmus test of class and social change.The underlying argument – in 1999 – was that the web represented a new public face for historical analysis, and that by posting the Old Bailey Proceedings we empowered everyone to be their own historian – to discover for themselves that landscape of unequal power. By 2003, when we posted the first iteration of the site – and more as a result of the creation of the online census’s rather the Old Bailey itself – the argument had changed somewhat to a simple acceptance of the worth and value of a demonstrable democratisation of access to the stuff of social history.The site did not have the explicit political content of Raphael Samuel’s work or Edward Thompson’s, but it both created an emphasis on the lived experience of the poor, and gave free public access to the raw materials of history to what are now some 23 million users.And it is important to remember at this point what most academic projects have looked like for the last decade, and the kinds of agendas that underpin them. If you wanted to characterise the average academic historical web resource, it would be a digitisation project aimed at the manuscripts of a philosopher or ‘scientist’. Newton, Bentham, the Philosophes, or founding fathers in the US; most digital projects have replicated the intellectual, and arguably rather intellectually old fashioned end, of the broader historical project. Gender history, the radical tradition, even economic and demographic history have been poorly represented on line – despite the low technical hurdles involved in posting the evidence for demographic and economic history in particular.The importance of the Old Bailey therefore was simply to grab an audience for the kind of history that I wanted people to be thinking about – empathetic, aware of social division and class, and focused on non-elite people. And to do so as a balance to what increasingly seems to me to be the emergence of a very conservative notion of what historical material looked like.The next step – the creation of the London Lives web site, was essentially driven by the same agenda, with the explicit addition that it should harness that wild community of family historians, and wild interest in the history of the individual, to creating an understanding of individuals in their specific contexts – of building lives, by way of building our understanding of communities, and essentially – of social relationships.3.5 million names, 240,000 pages of transcribed manuscripts reflecting social welfare and crimeand a framework that allowed individual users to create individual lives, that could in turn be built in to micro-histories. This was social history online – the stuff of a digital history from below.This hasn’t garnered quite the same audience, or had the same impact as the Old Bailey Online (it does not contain the glorious narrative drama inherent in a trial account), and the history it contains is just harder work to make real.But, from my perspective, the character and end of the two projects were absolutely consistent. Designed around 2004 (and completed in 2010), in some respects London Lives was a naïve attempt to make crowd sourcing an integral part of the process – though not in order to get work done for free (which seems to be the motivation for applying crowdsourcing in a lot of instances), but more as a way of helping to create communities of users, who in turn become both communities of consumers of history, and communities of creators, of their own histories.Around the same time as London Lives was kicking off, starting in 2005, and in collaboration with Mark Greengrass, we began to experiment with Semantic Web methodologies, Natural Language Processing, and a bunch of Web 2.0 techniques – all of which were driven in part by the engagement of people like Jamie McLaughlin, Sharon Howard, Ed McKenzie and Katherine Rogers at the Humanities Research Institute in Sheffield, and in part by the interest generated by the Old Bailey as a ‘Massive Text Object’ from digital humanists such as Bill Turkel. In other words, during the middle of the last decade, the balance between the technology and its use as a mode of delivery began to shift. We became more technically engaged with the Digital Humanities, and this began to create a tension with the historical agenda we were pursuing.And as a result, it was around this point that the basic coherence of the underlying project became more confused. Just as the demise of the Arts and Humanities Data Service in 2007 signalled the end of a coherent British digitisation policy (and the end of a particular vision of how history online might work), the rising significance of external technical developments began to impact significantly on our agenda, as we worked to amalgamate rapid technical innovation with the values and expectations of a public, democratic form of history. In other words the technology began to overtake our initial and underlying purpose.And the first upshot of that elision was the Connected Histories site: 15 Major web resources 10 billion words150,000 imagesAll made available through a federated search facility. Everything from Parliamentary Papers, to collections of ephemera and the British Museum’s collection of prints and drawings, were brought together and made keyword searchable through an abstracted index. With its distributed API architecture and use of NLP to tag a wide variety of source types, it represented a serious application of what at the time were relatively new methodologies.And unlike the previous sites, it was effectively driven by a changing national context, and by technology, and included a range of partners far beyond those involved in previous projects - most significantly Jane Winter and the Institute of Historical Research. In part this project was driven by a critique of data ‘silos’, but more fundamentally, we saw it as an answer to the incoherence of the digitisation project as a whole, following the withdrawal of funding to the AHDS, and the closure of the Arts and Humanities Research Council’s Resource Enhancement Scheme. It also formed an answer to the firewalls of privilege that were increasingly being thrown up around newspapers and other big digital resources – an important epiphenomenon of Britain’s mixed ecology of web delivery. In other words, while trying desperately to maintain a democratic model of intellectual access, we were forced to respond to a rapidly changing techno-cultural environment.In many respects, Connected Histories was an attempt to design an architecture, including an integral role for APIs, RDF indexes, and a comprehensive division between scholarly resources, and front end analytical functionality, that would keep the work of the previous decade safe from complete irrelevance. At its most powerful we believed the architecture would allow the underlying data to be curated, logged and preserved, even as the ‘front end’ grew tired and ridiculous.Early attempts to make the project automatic and fully self-sustaining through the use of crawlers, and hackerish scraping methodologies fell by the way, as even the great national memory institutions and commercial operations like ProQuest and Gale, signed up to the project.But, we also kept the hope that Connected Histories would effectively allow democratic access (or at least a democratically available map of the online landscape) to every internet user. There was no real, popular demand for this. Google has frightened us all in to believing there is an infinite body of material out there, so we can’t know its full extent. But it seemed important to us that what the public has paid for should be knowable by the public.And here is where the conundrums of ‘Big Data’ begin. And these conundrums are of two sorts – the first simple and technical; and the second more awkward and philosophical.By this time, two years ago or so, we had what looked like ‘pretty big data’, and the outline of a robust technical architecture that separated out academic resources from search facilities, both making the data much more sustainable and easily curated, and the analysis much more challenging and interesting. Suddenly, all the joys of datamining, corpus linguistics, textmining, of network analysis and interactive visualisations beckoned.And it is this latter challenging and exciting analytical environment that is so fundamentally problematic. Because we had ‘pretty big data’, and the architecture to do something serious with it, we suddenly found ourselves very much in danger of excluding precisely the audience for history that we started out to address. The intellectual politics of the projects (the commitment to a history from below), and the technology actually came in to conflict for the first time – though this would only be apparent if you looked under the bonnet, at the underlying architecture and the modes of working it assumed.One problem is that these new methodologies are and will continue to be reasonably technically challenging. If you need to be command-line comfortable to do good history – there is no way the web resources created are going to reach a wider democratic audience, or allow them to create histories that can compete for attention with those created within the academy – you end up giving over the creation of history to a top down, technocratic elite. In other words, you build in ‘history from above’, rather than ‘history from below’, and arguably privilege conservative versions of the past. One way forward, therefore, lay in attempting to make this new architecture work more effectively for an audience without substantial technical skills.In collaboration with Matthew Davies and Mark Merry at the Centre for Metropolitan History and with the Museum of London Archaeological Service, we tried to do just this with Locating London’s Past.Seventeen datasets4.9 million geo-referenced place names 29,000 individually defined polygons.But the main point is that it is a shot at creating the most intuitive front end version we could imagine of the sort of ‘mash up’ that the API architecture makes both possible, and effectively encourages.In other words, this was an attempt to take what a programmer might want to achieve with an API, and put it directly into the hands of a wider non-technical public. And we chose maps and geography as the exemplar data, and GIS as the best methodology, simply because, while every geographer will tell you maps are profound ideological constructs embedding a complex discourse, they are understood by a wider public in an intuitive and unproblematic way – allowing that public to make use of the statistics derivable from ‘big data’ in a way that intellectually feels like a classic ‘mash up’, but which requires little more expertise than navigating between stations on the London underground.So arguably, Locating London’s Past is in a direct line from the Old Bailey, and London Lives – seeking to engage and encourage the same basic audience to use the web to make their own history – and to do so from below – to create a humane, individualistic, and empathetic history that contributes to a simple politics of humanism.But it is not a complete answer, and the next project highlighted the problem even more comprehensively. At the same time as we were working on Connected Historiesand Locating London’s Past, by way of engaging that history from below audience, making all this stuff safe for a democratic and universal audience - we were also involved with the first round of the Digging Into Data Programme, with a project called Data Mining With Criminal Intent.The Data Mining with Criminal Intent project brought together three teams of scholars including Dan Cohen and Fred Gibbs from CHNM, and Geoffrey Rockwell and Stefan Sinclair of Voyant Tools, along with Bill Turkel from the University of Western Ontario, and Jamie McLaughlin from the HRI in Sheffield. It was intened to achieve just a few things. First, to build on that new distributed architecture to illustrate how tools and data in the humanities might be shared across the net - to embed an API methodology within a more complex network of distributed sites and tools; and second, to create an environment in which some ‘big data’ might be made available for use with the innovative tools created by linguists for textual analysis. And finally to begin to explore what kinds of new questions, these new tools and architecture would allow us to ask and answer.To achieve these ends, we brought onto a single metaphorical page, the Old Bailey material with the browser based citation management system, Zotero, and Voyant Tools – new tools for working with large numbers of words.Much of this was a simple working out of the API architecture and the implications inherent in separating data from analysis. But, it also led me to work with Bill Turkel, using Mathematica to do some macro-analysis of the Old Bailey Proceedings themselves.One of the interesting things about this is that simply because we did it so long ago, rekeying the text instead of using an OCR methodology, the Proceedings are now one of the few big resources relating to the period before 1840 or so, that is actually much use for text mining. Try creating an RDF triple out of the Burney Collection’s OCR and you get nothing that can be used as the basis for a semantic analysis – there is just too much noise. The exact opposite is true of the Proceedings because of their semi-structured character, highly tagged content, and precise transcription. And at 127 million words, they are just about big enough to do something sensible. And where Bill and I ended up was with a basic analysis of trial length and verdict over 240 years, that allowed us to critique and revise the history of the evolution of the criminal justice system, and the rise of plea bargaining. And we came to this conclusion through a methodology that I can only describe as ‘staring at data’ – looking open-eyed at endless iterations of the material, cut and sliced in different ways. It is a methodology that is central to much scientific analysis, and it is fun.But it is also where my conundrum comes in. However compelling the process is, it does not normally result in the kind of history I do. It is not ‘history from below’, it is not humanistic, nor is it very humane. It can only rarely be done by someone working part time out of interest, and it does not feed in to ‘public history’ or memory in any obvious way. The result is powerful, and intellectually engaging – it is the tools of the digital humanities wielded to create a compelling story that changes how we understand the past (which is fun); but it is a contribution to a kind of legal and academic history I do not normally write.And the point is, that the kind of history created in this instance, is precisely the natural upshot of ‘big data’ analysis. In other words, what has become self-evident to me, is that ‘big data’, and even ‘pretty big data’ inevitably creates a different and generically distinct form of historical analysis, and fundamentally changes the character of the historical agenda that is otherwise in place. This may seem obvious – but it needs to be stated explicitly.To illustrate this in a slightly different way, we need look no further than the doyens of ‘big data’; the creators of the Googe Ngram viewer.I love the Google ngram viewer, and it clearly points the way forward in lots of ways. But if you look at what Erez Lieberman Aiden and Jean-Baptiste Michel do with it, its impact on the form of historical scholarship begins to look problematic. Rather like what Bill Turkel and I did with the Old Bailey material, Lieberman Aiden and Michel appear to claim to be able to read history from the patterns the ngram viewer exposes - to decipher significant changes from the data itself. Their usual examples include the analysis of the decline of irregular verbs to a precise mathematical equation, and the rise of 'celebrity' as measured by the number of times an individual is mentioned in print.These imply that all historical development can, like irregular verbs, be described in mathematical terms, and that 'human nature', like the desire for fame, can be used as a constant to measure the changing technologies of culture. And that like the Old Bailey – we can discover change and effect through exploring the raw data. And that once we do this, it will become newly available, in the words of Lieberman Aiden and Michel, for 'scientific purposes'.In other words, there is a kind of scientific positivism that is actively encouraged by the model of ‘big data’ analysis. All the ambiguities of theory and structuralism, thick description and post modernism are simply irrelevant.In some respects, I have no problem with this whatsoever. I have never been a fully paid up post-modernist, and put most simply, unlike a thorough-going post-modernist, I think we can know stuff about the past.I do, however, have two particular issues. First, if I work towards a more big data-like approach, I am forced to rework and rethink my own ‘public history’ stance. I am no longer simply making material and empathetic engagement available to a wider audience; and therefore, the purpose of my labours is left open to doubt (by myself at the very least). But second, I am being drawn into a kind of positivism that assumes what will come out of the equations (the code breaking to use the dominate metaphor of the last 60 years) is socially usefully or morally valuable.In a sense, what ‘big data’ encourages is a morality-free engagement with a positivist understanding of human history. In contrast, the core of the historical tradition has been focused on the dialogue between the present and the past, and the usefulness of history in creating a working civil society. The lessons we take from the past are those which we need, rather than those which are most self-evident. If the project of history I bought in to was politically and morally framed (and it was), the advent of big data challenges the very root of that project.Of course, this should not really be a problem, if only because history has always been a dialogue between irrefutable evidence, and discursive construction (between what you find in the archive and what you write in a book). And science and its positivist pretentions have always been framed within a recognised sociology of knowledge and constructed hermeneutic.But, for me, I remain with a conundrum – how to turn big data in to good history? How do we preserve the democratic and accessible character of the web, while using the tools of a technocratic science model in which popular engagement is generally an afterthought rather than the point.I really just want to conclude about there – with the conundrum. For me, and for most of the digital humanities in the UK, the journey of the last fifteen years or so has been about access and audience – issues that are fundamentally un-problematic – which can be politically engaging and beautiful; and for this, one needs look no further than Tim Sherratt’s Invisible Australian’s project.Even if you prefer your history in a more elite vein than me, more people being able to read more sources is an unproblematic good thing, a simple moral good. And arguably, having the opportunity to stare hard at data, and look for anomalies, and weirdness, is also an unproblematic good.But, if we are now being led by the technology itself to write different kinds of history – the tools are shaping the product. If we end up losing the humane and the individual, because the data doesn’t quite work so easily that way, we are in danger of giving up the power of historical narrative (the ability to conjure up a person and emotions with words), without thinking through the nature of what will be gained in exchange. I am tempted to go back to my structuralist / Marxist roots and start ensuring my questions are sound before the data is assayed, but this seems to deny the joys of an open-eyed search for the weird. I am caught between audience and public engagement, on the one hand, and the positivist implications of big data, on the other.And I am left in a conundrum. In the synopses I wrote back in October or so, I thought I would be arguing: “that the analysis and exploration of 'big data' provides an opportunity to re-incorporate historical understandings in to a positivist analysis, while challenging historians to engage directly and critically with the tools of computational linguistics.”The challenge is certainly there, but I am less clear that the re-integration of history and positivism can be pursued without losing history’s fundamental and humanist purpose. For me, there remain big issues with big data; and a challenge to historians to figure out how to turn big data, to real historical account.I appreciated Ian Bogost's post from a few weeks ago on the state of digital humanities navel-gazing. As he concludes, "currently, what one does in the humanities is talk about the humanities. This is particularly true of the digital humanities, some of whose proponents are actually using computers to do new kinds of humanistic scholarly work in breaks between debates about the potential to use computers for new kinds of humanistic scholarly work." Perhaps there are historical reasons why, at this particular moment, the humanities are so self-reflective. No perhaps about it, actually. We are somewhat lost at sea and the "digital" is part of the reason. This does not mean, however, that reflection is productive, and certainly not all reflection is productive. At some point, as Ian suggests, it's a matter of getting back to work. A point that Mark Sample also makes.Maybe, from this point of view, rhetoric and composition has a built-in self-reflexivity in its research into writing and the teaching of writing: we research what we do and what our students do. (Of course, it's a big field so that's not all rhet/comp scholars do.) Getting back to work for me means continuing to investigate the ways in which digital technologies and networks shape rhetorical practices. I am particularly interested in the problem of rethinking rhetorical education to address shifting literacy practices. This, to me, is not narcissistic, though it does involve looking at the rhetorical practices of humanisits since it is fairly clear that what we will teach students is a function of what we do ourselves.To this end, I am interested in Stanely Fish's most recent DH post in the NY Times and Ted Underwood's response. Fish concludes (complains?) that digital humanities "will have little place for the likes of me and for the kind of criticism I practice: a criticism that narrows meaning to the significances designed by an author, a criticism that generalizes from a text as small as half a line, a criticism that insists on the distinction between the true and the false, between what is relevant and what is noise, between what is serious and what is mere play." Maybe so. Though there are probably a dozen other interpretive methods that also have little place for this method. But his more salient criticism (and you can read the article to see how he makes it) regards the ways in which he suggests digital humanities scholars develop their arguments and provide evidence for them. He puts it this way:The direction of my inferences is critical: first the interpretive hypothesis and then the formal pattern, which attains the status of noticeability only because an interpretation already in place is picking it out.The direction is the reverse in the digital humanities: first you run the numbers, and then you see if they prompt an interpretive hypothesis. The method, if it can be called that, is dictated by the capability of the tool.I would suggest that Fish's method is also "dictated by the capability of the tool." The primary tool in this case is Fish himself (please excuse the pejorative connotations of that sentence). The point is that Fish's method is also constrained by the capacities available to the relations between a human reader and a text. In either case, I wouldn't say "dictated" as I wouldn't attribute a deterministic relationship here. That's another misunderstanding. Fish appears to suggest here that his "hypothesis" arrives ex nihilo. I doubt that's actually the case. My first guess is that Fish, like most literary critics, would build an interpretation while reading a text, and that while certainly one might have some tenative hypothesis before cracking the book open, it would be based on readings of other books. I mean this is why literary scholars focus on a particular period, right? Because it is part of the disciplinary paradigm that books written in a particular time and place impact one another with special significance.So I believe what we have here is a significant misunderstanding of invention. Underwood makes a similar observation. He writesThe basic mistake that Fish is making is this: he pretends that humanists have no discovery process at all. For Fish, the interpretive act is always fully contained in an encounter with a single piece of evidence. How your “interpretive proposition” got framed in the first place is a matter of no consequence: some readers are just fortunate to have propositions that turn out to be correct. Fish is not alone in this idealized model of interpretation; it’s widespread among humanists.The underlying misunderstanding is painfully ironic. Fish is resisting the assistance of digital techniques — not because they would impose scientism on the humanities — but because they would force us to acknowledge that our ideas do after all come from somewhere.Hmmmm.... if this is actually the case it would explain why humanists struggle with teaching writing. It would also be ironic in the sense that so much of literary criticism focuses on exploring the historical and cultural contexts that inform the production of literary work while denying that similar contexts inform their own compositions. However I will go a little farther with Underwood's argument and suggest that it is precisely a scientistic conception of invention/discovery that leads to this belief in the eureka moment.Let's recall that at the heart of the typical scientist objection to Latour is his contention that scientific knowledge is constructed. In the conventional way of thinking, scientists want to claim that scientific knowledge is discovered in nature rather than "made up" in the lab. In this view, construction equals fiction. On the flipside, literary scholars in Fish's tradition want to claim that literary knowledge is discovered in the literary texts rather than being constructed in a lab (or a DH computerized process). In rhetoric, not surprisingly, we have made an extensive study of the history of invention. We can see how rhetoric gets slowly stripped of its canonical elements until it arrives in the Modern era as little more that Style. The processes of invention get supplanted by scientific methods of discovery on the one hand and the eventual emergence of a Romantic conception of inspiration on the other.It's for reasons like this that I can appreciate Cheryl Ball's response to the recent Profession issue on digital scholarship. That is, I think it's fair to suggest that rhetoricians have some understanding of how invention, evidence, and argument operate, and, from this rhetorician's perspective, this whole discussion seems a little simplistic.So let's rehearse some of the basic concepts here. Composing is a networked phenomenon because thinking is always already relational. I mean you are composing/thinking in words right? You didn't invent that language, right? So, that's obvious. Thoughts are constructed. Arguments are constructed. Evidence is constructed. Scholarship is constructed. That is, these things are composed of other things. That doesn't mean they can be explained solely by those other things. Nor does it mean that we fully understand (let alone control) the processes of construction. However, I think we can recognize in this current (if somewhat navel-gazing) conversation, that a shift in the available compositional networks has created some discomfort. It causes us to recognize the constructed, networked characteristics of our legacy practices, characteristics that we had come to ignore (or forget) because they had been normalized.In turn we are faced with choices that we never really recognized as choices before. We took invention to be natural. We read a book, and an interpretation came to us. Of course we had training and such to help us, but regardless of our methodological/theoretical preferences, in literary studies it ultimately came down to reading a text and developing an interpretation. Regardless of methodological differences literary interpretations were generically the same in general length, uses of evidence, structure of argument, etc. etc. The same was true in rhetoric and, I imagine, the rest of the humanities.But that's no longer the case. Now we have real choices to make that make real differences in the knowledge that we produce and the communities in which we participate. They make real differences in what we understand literacy to be and as such what we will teach to future generations of students (which is ultimately what concerns me). And here I would stretch far beyond the arguments related to "distant reading" which are the focus of Fish's piece (and another error he makes, conflating this one practice with the entirety of DH).In the end, I think it is entirely accurate to say that digital humanities from distant reading to middle-state publishing (like this blog) does little to improve our ability to conduct pre-digital scholarly practices or answer the questions of a pre-digital humanistic disciplinary paradigm. Digital technologies did not arrive to resolve the questions of the print humanities anymore than late industrial technologies arrived to resolve the questions of a pre-industrial humanities. As I often argue here, we are faced with ethical questions regarding how to proceed. What do we take from the 20th century into this one? Set aside, for a moment, the navel-gazing debates over methodological minutiae and consider what larger questions the humanities seek to answer in this century.I certainly do not want to make a claim that I know what the humanities should be. I have a hard enough time finding my own scholarly way. However, I will make the argument that what the humanities was in the last century was largely conditioned (though not determined!) by the capacities of an industrial culture and that now those capacities are different and as such the humanities will change. Maybe the humanities will diminish or even disappear, but not necessarily. However they will change if for no other reason than the technological infrastructures on which they once relied have changed.It's up to all of us, individually and collectively, to figure out how to get back to work.